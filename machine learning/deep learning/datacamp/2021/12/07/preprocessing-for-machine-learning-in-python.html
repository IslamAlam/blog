<h1 id="preprocessing-for-machine-learning-in-python">Preprocessing for Machine Learning in Python</h1>

<p>This is the memo of the 8th course (23 courses in all) of ‘Machine Learning Scientist with Python’ skill track.</p>

<p><strong>You can find the original course
 <a href="https://www.datacamp.com/courses/preprocessing-for-machine-learning-in-python">HERE</a></strong>
 .</p>

<p>###
<strong>Course Description</strong></p>

<p>This course covers the basics of how and when to perform data preprocessing. This essential step in any machine learning project is when you get your data ready for modeling. Between importing and cleaning your data and fitting your machine learning model is when preprocessing comes into play. You’ll learn how to standardize your data so that it’s in the right form for your model, create new features to best leverage the information in your dataset, and select the best features to improve your model fit. Finally, you’ll have some practice preprocessing by getting a dataset on UFO sightings ready for modeling.</p>

<p>###
<strong>Table of contents</strong></p>

<ol>
  <li>Introduction to Data Preprocessing</li>
  <li><a href="https://datascience103579984.wordpress.com/2019/12/23/preprocessing-for-machine-learning-in-python-from-datacamp/2/">Standardizing Data</a></li>
  <li><a href="https://datascience103579984.wordpress.com/2019/12/23/preprocessing-for-machine-learning-in-python-from-datacamp/3/">Feature Engineering</a></li>
  <li><a href="https://datascience103579984.wordpress.com/2019/12/23/preprocessing-for-machine-learning-in-python-from-datacamp/4/">Selecting features for modeling</a></li>
  <li><a href="https://datascience103579984.wordpress.com/2019/12/23/preprocessing-for-machine-learning-in-python-from-datacamp/5/">Putting it all together</a></li>
</ol>

<h1 id="1-introduction-to-data-preprocessing"><strong>1. Introduction to Data Preprocessing</strong></h1>
<hr />

<h2 id="11-what-is-data-preprocessing"><strong>1.1 What is data preprocessing?</strong></h2>

<p><img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/1-11.png?w=835" alt="Desktop View" /></p>

<h3 id="111-missing-data--columns"><strong>1.1.1 Missing data – columns</strong></h3>

<p>We have a dataset comprised of volunteer information from New York City. The dataset has a number of features, but we want to get rid of features that have at least 3 missing values.</p>

<p>How many features are in the original dataset, and how many features are in the set after columns with at least 3 missing values are removed?</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
volunteer.shape
# (665, 35)

volunteer.dropna(axis=1,thresh=3).shape
# (665, 24)

</code></pre></div></div>

<h3 id="112-missing-data--rows"><strong>1.1.2 Missing data – rows</strong></h3>

<p>Taking a look at the
 <code class="language-plaintext highlighter-rouge">volunteer</code>
 dataset again, we want to drop rows where the
 <code class="language-plaintext highlighter-rouge">category_desc</code>
 column values are missing. We’re going to do this using boolean indexing, by checking to see if we have any null values, and then filtering the dataset so that we only have rows with those values.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Check how many values are missing in the category_desc column
</span><span class="k">print</span><span class="p">(</span><span class="n">volunteer</span><span class="p">[</span><span class="s">'category_desc'</span><span class="p">].</span><span class="n">isnull</span><span class="p">().</span><span class="nb">sum</span><span class="p">())</span>
<span class="c1"># 48
</span>
<span class="c1"># Subset the volunteer dataset
</span><span class="n">volunteer_subset</span> <span class="o">=</span> <span class="n">volunteer</span><span class="p">[</span><span class="n">volunteer</span><span class="p">[</span><span class="s">'category_desc'</span><span class="p">].</span><span class="n">notnull</span><span class="p">()]</span>

<span class="c1"># Print out the shape of the subset
</span><span class="k">print</span><span class="p">(</span><span class="n">volunteer_subset</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># (617, 35)
</span>
</code></pre></div></div>

<hr />

<h2 id="12-working-with-data-types"><strong>1.2 Working with data types</strong></h2>

<p><img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/2-12.png?w=975" alt="Desktop View" /></p>

<h3 id="121-exploring-data-types"><strong>1.2.1 Exploring data types</strong></h3>

<p>Taking another look at the dataset comprised of volunteer information from New York City, we want to know what types we’ll be working with as we start to do more preprocessing.</p>

<p>Which data types are present in the
 <code class="language-plaintext highlighter-rouge">volunteer</code>
 dataset?</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
set(volunteer.dtypes.values)
{dtype('int64'), dtype('float64'), dtype('O')}

</code></pre></div></div>

<h3 id="122-converting-a-column-type"><strong>1.2.2 Converting a column type</strong></h3>

<p>If you take a look at the
 <code class="language-plaintext highlighter-rouge">volunteer</code>
 dataset types, you’ll see that the column
 <code class="language-plaintext highlighter-rouge">hits</code>
 is type
 <code class="language-plaintext highlighter-rouge">object</code>
 . But, if you actually look at the column, you’ll see that it consists of integers. Let’s convert that column to type
 <code class="language-plaintext highlighter-rouge">int</code>
 .</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
volunteer["hits"].dtype
# dtype('O')

# Convert the hits column to type int
volunteer["hits"] = volunteer["hits"].astype('int')

volunteer["hits"].dtype
# dtype('int64')

</code></pre></div></div>

<hr />

<h2 id="13-class-distribution"><strong>1.3 Class distribution</strong></h2>

<p><img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/3-12.png?w=996" alt="Desktop View" />
<img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/4-12.png?w=980" alt="Desktop View" /></p>

<h3 id="131-class-imbalance"><strong>1.3.1 Class imbalance</strong></h3>

<p>In the
 <code class="language-plaintext highlighter-rouge">volunteer</code>
 dataset, we’re thinking about trying to predict the
 <code class="language-plaintext highlighter-rouge">category_desc</code>
 variable using the other features in the dataset. First, though, we need to know what the class distribution (and imbalance) is for that label.</p>

<p>Which descriptions occur less than 50 times in the
 <code class="language-plaintext highlighter-rouge">volunteer</code>
 dataset?</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
volunteer.category_desc.value_counts()
Strengthening Communities    307
Helping Neighbors in Need    119
Education                     92
Health                        52
Environment                   32
Emergency Preparedness        15
Name: category_desc, dtype: int64

</code></pre></div></div>

<h3 id="132-stratified-sampling"><strong>1.3.2 Stratified sampling</strong></h3>

<p>We know that the distribution of variables in the
 <code class="language-plaintext highlighter-rouge">category_desc</code>
 column in the
 <code class="language-plaintext highlighter-rouge">volunteer</code>
 dataset is uneven. If we wanted to train a model to try to predict
 <code class="language-plaintext highlighter-rouge">category_desc</code>
 , we would want to train the model on a sample of data that is representative of the entire dataset. Stratified sampling is a way to achieve this.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Create a data with all columns except category_desc
</span><span class="n">volunteer_X</span> <span class="o">=</span> <span class="n">volunteer</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'category_desc'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Create a category_desc labels dataset
</span><span class="n">volunteer_y</span> <span class="o">=</span> <span class="n">volunteer</span><span class="p">[[</span><span class="s">'category_desc'</span><span class="p">]]</span>

<span class="c1"># Use stratified sampling to split up the dataset according to the volunteer_y dataset
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">volunteer_X</span><span class="p">,</span> <span class="n">volunteer_y</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">volunteer_y</span><span class="p">)</span>

<span class="c1"># Print out the category_desc counts on the training y labels
</span><span class="k">print</span><span class="p">(</span><span class="n">y_train</span><span class="p">[</span><span class="s">'category_desc'</span><span class="p">].</span><span class="n">value_counts</span><span class="p">())</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Strengthening Communities    230
Helping Neighbors in Need     89
Education                     69
Health                        39
Environment                   24
Emergency Preparedness        11
Name: category_desc, dtype: int64

</code></pre></div></div>

<h1 id="2-standardizing-data"><strong>2. Standardizing Data</strong></h1>
<hr />

<h2 id="21-standardizing-data"><strong>2.1 Standardizing Data</strong></h2>

<p><img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/5-12.png?w=813" alt="Desktop View" />
<img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/6-12.png?w=863" alt="Desktop View" /></p>

<h3 id="211-when-to-standardize"><strong>2.1.1 When to standardize</strong></h3>

<ul>
  <li>A column you want to use for modeling has extremely high variance.</li>
  <li>You have a dataset with several continuous columns on different scales and you’d like to use a linear model to train the data.</li>
  <li>The models you’re working with use some sort of distance metric in a linear space, like the Euclidean metric.</li>
</ul>

<h3 id="212-modeling-without-normalizing"><strong>2.1.2 Modeling without normalizing</strong></h3>

<p>Let’s take a look at what might happen to your model’s accuracy if you try to model data without doing some sort of standardization first. Here we have a subset of the
 <code class="language-plaintext highlighter-rouge">wine</code>
 dataset. One of the columns,
 <code class="language-plaintext highlighter-rouge">Proline</code>
 , has an extremely high variance compared to the other columns. This is an example of where a technique like log normalization would come in handy, which you’ll learn about in the next section.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Split the dataset and labels into training and test sets
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Fit the k-nearest neighbors model to the training data
</span><span class="n">knn</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Score the model on the test data
</span><span class="k">print</span><span class="p">(</span><span class="n">knn</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span>
<span class="c1"># 0.5333333333333333
</span>
</code></pre></div></div>

<hr />

<h2 id="22-log-normalization"><strong>2.2 Log normalization</strong></h2>

<p><img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/7-12.png?w=789" alt="Desktop View" />
<img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/8-11.png?w=996" alt="Desktop View" /></p>

<h3 id="221-checking-the-variance"><strong>2.2.1 Checking the variance</strong></h3>

<p>Check the variance of the columns in the
 <code class="language-plaintext highlighter-rouge">wine</code>
 dataset. Which column is a candidate for normalization?</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
wine.var()
Type                                0.600679
Alcohol                             0.659062
Malic acid                          1.248015
Ash                                 0.075265
Alcalinity of ash                  11.152686
Magnesium                         203.989335
Total phenols                       0.391690
Flavanoids                          0.997719
Nonflavanoid phenols                0.015489
Proanthocyanins                     0.327595
Color intensity                     5.374449
Hue                                 0.052245
OD280/OD315 of diluted wines        0.504086
Proline                         99166.717355
dtype: float64

</code></pre></div></div>

<p><strong>Proline 99166.717355</strong></p>

<h3 id="222-log-normalization-in-python"><strong>2.2.2 Log normalization in Python</strong></h3>

<p>Now that we know that the
 <code class="language-plaintext highlighter-rouge">Proline</code>
 column in our wine dataset has a large amount of variance, let’s log normalize it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Print out the variance of the Proline column
</span><span class="k">print</span><span class="p">(</span><span class="n">wine</span><span class="p">.</span><span class="n">Proline</span><span class="p">.</span><span class="n">var</span><span class="p">())</span>
<span class="c1"># 99166.71735542436
</span>
<span class="c1"># Apply the log normalization function to the Proline column
</span><span class="n">wine</span><span class="p">[</span><span class="s">'Proline_log'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">wine</span><span class="p">.</span><span class="n">Proline</span><span class="p">)</span>

<span class="c1"># Check the variance of the Proline column again
</span><span class="k">print</span><span class="p">(</span><span class="n">wine</span><span class="p">.</span><span class="n">Proline_log</span><span class="p">.</span><span class="n">var</span><span class="p">())</span>
<span class="c1"># 0.17231366191842012
</span>
</code></pre></div></div>

<hr />

<h2 id="23-scaling-data-for-feature-comparison"><strong>2.3 Scaling data for feature comparison</strong></h2>

<p><img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/9-11.png?w=852" alt="Desktop View" />
<img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/10-11.png?w=559" alt="Desktop View" />
<img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/11-10.png?w=750" alt="Desktop View" /></p>

<h3 id="231-scaling-data--investigating-columns"><strong>2.3.1 Scaling data – investigating columns</strong></h3>

<p>We want to use the
 <code class="language-plaintext highlighter-rouge">Ash</code>
 ,
 <code class="language-plaintext highlighter-rouge">Alcalinity of ash</code>
 , and
 <code class="language-plaintext highlighter-rouge">Magnesium</code>
 columns in the wine dataset to train a linear model, but it’s possible that these columns are all measured in different ways, which would bias a linear model.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
wine[['Ash','Alcalinity of ash','Magnesium']].describe()
              Ash  Alcalinity of ash   Magnesium
count  178.000000         178.000000  178.000000
mean     2.366517          19.494944   99.741573
std      0.274344           3.339564   14.282484
min      1.360000          10.600000   70.000000
25%      2.210000          17.200000   88.000000
50%      2.360000          19.500000   98.000000
75%      2.557500          21.500000  107.000000
max      3.230000          30.000000  162.000000

</code></pre></div></div>

<h3 id="232-scaling-data--standardizing-columns"><strong>2.3.2 Scaling data – standardizing columns</strong></h3>

<p>Since we know that the
 <code class="language-plaintext highlighter-rouge">Ash</code>
 ,
 <code class="language-plaintext highlighter-rouge">Alcalinity of ash</code>
 , and
 <code class="language-plaintext highlighter-rouge">Magnesium</code>
 columns in the wine dataset are all on different scales, let’s standardize them in a way that allows for use in a linear model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Import StandardScaler from scikit-learn
</span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># Create the scaler
</span><span class="n">ss</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>

<span class="c1"># Take a subset of the DataFrame you want to scale
</span><span class="n">wine_subset</span> <span class="o">=</span> <span class="n">wine</span><span class="p">[[</span><span class="s">'Ash'</span><span class="p">,</span><span class="s">'Alcalinity of ash'</span><span class="p">,</span><span class="s">'Magnesium'</span><span class="p">]]</span>

<span class="c1"># Apply the scaler to the DataFrame subset
</span><span class="n">wine_subset_scaled</span> <span class="o">=</span> <span class="n">ss</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">wine_subset</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
wine_subset_scaled[:5]
array([[ 0.23205254, -1.16959318,  1.91390522],
       [-0.82799632, -2.49084714,  0.01814502],
       [ 1.10933436, -0.2687382 ,  0.08835836],
       [ 0.4879264 , -0.80925118,  0.93091845],
       [ 1.84040254,  0.45194578,  1.28198515]])

</code></pre></div></div>

<hr />

<h2 id="24-standardized-data-and-modeling"><strong>2.4 Standardized data and modeling</strong></h2>

<h3 id="241-knn-on-non-scaled-data"><strong>2.4.1 KNN on non-scaled data</strong></h3>

<p>Let’s first take a look at the accuracy of a K-nearest neighbors model on the
 <code class="language-plaintext highlighter-rouge">wine</code>
 dataset without standardizing the data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Split the dataset and labels into training and test sets
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Fit the k-nearest neighbors model to the training data
</span><span class="n">knn</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Score the model on the test data
</span><span class="k">print</span><span class="p">(</span><span class="n">knn</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<span class="c1"># 0.6444444444444445
</span>
</code></pre></div></div>

<h3 id="242-knn-on-scaled-data"><strong>2.4.2 KNN on scaled data</strong></h3>

<p>The accuracy score on the unscaled
 <code class="language-plaintext highlighter-rouge">wine</code>
 dataset was decent, but we can likely do better if we scale the dataset. The process is mostly the same as the previous exercise, with the added step of scaling the data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Create the scaling method.
</span><span class="n">ss</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>

<span class="c1"># Apply the scaling method to the dataset used for modeling.
</span><span class="n">X_scaled</span> <span class="o">=</span> <span class="n">ss</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Fit the k-nearest neighbors model to the training data.
</span><span class="n">knn</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Score the model on the test data.
</span><span class="k">print</span><span class="p">(</span><span class="n">knn</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span>
<span class="c1"># 0.9555555555555556
</span>
</code></pre></div></div>

<h1 id="3-feature-engineering"><strong>3. Feature Engineering</strong></h1>
<hr />

<h2 id="31-feature-engineering"><strong>3.1 Feature engineering</strong></h2>

<p><img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/12-11.png?w=862" alt="Desktop View" />
<img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/13-10.png?w=895" alt="Desktop View" />
<img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/14-8.png?w=880" alt="Desktop View" /></p>

<h3 id="311-examples-for-creating-new-features"><strong>3.1.1 Examples for creating new features</strong></h3>

<ul>
  <li>timestamps</li>
  <li>newspaper headlines</li>
</ul>

<p>Timestamps can be broken into days or months, and headlines can be used for natural language processing.</p>

<h3 id="312-identifying-areas-for-feature-engineering"><strong>3.1.2 Identifying areas for feature engineering</strong></h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
volunteer[['title','created_date','category_desc']].head(1)
                                               title     created_date category_desc
0  Volunteers Needed For Rise Up &amp; Stay Put! Home...  January 13 2011           Strengthening Communities

</code></pre></div></div>

<p>All of these columns will require some feature engineering before modeling.</p>

<hr />

<h2 id="32-encoding-categorical-variables"><strong>3.2 Encoding categorical variables</strong></h2>

<p><img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/18-5.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/15-8.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/16-6.png?w=756" alt="Desktop View" />
<img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/17-4.png?w=669" alt="Desktop View" /></p>

<h3 id="321-encoding-categorical-variables--binary"><strong>3.2.1 Encoding categorical variables – binary</strong></h3>

<p>Take a look at the
 <code class="language-plaintext highlighter-rouge">hiking</code>
 dataset. There are several columns here that need encoding, one of which is the
 <code class="language-plaintext highlighter-rouge">Accessible</code>
 column, which needs to be encoded in order to be modeled.
 <code class="language-plaintext highlighter-rouge">Accessible</code>
 is a binary feature, so it has two values – either
 <code class="language-plaintext highlighter-rouge">Y</code>
 or
 <code class="language-plaintext highlighter-rouge">N</code>
 – so it needs to be encoded into 1s and 0s. Use scikit-learn’s
 <code class="language-plaintext highlighter-rouge">LabelEncoder</code>
 method to do that transformation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Set up the LabelEncoder object
</span><span class="n">enc</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>

<span class="c1"># Apply the encoding to the "Accessible" column
</span><span class="n">hiking</span><span class="p">[</span><span class="s">'Accessible_enc'</span><span class="p">]</span> <span class="o">=</span> <span class="n">enc</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">hiking</span><span class="p">.</span><span class="n">Accessible</span><span class="p">)</span>

<span class="c1"># Compare the two columns
</span><span class="k">print</span><span class="p">(</span><span class="n">hiking</span><span class="p">[[</span><span class="s">'Accessible'</span><span class="p">,</span> <span class="s">'Accessible_enc'</span><span class="p">]].</span><span class="n">head</span><span class="p">())</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
  Accessible  Accessible_enc
0          Y               1
1          N               0
2          N               0
3          N               0
4          N               0

</code></pre></div></div>

<h3 id="322-encoding-categorical-variables--one-hot"><strong>3.2.2 Encoding categorical variables – one-hot</strong></h3>

<p>One of the columns in the
 <code class="language-plaintext highlighter-rouge">volunteer</code>
 dataset,
 <code class="language-plaintext highlighter-rouge">category_desc</code>
 , gives category descriptions for the volunteer opportunities listed. Because it is a categorical variable with more than two categories, we need to use one-hot encoding to transform this column numerically.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Transform the category_desc column
</span><span class="n">category_enc</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">volunteer</span><span class="p">[</span><span class="s">"category_desc"</span><span class="p">])</span>

<span class="c1"># Take a look at the encoded columns
</span><span class="k">print</span><span class="p">(</span><span class="n">category_enc</span><span class="p">.</span><span class="n">head</span><span class="p">())</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
   Education  Emergency Preparedness            ...              Helping Neighbors in Need  Strengthening Communities
0          0                       0            ...                                      0                          0
1          0                       0            ...                                      0                          1
2          0                       0            ...                                      0                          1
3          0                       0            ...                                      0                          1
4          0                       0            ...                                      0                          0

[5 rows x 6 columns]

</code></pre></div></div>

<hr />

<h2 id="33-engineering-numerical-features"><strong>3.3 Engineering numerical features</strong></h2>

<p><img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/19-5.png?w=856" alt="Desktop View" />
<img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/20-5.png?w=1009" alt="Desktop View" /></p>

<h3 id="331-engineering-numerical-features--taking-an-average"><strong>3.3.1 Engineering numerical features – taking an average</strong></h3>

<p>A good use case for taking an aggregate statistic to create a new feature is to take the mean of columns. Here, you have a DataFrame of running times named
 <code class="language-plaintext highlighter-rouge">running_times_5k</code>
 . For each
 <code class="language-plaintext highlighter-rouge">name</code>
 in the dataset, take the mean of their 5 run times.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Create a list of the columns to average
</span><span class="n">run_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'run1'</span><span class="p">,</span> <span class="s">'run2'</span><span class="p">,</span> <span class="s">'run3'</span><span class="p">,</span> <span class="s">'run4'</span><span class="p">,</span> <span class="s">'run5'</span><span class="p">]</span>

<span class="c1"># Use apply to create a mean column
# axis=1 = row wise
</span><span class="n">running_times_5k</span><span class="p">[</span><span class="s">"mean"</span><span class="p">]</span> <span class="o">=</span> <span class="n">running_times_5k</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">row</span><span class="p">[</span><span class="n">run_columns</span><span class="p">].</span><span class="n">mean</span><span class="p">(),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Take a look at the results
</span><span class="k">print</span><span class="p">(</span><span class="n">running_times_5k</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
      name  run1  run2  run3  run4  run5   mean
0      Sue  20.1  18.5  19.6  20.3  18.3  19.36
1     Mark  16.5  17.1  16.9  17.6  17.3  17.08
2     Sean  23.5  25.1  25.2  24.6  23.9  24.46
3     Erin  21.7  21.1  20.9  22.1  22.2  21.60
4    Jenny  25.8  27.1  26.1  26.7  26.9  26.52
5  Russell  30.9  29.6  31.4  30.4  29.9  30.44

</code></pre></div></div>

<h3 id="332-engineering-numerical-features--datetime"><strong>3.3.2 Engineering numerical features – datetime</strong></h3>

<p>There are several columns in the
 <code class="language-plaintext highlighter-rouge">volunteer</code>
 dataset comprised of datetimes. Let’s take a look at the
 <code class="language-plaintext highlighter-rouge">start_date_date</code>
 column and extract just the month to use as a feature for modeling.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># First, convert string column to date column
</span><span class="n">volunteer</span><span class="p">[</span><span class="s">"start_date_converted"</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">volunteer</span><span class="p">[</span><span class="s">"start_date_date"</span><span class="p">])</span>

<span class="c1"># Extract just the month from the converted column
</span><span class="n">volunteer</span><span class="p">[</span><span class="s">"start_date_month"</span><span class="p">]</span> <span class="o">=</span> <span class="n">volunteer</span><span class="p">[</span><span class="s">"start_date_converted"</span><span class="p">].</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">row</span><span class="p">.</span><span class="n">month</span><span class="p">)</span>

<span class="c1"># Take a look at the converted and new month columns
</span><span class="k">print</span><span class="p">(</span><span class="n">volunteer</span><span class="p">[[</span><span class="s">'start_date_converted'</span><span class="p">,</span> <span class="s">'start_date_month'</span><span class="p">]].</span><span class="n">head</span><span class="p">())</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
  start_date_converted  start_date_month
0           2011-07-30                 7
1           2011-02-01                 2
2           2011-01-29                 1
3           2011-02-14                 2
4           2011-02-05                 2

</code></pre></div></div>

<hr />

<h2 id="34-text-classification"><strong>3.4 Text classification</strong></h2>

<p><img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/21-5.png?w=662" alt="Desktop View" />
<img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/24-5.png?w=540" alt="Desktop View" />
<img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/22-5.png?w=1005" alt="Desktop View" />
<img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/23-5.png?w=718" alt="Desktop View" /></p>

<h3 id="341-engineering-features-from-strings--extraction"><strong>3.4.1 Engineering features from strings – extraction</strong></h3>

<p>The
 <code class="language-plaintext highlighter-rouge">Length</code>
 column in the
 <code class="language-plaintext highlighter-rouge">hiking</code>
 dataset is a column of strings, but contained in the column is the mileage for the hike. We’re going to extract this mileage using regular expressions, and then use a lambda in Pandas to apply the extraction to the DataFrame.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Write a pattern to extract numbers and decimals
</span><span class="k">def</span> <span class="nf">return_mileage</span><span class="p">(</span><span class="n">length</span><span class="p">):</span>
    <span class="n">pattern</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="sa">r</span><span class="s">"\d+\.\d+"</span><span class="p">)</span>

    <span class="c1"># Search the text for matches
</span>    <span class="n">mile</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">match</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="n">length</span><span class="p">)</span>

    <span class="c1"># If a value is returned, use group(0) to return the found value
</span>    <span class="k">if</span> <span class="n">mile</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">mile</span><span class="p">.</span><span class="n">group</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="c1"># Apply the function to the Length column and take a look at both columns
</span><span class="n">hiking</span><span class="p">[</span><span class="s">"Length_num"</span><span class="p">]</span> <span class="o">=</span> <span class="n">hiking</span><span class="p">[</span><span class="s">'Length'</span><span class="p">].</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">return_mileage</span><span class="p">(</span><span class="n">row</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">hiking</span><span class="p">[[</span><span class="s">"Length"</span><span class="p">,</span> <span class="s">"Length_num"</span><span class="p">]].</span><span class="n">head</span><span class="p">())</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
       Length  Length_num
0   0.8 miles        0.80
1    1.0 mile        1.00
2  0.75 miles        0.75
3   0.5 miles        0.50
4   0.5 miles        0.50

</code></pre></div></div>

<h3 id="342-engineering-features-from-strings--tfidf"><strong>3.4.2 Engineering features from strings – tf/idf</strong></h3>

<p>Let’s transform the
 <code class="language-plaintext highlighter-rouge">volunteer</code>
 dataset’s
 <code class="language-plaintext highlighter-rouge">title</code>
 column into a text vector, to use in a prediction task in the next exercise.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Take the title text
</span><span class="n">title_text</span> <span class="o">=</span> <span class="n">volunteer</span><span class="p">[</span><span class="s">'title'</span><span class="p">]</span>

<span class="c1"># Create the vectorizer method
</span><span class="n">tfidf_vec</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">()</span>

<span class="c1"># Transform the text into tf-idf vectors
</span><span class="n">text_tfidf</span> <span class="o">=</span> <span class="n">tfidf_vec</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">title_text</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
text_tfidf
&lt;665x1136 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
	with 3397 stored elements in Compressed Sparse Row format&gt;

</code></pre></div></div>

<h3 id="343-text-classification-using-tfidf-vectors"><strong>3.4.3 Text classification using tf/idf vectors</strong></h3>

<p>Now that we’ve encoded the
 <code class="language-plaintext highlighter-rouge">volunteer</code>
 dataset’s
 <code class="language-plaintext highlighter-rouge">title</code>
 column into tf/idf vectors, let’s use those vectors to try to predict the
 <code class="language-plaintext highlighter-rouge">category_desc</code>
 column.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
text_tfidf.toarray()
array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]])

text_tfidf.toarray().shape
# (617, 1089)

volunteer["category_desc"].head()
1    Strengthening Communities
2    Strengthening Communities
3    Strengthening Communities
4                  Environment
5                  Environment
Name: category_desc, dtype: object

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Split the dataset according to the class distribution of category_desc
</span><span class="n">y</span> <span class="o">=</span> <span class="n">volunteer</span><span class="p">[</span><span class="s">"category_desc"</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">text_tfidf</span><span class="p">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">y</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Fit the model to the training data
</span><span class="n">nb</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Print out the model's accuracy
</span><span class="k">print</span><span class="p">(</span><span class="n">nb</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<span class="c1"># 0.567741935483871
</span>
</code></pre></div></div>

<p>Notice that the model doesn’t score very well. We’ll work on selecting the best features for modeling in the next chapter.</p>

<h1 id="4-selecting-features-for-modeling"><strong>4. Selecting features for modeling</strong></h1>
<hr />

<h2 id="41-feature-selection"><strong>4.1 Feature selection</strong></h2>

<p><img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/1-12.png?w=770" alt="Desktop View" /></p>

<h3 id="411-identifying-areas-for-feature-selection"><strong>4.1.1 Identifying areas for feature selection</strong></h3>

<p>Take an exploratory look at the post-feature engineering
 <code class="language-plaintext highlighter-rouge">hiking</code>
 dataset. Which of the following columns is a good candidate for feature selection?</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
hiking.columns
Index(['Accessible', 'Difficulty', 'Length', 'Limited_Access', 'Location',
       'Name', 'Other_Details', 'Park_Name', 'Prop_ID', 'lat', 'lon',
       'Length_extract', 'accessible_enc', '', 'Easy', 'Easy ',
       'Easy/Moderate', 'Moderate', 'Moderate/Difficult', 'Various'],
      dtype='object')

</code></pre></div></div>

<ul>
  <li>Length</li>
  <li>Difficulty</li>
  <li>Accessible</li>
</ul>

<p>All three of these columns are good candidates for feature selection.</p>

<hr />

<h2 id="42-removing-redundant-features"><strong>4.2 Removing redundant features</strong></h2>

<p><img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/2-13.png?w=596" alt="Desktop View" />
<img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/3-13.png?w=912" alt="Desktop View" /></p>

<h3 id="421-selecting-relevant-features"><strong>4.2.1 Selecting relevant features</strong></h3>

<p>Now let’s identify the redundant columns in the
 <code class="language-plaintext highlighter-rouge">volunteer</code>
 dataset and perform feature selection on the dataset to return a DataFrame of the relevant features.</p>

<p>For example, if you explore the
 <code class="language-plaintext highlighter-rouge">volunteer</code>
 dataset in the console, you’ll see three features which are related to location:
 <code class="language-plaintext highlighter-rouge">locality</code>
 ,
 <code class="language-plaintext highlighter-rouge">region</code>
 , and
 <code class="language-plaintext highlighter-rouge">postalcode</code>
 . They contain repeated information, so it would make sense to keep only one of the features.</p>

<p>There are also features that have gone through the feature engineering process: columns like
 <code class="language-plaintext highlighter-rouge">Education</code>
 and
 <code class="language-plaintext highlighter-rouge">Emergency Preparedness</code>
 are a product of encoding the categorical variable
 <code class="language-plaintext highlighter-rouge">category_desc</code>
 , so
 <code class="language-plaintext highlighter-rouge">category_desc</code>
 itself is redundant now.</p>

<p>Take a moment to examine the features of
 <code class="language-plaintext highlighter-rouge">volunteer</code>
 in the console, and try to identify the redundant features.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Create a list of redundant column names to drop
</span><span class="n">to_drop</span> <span class="o">=</span> <span class="p">[</span><span class="s">"locality"</span><span class="p">,</span> <span class="s">"region"</span><span class="p">,</span> <span class="s">"category_desc"</span><span class="p">,</span> <span class="s">"created_date"</span><span class="p">,</span> <span class="s">"vol_requests"</span><span class="p">]</span>

<span class="c1"># Drop those columns from the dataset
</span><span class="n">volunteer_subset</span> <span class="o">=</span> <span class="n">volunteer</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">to_drop</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Print out the head of the new dataset
</span><span class="k">print</span><span class="p">(</span><span class="n">volunteer_subset</span><span class="p">.</span><span class="n">head</span><span class="p">())</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
volunteer_subset.columns
Index(['title', 'hits', 'postalcode', 'vol_requests_lognorm', 'created_month',
       'Education', 'Emergency Preparedness', 'Environment', 'Health',
       'Helping Neighbors in Need', 'Strengthening Communities'],
      dtype='object')

</code></pre></div></div>

<h3 id="422-checking-for-correlated-features"><strong>4.2.2 Checking for correlated features</strong></h3>

<p>Let’s take a look at the
 <code class="language-plaintext highlighter-rouge">wine</code>
 dataset again, which is made up of continuous, numerical features. Run Pearson’s correlation coefficient on the dataset to determine which columns are good candidates for eliminating. Then, remove those columns from the DataFrame.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Print out the column correlations of the wine dataset
</span><span class="k">print</span><span class="p">(</span><span class="n">wine</span><span class="p">.</span><span class="n">corr</span><span class="p">())</span>

<span class="c1"># Take a minute to find the column where the correlation value is greater than 0.75 at least twice
</span><span class="n">to_drop</span> <span class="o">=</span> <span class="s">"Flavanoids"</span>

<span class="c1"># Drop that column from the DataFrame
</span><span class="n">wine</span> <span class="o">=</span> <span class="n">wine</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">to_drop</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
print(wine.corr())
                              Flavanoids  Total phenols  Malic acid  OD280/OD315 of diluted wines       Hue
Flavanoids                      1.000000       0.864564   -0.411007                      0.787194  0.543479
Total phenols                   0.864564       1.000000   -0.335167                      0.699949  0.433681
Malic acid                     -0.411007      -0.335167    1.000000                     -0.368710 -0.561296
OD280/OD315 of diluted wines    0.787194       0.699949   -0.368710                      1.000000  0.565468
Hue                             0.543479       0.433681   -0.561296                      0.565468  1.000000

</code></pre></div></div>

<hr />

<h2 id="43-selecting-features-using-text-vectors"><strong>4.3 Selecting features using text vectors</strong></h2>

<p><img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/4-13.png?w=985" alt="Desktop View" />
<img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/5-13.png?w=997" alt="Desktop View" />
<img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/6-13.png?w=945" alt="Desktop View" /></p>

<h3 id="431-exploring-text-vectors-part-1"><strong>4.3.1 Exploring text vectors, part 1</strong></h3>

<p>Let’s expand on the text vector exploration method we just learned about, using the
 <code class="language-plaintext highlighter-rouge">volunteer</code>
 dataset’s
 <code class="language-plaintext highlighter-rouge">title</code>
 tf/idf vectors. In this first part of text vector exploration, we’re going to add to that function we learned about in the slides. We’ll return a list of numbers with the function. In the next exercise, we’ll write another function to collect the top words across all documents, extract them, and then use that list to filter down our
 <code class="language-plaintext highlighter-rouge">text_tfidf</code>
 vector.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
vocab
{1048: 'web',
 278: 'designer',
 1017: 'urban',
...}

tfidf_vec
TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)

tfidf_vec.vocabulary_
{'web': 1048,
 'designer': 278,
 'urban': 1017,
...}

text_tfidf
&lt;617x1089 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
	with 3172 stored elements in Compressed Sparse Row format&gt;

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Add in the rest of the parameters
</span><span class="k">def</span> <span class="nf">return_weights</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">original_vocab</span><span class="p">,</span> <span class="n">vector</span><span class="p">,</span> <span class="n">vector_index</span><span class="p">,</span> <span class="n">top_n</span><span class="p">):</span>
    <span class="n">zipped</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">vector</span><span class="p">[</span><span class="n">vector_index</span><span class="p">].</span><span class="n">indices</span><span class="p">,</span> <span class="n">vector</span><span class="p">[</span><span class="n">vector_index</span><span class="p">].</span><span class="n">data</span><span class="p">))</span>

    <span class="c1"># Let's transform that zipped dict into a series
</span>    <span class="n">zipped_series</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">({</span><span class="n">vocab</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span><span class="n">zipped</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">vector</span><span class="p">[</span><span class="n">vector_index</span><span class="p">].</span><span class="n">indices</span><span class="p">})</span>

    <span class="c1"># Let's sort the series to pull out the top n weighted words
</span>    <span class="n">zipped_index</span> <span class="o">=</span> <span class="n">zipped_series</span><span class="p">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)[:</span><span class="n">top_n</span><span class="p">].</span><span class="n">index</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">original_vocab</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">zipped_index</span><span class="p">]</span>

<span class="c1"># Print out the weighted words
</span><span class="k">print</span><span class="p">(</span><span class="n">return_weights</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">tfidf_vec</span><span class="p">.</span><span class="n">vocabulary_</span><span class="p">,</span> <span class="n">text_tfidf</span><span class="p">,</span> <span class="n">vector_index</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>
<span class="c1"># [189, 942, 466]
</span>
</code></pre></div></div>

<h3 id="432-exploring-text-vectors-part-2"><strong>4.3.2 Exploring text vectors, part 2</strong></h3>

<p>Using the function we wrote in the previous exercise, we’re going to extract the top words from each document in the text vector, return a list of the word indices, and use that list to filter the text vector down to those top words.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
def words_to_filter(vocab, original_vocab, vector, top_n):
    filter_list = []
    for i in range(0, vector.shape[0]):

        # Here we'll call the function from the previous exercise, and extend the list we're creating
        filtered = return_weights(vocab, original_vocab, vector, i, top_n)
        filter_list.extend(filtered)
    # Return the list in a set, so we don't get duplicate word indices
    return set(filter_list)

# Call the function to get the list of word indices
filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)

# By converting filtered_words back to a list, we can use it to filter the columns in the text vector
filtered_text = text_tfidf[:, list(filtered_words)]

filtered_text
&lt;617x1008 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
	with 2948 stored elements in Compressed Sparse Row format&gt;

</code></pre></div></div>

<h3 id="433-training-naive-bayes-with-feature-selection"><strong>4.3.3 Training Naive Bayes with feature selection</strong></h3>

<p>Let’s re-run the Naive Bayes text classification model we ran at the end of chapter 3, with our selection choices from the previous exercise, on the
 <code class="language-plaintext highlighter-rouge">volunteer</code>
 dataset’s
 <code class="language-plaintext highlighter-rouge">title</code>
 and
 <code class="language-plaintext highlighter-rouge">category_desc</code>
 columns.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Split the dataset according to the class distribution of category_desc, using the filtered_text vector
</span><span class="n">train_X</span><span class="p">,</span> <span class="n">test_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">test_y</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">filtered_text</span><span class="p">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">y</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Fit the model to the training data
</span><span class="n">nb</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>

<span class="c1"># Print out the model's accuracy
</span><span class="k">print</span><span class="p">(</span><span class="n">nb</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_X</span><span class="p">,</span><span class="n">test_y</span><span class="p">))</span>
<span class="c1"># 0.567741935483871
</span>
</code></pre></div></div>

<p>You can see that our accuracy score wasn’t that different from the score at the end of chapter 3. That’s okay; the
 <code class="language-plaintext highlighter-rouge">title</code>
 field is a very small text field, appropriate for demonstrating how filtering vectors works.</p>

<hr />

<h2 id="44-dimensionality-reduction"><strong>4.4 Dimensionality reduction</strong></h2>

<p><img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/7-13.png?w=830" alt="Desktop View" /></p>

<h3 id="441-using-pca"><strong>4.4.1 Using PCA</strong></h3>

<p>Let’s apply PCA to the
 <code class="language-plaintext highlighter-rouge">wine</code>
 dataset, to see if we can get an increase in our model’s accuracy.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
from sklearn.decomposition import PCA

# Set up PCA and the X vector for diminsionality reduction
pca = PCA()
wine_X = wine.drop("Type", axis=1)

# Apply PCA to the wine dataset X vector
transformed_X = pca.fit_transform(wine_X)

# Look at the percentage of variance explained by the different components
print(pca.explained_variance_ratio_)

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
[9.98091230e-01 1.73591562e-03 9.49589576e-05 5.02173562e-05
 1.23636847e-05 8.46213034e-06 2.80681456e-06 1.52308053e-06
 1.12783044e-06 7.21415811e-07 3.78060267e-07 2.12013755e-07
 8.25392788e-08]

</code></pre></div></div>

<h3 id="442-training-a-model-with-pca"><strong>4.4.2 Training a model with PCA</strong></h3>

<p>Now that we have run PCA on the
 <code class="language-plaintext highlighter-rouge">wine</code>
 dataset, let’s try training a model with it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Split the transformed X and the y labels into training and test sets
</span><span class="n">X_wine_train</span><span class="p">,</span> <span class="n">X_wine_test</span><span class="p">,</span> <span class="n">y_wine_train</span><span class="p">,</span> <span class="n">y_wine_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">transformed_X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Fit knn to the training data
</span><span class="n">knn</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_wine_train</span><span class="p">,</span><span class="n">y_wine_train</span><span class="p">)</span>

<span class="c1"># Score knn on the test data and print it out
</span><span class="n">knn</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_wine_test</span><span class="p">,</span><span class="n">y_wine_test</span><span class="p">)</span>
<span class="c1"># 0.6444444444444445
</span>
</code></pre></div></div>

<h1 id="5-putting-it-all-together"><strong>5. Putting it all together</strong></h1>
<hr />

<h2 id="51-ufos-and-preprocessing"><strong>5.1 UFOs and preprocessing</strong></h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
ufo.head()
                 date               city state country      type  seconds  \
2 2002-11-21 05:45:00           clemmons    nc      us  triangle    300.0
4 2012-06-16 23:00:00          san diego    ca      us     light    600.0
7 2013-06-09 00:00:00  oakville (canada)    on      ca     light    120.0
8 2013-04-26 23:27:00              lacey    wa      us     light    120.0
9 2013-09-13 20:30:00           ben avon    pa      us    sphere    300.0

    length_of_time                                               desc  \
2  about 5 minutes  It was a large, triangular shaped flying ob...
4       10 minutes  Dancing lights that would fly around and then ...
7        2 minutes  Brilliant orange light or chinese lantern at o...
8        2 minutes  Bright red light moving north to north west fr...
9        5 minutes  North-east moving south-west. First 7 or so li...

     recorded        lat        long
2  12/23/2002  36.021389  -80.382222
4    7/4/2012  32.715278 -117.156389
7    7/3/2013  43.433333  -79.666667
8   5/15/2013  47.034444 -122.821944
9   9/30/2013  40.508056  -80.083333

</code></pre></div></div>

<p><img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/8-12.png?w=977" alt="Desktop View" /></p>

<h3 id="511-checking-column-types"><strong>5.1.1 Checking column types</strong></h3>

<p>Take a look at the UFO dataset’s column types using the
 <code class="language-plaintext highlighter-rouge">dtypes</code>
 attribute. Two columns jump out for transformation: the seconds column, which is a numeric column but is being read in as
 <code class="language-plaintext highlighter-rouge">object</code>
 , and the
 <code class="language-plaintext highlighter-rouge">date</code>
 column, which can be transformed into the
 <code class="language-plaintext highlighter-rouge">datetime</code>
 type. That will make our feature engineering efforts easier later on.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Check the column types
</span><span class="k">print</span><span class="p">(</span><span class="n">ufo</span><span class="p">.</span><span class="n">dtypes</span><span class="p">)</span>

<span class="c1"># Change the type of seconds to float
</span><span class="n">ufo</span><span class="p">[</span><span class="s">"seconds"</span><span class="p">]</span> <span class="o">=</span> <span class="n">ufo</span><span class="p">.</span><span class="n">seconds</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float'</span><span class="p">)</span>

<span class="c1"># Change the date column to type datetime
</span><span class="n">ufo</span><span class="p">[</span><span class="s">"date"</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">ufo</span><span class="p">[</span><span class="s">'date'</span><span class="p">])</span>

<span class="c1"># Check the column types
</span><span class="k">print</span><span class="p">(</span><span class="n">ufo</span><span class="p">[[</span><span class="s">'seconds'</span><span class="p">,</span><span class="s">'date'</span><span class="p">]].</span><span class="n">dtypes</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
date               object
city               object
state              object
country            object
type               object
seconds            object
length_of_time     object
desc               object
recorded           object
lat                object
long              float64
dtype: object
seconds           float64
date       datetime64[ns]
dtype: object

</code></pre></div></div>

<h3 id="512-dropping-missing-data"><strong>5.1.2 Dropping missing data</strong></h3>

<p>Let’s remove some of the rows where certain columns have missing values. We’re going to look at the
 <code class="language-plaintext highlighter-rouge">length_of_time</code>
 column, the
 <code class="language-plaintext highlighter-rouge">state</code>
 column, and the
 <code class="language-plaintext highlighter-rouge">type</code>
 column. If any of the values in these columns are missing, we’re going to drop the rows.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Check how many values are missing in the length_of_time, state, and type columns
</span><span class="k">print</span><span class="p">(</span><span class="n">ufo</span><span class="p">[[</span><span class="s">'length_of_time'</span><span class="p">,</span> <span class="s">'state'</span><span class="p">,</span> <span class="s">'type'</span><span class="p">]].</span><span class="n">isnull</span><span class="p">().</span><span class="nb">sum</span><span class="p">())</span>

<span class="c1"># Keep only rows where length_of_time, state, and type are not null
</span><span class="n">ufo_no_missing</span> <span class="o">=</span> <span class="n">ufo</span><span class="p">[</span><span class="n">ufo</span><span class="p">[</span><span class="s">'length_of_time'</span><span class="p">].</span><span class="n">notnull</span><span class="p">()</span> <span class="o">&amp;</span>
          <span class="n">ufo</span><span class="p">[</span><span class="s">'state'</span><span class="p">].</span><span class="n">notnull</span><span class="p">()</span> <span class="o">&amp;</span>
          <span class="n">ufo</span><span class="p">[</span><span class="s">'type'</span><span class="p">].</span><span class="n">notnull</span><span class="p">()]</span>

<span class="c1"># Print out the shape of the new dataset
</span><span class="k">print</span><span class="p">(</span><span class="n">ufo_no_missing</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
length_of_time    143
state             419
type              159
dtype: int64
(4283, 4)

</code></pre></div></div>

<hr />

<h2 id="52-categorical-variables-and-standardization"><strong>5.2 Categorical variables and standardization</strong></h2>

<p><img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/9-12.png?w=731" alt="Desktop View" />
<img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/10-12.png?w=486" alt="Desktop View" /></p>

<h3 id="521-extracting-numbers-from-strings"><strong>5.2.1 Extracting numbers from strings</strong></h3>

<p>The
 <code class="language-plaintext highlighter-rouge">length_of_time</code>
 field in the UFO dataset is a text field that has the number of minutes within the string. Here, you’ll extract that number from that text field using regular expressions.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
def return_minutes(time_string):

    # Use \d+ to grab digits
    pattern = re.compile(r"\d+")

    # Use match on the pattern and column
    num = re.match(pattern, time_string)
    if num is not None:
        return int(num.group(0))

# Apply the extraction to the length_of_time column
ufo["minutes"] = ufo["length_of_time"].apply(return_minutes)

# Take a look at the head of both of the columns
print(ufo[['length_of_time','minutes']].head())

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
    length_of_time  minutes
2  about 5 minutes      NaN
4       10 minutes     10.0
7        2 minutes      2.0
8        2 minutes      2.0
9        5 minutes      5.0

</code></pre></div></div>

<h3 id="522-identifying-features-for-standardization"><strong>5.2.2 Identifying features for standardization</strong></h3>

<p>In this section, you’ll investigate the variance of columns in the UFO dataset to determine which features should be standardized. After taking a look at the variances of the
 <code class="language-plaintext highlighter-rouge">seconds</code>
 and
 <code class="language-plaintext highlighter-rouge">minutes</code>
 column, you’ll see that the variance of the
 <code class="language-plaintext highlighter-rouge">seconds</code>
 column is extremely high. Because
 <code class="language-plaintext highlighter-rouge">seconds</code>
 and
 <code class="language-plaintext highlighter-rouge">minutes</code>
 are related to each other (an issue we’ll deal with when we select features for modeling), let’s log normlize the
 <code class="language-plaintext highlighter-rouge">seconds</code>
 column.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Check the variance of the seconds and minutes columns
</span><span class="k">print</span><span class="p">(</span><span class="n">ufo</span><span class="p">[[</span><span class="s">'seconds'</span><span class="p">,</span><span class="s">'minutes'</span><span class="p">]].</span><span class="n">var</span><span class="p">())</span>

<span class="c1"># Log normalize the seconds column
</span><span class="n">ufo</span><span class="p">[</span><span class="s">"seconds_log"</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">ufo</span><span class="p">[[</span><span class="s">'seconds'</span><span class="p">]])</span>

<span class="c1"># Print out the variance of just the seconds_log column
</span><span class="k">print</span><span class="p">(</span><span class="n">ufo</span><span class="p">[</span><span class="s">"seconds_log"</span><span class="p">].</span><span class="n">var</span><span class="p">())</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
seconds    424087.417474
minutes       117.546372
dtype: float64
1.1223923881183004

</code></pre></div></div>

<hr />

<h2 id="53-engineering-new-features"><strong>5.3 Engineering new features</strong></h2>

<p><img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/11-11.png?w=995" alt="Desktop View" /></p>

<h3 id="531-encoding-categorical-variables"><strong>5.3.1 Encoding categorical variables</strong></h3>

<p>There are couple of columns in the UFO dataset that need to be encoded before they can be modeled through scikit-learn. You’ll do that transformation here, using both binary and one-hot encoding methods.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Use Pandas to encode us values as 1 and others as 0
</span><span class="n">ufo</span><span class="p">[</span><span class="s">"country_enc"</span><span class="p">]</span> <span class="o">=</span> <span class="n">ufo</span><span class="p">[</span><span class="s">"country"</span><span class="p">].</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span><span class="o">==</span><span class="s">'us'</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># Print the number of unique type values
</span><span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ufo</span><span class="p">[</span><span class="s">'type'</span><span class="p">].</span><span class="n">unique</span><span class="p">()))</span>
<span class="c1"># 21
</span>
<span class="c1"># Create a one-hot encoded set of the type values
</span><span class="n">type_set</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">ufo</span><span class="p">[</span><span class="s">'type'</span><span class="p">])</span>

<span class="c1"># Concatenate this set back to the ufo DataFrame
</span><span class="n">ufo</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">ufo</span><span class="p">,</span> <span class="n">type_set</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

</code></pre></div></div>

<h3 id="532-features-from-dates"><strong>5.3.2 Features from dates</strong></h3>

<p>Another feature engineering task to perform is month and year extraction. Perform this task on the
 <code class="language-plaintext highlighter-rouge">date</code>
 column of the
 <code class="language-plaintext highlighter-rouge">ufo</code>
 dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Look at the first 5 rows of the date column
</span><span class="k">print</span><span class="p">(</span><span class="n">ufo</span><span class="p">[</span><span class="s">'date'</span><span class="p">].</span><span class="n">head</span><span class="p">())</span>

<span class="c1"># Extract the month from the date column
</span><span class="n">ufo</span><span class="p">[</span><span class="s">"month"</span><span class="p">]</span> <span class="o">=</span> <span class="n">ufo</span><span class="p">[</span><span class="s">"date"</span><span class="p">].</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="p">.</span><span class="n">month</span><span class="p">)</span>

<span class="c1"># Extract the year from the date column
</span><span class="n">ufo</span><span class="p">[</span><span class="s">"year"</span><span class="p">]</span> <span class="o">=</span> <span class="n">ufo</span><span class="p">[</span><span class="s">"date"</span><span class="p">].</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="p">.</span><span class="n">year</span><span class="p">)</span>

<span class="c1"># Take a look at the head of all three columns
</span><span class="k">print</span><span class="p">(</span><span class="n">ufo</span><span class="p">[[</span><span class="s">'date'</span><span class="p">,</span><span class="s">'month'</span><span class="p">,</span><span class="s">'year'</span><span class="p">]].</span><span class="n">head</span><span class="p">())</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
0   2002-11-21 05:45:00
1   2012-06-16 23:00:00
2   2013-06-09 00:00:00
3   2013-04-26 23:27:00
4   2013-09-13 20:30:00
Name: date, dtype: datetime64[ns]
                 date  month  year
0 2002-11-21 05:45:00     11  2002
1 2012-06-16 23:00:00      6  2012
2 2013-06-09 00:00:00      6  2013
3 2013-04-26 23:27:00      4  2013
4 2013-09-13 20:30:00      9  2013

</code></pre></div></div>

<h3 id="533-text-vectorization"><strong>5.3.3 Text vectorization</strong></h3>

<p>Let’s transform the
 <code class="language-plaintext highlighter-rouge">desc</code>
 column in the UFO dataset into tf/idf vectors, since there’s likely something we can learn from this field.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Take a look at the head of the desc field
</span><span class="k">print</span><span class="p">(</span><span class="n">ufo</span><span class="p">[</span><span class="s">"desc"</span><span class="p">].</span><span class="n">head</span><span class="p">())</span>

<span class="c1"># Create the tfidf vectorizer object
</span><span class="n">vec</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">()</span>

<span class="c1"># Use vec's fit_transform method on the desc field
</span><span class="n">desc_tfidf</span> <span class="o">=</span> <span class="n">vec</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">ufo</span><span class="p">[</span><span class="s">"desc"</span><span class="p">])</span>

<span class="c1"># Look at the number of columns this creates
</span><span class="k">print</span><span class="p">(</span><span class="n">desc_tfidf</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
0    It was a large, triangular shaped flying ob...
1    Dancing lights that would fly around and then ...
2    Brilliant orange light or chinese lantern at o...
3    Bright red light moving north to north west fr...
4    North-east moving south-west. First 7 or so li...
Name: desc, dtype: object
(1866, 3422)

</code></pre></div></div>

<hr />

<h2 id="54-feature-selection-and-modeling"><strong>5.4 Feature selection and modeling</strong></h2>

<p><img src="/blog/assets/datacamp/preprocessing-for-machine-learning-in-python/12-12.png?w=925" alt="Desktop View" /></p>

<h3 id="541-selecting-the-ideal-dataset"><strong>5.4.1 Selecting the ideal dataset</strong></h3>

<p>Let’s get rid of some of the unnecessary features. Because we have an encoded country column,
 <code class="language-plaintext highlighter-rouge">country_enc</code>
 , keep it and drop other columns related to location:
 <code class="language-plaintext highlighter-rouge">city</code>
 ,
 <code class="language-plaintext highlighter-rouge">country</code>
 ,
 <code class="language-plaintext highlighter-rouge">lat</code>
 ,
 <code class="language-plaintext highlighter-rouge">long</code>
 ,
 <code class="language-plaintext highlighter-rouge">state</code>
 .</p>

<p>We have columns related to
 <code class="language-plaintext highlighter-rouge">month</code>
 and
 <code class="language-plaintext highlighter-rouge">year</code>
 , so we don’t need the
 <code class="language-plaintext highlighter-rouge">date</code>
 or
 <code class="language-plaintext highlighter-rouge">recorded</code>
 columns.</p>

<p>We vectorized
 <code class="language-plaintext highlighter-rouge">desc</code>
 , so we don’t need it anymore. For now we’ll keep
 <code class="language-plaintext highlighter-rouge">type</code>
 .</p>

<p>We’ll keep
 <code class="language-plaintext highlighter-rouge">seconds_log</code>
 and drop
 <code class="language-plaintext highlighter-rouge">seconds</code>
 and
 <code class="language-plaintext highlighter-rouge">minutes</code>
 .</p>

<p>Let’s also get rid of the
 <code class="language-plaintext highlighter-rouge">length_of_time</code>
 column, which is unnecessary after extracting
 <code class="language-plaintext highlighter-rouge">minutes</code>
 .</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Check the correlation between the seconds, seconds_log, and minutes columns
</span><span class="k">print</span><span class="p">(</span><span class="n">ufo</span><span class="p">[[</span><span class="s">'seconds'</span><span class="p">,</span><span class="s">'seconds_log'</span><span class="p">,</span><span class="s">'minutes'</span><span class="p">]].</span><span class="n">corr</span><span class="p">())</span>

<span class="c1"># Make a list of features to drop
</span><span class="n">to_drop</span> <span class="o">=</span> <span class="p">[</span><span class="s">'city'</span><span class="p">,</span> <span class="s">'country'</span><span class="p">,</span> <span class="s">'date'</span><span class="p">,</span> <span class="s">'desc'</span><span class="p">,</span> <span class="s">'lat'</span><span class="p">,</span> <span class="s">'length_of_time'</span><span class="p">,</span> <span class="s">'long'</span><span class="p">,</span> <span class="s">'minutes'</span><span class="p">,</span> <span class="s">'recorded'</span><span class="p">,</span> <span class="s">'seconds'</span><span class="p">,</span> <span class="s">'state'</span><span class="p">]</span>

<span class="c1"># Drop those features
</span><span class="n">ufo_dropped</span> <span class="o">=</span> <span class="n">ufo</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">to_drop</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Let's also filter some words out of the text vector we created
</span><span class="n">filtered_words</span> <span class="o">=</span> <span class="n">words_to_filter</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">vec</span><span class="p">.</span><span class="n">vocabulary_</span><span class="p">,</span> <span class="n">desc_tfidf</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
              seconds  seconds_log   minutes
seconds      1.000000     0.853371  0.980341
seconds_log  0.853371     1.000000  0.824493
minutes      0.980341     0.824493  1.000000

</code></pre></div></div>

<h3 id="542-modeling-the-ufo-dataset-part-1"><strong>5.4.2 Modeling the UFO dataset, part 1</strong></h3>

<p>In this exercise, we’re going to build a k-nearest neighbor model to predict which country the UFO sighting took place in. Our
 <code class="language-plaintext highlighter-rouge">X</code>
 dataset has the log-normalized seconds column, the one-hot encoded type columns, as well as the month and year when the sighting took place. The
 <code class="language-plaintext highlighter-rouge">y</code>
 labels are the encoded country column, where 1 is
 <code class="language-plaintext highlighter-rouge">us</code>
 and 0 is
 <code class="language-plaintext highlighter-rouge">ca</code>
 .</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Take a look at the features in the X set of data
</span><span class="k">print</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>

<span class="c1"># Split the X and y sets using train_test_split, setting stratify=y
</span><span class="n">train_X</span><span class="p">,</span> <span class="n">test_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">test_y</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Fit knn to the training sets
</span><span class="n">knn</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span><span class="n">train_y</span><span class="p">)</span>

<span class="c1"># Print the score of knn on the test sets
</span><span class="k">print</span><span class="p">(</span><span class="n">knn</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_X</span><span class="p">,</span><span class="n">test_y</span><span class="p">))</span>
<span class="c1"># 0.8693790149892934
</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Index(['seconds_log', 'changing', 'chevron', 'cigar', 'circle', 'cone',
       'cross', 'cylinder', 'diamond', 'disk', 'egg', 'fireball', 'flash',
       'formation', 'light', 'other', 'oval', 'rectangle', 'sphere',
       'teardrop', 'triangle', 'unknown', 'month', 'year'],
      dtype='object')

</code></pre></div></div>

<h3 id="543-modeling-the-ufo-dataset-part-2"><strong>5.4.3 Modeling the UFO dataset, part 2</strong></h3>

<p>Finally, let’s build a model using the text vector we created,
 <code class="language-plaintext highlighter-rouge">desc_tfidf</code>
 , using the
 <code class="language-plaintext highlighter-rouge">filtered_words</code>
 list to create a filtered text vector. Let’s see if we can predict the
 <code class="language-plaintext highlighter-rouge">type</code>
 of the sighting based on the text. We’ll use a Naive Bayes model for this.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Use the list of filtered words we created to filter the text vector
</span><span class="n">filtered_text</span> <span class="o">=</span> <span class="n">desc_tfidf</span><span class="p">[:,</span> <span class="nb">list</span><span class="p">(</span><span class="n">filtered_words</span><span class="p">)]</span>

<span class="c1"># Split the X and y sets using train_test_split, setting stratify=y
</span><span class="n">train_X</span><span class="p">,</span> <span class="n">test_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">test_y</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">filtered_text</span><span class="p">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">y</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Fit nb to the training sets
</span><span class="n">nb</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span><span class="n">train_y</span><span class="p">)</span>

<span class="c1"># Print the score of nb on the test sets
</span><span class="k">print</span><span class="p">(</span><span class="n">nb</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_X</span><span class="p">,</span><span class="n">test_y</span><span class="p">))</span>
<span class="c1"># 0.16274089935760172
</span>
</code></pre></div></div>

<p>As you can see, this model performs very poorly on this text data. This is a clear case where iteration would be necessary to figure out what subset of text improves the model, and if perhaps any of the other features are useful in predicting
 <code class="language-plaintext highlighter-rouge">type</code>
 .</p>

<hr />

<p>Thank you for reading and hope you’ve learned a lot.</p>

