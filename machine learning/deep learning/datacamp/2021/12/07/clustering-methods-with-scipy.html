<h1 id="clustering-methods-with-scipy">Clustering Methods with SciPy</h1>

<p>This is the memo of the 6th course (23 courses in all) of ‘Machine Learning Scientist with Python’ skill track.</p>

<p><strong>You can find the original course
 <a href="https://www.datacamp.com/courses/clustering-methods-with-scipy">HERE</a></strong>
 .</p>

<p>###
<strong>Course Description</strong></p>

<p>You have probably come across Google News, which automatically groups similar news articles under a topic. Have you ever wondered what process runs in the background to arrive at these groups? In this course, you will be introduced to unsupervised learning through clustering using the SciPy library in Python. This course covers pre-processing of data and application of hierarchical and k-means clustering. Through the course, you will explore player statistics from a popular football video game, FIFA 18. After completing the course, you will be able to quickly apply various clustering algorithms on data, visualize the clusters formed and analyze results.</p>

<p>###
<strong>Table of contents</strong></p>

<ol>
  <li>Introduction to Clustering</li>
  <li><a href="https://datascience103579984.wordpress.com/2019/12/05/clustering-methods-with-scipy-from-datacamp/2/">Hierarchical Clustering</a></li>
  <li><a href="https://datascience103579984.wordpress.com/2019/12/05/clustering-methods-with-scipy-from-datacamp/3/">K-Means Clustering</a></li>
  <li><a href="https://datascience103579984.wordpress.com/2019/12/05/clustering-methods-with-scipy-from-datacamp/4/">Clustering in Real World</a></li>
</ol>

<hr />

<h1 id="1-introduction-to-clustering"><strong>1. Introduction to Clustering</strong></h1>
<hr />

<h2 id="11-unsupervised-learning-basics"><strong>1.1 Unsupervised learning: basics</strong></h2>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/1-1.png?w=1024" alt="Desktop View" /></p>

<h3 id="111-unsupervised-learning-in-real-world"><strong>1.1.1 Unsupervised learning in real world</strong></h3>

<p>Which of the following examples can be solved with unsupervised learning?</p>

<ul>
  <li>A list of tweets to be classified based on their sentiment, the data has tweets associated with a positive or negative sentiment.</li>
  <li>A spam recognition system that marks incoming emails as spam, the data has emails marked as spam and not spam.</li>
  <li><strong>Segmentation of learners at DataCamp based on courses they complete. The training data has no labels.press</strong></li>
</ul>

<h3 id="112-pokémon-sightings"><strong>1.1.2 Pokémon sightings</strong></h3>

<p>There have been reports of sightings of rare, legendary Pokémon. You have been asked to investigate! Plot the coordinates of sightings to find out where the Pokémon might be. The X and Y coordinates of the points are stored in list
 <code class="language-plaintext highlighter-rouge">x</code>
 and
 <code class="language-plaintext highlighter-rouge">y</code>
 , respectively.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Import plotting class from matplotlib library
</span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Create a scatter plot
</span><span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Display the scatter plot
</span><span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/2-1.png?w=1024" alt="Desktop View" /></p>

<p>Notice the areas where the sightings are dense. This indicates that there is not one, but two legendary Pokémon out there!</p>

<hr />

<h2 id="12-basics-of-cluster-analysis"><strong>1.2 Basics of cluster analysis</strong></h2>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/3-1.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/4-1.png?w=876" alt="Desktop View" /></p>

<p>####
<strong>Hierarchical clustering algorithms</strong></p>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/5-1.png?w=596" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/6-1.png?w=589" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/7-1.png?w=591" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/8.png?w=589" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/9.png?w=908" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/10.png?w=965" alt="Desktop View" /></p>

<p>####
 K-means clustering algorithms</p>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/11.png?w=561" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/12.png?w=551" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/13.png?w=540" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/14.png?w=813" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/15.png?w=982" alt="Desktop View" /></p>

<h3 id="121-pokémon-sightings-hierarchical-clustering"><strong>1.2.1 Pokémon sightings: hierarchical clustering</strong></h3>

<p>We are going to continue the investigation into the sightings of legendary Pokémon from the previous exercise. Remember that in the scatter plot of the previous exercise, you identified two areas where Pokémon sightings were dense. This means that the points seem to separate into two clusters. In this exercise, you will form two clusters of the sightings using hierarchical clustering.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
df.head()
   x   y
0  9   8
1  6   4
2  2  10
3  3   6
4  1   0

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Import linkage and fcluster functions
</span><span class="kn">from</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="kn">import</span> <span class="n">linkage</span><span class="p">,</span> <span class="n">fcluster</span>

<span class="c1"># Use the linkage() function to compute distance
</span><span class="n">Z</span> <span class="o">=</span> <span class="n">linkage</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s">'ward'</span><span class="p">)</span>

<span class="c1"># Generate cluster labels
</span><span class="n">df</span><span class="p">[</span><span class="s">'cluster_labels'</span><span class="p">]</span> <span class="o">=</span> <span class="n">fcluster</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s">'maxclust'</span><span class="p">)</span>

<span class="c1"># Plot the points with seaborn
</span><span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s">'x'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'y'</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">'cluster_labels'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
type(Z)
numpy.ndarray

Z[:3]
array([[10., 13.,  0.,  2.],
       [15., 19.,  0.,  2.],
       [ 1.,  5.,  1.,  2.]])

df
     x   y  cluster_labels
0    9   8               2
1    6   4               2
...
8    1   6               2
9    7   1               2
10  23  29               1
11  26  25               1
...

</code></pre></div></div>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/image.png?w=1024" alt="Desktop View" /></p>

<p>Notice that the cluster labels are plotted with different colors. You will notice that the resulting plot has an extra cluster labelled 0 in the legend. This will be explained later in the course.</p>

<h3 id="122-pokémon-sightings-k-means-clustering"><strong>1.2.2 Pokémon sightings: k-means clustering</strong></h3>

<p>We are going to continue the investigation into the sightings of legendary Pokémon from the previous exercise. Just like the previous exercise, we will use the same example of Pokémon sightings. In this exercise, you will form clusters of the sightings using k-means clustering.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Import kmeans and vq functions
</span><span class="kn">from</span> <span class="nn">scipy.cluster.vq</span> <span class="kn">import</span> <span class="n">kmeans</span><span class="p">,</span> <span class="n">vq</span>

<span class="c1"># Compute cluster centers
</span><span class="n">centroids</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Assign cluster labels
</span><span class="n">df</span><span class="p">[</span><span class="s">'cluster_labels'</span><span class="p">],</span> <span class="n">_</span> <span class="o">=</span> <span class="n">vq</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">centroids</span><span class="p">)</span>

<span class="c1"># Plot the points with seaborn
</span><span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s">'x'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'y'</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">'cluster_labels'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
centroids
array([[23.7, 28. ],
       [ 4.3,  5.9]])

</code></pre></div></div>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/2-2.png?w=1024" alt="Desktop View" /></p>

<p>Notice that in this case, the results of both types of clustering are similar. We will look at distinctly different results later in the course.</p>

<hr />

<h2 id="13-data-preparation-for-cluster-analysis"><strong>1.3 Data preparation for cluster analysis</strong></h2>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/3-2.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/4-2.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/5-2.png?w=1024" alt="Desktop View" /></p>

<h3 id="131-normalize-basic-list-data"><strong>1.3.1 Normalize basic list data</strong></h3>

<p>Now that you are aware of normalization, let us try to normalize some data.
 <code class="language-plaintext highlighter-rouge">goals_for</code>
 is a list of goals scored by a football team in their last ten matches. Let us standardize the data using the
 <code class="language-plaintext highlighter-rouge">whiten()</code>
 function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Import the whiten function
</span><span class="kn">from</span> <span class="nn">scipy.cluster.vq</span> <span class="kn">import</span> <span class="n">whiten</span>

<span class="n">goals_for</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>

<span class="c1"># Use the whiten() function to standardize the data
</span><span class="n">scaled_data</span> <span class="o">=</span> <span class="n">whiten</span><span class="p">(</span><span class="n">goals_for</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">scaled_data</span><span class="p">)</span>
<span class="c1"># [3.07692308 2.30769231 1.53846154 2.30769231 0.76923077 0.76923077
</span> <span class="mf">1.53846154</span> <span class="mf">0.</span>         <span class="mf">0.76923077</span> <span class="mf">3.07692308</span><span class="p">]</span>

</code></pre></div></div>

<h3 id="132-visualize-normalized-data"><strong>1.3.2 Visualize normalized data</strong></h3>

<p>After normalizing your data, you can compare the scaled data to the original data to see the difference. The variables from the last exercise,
 <code class="language-plaintext highlighter-rouge">goals_for</code>
 and
 <code class="language-plaintext highlighter-rouge">scaled_data</code>
 are already available to you.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Plot original data
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">goals_for</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'original'</span><span class="p">)</span>

<span class="c1"># Plot scaled data
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">scaled_data</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'scaled'</span><span class="p">)</span>

<span class="c1"># Show the legend in the plot
</span><span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Display the plot
</span><span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/6-2.png?w=1024" alt="Desktop View" /></p>

<h3 id="133-normalization-of-small-numbers"><strong>1.3.3 Normalization of small numbers</strong></h3>

<p>In earlier examples, you have normalization of whole numbers. In this exercise, you will look at the treatment of fractional numbers – the change of interest rates in the country of Bangalla over the years.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Prepare data
</span><span class="n">rate_cuts</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0025</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0005</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.001</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0005</span><span class="p">,</span> <span class="mf">0.0025</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.001</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0015</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0005</span><span class="p">]</span>

<span class="c1"># Use the whiten() function to standardize the data
</span><span class="n">scaled_data</span> <span class="o">=</span> <span class="n">whiten</span><span class="p">(</span><span class="n">rate_cuts</span><span class="p">)</span>

<span class="c1"># Plot original data
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rate_cuts</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'original'</span><span class="p">)</span>

<span class="c1"># Plot scaled data
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">scaled_data</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'scaled'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/7-2.png?w=1024" alt="Desktop View" /></p>

<p>Notice how the changes in the original data are negligible as compared to the scaled data</p>

<h3 id="134-fifa-18-normalize-data"><strong>1.3.4 FIFA 18: Normalize data</strong></h3>

<p>FIFA 18 is a football video game that was released in 2017 for PC and consoles. The dataset that you are about to work on contains data on the 1000 top individual players in the game. You will explore various features of the data as we move ahead in the course. In this exercise, you will work with two columns,
 <code class="language-plaintext highlighter-rouge">eur_wage</code>
 , the wage of a player in Euros and
 <code class="language-plaintext highlighter-rouge">eur_value</code>
 , their current transfer market value.</p>

<p>The data for this exercise is stored in a Pandas dataframe,
 <code class="language-plaintext highlighter-rouge">fifa</code>
 .
 <code class="language-plaintext highlighter-rouge">whiten</code>
 from
 <code class="language-plaintext highlighter-rouge">scipy.cluster.vq</code>
 and
 <code class="language-plaintext highlighter-rouge">matplotlib.pyplot</code>
 as
 <code class="language-plaintext highlighter-rouge">plt</code>
 have been pre-loaded.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Scale wage and value
</span><span class="n">fifa</span><span class="p">[</span><span class="s">'scaled_wage'</span><span class="p">]</span> <span class="o">=</span> <span class="n">whiten</span><span class="p">(</span><span class="n">fifa</span><span class="p">[</span><span class="s">'eur_wage'</span><span class="p">])</span>
<span class="n">fifa</span><span class="p">[</span><span class="s">'scaled_value'</span><span class="p">]</span> <span class="o">=</span> <span class="n">whiten</span><span class="p">(</span><span class="n">fifa</span><span class="p">[</span><span class="s">'eur_value'</span><span class="p">])</span>

<span class="c1"># Plot the two columns in a scatter plot
</span><span class="n">fifa</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s">'scaled_wage'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'scaled_value'</span><span class="p">,</span> <span class="n">kind</span> <span class="o">=</span> <span class="s">'scatter'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Check mean and standard deviation of scaled values
</span><span class="k">print</span><span class="p">(</span><span class="n">fifa</span><span class="p">[[</span><span class="s">'scaled_wage'</span><span class="p">,</span> <span class="s">'scaled_value'</span><span class="p">]].</span><span class="n">describe</span><span class="p">())</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
       scaled_wage  scaled_value
count      1000.00       1000.00
mean          1.12          1.31
std           1.00          1.00
min           0.00          0.00
25%           0.47          0.73
50%           0.85          1.02
75%           1.41          1.54
max           9.11          8.98

</code></pre></div></div>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/8-1.png?w=1024" alt="Desktop View" /></p>

<p>As you can see the scaled values have a standard deviation of 1.</p>

<h1 id="2-hierarchical-clustering"><strong>2. Hierarchical Clustering</strong></h1>
<hr />

<h2 id="21-basics-of-hierarchical-clustering"><strong>2.1 Basics of hierarchical clustering</strong></h2>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/9-1.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/10-1.png?w=806" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/11-1.png?w=1022" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/15-1.png?w=1024" alt="Desktop View" /></p>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/12-1.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/13-1.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/14-1.png?w=1024" alt="Desktop View" /></p>

<h3 id="211-hierarchical-clustering-ward-method"><strong>2.1.1 Hierarchical clustering: ward method</strong></h3>

<p>It is time for Comic-Con! Comic-Con is an annual comic-based convention held in major cities in the world. You have the data of last year’s footfall, the number of people at the convention ground at a given time.</p>

<p>You would like to decide the location of your stall to maximize sales. Using the ward method, apply hierarchical clustering to find the two points of attraction in the area.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
comic_con
    x_coordinate  y_coordinate  x_scaled  y_scaled
0             17             4  0.509349  0.090010
1             20             6  0.599234  0.135015
2             35             0  1.048660  0.000000
3             14             0  0.419464  0.000000
4             37             4  1.108583  0.090010
5             33             3  0.988736  0.067507
6             14             1  0.419464  0.022502

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Import the fcluster and linkage functions
</span><span class="kn">from</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="kn">import</span> <span class="n">linkage</span><span class="p">,</span> <span class="n">fcluster</span>

<span class="c1"># Use the linkage() function
</span><span class="n">distance_matrix</span> <span class="o">=</span> <span class="n">linkage</span><span class="p">(</span><span class="n">comic_con</span><span class="p">[[</span><span class="s">'x_scaled'</span><span class="p">,</span> <span class="s">'y_scaled'</span><span class="p">]],</span> <span class="n">method</span> <span class="o">=</span> <span class="s">'ward'</span><span class="p">,</span> <span class="n">metric</span> <span class="o">=</span> <span class="s">'euclidean'</span><span class="p">)</span>

<span class="c1"># Assign cluster labels
</span><span class="n">comic_con</span><span class="p">[</span><span class="s">'cluster_labels'</span><span class="p">]</span> <span class="o">=</span> <span class="n">fcluster</span><span class="p">(</span><span class="n">distance_matrix</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s">'maxclust'</span><span class="p">)</span>

<span class="c1"># Plot clusters
</span><span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s">'x_scaled'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'y_scaled'</span><span class="p">,</span>
                <span class="n">hue</span><span class="o">=</span><span class="s">'cluster_labels'</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">comic_con</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/1-2.png?w=1024" alt="Desktop View" /></p>

<p>Notice the two clusters correspond to the points of attractions in the figure towards the bottom (a stage) and the top right (an interesting stall).</p>

<h3 id="212-hierarchical-clustering-single-method"><strong>2.1.2 Hierarchical clustering: single method</strong></h3>

<p>Let us use the same footfall dataset and check if any changes are seen if we use a different method for clustering.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Import the fcluster and linkage functions
</span><span class="kn">from</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="kn">import</span> <span class="n">fcluster</span><span class="p">,</span> <span class="n">linkage</span>

<span class="c1"># Use the linkage() function
</span><span class="n">distance_matrix</span> <span class="o">=</span> <span class="n">linkage</span><span class="p">(</span><span class="n">comic_con</span><span class="p">[[</span><span class="s">'x_scaled'</span><span class="p">,</span> <span class="s">'y_scaled'</span><span class="p">]],</span> <span class="n">method</span> <span class="o">=</span> <span class="s">'single'</span><span class="p">,</span> <span class="n">metric</span> <span class="o">=</span> <span class="s">'euclidean'</span><span class="p">)</span>

<span class="c1"># Assign cluster labels
</span><span class="n">comic_con</span><span class="p">[</span><span class="s">'cluster_labels'</span><span class="p">]</span> <span class="o">=</span> <span class="n">fcluster</span><span class="p">(</span><span class="n">distance_matrix</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s">'maxclust'</span><span class="p">)</span>

<span class="c1"># Plot clusters
</span><span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s">'x_scaled'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'y_scaled'</span><span class="p">,</span>
                <span class="n">hue</span><span class="o">=</span><span class="s">'cluster_labels'</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">comic_con</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p>Notice that in this example, the clusters formed are not different from the ones created using the ward method.</p>

<h3 id="213-hierarchical-clustering-complete-method"><strong>2.1.3 Hierarchical clustering: complete method</strong></h3>

<p>For the third and final time, let us use the same footfall dataset and check if any changes are seen if we use a different method for clustering.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Import the fcluster and linkage functions
</span><span class="kn">from</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="kn">import</span> <span class="n">linkage</span><span class="p">,</span> <span class="n">fcluster</span>

<span class="c1"># Use the linkage() function
</span><span class="n">distance_matrix</span> <span class="o">=</span> <span class="n">linkage</span><span class="p">(</span><span class="n">comic_con</span><span class="p">[[</span><span class="s">'x_scaled'</span><span class="p">,</span> <span class="s">'y_scaled'</span><span class="p">]],</span> <span class="n">method</span><span class="o">=</span><span class="s">'complete'</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s">'euclidean'</span><span class="p">)</span>

<span class="c1"># Assign cluster labels
</span><span class="n">comic_con</span><span class="p">[</span><span class="s">'cluster_labels'</span><span class="p">]</span> <span class="o">=</span> <span class="n">fcluster</span><span class="p">(</span><span class="n">distance_matrix</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">criterion</span><span class="o">=</span><span class="s">'maxclust'</span><span class="p">)</span>

<span class="c1"># Plot clusters
</span><span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s">'x_scaled'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'y_scaled'</span><span class="p">,</span>
                <span class="n">hue</span><span class="o">=</span><span class="s">'cluster_labels'</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">comic_con</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p>Coincidentally, the clusters formed are not different from the ward or single methods. Next, let us learn how to visualize clusters.</p>

<hr />

<h2 id="22-visualize-clusters"><strong>2.2 Visualize clusters</strong></h2>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/3-3.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/4-3.png?w=1000" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/5-3.png?w=1024" alt="Desktop View" /></p>

<h3 id="221-visualize-clusters-with-matplotlib"><strong>2.2.1 Visualize clusters with matplotlib</strong></h3>

<p>We have discussed that visualizations are necessary to assess the clusters that are formed and spot trends in your data. Let us now focus on visualizing the footfall dataset from Comic-Con using the
 <code class="language-plaintext highlighter-rouge">matplotlib</code>
 module.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Import the pyplot class
</span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Define a colors dictionary for clusters
</span><span class="n">colors</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="s">'red'</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span><span class="s">'blue'</span><span class="p">}</span>

<span class="c1"># Plot a scatter plot
</span><span class="n">comic_con</span><span class="p">.</span><span class="n">plot</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s">'x_scaled'</span><span class="p">,</span>
                	   <span class="n">y</span> <span class="o">=</span> <span class="s">'y_scaled'</span><span class="p">,</span>
                	   <span class="n">c</span> <span class="o">=</span> <span class="n">comic_con</span><span class="p">[</span><span class="s">'cluster_labels'</span><span class="p">].</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">colors</span><span class="p">[</span><span class="n">x</span><span class="p">]))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/6-3.png?w=1024" alt="Desktop View" /></p>

<h3 id="222-visualize-clusters-with-seaborn"><strong>2.2.2 Visualize clusters with seaborn</strong></h3>

<p>Let us now visualize the footfall dataset from Comic Con using the
 <code class="language-plaintext highlighter-rouge">seaborn</code>
 module. Visualizing clusters using
 <code class="language-plaintext highlighter-rouge">seaborn</code>
 is easier with the inbuild
 <code class="language-plaintext highlighter-rouge">hue</code>
 function for cluster labels.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Import the seaborn module
</span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="c1"># Plot a scatter plot using seaborn
</span><span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s">'x_scaled'</span><span class="p">,</span>
                <span class="n">y</span><span class="o">=</span><span class="s">'y_scaled'</span><span class="p">,</span>
                <span class="n">hue</span><span class="o">=</span><span class="s">'cluster_labels'</span><span class="p">,</span>
                <span class="n">data</span> <span class="o">=</span> <span class="n">comic_con</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/7-3.png?w=1024" alt="Desktop View" /></p>

<p>Notice the legend is automatically shown when using the
 <code class="language-plaintext highlighter-rouge">hue</code>
 argument.</p>

<hr />

<h2 id="23-how-many-clusters"><strong>2.3 How many clusters?</strong></h2>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/8-2.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/9-2.png?w=869" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/10-2.png?w=1024" alt="Desktop View" /></p>

<h3 id="231-create-a-dendrogram"><strong>2.3.1 Create a dendrogram</strong></h3>

<p>Dendrograms are branching diagrams that show the merging of clusters as we move through the distance matrix. Let us use the Comic Con footfall data to create a dendrogram.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Import the dendrogram function
</span><span class="kn">from</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="kn">import</span> <span class="n">dendrogram</span>

<span class="c1"># Create a dendrogram
</span><span class="n">dn</span> <span class="o">=</span> <span class="n">dendrogram</span><span class="p">(</span><span class="n">distance_matrix</span><span class="p">)</span>

<span class="c1"># Display the dendogram
</span><span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/11-2.png?w=1024" alt="Desktop View" /></p>

<hr />

<h2 id="24-limitations-of-hierarchical-clustering"><strong>2.4 Limitations of hierarchical clustering</strong></h2>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/12-2.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/13-2.png?w=1024" alt="Desktop View" /></p>

<h3 id="241-fifa-18-exploring-defenders"><strong>2.4.1 FIFA 18: exploring defenders</strong></h3>

<p>In the FIFA 18 dataset, various attributes of players are present. Two such attributes are:</p>

<ul>
  <li>
    <dl>
      <dt><em>sliding tackle</em></dt>
      <dd>a number between 0-99 which signifies how accurate a player is able to perform sliding tackles</dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><em>aggression</em></dt>
      <dd>a number between 0-99 which signifies the commitment and will of a player</dd>
    </dl>
  </li>
</ul>

<p>These are typically high in defense-minded players. In this exercise, you will perform clustering based on these attributes in the data.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
fifa.head()
   sliding_tackle  aggression  ...  scaled_aggression  cluster_labels
0              23          63  ...               3.72               3
1              26          48  ...               2.84               3
2              33          56  ...               3.31               3
3              38          78  ...               4.61               3
4              11          29  ...               1.71               2

[5 rows x 5 columns]

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Fit the data into a hierarchical clustering algorithm
</span><span class="n">distance_matrix</span> <span class="o">=</span> <span class="n">linkage</span><span class="p">(</span><span class="n">fifa</span><span class="p">[[</span><span class="s">'scaled_sliding_tackle'</span><span class="p">,</span> <span class="s">'scaled_aggression'</span><span class="p">]],</span> <span class="s">'ward'</span><span class="p">)</span>

<span class="c1"># Assign cluster labels to each row of data
</span><span class="n">fifa</span><span class="p">[</span><span class="s">'cluster_labels'</span><span class="p">]</span> <span class="o">=</span> <span class="n">fcluster</span><span class="p">(</span><span class="n">distance_matrix</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s">'maxclust'</span><span class="p">)</span>

<span class="c1"># Display cluster centers of each cluster
</span><span class="k">print</span><span class="p">(</span><span class="n">fifa</span><span class="p">[[</span><span class="s">'scaled_sliding_tackle'</span><span class="p">,</span> <span class="s">'scaled_aggression'</span><span class="p">,</span> <span class="s">'cluster_labels'</span><span class="p">]].</span><span class="n">groupby</span><span class="p">(</span><span class="s">'cluster_labels'</span><span class="p">).</span><span class="n">mean</span><span class="p">())</span>

<span class="c1"># Create a scatter plot through seaborn
</span><span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s">'scaled_sliding_tackle'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'scaled_aggression'</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">'cluster_labels'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">fifa</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
                scaled_sliding_tackle  scaled_aggression
cluster_labels
1                                2.99               4.35
2                                0.74               1.94
3                                1.34               3.62

</code></pre></div></div>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/14-2.png?w=1024" alt="Desktop View" /></p>
<h1 id="3-k-means-clustering"><strong>3. K-Means Clustering</strong></h1>
<hr />

<h2 id="31-basics-of-k-means-clustering"><strong>3.1 Basics of k-means clustering</strong></h2>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/1-3.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/2-4.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/3-4.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/4-4.png?w=723" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/5-4.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/6-4.png?w=1024" alt="Desktop View" /></p>

<h3 id="311-k-means-clustering-first-exercise"><strong>3.1.1 K-means clustering: first exercise</strong></h3>

<p>This exercise will familiarize you with the usage of k-means clustering on a dataset. Let us use the Comic Con dataset and check how k-means clustering works on it.</p>

<p>Recall the two steps of k-means clustering:</p>

<ul>
  <li>Define cluster centers through
 <code class="language-plaintext highlighter-rouge">kmeans()</code>
 function. It has two required arguments: observations and number of clusters.</li>
  <li>Assign cluster labels through the
 <code class="language-plaintext highlighter-rouge">vq()</code>
 function. It has two required arguments: observations and cluster centers.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Import the kmeans and vq functions
</span><span class="kn">from</span> <span class="nn">scipy.cluster.vq</span> <span class="kn">import</span> <span class="n">kmeans</span><span class="p">,</span> <span class="n">vq</span>

<span class="c1"># Generate cluster centers
</span><span class="n">cluster_centers</span><span class="p">,</span> <span class="n">distortion</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">(</span><span class="n">comic_con</span><span class="p">[[</span><span class="s">'x_scaled'</span><span class="p">,</span><span class="s">'y_scaled'</span><span class="p">]],</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Assign cluster labels
</span><span class="n">comic_con</span><span class="p">[</span><span class="s">'cluster_labels'</span><span class="p">],</span> <span class="n">distortion_list</span> <span class="o">=</span> <span class="n">vq</span><span class="p">(</span><span class="n">comic_con</span><span class="p">[[</span><span class="s">'x_scaled'</span><span class="p">,</span><span class="s">'y_scaled'</span><span class="p">]],</span><span class="n">cluster_centers</span><span class="p">)</span>

<span class="c1"># Plot clusters
</span><span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s">'x_scaled'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'y_scaled'</span><span class="p">,</span>
                <span class="n">hue</span><span class="o">=</span><span class="s">'cluster_labels'</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">comic_con</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/7-4.png?w=1024" alt="Desktop View" /></p>

<p>Notice that the clusters formed are exactly the same as hierarchical clustering that you did in the previous chapter.</p>

<h3 id="312-runtime-of-k-means-clustering"><strong>3.1.2 Runtime of k-means clustering</strong></h3>

<p>Recall that it took a significantly long time to run hierarchical clustering. How long does it take to run the
 <code class="language-plaintext highlighter-rouge">kmeans()</code>
 function on the FIFA dataset?</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
%timeit kmeans(fifa[['scaled_sliding_tackle','scaled_aggression']],3)
# 10 loops, best of 3: 69.7 ms per loop

%timeit linkage(fifa[['scaled_sliding_tackle','scaled_aggression']], method = 'ward', metric = 'euclidean')
# 1 loop, best of 3: 703 ms per loop

</code></pre></div></div>

<hr />

<h2 id="32-how-many-clusters"><strong>3.2 How many clusters?</strong></h2>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/8-3.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/9-3.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/10-3.png?w=900" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/11-3.png?w=779" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/12-3.png?w=980" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/13-3.png?w=1024" alt="Desktop View" /></p>

<h3 id="321-elbow-method-on-distinct-clusters"><strong>3.2.1 Elbow method on distinct clusters</strong></h3>

<p>Let us use the comic con data set to see how the elbow plot looks on a data set with distinct, well-defined clusters. You may want to display the data points before proceeding with the exercise.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
distortions = []
num_clusters = range(1, 7)

# Create a list of distortions from the kmeans function
for i in num_clusters:
    cluster_centers, distortion = kmeans(comic_con[['x_scaled','y_scaled']],i)
    distortions.append(distortion)

# Create a data frame with two lists - num_clusters, distortions
elbow_plot = pd.DataFrame({'num_clusters': num_clusters, 'distortions': distortions})

# Creat a line plot of num_clusters and distortions
sns.lineplot(x='num_clusters', y='distortions', data = elbow_plot)
plt.xticks(num_clusters)
plt.show()

</code></pre></div></div>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/14-3.png?w=1024" alt="Desktop View" /></p>

<p>From the elbow plot, there are 2 clusters in the data.</p>

<h3 id="322-elbow-method-on-uniform-data"><strong>3.2.2 Elbow method on uniform data</strong></h3>

<p>In the earlier exercise, you constructed an elbow plot on data with well-defined clusters. Let us now see how the elbow plot looks on a data set with uniformly distributed points. You may want to display the data points on the console before proceeding with the exercise.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
distortions = []
num_clusters = range(2, 7)

# Create a list of distortions from the kmeans function
for i in num_clusters:
    cluster_centers, distortion = kmeans(uniform_data[['x_scaled','y_scaled']],i)
    distortions.append(distortion)

# Create a data frame with two lists - number of clusters and distortions
elbow_plot = pd.DataFrame({'num_clusters': num_clusters, 'distortions': distortions})

# Creat a line plot of num_clusters and distortions
sns.lineplot(x='num_clusters', y='distortions', data = elbow_plot)
plt.xticks(num_clusters)
plt.show()

</code></pre></div></div>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/15-2.png?w=1024" alt="Desktop View" /></p>

<p>From the elbow plot, we can not determine how many clusters in the data.</p>

<hr />

<h2 id="33-limitations-of-k-means-clustering"><strong>3.3 Limitations of k-means clustering</strong></h2>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/16.png?w=973" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/17.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/18.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/19.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/20.png?w=983" alt="Desktop View" /></p>

<h3 id="331-impact-of-seeds-on-distinct-clusters"><strong>3.3.1 Impact of seeds on distinct clusters</strong></h3>

<p>You noticed the impact of seeds on a dataset that did not have well-defined groups of clusters. In this exercise, you will explore whether seeds impact the clusters in the Comic Con data, where the clusters are well-defined.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Import random class
</span><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">random</span>

<span class="c1"># Initialize seed
</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1000</span><span class="p">])</span>

<span class="c1"># Run kmeans clustering
</span><span class="n">cluster_centers</span><span class="p">,</span> <span class="n">distortion</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">(</span><span class="n">comic_con</span><span class="p">[[</span><span class="s">'x_scaled'</span><span class="p">,</span> <span class="s">'y_scaled'</span><span class="p">]],</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">comic_con</span><span class="p">[</span><span class="s">'cluster_labels'</span><span class="p">],</span> <span class="n">distortion_list</span> <span class="o">=</span> <span class="n">vq</span><span class="p">(</span><span class="n">comic_con</span><span class="p">[[</span><span class="s">'x_scaled'</span><span class="p">,</span> <span class="s">'y_scaled'</span><span class="p">]],</span> <span class="n">cluster_centers</span><span class="p">)</span>

<span class="c1"># Plot the scatterplot
</span><span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s">'x_scaled'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'y_scaled'</span><span class="p">,</span>
                <span class="n">hue</span><span class="o">=</span><span class="s">'cluster_labels'</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">comic_con</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/21.png?w=1024" alt="Desktop View" /></p>

<p>Notice that the plots have not changed after changing the seed as the clusters are well-defined.</p>

<h3 id="332-uniform-clustering-patterns"><strong>3.3.2 Uniform clustering patterns</strong></h3>

<p>Now that you are familiar with the impact of seeds, let us look at the bias in k-means clustering towards the formation of uniform clusters.</p>

<p>Let us use a mouse-like dataset for our next exercise. A mouse-like dataset is a group of points that resemble the head of a mouse: it has three clusters of points arranged in circles, one each for the face and two ears of a mouse.</p>

<p>Here is how a typical mouse-like dataset looks like (
 <a href="https://www.researchgate.net/figure/Clustering-results-for-the-Mouse-data-set-where-the-black-boxes-represent-the-centroids_fig3_256378655">Source</a>
 ).</p>

<p><img src="https://assets.datacamp.com/production/repositories/3842/datasets/fa03a65258018a0c945528a987cdd250010de1ee/Clustering-results-for-the-Mouse-data-set-where-the-black-boxes-represent-the-centroids.ppm" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Import the kmeans and vq functions
</span><span class="kn">from</span> <span class="nn">scipy.cluster.vq</span> <span class="kn">import</span> <span class="n">kmeans</span><span class="p">,</span> <span class="n">vq</span>

<span class="c1"># Generate cluster centers
</span><span class="n">cluster_centers</span><span class="p">,</span> <span class="n">distortion</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">(</span><span class="n">mouse</span><span class="p">[[</span><span class="s">'x_scaled'</span><span class="p">,</span><span class="s">'y_scaled'</span><span class="p">]],</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Assign cluster labels
</span><span class="n">mouse</span><span class="p">[</span><span class="s">'cluster_labels'</span><span class="p">],</span> <span class="n">distortion_list</span> <span class="o">=</span> <span class="n">vq</span><span class="p">(</span><span class="n">mouse</span><span class="p">[[</span><span class="s">'x_scaled'</span><span class="p">,</span><span class="s">'y_scaled'</span><span class="p">]],</span><span class="n">cluster_centers</span><span class="p">)</span>

<span class="c1"># Plot clusters
</span><span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s">'x_scaled'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'y_scaled'</span><span class="p">,</span>
                <span class="n">hue</span><span class="o">=</span><span class="s">'cluster_labels'</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">mouse</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/22.png?w=1024" alt="Desktop View" /></p>

<p>Notice that kmeans is unable to capture the three visible clusters clearly, and the two clusters towards the top have taken in some points along the boundary. This happens due to the underlying assumption in kmeans algorithm to minimize distortions which leads to clusters that are similar in terms of area.</p>

<h3 id="333-fifa-18-defenders-revisited"><strong>3.3.3 FIFA 18: defenders revisited</strong></h3>

<p>In the FIFA 18 dataset, various attributes of players are present. Two such attributes are:</p>

<ul>
  <li>
    <dl>
      <dt><em>defending</em></dt>
      <dd>a number which signifies the defending attributes of a player</dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><em>physical</em></dt>
      <dd>a number which signifies the physical attributes of a player</dd>
    </dl>
  </li>
</ul>

<p>These are typically defense-minded players. In this exercise, you will perform clustering based on these attributes in the data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Set up a random seed in numpy
</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">([</span><span class="mi">1000</span><span class="p">,</span><span class="mi">2000</span><span class="p">])</span>

<span class="c1"># Fit the data into a k-means algorithm
</span><span class="n">cluster_centers</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">(</span><span class="n">fifa</span><span class="p">[[</span><span class="s">'scaled_def'</span><span class="p">,</span> <span class="s">'scaled_phy'</span><span class="p">]],</span> <span class="mi">3</span><span class="p">)</span>

<span class="c1"># Assign cluster labels
</span><span class="n">fifa</span><span class="p">[</span><span class="s">'cluster_labels'</span><span class="p">],</span> <span class="n">_</span> <span class="o">=</span> <span class="n">vq</span><span class="p">(</span><span class="n">fifa</span><span class="p">[[</span><span class="s">'scaled_def'</span><span class="p">,</span> <span class="s">'scaled_phy'</span><span class="p">]],</span> <span class="n">cluster_centers</span><span class="p">)</span>

<span class="c1"># Display cluster centers
</span><span class="k">print</span><span class="p">(</span><span class="n">fifa</span><span class="p">[[</span><span class="s">'scaled_def'</span><span class="p">,</span> <span class="s">'scaled_phy'</span><span class="p">,</span> <span class="s">'cluster_labels'</span><span class="p">]].</span><span class="n">groupby</span><span class="p">(</span><span class="s">'cluster_labels'</span><span class="p">).</span><span class="n">mean</span><span class="p">())</span>

<span class="c1"># Create a scatter plot through seaborn
</span><span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s">'scaled_def'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'scaled_phy'</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">'cluster_labels'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">fifa</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
                scaled_def  scaled_phy
cluster_labels
0                     3.74        8.87
1                     1.87        7.08
2                     2.10        8.94

</code></pre></div></div>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/23.png?w=1024" alt="Desktop View" /></p>

<p>Notice that the seed has an impact on clustering as the data is uniformly distributed.</p>

<h1 id="4-clustering-in-real-world"><strong>4. Clustering in Real World</strong></h1>
<hr />

<h2 id="41-dominant-colors-in-images"><strong>4.1 Dominant colors in images</strong></h2>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/24.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/25.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/26.png?w=968" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/27.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/28.png?w=854" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/29.png?w=841" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/30.png?w=906" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/31.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/32.png?w=797" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/33.png?w=1010" alt="Desktop View" /></p>

<h3 id="411-extract-rgb-values-from-image"><strong>4.1.1 Extract RGB values from image</strong></h3>

<p>There are broadly three steps to find the dominant colors in an image:</p>

<ul>
  <li>Extract RGB values into three lists.</li>
  <li>Perform k-means clustering on scaled RGB values.</li>
  <li>Display the colors of cluster centers.</li>
</ul>

<p>To extract RGB values, we use the
 <code class="language-plaintext highlighter-rouge">imread()</code>
 function of the
 <code class="language-plaintext highlighter-rouge">image</code>
 class of
 <code class="language-plaintext highlighter-rouge">matplotlib</code>
 . Empty lists,
 <code class="language-plaintext highlighter-rouge">r</code>
 ,
 <code class="language-plaintext highlighter-rouge">g</code>
 and
 <code class="language-plaintext highlighter-rouge">b</code>
 have been initialized.</p>

<p>For the purpose of finding dominant colors, we will be using the following image.</p>

<p><img src="https://assets.datacamp.com/production/repositories/3842/datasets/57d0d6d409bfd543e86c7f7398239fa0722e9b48/batman.jpg" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Import image class of matplotlib
</span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">image</span> <span class="k">as</span> <span class="n">img</span>

<span class="c1"># Read batman image and print dimensions
</span><span class="n">batman_image</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="n">imread</span><span class="p">(</span><span class="s">'batman.jpg'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">batman_image</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># (57, 90, 3)
</span>
<span class="c1"># Store RGB values of all pixels in lists r, g and b
</span><span class="k">for</span> <span class="n">rows</span> <span class="ow">in</span> <span class="n">batman_image</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">temp_r</span><span class="p">,</span> <span class="n">temp_g</span><span class="p">,</span> <span class="n">temp_b</span> <span class="ow">in</span> <span class="n">rows</span><span class="p">:</span>
        <span class="n">r</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_r</span><span class="p">)</span>
        <span class="n">g</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_g</span><span class="p">)</span>
        <span class="n">b</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_b</span><span class="p">)</span>

</code></pre></div></div>

<p>You have successfully extracted the RGB values of the image into three lists, one for each color channel.</p>

<p>####
 4.1.2 How many dominant colors?</p>

<p>The RGB values are stored in a data frame,
 <code class="language-plaintext highlighter-rouge">batman_df</code>
 . The RGB values have been standardized used the
 <code class="language-plaintext highlighter-rouge">whiten()</code>
 function, stored in columns,
 <code class="language-plaintext highlighter-rouge">scaled_red</code>
 ,
 <code class="language-plaintext highlighter-rouge">scaled_blue</code>
 and
 <code class="language-plaintext highlighter-rouge">scaled_green</code>
 .</p>

<p>Construct an elbow plot with the data frame. How many dominant colors are present?</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
distortions = []
num_clusters = range(1, 7)

# Create a list of distortions from the kmeans function
for i in num_clusters:
    cluster_centers, distortion = kmeans(batman_df[['scaled_red', 'scaled_blue', 'scaled_green']], i)
    distortions.append(distortion)

# Create a data frame with two lists, num_clusters and distortions
elbow_plot = pd.DataFrame({'num_clusters':num_clusters,'distortions':distortions})

# Create a line plot of num_clusters and distortions
sns.lineplot(x='num_clusters', y='distortions', data = elbow_plot)
plt.xticks(num_clusters)
plt.show()

</code></pre></div></div>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/1-4.png?w=1024" alt="Desktop View" /></p>

<p>Notice that there are three distinct colors present in the image, which is supported by the elbow plot.</p>

<h3 id="413-display-dominant-colors"><strong>4.1.3 Display dominant colors</strong></h3>

<p>To display the dominant colors, convert the colors of the cluster centers to their raw values and then converted them to the range of 0-1, using the following formula:
 <code class="language-plaintext highlighter-rouge">converted_pixel = standardized_pixel * pixel_std / 255</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Get standard deviations of each color
</span><span class="n">r_std</span><span class="p">,</span> <span class="n">g_std</span><span class="p">,</span> <span class="n">b_std</span> <span class="o">=</span> <span class="n">batman_df</span><span class="p">[[</span><span class="s">'red'</span><span class="p">,</span> <span class="s">'green'</span><span class="p">,</span> <span class="s">'blue'</span><span class="p">]].</span><span class="n">std</span><span class="p">()</span>

<span class="k">for</span> <span class="n">cluster_center</span> <span class="ow">in</span> <span class="n">cluster_centers</span><span class="p">:</span>
    <span class="n">scaled_r</span><span class="p">,</span> <span class="n">scaled_g</span><span class="p">,</span> <span class="n">scaled_b</span> <span class="o">=</span> <span class="n">cluster_center</span>
    <span class="c1"># Convert each standardized value to scaled value
</span>    <span class="n">colors</span><span class="p">.</span><span class="n">append</span><span class="p">((</span>
        <span class="n">scaled_r</span> <span class="o">*</span> <span class="n">r_std</span> <span class="o">/</span> <span class="mi">255</span><span class="p">,</span>
        <span class="n">scaled_g</span> <span class="o">*</span> <span class="n">g_std</span> <span class="o">/</span> <span class="mi">255</span><span class="p">,</span>
        <span class="n">scaled_b</span> <span class="o">*</span> <span class="n">b_std</span> <span class="o">/</span> <span class="mi">255</span>
    <span class="p">))</span>

<span class="c1"># Display colors of cluster centers
</span><span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">([</span><span class="n">colors</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/2-5.png?w=1024" alt="Desktop View" /></p>

<p>Notice the three colors resemble the three that are indicative from visual inspection of the image.</p>

<hr />

<h2 id="42-document-clustering"><strong>4.2 Document clustering</strong></h2>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/5-5.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/6-5.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/3-5.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/4-5.png?w=753" alt="Desktop View" /></p>

<h3 id="421-tf-idfterm-frequencyinverse-document-frequency-of-movie-plots"><strong>4.2.1 TF-IDF(term frequency–inverse document frequency) of movie plots</strong></h3>

<p>Let us use the plots of randomly selected movies to perform document clustering on. Before performing clustering on documents, they need to be cleaned of any unwanted noise (such as special characters and stop words) and converted into a sparse matrix through TF-IDF of the documents.</p>

<p>Use the
 <code class="language-plaintext highlighter-rouge">TfidfVectorizer</code>
 class to perform the TF-IDF of movie plots stored in the list
 <code class="language-plaintext highlighter-rouge">plots</code>
 . The
 <code class="language-plaintext highlighter-rouge">remove_noise()</code>
 function is available to use as a
 <code class="language-plaintext highlighter-rouge">tokenizer</code>
 in the
 <code class="language-plaintext highlighter-rouge">TfidfVectorizer</code>
 class. The
 <code class="language-plaintext highlighter-rouge">.fit_transform()</code>
 method fits the data into the
 <code class="language-plaintext highlighter-rouge">TfidfVectorizer</code>
 objects and then generates the TF-IDF sparse matrix.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
plots[:1]
['Cable Hogue is isolated in the desert, awaiting his partners, Taggart and Bowen,
...
...
A coyote wanders into the abandoned Cable Springs. But the coyote has a collar – possibly symbolising the taming of the wilderness.']


</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Import TfidfVectorizer class from sklearn
</span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>

<span class="c1"># Initialize TfidfVectorizer
</span><span class="n">tfidf_vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_df</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">remove_noise</span><span class="p">)</span>

<span class="c1"># Use the .fit_transform() method on the list plots
</span><span class="n">tfidf_matrix</span> <span class="o">=</span> <span class="n">tfidf_vectorizer</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">plots</span><span class="p">)</span>

</code></pre></div></div>

<h3 id="422-top-terms-in-movie-clusters"><strong>4.2.2 Top terms in movie clusters</strong></h3>

<p>Now that you have created a sparse matrix, generate cluster centers and print the top three terms in each cluster. Use the
 <code class="language-plaintext highlighter-rouge">.todense()</code>
 method to convert the sparse matrix,
 <code class="language-plaintext highlighter-rouge">tfidf_matrix</code>
 to a normal matrix for the
 <code class="language-plaintext highlighter-rouge">kmeans()</code>
 function to process. Then, use the
 <code class="language-plaintext highlighter-rouge">.get_feature_names()</code>
 method to get a list of terms in the
 <code class="language-plaintext highlighter-rouge">tfidf_vectorizer</code>
 object. The
 <code class="language-plaintext highlighter-rouge">zip()</code>
 function in Python joins two lists.</p>

<p>The
 <code class="language-plaintext highlighter-rouge">tfidf_vectorizer</code>
 object and sparse matrix,
 <code class="language-plaintext highlighter-rouge">tfidf_matrix</code>
 , from the previous have been retained in this exercise.
 <code class="language-plaintext highlighter-rouge">kmeans</code>
 has been imported from SciPy.</p>

<p>With a higher number of data points, the clusters formed would be defined more clearly. However, this requires some computational power, making it difficult to accomplish in an exercise here.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
num_clusters = 2

# Generate cluster centers through the kmeans function
cluster_centers, distortion = kmeans(tfidf_matrix.todense(),num_clusters)

# Generate terms from the tfidf_vectorizer object
terms = tfidf_vectorizer.get_feature_names()

for i in range(num_clusters):
    # Sort the terms and print top 3 terms
    center_terms = dict(zip(terms, cluster_centers[i]))
    sorted_terms = sorted(center_terms, key=center_terms.get, reverse=True)
    print(sorted_terms[:3])

# ['back', 'father', 'one']
# ['man', 'police', 'killed']

</code></pre></div></div>

<p>Notice positive, warm words in the first cluster and words referring to action in the second cluster.</p>

<hr />

<h2 id="43-clustering-with-multiple-features"><strong>4.3 Clustering with multiple features</strong></h2>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/7-5.png?w=1024" alt="Desktop View" />
<img src="/blog/assets/datacamp/clustering-methods-with-scipy/8-4.png?w=553" alt="Desktop View" /></p>

<h3 id="431-clustering-with-many-features"><strong>4.3.1 Clustering with many features</strong></h3>

<p>What should you do if you have too many features for clustering?</p>

<p>Reduce features using a technique like Factor Analysis</p>

<h3 id="432-basic-checks-on-clusters"><strong>4.3.2 Basic checks on clusters</strong></h3>

<p>In the FIFA 18 dataset, we have concentrated on defenders in previous exercises. Let us try to focus on attacking attributes of a player. Pace (
 <code class="language-plaintext highlighter-rouge">pac</code>
 ), Dribbling (
 <code class="language-plaintext highlighter-rouge">dri</code>
 ) and Shooting (
 <code class="language-plaintext highlighter-rouge">sho</code>
 ) are features that are present in attack minded players. In this exercise, k-means clustering has already been applied on the data using the scaled values of these three attributes. Try some basic checks on the clusters so formed.</p>

<p>The data is stored in a Pandas data frame,
 <code class="language-plaintext highlighter-rouge">fifa</code>
 . The scaled column names are present in a list
 <code class="language-plaintext highlighter-rouge">scaled_features</code>
 . The cluster labels are stored in the
 <code class="language-plaintext highlighter-rouge">cluster_labels</code>
 column. Recall the
 <code class="language-plaintext highlighter-rouge">.count()</code>
 and
 <code class="language-plaintext highlighter-rouge">.mean()</code>
 methods in Pandas help you find the number of observations and mean of observations in a data frame.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Print the size of the clusters
</span><span class="k">print</span><span class="p">(</span><span class="n">fifa</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">'cluster_labels'</span><span class="p">)[</span><span class="s">'ID'</span><span class="p">].</span><span class="n">count</span><span class="p">())</span>

<span class="c1"># Print the mean value of wages in each cluster
</span><span class="k">print</span><span class="p">(</span><span class="n">fifa</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">'cluster_labels'</span><span class="p">)[</span><span class="s">'eur_wage'</span><span class="p">].</span><span class="n">mean</span><span class="p">())</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
cluster_labels
0     83
1    107
2     60
Name: ID, dtype: int64

cluster_labels
0   132108.43
1   130308.41
2   117583.33
Name: eur_wage, dtype: float64

</code></pre></div></div>

<p>In this example, the cluster sizes are not very different, and there are no significant differences that can be seen in the wages. Further analysis is required to validate these clusters.</p>

<h3 id="433-fifa-18-what-makes-a-complete-player"><strong>4.3.3 FIFA 18: what makes a complete player?</strong></h3>

<p>The overall level of a player in FIFA 18 is defined by six characteristics: pace (
 <code class="language-plaintext highlighter-rouge">pac</code>
 ), shooting (
 <code class="language-plaintext highlighter-rouge">sho</code>
 ), passing (
 <code class="language-plaintext highlighter-rouge">pas</code>
 ), dribbling (
 <code class="language-plaintext highlighter-rouge">dri</code>
 ), defending (
 <code class="language-plaintext highlighter-rouge">def</code>
 ), physical (
 <code class="language-plaintext highlighter-rouge">phy</code>
 ).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Create centroids with kmeans for 2 clusters
</span><span class="n">cluster_centers</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">(</span><span class="n">fifa</span><span class="p">[</span><span class="n">scaled_features</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Assign cluster labels and print cluster centers
</span><span class="n">fifa</span><span class="p">[</span><span class="s">'cluster_labels'</span><span class="p">],</span> <span class="n">_</span> <span class="o">=</span> <span class="n">vq</span><span class="p">(</span><span class="n">fifa</span><span class="p">[</span><span class="n">scaled_features</span><span class="p">],</span> <span class="n">cluster_centers</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">fifa</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">'cluster_labels'</span><span class="p">)[</span><span class="n">scaled_features</span><span class="p">].</span><span class="n">mean</span><span class="p">())</span>

<span class="c1"># Plot cluster centers to visualize clusters
</span><span class="n">fifa</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">'cluster_labels'</span><span class="p">)[</span><span class="n">scaled_features</span><span class="p">].</span><span class="n">mean</span><span class="p">().</span><span class="n">plot</span><span class="p">(</span><span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s">'bar'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Get the name column of top 5 players in each cluster
</span><span class="k">for</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="n">fifa</span><span class="p">[</span><span class="s">'cluster_labels'</span><span class="p">].</span><span class="n">unique</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="n">cluster</span><span class="p">,</span> <span class="n">fifa</span><span class="p">[</span><span class="n">fifa</span><span class="p">[</span><span class="s">'cluster_labels'</span><span class="p">]</span> <span class="o">==</span> <span class="n">cluster</span><span class="p">][</span><span class="s">'name'</span><span class="p">].</span><span class="n">values</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
                scaled_pac  scaled_sho  scaled_pas  scaled_dri  scaled_def  \
cluster_labels
0                     6.68        5.43        8.46        8.51        2.50
1                     5.44        3.66        7.17        6.76        3.97

                scaled_phy
cluster_labels
0                     8.34
1                     9.21


0 ['Cristiano Ronaldo' 'L. Messi' 'Neymar' 'L. Suárez' 'M. Neuer']
1 ['Sergio Ramos' 'G. Chiellini' 'D. Godín' 'Thiago Silva' 'M. Hummels']

</code></pre></div></div>

<p><img src="/blog/assets/datacamp/clustering-methods-with-scipy/9-4.png?w=1024" alt="Desktop View" /></p>

<p>Notice the top players in each cluster are representative of the overall characteristics of the cluster – one of the clusters primarily represents attackers, whereas the other represents defenders.</p>

<p>Surprisingly, a top goalkeeper Manuel Neuer is seen in the attackers group, but he is known for going out of the box and participating in open play, which are reflected in his FIFA 18 attributes.</p>

<hr />

<p>Thank you for reading and hope you’ve learned a lot.</p>

