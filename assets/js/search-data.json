{
  
    
        "post0": {
            "title": "1. Kaggle competitions process",
            "content": "Winning a Kaggle Competition in Python . Course Description . Kaggle is the most famous platform for Data Science competitions. Taking part in such competitions allows you to work with real-world datasets, explore various machine learning problems, compete with other participants and, finally, get invaluable hands-on experience. In this course, you will learn how to approach and structure any Data Science competition. You will be able to select the correct local validation scheme and to avoid overfitting. Moreover, you will master advanced feature engineering together with model ensembling approaches. All these techniques will be practiced on Kaggle competitions datasets. . link text link text link text link text . import pandas as pd import numpy as np import matplotlib.pyplot as plt . 1.1 Competitions overview . Kaggle benefits Get practical experience on the real-world data | Develop portfolio projects | Meet a great Data Science community | Try new domain or model type | Keep up-to-date with the best performing methods | . | Process | . . Explore train data . You will work with another Kaggle competition called &quot;Store Item Demand Forecasting Challenge&quot;. In this competition, you are given 5 years of store-item sales data, and asked to predict 3 months of sales for 50 different items in 10 different stores. . To begin, let&#39;s explore the train data for this competition. For the faster performance, you will work with a subset of the train data containing only a single month history. . Your initial goal is to read the input data and take the first look at it. . Instructions . 100 XP . Import pandas as pd. | Read train data using pandas&#39; read_csv() method. | Print the head of the train data (using head() method) to see the data sample. | . import pandas as pd # Read train data train = pd.read_csv(&#39;train.csv&#39;) # Look at the shape of the data print(&#39;Train shape:&#39;, train.shape) # Look at the head() of the data print(train.head()) . Explore test data . Having looked at the train data, let&#39;s explore the test data in the &quot;Store Item Demand Forecasting Challenge&quot;. Remember, that the test dataset generally contains one column less than the train one. . This column, together with the output format, is presented in the sample submission file. Before making any progress in the competition, you should get familiar with the expected output. . That is why, let&#39;s look at the columns of the test dataset and compare it to the train columns. Additionally, let&#39;s explore the format of the sample submission. The train DataFrame is available in your workspace. . Instructions 1/2 . 50 XP . Read the test dataset. | Print the column names of the train and test datasets. | . import pandas as pd # Read the test data test = pd.read_csv(&#39;test.csv&#39;) # Print train and test columns print(&#39;Train columns:&#39;, train.columns.tolist()) print(&#39;Test columns:&#39;, test.columns.tolist()) . Instructions 2/2 . 50 XP . 2) . | Notice that test columns do not have the target &quot;sales&quot; column. Now, read the sample submission file. . | Look at the head of the submission file to get the output format. | . import pandas as pd # Read the test data test = pd.read_csv(&#39;test.csv&#39;) # Print train and test columns print(&#39;Train columns:&#39;, train.columns.tolist()) print(&#39;Test columns:&#39;, test.columns.tolist()) # Read the sample submission file sample_submission = pd.read_csv(&#39;sample_submission.csv&#39;) # Look at the head() of the sample submission print(sample_submission.head()) . The sample submission file consists of two columns: id of the observation and sales column for your predictions. Kaggle will evaluate your predictions on the true sales data for the corresponding id. So, it’s important to keep track of the predictions by id before submitting them. Let’s jump in the next lesson to see how to prepare a submission file! . Prepare your first submission . Determine a problem type . You will keep working on the Store Item Demand Forecasting Challenge. Recall that you are given a history of store-item sales data, and asked to predict 3 months of the future sales. . Before building a model, you should determine the problem type you are addressing. The goal of this exercise is to look at the distribution of the target variable, and select the correct problem type you will be building a model for. . The train DataFrame is already available in your workspace. It has the target variable column called &quot;sales&quot;. Also, matplotlib.pyplot is already imported as plt. . Instructions . 50 XP . Possible Answers . Classification. . | Regression. . | Clustering. . | . That&#39;s correct! The sales variable is continuous, so you&#39;re solving a regression problem. . . plt.plot(train.sales) plt.show() . Train a simple model . As you determined, you are dealing with a regression problem. So, now you&#39;re ready to build a model for a subsequent submission. But now, instead of building the simplest Linear Regression model as in the slides, let&#39;s build an out-of-box Random Forest model. . You will use the RandomForestRegressor class from the scikit-learn library. . Your objective is to train a Random Forest model with default parameters on the &quot;store&quot; and &quot;item&quot; features. . Instructions . 100 XP . Read the train data using pandas. | Create a Random Forest object. | Train the Random Forest model on the &quot;store&quot; and &quot;item&quot; features with &quot;sales&quot; as a target. | . import pandas as pd from sklearn.ensemble import RandomForestRegressor # Read the train data train = pd.read_csv(&#39;train.csv&#39;) # Create a Random Forest object rf = RandomForestRegressor() # Train a model rf.fit(X=train[[&#39;store&#39;, &#39;item&#39;]], y=train[&#39;sales&#39;]) . Congratulations, you&#39;ve built the first simple model. Now it&#39;s time to use it for the test predictions. Go on to the next step! . Prepare a submission . You&#39;ve already built a model on the training data from the Kaggle Store Item Demand Forecasting Challenge. Now, it&#39;s time to make predictions on the test data and create a submission file in the specified format. . Your goal is to read the test data, make predictions, and save these in the format specified in the &quot;sample_submission.csv&quot; file. The rf object you created in the previous exercise is available in your workspace. . Note that starting from now and for the rest of the course, pandas library will be always imported for you and could be accessed as pd. . Instructions 1/2 . 50 XP . Read &quot;test.csv&quot; and &quot;sample_submission.csv&quot; files using pandas. | Look at the head of the sample submission to determine the format. | . test = pd.read_csv(&quot;test.csv&quot;) sample_submission = pd.read_csv(&quot;sample_submission.csv&quot;) # Show the head() of the sample_submission print(sample_submission.head()) . Instructions 2/2 . 50 XP . Note that sample submission has id and sales columns. Now, make predictions on the test data using the rf model, that you fitted on the train data. | Using the format given in the sample submission, write your results to a new file. | . test = pd.read_csv(&#39;test.csv&#39;) sample_submission = pd.read_csv(&#39;sample_submission.csv&#39;) # Show the head() of the sample_submission print(sample_submission.head()) # Get predictions for the test set test[&#39;sales&#39;] = rf.predict(test[[&#39;store&#39;, &#39;item&#39;]]) # Write test predictions using the sample_submission format test[[&#39;id&#39;, &#39;sales&#39;]].to_csv(&#39;kaggle_submission.csv&#39;, index=False) . Congratulations! You&#39;ve prepared your first Kaggle submission. Now, you could upload it to the Kaggle platform and see your score and current position on the Leaderboard. Move forward to learn more about the Leaderboard itself! . Public vs Private leaderboard . What model is overfitting? . Let&#39;s say you&#39;ve trained 4 different models and calculated a metric for both train and validation data sets. For example, the metric is Mean Squared Error (the lower its value the better). Train and validation metrics for all the models are presented in the table below. . Please, select the model that overfits to train data. . Model Train MSE Validation MSE . Model 1 | 2.35 | 2.46 | . Model 2 | 2.20 | 2.15 | . Model 3 | 2.10 | 2.14 | . Model 4 | 1.90 | 2.35 | . Answer the question . 50XP . Possible Answers . Model 1. | . Model 2. | . Model 3. | . Model 4. | . That&#39;s right! Model 4 has considerably lower train MSE compared to other models. However, validation MSE started growing again. . Train XGBoost models . Every Machine Learning method could potentially overfit. You will see it on this example with XGBoost. Again, you are working with the Store Item Demand Forecasting Challenge. The train DataFrame is available in your workspace. . Firstly, let&#39;s train multiple XGBoost models with different sets of hyperparameters using XGBoost&#39;s learning API. The single hyperparameter you will change is: . max_depth - maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. | . Instructions 1/3 . 35 XP . - Set the maximum depth to 2. Then hit Submit Answer button to train the first model. . import xgboost as xgb # Create DMatrix on train data dtrain = xgb.DMatrix(data=train[[&#39;store&#39;, &#39;item&#39;]], label=train[&#39;sales&#39;]) # Define xgboost parameters params = {&#39;objective&#39;: &#39;reg:linear&#39;, &#39;max_depth&#39;: 2, &#39;silent&#39;: 1} # Train xgboost model xg_depth_2 = xgb.train(params=params, dtrain=dtrain) . - Now, set the maximum depth to 8. Then hit Submit Answer button to train the second model. . import xgboost as xgb # Create DMatrix on train data dtrain = xgb.DMatrix(data=train[[&#39;store&#39;, &#39;item&#39;]], label=train[&#39;sales&#39;]) # Define xgboost parameters params = {&#39;objective&#39;: &#39;reg:linear&#39;, &#39;max_depth&#39;: 8, &#39;silent&#39;: 1} # Train xgboost model xg_depth_8 = xgb.train(params=params, dtrain=dtrain) . - Finally, set the maximum depth to 15. Then hit Submit Answer button to train the third model. . import xgboost as xgb # Create DMatrix on train data dtrain = xgb.DMatrix(data=train[[&#39;store&#39;, &#39;item&#39;]], label=train[&#39;sales&#39;]) # Define xgboost parameters params = {&#39;objective&#39;: &#39;reg:linear&#39;, &#39;max_depth&#39;: 15, &#39;silent&#39;: 1} # Train xgboost model xg_depth_15 = xgb.train(params=params, dtrain=dtrain) . All right, now you have 3 different XGBoost models trained. Let&#39;s explore them further! . Explore overfitting XGBoost . Having trained 3 XGBoost models with different maximum depths, you will now evaluate their quality. For this purpose, you will measure the quality of each model on both the train data and the test data. As you know by now, the train data is the data models have been trained on. The test data is the next month sales data that models have never seen before. . The goal of this exercise is to determine whether any of the models trained is overfitting. To measure the quality of the models you will use Mean Squared Error (MSE). It&#39;s available in sklearn.metrics as mean_squared_error() function that takes two arguments: true values and predicted values. . train and test DataFrames together with 3 models trained (xg_depth_2, xg_depth_8, xg_depth_15) are available in your workspace. . Instructions . 100 XP . Make predictions for each model on both the train and test data. | Calculate the MSE between the true values and your predictions for both the train and test data. | . from sklearn.metrics import mean_squared_error dtrain = xgb.DMatrix(data=train[[&#39;store&#39;, &#39;item&#39;]]) dtest = xgb.DMatrix(data=test[[&#39;store&#39;, &#39;item&#39;]]) # For each of 3 trained models for model in [xg_depth_2, xg_depth_8, xg_depth_15]: # Make predictions train_pred = model.predict(dtrain) test_pred = model.predict(dtest) # Calculate metrics mse_train = mean_squared_error(train[&#39;sales&#39;], train_pred) mse_test = mean_squared_error(test[&#39;sales&#39;], test_pred) print(&#39;MSE Train: {:.3f}. MSE Test: {:.3f}&#39;.format(mse_train, mse_test)) . So, you see that the third model with depth 15 is already overfitting. It has considerably lower train error compared to the second model, however test error is higher. Be aware of overfitting and move on to the next chapter to know how to beat it! . 2. Dive into the Competition . Now that you know the basics of Kaggle competitions, you will learn how to study the specific problem at hand. You will practice EDA and get to establish correct local validation strategies. You will also learn about data leakage. . Understand the problem . Understand the problem type . As you&#39;ve just seen, the first step of the solution workflow is to skim through the problem statement. Your goal now is to determine data types available as well as the problem type for the Avito Demand Prediction Challenge. The evaluation metric in this competition is the Root Mean Squared Error. The problem definition is presented below. . In this Kaggle competition, Avito is challenging you to predict demand for an online advertisement based on its full description (price, title, images, etc.), its context (geo position, similar ads already posted) and historical demand for similar ads in the past. . . What problem type are you facing, and what data do you have at your disposal? . Answer the question . 50XP . Possible Answers . This is a regression problem with tabular, time series, image and text data. . | This is a regression problem with tabular, text and image data. . | This is a classification problem with tabular, time series, image and text data. . | This is a clustering problem with tabular, text and image data. . | . That&#39;s correct! This competition contains a mix of various structured and unstructured data. . Define a competition metric . Competition metric is used by Kaggle to evaluate your submissions. Moreover, you also need to measure the performance of different models on a local validation set. . For now, your goal is to manually develop a couple of competition metrics in case if they are not available in sklearn.metrics. . In particular, you will define: . Mean Squared Error (MSE) for the regression problem: $$ MSE= frac{1}{N} ∑ limits_{i=1}^{N}{(y_i− hat{y}_i)^2}$$ . | Logarithmic Loss (LogLoss) for the binary classification problem: $$LogLoss=− frac{1}{N}∑ limits_{i=1}^{N} {( y_i ln⁡ (p_i) +(1−y_i) ln⁡(1−p_i))}$$ . | . Instructions 1/2 . 50 XP . - Using `numpy`, define MSE metric. As a function input, you&#39;re given true `y_true` and predicted `y_pred` arrays. . import numpy as np # Import MSE from sklearn from sklearn.metrics import mean_squared_error # Define your own MSE function def own_mse(y_true, y_pred): # Raise differences to the power of 2 squares = np.power(y_true - y_pred, 2) # Find mean over all observations err = np.mean(squares) return err print(&#39;Sklearn MSE: {:.5f}. &#39;.format(mean_squared_error(y_regression_true, y_regression_pred))) print(&#39;Your MSE: {:.5f}. &#39;.format(own_mse(y_regression_true, y_regression_pred))) . - Using `numpy`, define LogLoss metric. As input, you&#39;re given true class `y_true` and probability predicted `prob_pred`. . import numpy as np # Import log_loss from sklearn from sklearn.metrics import log_loss # Define your own LogLoss function def own_logloss(y_true, prob_pred): # Find loss for each observation terms = y_true * np.log(prob_pred) + (1 - y_true) * np.log(1 - prob_pred) # Find mean over all observations err = np.mean(terms) return -err print(&#39;Sklearn LogLoss: {:.5f}&#39;.format(log_loss(y_classification_true, y_classification_pred))) print(&#39;Your LogLoss: {:.5f}&#39;.format(own_logloss(y_classification_true, y_classification_pred))) . Great! You see that your functions work the same way that built-in sklearn metrics. Knowing the problem type and evaluation metric, it&#39;s time to start Data Analysis. Let&#39;s move on to the next lesson on EDA! . Initial EDA . EDA statistics . As mentioned in the slides, you&#39;ll work with New York City taxi fare prediction data. You&#39;ll start with finding some basic statistics about the data. Then you&#39;ll move forward to plot some dependencies and generate hypotheses on them. . The train and test DataFrames are already available in your workspace. . Instructions 1/2 . 50 XP . Find the shapes of the train and test data. | Look at the head of the train data. | . print(&#39;Train shape:&#39;, train.shape) print(&#39;Test shape:&#39;, test.shape) # Train head() print(train.head()) . Instructions 2/2 . 50 XP . Describe the &quot;fare_amount&quot; column to get some statistics about the target variable. | Find the distribution of the &quot;passenger_count&quot; in the train data (using the value_counts() method). | . print(&#39;Train shape:&#39;, train.shape) print(&#39;Test shape:&#39;, test.shape) # Train head() print(train.head()) # Describe the target variable print(train.fare_amount.describe()) # Train distribution of passengers within rides print(train.passenger_count.value_counts()) . All right! You just obtained a couple of descriptive statistics about the data. You can look at them to understand the data structure. However, they are not informative enough to get ideas for the future solution. Let&#39;s get down to more practical EDA! . EDA plots I . After generating a couple of basic statistics, it&#39;s time to come up with and validate some ideas about the data dependencies. Again, the train DataFrame from the taxi competition is already available in your workspace. . To begin with, let&#39;s make a scatterplot plotting the relationship between the fare amount and the distance of the ride. Intuitively, the longer the ride, the higher its price. . To get the distance in kilometers between two geo-coordinates, you will use Haversine distance. Its calculation is available with the haversine_distance() function defined for you. The function expects train DataFrame as input. . Instructions . 100 XP . Create a new variable &quot;distance_km&quot; as Haversine distance between pickup and dropoff points. | Plot a scatterplot with &quot;fare_amount&quot; on the x axis and &quot;distance_km&quot; on the y axis. To draw a scatterplot use matplotlib scatter() method. | Set a limit on a ride distance to be between 0 and 50 kilometers to avoid plotting outliers. | . train[&#39;distance_km&#39;] = haversine_distance(train) # Draw a scatterplot plt.scatter(x=train[&#39;fare_amount&#39;], y=train[&#39;distance_km&#39;], alpha=0.5) plt.xlabel(&#39;Fare amount&#39;) plt.ylabel(&#39;Distance, km&#39;) plt.title(&#39;Fare amount based on the distance&#39;) # Limit on the distance plt.ylim(0, 50) plt.show() . Nice plot! It&#39;s obvious now that there is a clear dependency between ride distance and fare amount. So, ride distance is, probably, a good feature. Let&#39;s find some others! . . EDA plots II . Another idea that comes to mind is that the price of a ride could change during the day. . Your goal is to plot the median fare amount for each hour of the day as a simple line plot. The hour feature is calculated for you. Don&#39;t worry if you do not know how to work with the date features. We will explore them in the chapter on Feature Engineering. . Instructions . 100 XP . Group train DataFrame by &quot;hour&quot; and calculate the median for the &quot;fare_amount&quot; column. | Using hour_price DataFrame obtained, plot a line with &quot;hour&quot; on the x axis and &quot;fare_amount&quot; on the y axis. | . train[&#39;pickup_datetime&#39;] = pd.to_datetime(train.pickup_datetime) train[&#39;hour&#39;] = train.pickup_datetime.dt.hour # Find median fare_amount for each hour hour_price = train.groupby(&#39;hour&#39;, as_index=False)[&#39;fare_amount&#39;].median() # Plot the line plot plt.plot(hour_price[&#39;hour&#39;], hour_price[&#39;fare_amount&#39;], marker=&#39;o&#39;) plt.xlabel(&#39;Hour of the day&#39;) plt.ylabel(&#39;Median fare amount&#39;) plt.title(&#39;Fare amount based on day time&#39;) plt.xticks(range(24)) plt.show() . Great! We see that prices are a bit higher during the night. It is a good indicator that we should include the &quot;hour&quot; feature in the final model, or at least add a binary feature &quot;is_night&quot;. Move on to the next lesson to learn how to check whether new features are useful for the model or not! . . Local validation . K-fold cross-validation . You will start by getting hands-on experience in the most commonly used K-fold cross-validation. . The data you&#39;ll be working with is from the &quot;Two sigma connect: rental listing inquiries&quot; Kaggle competition. The competition problem is a multi-class classification of the rental listings into 3 classes: low interest, medium interest and high interest. For faster performance, you will work with a subsample consisting of 1,000 observations. . You need to implement a K-fold validation strategy and look at the sizes of each fold obtained. train DataFrame is already available in your workspace. . Instructions . 100 XP . Create a KFold object with 3 folds. | Loop over each split using the kf object. | For each split select training and testing folds using train_index and test_index. | . from sklearn.model_selection import KFold # Create a KFold object kf = KFold(n_splits=3, shuffle=True, random_state=123) # Loop through each split fold = 0 for train_index, test_index in kf.split(train): # Obtain training and testing folds cv_train, cv_test = train.iloc[train_index], train.iloc[test_index] print(&#39;Fold: {}&#39;.format(fold)) print(&#39;CV train shape: {}&#39;.format(cv_train.shape)) print(&#39;Medium interest listings in CV train: {} n&#39;.format(sum(cv_train.interest_level == &#39;medium&#39;))) fold += 1 . So, we see that the number of observations in each fold is almost uniform. It means that we&#39;ve just splitted the train data into 3 equal folds. However, if we look at the number of medium-interest listings, it&#39;s varying from 162 to 175 from one fold to another. To make them uniform among the folds, let&#39;s use Stratified K-fold! . Stratified K-fold . As you&#39;ve just noticed, you have a pretty different target variable distribution among the folds due to the random splits. It&#39;s not crucial for this particular competition, but could be an issue for the classification competitions with the highly imbalanced target variable. . To overcome this, let&#39;s implement the stratified K-fold strategy with the stratification on the target variable. train DataFrame is already available in your workspace. . Instructions . 100 XP . Create a StratifiedKFold object with 3 folds and shuffling. | Loop over each split using str_kf object. Stratification is based on the &quot;interest_level&quot; column. | For each split select training and testing folds using train_index and test_index. | . from sklearn.model_selection import StratifiedKFold # Create a StratifiedKFold object str_kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=123) # Loop through each split fold = 0 for train_index, test_index in str_kf.split(train, train[&#39;interest_level&#39;]): # Obtain training and testing folds cv_train, cv_test = train.iloc[train_index], train.iloc[test_index] print(&#39;Fold: {}&#39;.format(fold)) print(&#39;CV train shape: {}&#39;.format(cv_train.shape)) print(&#39;Medium interest listings in CV train: {} n&#39;.format(sum(cv_train.interest_level == &#39;medium&#39;))) fold += 1 . Great! Now you see that both size and target distribution are the same among the folds. The general rule is to prefer Stratified K-Fold over usual K-Fold in any classification problem. Move to the next lesson to learn about other cross-validation strategies! . Validation usage . Time K-fold . Remember the &quot;Store Item Demand Forecasting Challenge&quot; where you are given store-item sales data, and have to predict future sales? . It&#39;s a competition with time series data. So, time K-fold cross-validation should be applied. Your goal is to create this cross-validation strategy and make sure that it works as expected. . Note that the train DataFrame is already available in your workspace, and that TimeSeriesSplit has been imported from sklearn.model_selection. . Instructions . 100 XP . Create a TimeSeriesSplit object with 3 splits. | Sort the train data by &quot;date&quot; column to apply time K-fold. | Loop over each time split using time_kfold object. | For each split select training and testing folds using train_index and test_index. | . time_kfold = TimeSeriesSplit(n_splits=3) # Sort train data by date train = train.sort_values(&#39;date&#39;) # Iterate through each split fold = 0 for train_index, test_index in time_kfold.split(train): cv_train, cv_test = train.iloc[train_index], train.iloc[test_index] print(&#39;Fold :&#39;, fold) print(&#39;Train date range: from {} to {}&#39;.format(cv_train.date.min(), cv_train.date.max())) print(&#39;Test date range: from {} to {} n&#39;.format(cv_test.date.min(), cv_test.date.max())) fold += 1 . Great! You&#39;ve applied time K-fold cross-validation strategy for the demand forecasting. Look at the output. It works as expected, training only on the past data and predicting the future. Progress to the next exercise to evaluate different models! . Overall validation score . Now it&#39;s time to get the actual model performance using cross-validation! How does our store item demand prediction model perform? . Your task is to take the Mean Squared Error (MSE) for each fold separately, and then combine these results into a single number. . For simplicity, you&#39;re given get_fold_mse() function that for each cross-validation split fits a Random Forest model and returns a list of MSE scores by fold. get_fold_mse() accepts two arguments: train and TimeSeriesSplit object. . Instructions 1/3 . 35 XP . Create time 3-fold cross-validation. | Print the numpy mean of MSE scores by folds. | . from sklearn.model_selection import TimeSeriesSplit import numpy as np # Sort train data by date train = train.sort_values(&#39;date&#39;) # Initialize 3-fold time cross-validation kf = TimeSeriesSplit(n_splits=3) # Get MSE scores for each cross-validation split mse_scores = get_fold_mse(train, kf) print(&#39;Mean validation MSE: {:.5f}&#39;.format(np.mean(mse_scores))) . Instructions 2/3 . 35 XP . Print the list of MSEs by fold. | . from sklearn.model_selection import TimeSeriesSplit import numpy as np # Sort train data by date train = train.sort_values(&#39;date&#39;) # Initialize 3-fold time cross-validation kf = TimeSeriesSplit(n_splits=3) # Get MSE scores for each cross-validation split mse_scores = get_fold_mse(train, kf) print(&#39;Mean validation MSE: {:.5f}&#39;.format(np.mean(mse_scores))) print(&#39;MSE by fold: {}&#39;.format(mse_scores)) . Instructions 3/3 . 30 XP . To calculate the overall score, find the sum of MSE mean and standard deviation. | . from sklearn.model_selection import TimeSeriesSplit import numpy as np # Sort train data by date train = train.sort_values(&#39;date&#39;) # Initialize 3-fold time cross-validation kf = TimeSeriesSplit(n_splits=3) # Get MSE scores for each cross-validation split mse_scores = get_fold_mse(train, kf) print(&#39;Mean validation MSE: {:.5f}&#39;.format(np.mean(mse_scores))) print(&#39;MSE by fold: {}&#39;.format(mse_scores)) print(&#39;Overall validation MSE: {:.5f}&#39;.format(np.mean(mse_scores) + np.std(mse_scores))) . Congratulations, you&#39;ve mastered it! Now, you know different validation strategies as well as how to use them to obtain overall model performance. It&#39;s time for the next and the most interesting part of the solution process: Feature Engineering and Modeling. See you in the next Chapters! . 3. Feature Engineering . You will now get exposure to different types of features. You will modify existing features and create new ones. Also, you will treat the missing data accordingly. . Feature engineering . Arithmetical features . To practice creating new features, you will be working with a subsample from the Kaggle competition called &quot;House Prices: Advanced Regression Techniques&quot;. The goal of this competition is to predict the price of the house based on its properties. It&#39;s a regression problem with Root Mean Squared Error as an evaluation metric. . Your goal is to create new features and determine whether they improve your validation score. To get the validation score from 5-fold cross-validation, you&#39;re given the get_kfold_rmse() function. Use it with the train DataFrame, available in your workspace, as an argument. . Instructions 1/3 . 50 XP . Create a new feature representing the total area (basement, 1st and 2nd floors) of the house. The columns &quot;TotalBsmtSF&quot;, &quot;FirstFlrSF&quot; and &quot;SecondFlrSF&quot; give the areas of the basement, 1st and 2nd floors, respectively. | . print(&#39;RMSE before feature engineering:&#39;, get_kfold_rmse(train)) # Find the total area of the house train[&#39;TotalArea&#39;] = train[&#39;TotalBsmtSF&#39;] + train[&#39;FirstFlrSF&#39;] + train[&#39;SecondFlrSF&#39;] # Look at the updated RMSE print(&#39;RMSE with total area:&#39;, get_kfold_rmse(train)) . Instructions 2/3 . 50 XP . Create a new feature representing the area of the garden. It is a difference between the total area of the property (&quot;LotArea&quot;) and the first floor area (&quot;FirstFlrSF&quot;). | . print(&#39;RMSE before feature engineering:&#39;, get_kfold_rmse(train)) # Find the total area of the house train[&#39;TotalArea&#39;] = train[&#39;TotalBsmtSF&#39;] + train[&#39;FirstFlrSF&#39;] + train[&#39;SecondFlrSF&#39;] print(&#39;RMSE with total area:&#39;, get_kfold_rmse(train)) # Find the area of the garden train[&#39;GardenArea&#39;] = train[&#39;LotArea&#39;] - train[&#39;FirstFlrSF&#39;] print(&#39;RMSE with garden area:&#39;, get_kfold_rmse(train)) . Instructions 3/3 . 0 XP . Create a new feature representing the total number of bathrooms in the house. It is a sum of full bathrooms (&quot;FullBath&quot;) and half bathrooms (&quot;HalfBath&quot;). | . print(&#39;RMSE before feature engineering:&#39;, get_kfold_rmse(train)) # Find the total area of the house train[&#39;TotalArea&#39;] = train[&#39;TotalBsmtSF&#39;] + train[&#39;FirstFlrSF&#39;] + train[&#39;SecondFlrSF&#39;] print(&#39;RMSE with total area:&#39;, get_kfold_rmse(train)) # Find the area of the garden train[&#39;GardenArea&#39;] = train[&#39;LotArea&#39;] - train[&#39;FirstFlrSF&#39;] print(&#39;RMSE with garden area:&#39;, get_kfold_rmse(train)) # Find total number of bathrooms train[&#39;TotalBath&#39;] = train[&#39;FullBath&#39;] + train[&#39;HalfBath&#39;] print(&#39;RMSE with number of bathrooms:&#39;, get_kfold_rmse(train)) . Nice! You&#39;ve created three new features. Here you see that house area improved the RMSE by almost $1,000. Adding garden area improved the RMSE by another $600. However, with the total number of bathrooms, the RMSE has increased. It means that you keep the new area features, but do not add &quot;TotalBath&quot; as a new feature. Let&#39;s now work with the datetime features! . Date features . You&#39;ve built some basic features using numerical variables. Now, it&#39;s time to create features based on date and time. You will practice on a subsample from the Taxi Fare Prediction Kaggle competition data. The data represents information about the taxi rides and the goal is to predict the price for each ride. . Your objective is to generate date features from the pickup datetime. Recall that it&#39;s better to create new features for train and test data simultaneously. After the features are created, split the data back into the train and test DataFrames. Here it&#39;s done using pandas&#39; isin() method. . The train and test DataFrames are already available in your workspace. . Instructions . 100 XP . Concatenate the train and test DataFrames into a single DataFrame taxi. | Convert the &quot;pickup_datetime&quot; column to a datetime object. | Create the day of week (using .dayofweek attribute) and hour (using .hour attribute) features from the &quot;pickup_datetime&quot; column. | . taxi = pd.concat([train, test]) # Convert pickup date to datetime object taxi[&#39;pickup_datetime&#39;] = pd.to_datetime(taxi[&#39;pickup_datetime&#39;]) # Create a day of week feature taxi[&#39;dayofweek&#39;] = taxi[&#39;pickup_datetime&#39;].dt.dayofweek # Create an hour feature taxi[&#39;hour&#39;] = taxi[&#39;pickup_datetime&#39;].dt.hour # Split back into train and test new_train = taxi[taxi[&#39;id&#39;].isin(train[&#39;id&#39;])] new_test = taxi[taxi[&#39;id&#39;].isin(test[&#39;id&#39;])] . Great! Now you know how to perform feature engineering for train and test DataFrames simultaneously. Having considered numerical and datetime features, move forward to master feature engineering for categorical ones! . Categorical features . Label encoding . Let&#39;s work on categorical variables encoding. You will again work with a subsample from the House Prices Kaggle competition. . Your objective is to encode categorical features &quot;RoofStyle&quot; and &quot;CentralAir&quot; using label encoding. The train and test DataFrames are already available in your workspace. . Instructions . 100 XP . Concatenate train and test DataFrames into a single DataFrame houses. | Create a LabelEncoder object without arguments and assign it to le. | Create new label-encoded features for &quot;RoofStyle&quot; and &quot;CentralAir&quot; using the same le object. | . houses = pd.concat([train, test]) # Label encoder from sklearn.preprocessing import LabelEncoder le = LabelEncoder() # Create new features houses[&#39;RoofStyle_enc&#39;] = le.fit_transform(houses[&#39;RoofStyle&#39;]) houses[&#39;CentralAir_enc&#39;] = le.fit_transform(houses[&#39;CentralAir&#39;]) # Look at new features print(houses[[&#39;RoofStyle&#39;, &#39;RoofStyle_enc&#39;, &#39;CentralAir&#39;, &#39;CentralAir_enc&#39;]].head()) . All right! You can see that categorical variables have been label encoded. However, as you already know, label encoder is not always a good choice for categorical variables. Let&#39;s go further and apply One-Hot encoding. . One-Hot encoding . The problem with label encoding is that it implicitly assumes that there is a ranking dependency between the categories. So, let&#39;s change the encoding method for the features &quot;RoofStyle&quot; and &quot;CentralAir&quot; to one-hot encoding. Again, the train and test DataFrames from House Prices Kaggle competition are already available in your workspace. . Recall that if you&#39;re dealing with binary features (categorical features with only two categories) it is suggested to apply label encoder only. . Your goal is to determine which of the mentioned features is not binary, and to apply one-hot encoding only to this one. . Instructions 1/4 . 35 XP . Determine the distribution of &quot;RoofStyle&quot; and &quot;CentralAir&quot; features using pandas&#39; value_counts() method. | . houses = pd.concat([train, test]) # Look at feature distributions print(houses[&#39;RoofStyle&#39;].value_counts(), &#39; n&#39;) print(houses[&#39;CentralAir&#39;].value_counts()) . Instructions 2/4 . 0 XP . Question . Which of the features is binary? | . Possible Answers . &quot;RoofStyle&quot;. . | &quot;CentralAir&quot;. . | . Instructions 3/4 . 35 XP . As long as &quot;CentralAir&quot; is a binary feature, encode it with a label encoder (0 - for one class and 1 - for another class). | . houses = pd.concat([train, test]) # Label encode binary &#39;CentralAir&#39; feature from sklearn.preprocessing import LabelEncoder le = LabelEncoder() houses[&#39;CentralAir_enc&#39;] = le.fit_transform(houses[&#39;CentralAir&#39;]) . Instructions 4/4 . 30 XP . For the categorical feature &quot;RoofStyle&quot; let&#39;s use the one-hot encoder. Firstly, create one-hot encoded features using the get_dummies() method. Then they are concatenated to the initial houses DataFrame. | . houses = pd.concat([train, test]) # Label encode binary &#39;CentralAir&#39; feature from sklearn.preprocessing import LabelEncoder le = LabelEncoder() houses[&#39;CentralAir_enc&#39;] = le.fit_transform(houses[&#39;CentralAir&#39;]) # Create One-Hot encoded features ohe = pd.get_dummies(houses[&#39;RoofStyle&#39;], prefix=&#39;RoofStyle&#39;) # Concatenate OHE features to houses houses = pd.concat([houses, ohe], axis=1) # Look at OHE features print(houses[[col for col in houses.columns if &#39;RoofStyle&#39; in col]].head(3)) . Congratulations! Now you&#39;ve mastered one-hot encoding as well! The one-hot encoded features look as expected. Remember to drop the initial string column, because models will not handle it automatically. OK, we&#39;re done with simple categorical encoders. Let&#39;s move to the target encoder! . Target encoding . Mean target encoding . First of all, you will create a function that implements mean target encoding. Remember that you need to develop the two following steps: . Calculate the mean on the train, apply to the test | Split train into K folds. Calculate the out-of-fold mean for each fold, apply to this particular fold | Each of these steps will be implemented in a separate function: test_mean_target_encoding() and train_mean_target_encoding(), respectively. . The final function mean_target_encoding() takes as arguments: the train and test DataFrames, the name of the categorical column to be encoded, the name of the target column and a smoothing parameter alpha. It returns two values: a new feature for train and test DataFrames, respectively. . Instructions 1/3 . 35 XP . You need to add smoothing to avoid overfitting. So, add $α$ parameter to the denominator in train_statistics calculations. | You need to treat new categories in the test data. So, pass a global mean as an argument to the fillna() method. | . def test_mean_target_encoding(train, test, target, categorical, alpha=5): # Calculate global mean on the train data global_mean = train[target].mean() # Group by the categorical feature and calculate its properties train_groups = train.groupby(categorical) category_sum = train_groups[target].sum() category_size = train_groups.size() # Calculate smoothed mean target statistics train_statistics = (category_sum + global_mean * alpha) / (category_size + alpha) # Apply statistics to the test data and fill new categories test_feature = test[categorical].map(train_statistics).fillna(global_mean) return test_feature.values . Instructions 2/3 . 35 XP . To calculate the train mean encoded feature you need to use out-of-fold statistics, splitting train into several folds. Specify the train and test indices for each validation split to access it. | . def train_mean_target_encoding(train, target, categorical, alpha=5): # Create 5-fold cross-validation kf = KFold(n_splits=5, random_state=123, shuffle=True) train_feature = pd.Series(index=train.index) # For each folds split for train_index, test_index in kf.split(train): cv_train, cv_test = train.iloc[train_index], train.iloc[test_index] # Calculate out-of-fold statistics and apply to cv_test cv_test_feature = test_mean_target_encoding(cv_train, cv_test, target, categorical, alpha) # Save new feature for this particular fold train_feature.iloc[test_index] = cv_test_feature return train_feature.values . Instructions 3/3 . 30 XP . Finally, you just calculate train and test target mean encoded features and return them from the function. So, return train_feature and test_feature obtained. | . def mean_target_encoding(train, test, target, categorical, alpha=5): # Get the train feature train_feature = train_mean_target_encoding(train, target, categorical, alpha) # Get the test feature test_feature = test_mean_target_encoding(train, test, target, categorical, alpha) # Return new features to add to the model return train_feature, test_feature . Great! Now you are equipped with a function that performs mean target encoding of any categorical feature. Move on to learn how to implement mean target encoding for the K-fold cross-validation using the mean_target_encoding() function you&#39;ve just built! . K-fold cross-validation . You will work with a binary classification problem on a subsample from Kaggle playground competition. The objective of this competition is to predict whether a famous basketball player Kobe Bryant scored a basket or missed a particular shot. . Train data is available in your workspace as bryant_shots DataFrame. It contains data on 10,000 shots with its properties and a target variable &quot;shot _made _flag&quot; -- whether shot was scored or not. . One of the features in the data is &quot;game_id&quot; -- a particular game where the shot was made. There are 541 distinct games. So, you deal with a high-cardinality categorical feature. Let&#39;s encode it using a target mean! . Suppose you&#39;re using 5-fold cross-validation and want to evaluate a mean target encoded feature on the local validation. . Instructions . 100 XP . To achieve this, you need to repeat encoding procedure for the &quot;game_id&quot; categorical feature inside each folds split separately. Your goal is to specify all the missing parameters for the mean_target_encoding() function call inside each folds split. | Recall that the train and test parameters expect the train and test DataFrames. | While the target and categorical parameters expect names of the target variable and categorical feature to be encoded. | . kf = KFold(n_splits=5, random_state=123, shuffle=True) # For each folds split for train_index, test_index in kf.split(bryant_shots): cv_train, cv_test = bryant_shots.iloc[train_index], bryant_shots.iloc[test_index] # Create mean target encoded feature cv_train[&#39;game_id_enc&#39;], cv_test[&#39;game_id_enc&#39;] = mean_target_encoding(train=cv_train, test=cv_test, target=&#39;shot_made_flag&#39;, categorical=&#39;game_id&#39;, alpha=5) # Look at the encoding print(cv_train[[&#39;game_id&#39;, &#39;shot_made_flag&#39;, &#39;game_id_enc&#39;]].sample(n=1)) . Nice! You could see different game encodings for each validation split in the output. The main conclusion you should make: while using local cross-validation, you need to repeat mean target encoding procedure inside each folds split separately. Go on to try other problem types beyond binary classification! . Beyond binary classification . Of course, binary classification is just a single special case. Target encoding could be applied to any target variable type: . For binary classification usually mean target encoding is used | For regression mean could be changed to median, quartiles, etc. | For multi-class classification with N classes we create N features with target mean for each category in one vs. all fashion | . The mean_target_encoding() function you&#39;ve created could be used for any target type specified above. Let&#39;s apply it for the regression problem on the example of House Prices Kaggle competition. . Your goal is to encode a categorical feature &quot;RoofStyle&quot; using mean target encoding. The train and test DataFrames are already available in your workspace. . Instructions . 100 XP . Specify all the missing parameters for the mean_target_encoding() function call. Target variable name is &quot;SalePrice&quot;. Set $α$ hyperparameter to 10. | Recall that the train and test parameters expect the train and test DataFrames. | While the target and categorical parameters expect names of the target variable and feature to be encoded. | . train[&#39;RoofStyle_enc&#39;], test[&#39;RoofStyle_enc&#39;] = mean_target_encoding(train=train, test=test, target=&#39;SalePrice&#39;, categorical=&#39;RoofStyle&#39;, alpha=10) # Look at the encoding print(test[[&#39;RoofStyle&#39;, &#39;RoofStyle_enc&#39;]].drop_duplicates()) . So, you observe that houses with the Hip roof are the most pricy, while houses with the Gambrel roof are the cheapest. It&#39;s exactly the goal of target encoding: you&#39;ve encoded categorical feature in such a manner that there is now a correlation between category values and target variable. We&#39;re done with categorical encoders. Not it&#39;s time to talk about the missing data! . Missing data . Find missing data . Let&#39;s impute missing data on a real Kaggle dataset. For this purpose, you will be using a data subsample from the Kaggle &quot;Two sigma connect: rental listing inquiries&quot; competition. . Before proceeding with any imputing you need to know the number of missing values for each of the features. Moreover, if the feature has missing values, you should explore the type of this feature. . Instructions 1/2 . 50 XP . Read the &quot;twosigma_train.csv&quot; file using pandas. | Find the number of missing values in each column. | . twosigma = pd.read_csv(&#39;twosigma_train.csv&#39;) # Find the number of missing values in each column print(twosigma.isnull().sum()) . Instructions 2/2 . 50 XP . Select the columns with the missing values and look at the head of the DataFrame. | . twosigma = pd.read_csv(&#39;twosigma_train.csv&#39;) # Find the number of missing values in each column print(twosigma.isnull().sum()) # Look at the columns with the missing values print(twosigma[[&#39;building_id&#39;, &#39;price&#39;]].head()) . All right, you&#39;ve found out that &#39;building_id&#39; and &#39;price&#39; columns have missing values. Looking at the head of the DataFrame, we may conclude that &#39;price&#39; is a numerical feature, while &#39;building_id&#39; is a categorical feature that is encoding buildings as hashes. . Impute missing data . You&#39;ve found that &quot;price&quot; and &quot;building_id&quot; columns have missing values in the Rental Listing Inquiries dataset. So, before passing the data to the models you need to impute these values. . Numerical feature &quot;price&quot; will be encoded with a mean value of non-missing prices. . Imputing categorical feature &quot;building_id&quot; with the most frequent category is a bad idea, because it would mean that all the apartments with a missing &quot;building_id&quot; are located in the most popular building. The better idea is to impute it with a new category. . The DataFrame rental_listings with competition data is read for you. . Instructions 1/2 . 50 XP . - Create a SimpleImputer object with &quot;mean&quot; strategy. - Impute missing prices with the mean value. . from sklearn.impute import SimpleImputer # Create mean imputer mean_imputer = SimpleImputer(strategy=&#39;mean&#39;) # Price imputation rental_listings[[&#39;price&#39;]] = mean_imputer.fit_transform(rental_listings[[&#39;price&#39;]]) . Instructions 2/2 . Create an imputer with &quot;constant&quot; strategy. Use &quot;MISSING&quot; as fill_value. | Impute missing buildings with a constant value. | . from sklearn.impute import SimpleImputer # Create constant imputer constant_imputer = SimpleImputer(strategy=&#39;constant&#39;, fill_value=&#39;MISSING&#39;) # building_id imputation rental_listings[[&#39;building_id&#39;]] = constant_imputer.fit_transform(rental_listings[[&#39;building_id&#39;]]) . Nice! Now our data is ready to be passed to any Machine Learning model. Move on to the next chapter to build and improve your models! . 4. Modeling . Time to bring everything together and build some models! In this last chapter, you will build a base model before tuning some hyperparameters and improving your results with ensembles. You will then get some final tips and tricks to help you compete more efficiently. . Baseline model . Replicate validation score . You&#39;ve seen both validation and Public Leaderboard scores in the video. However, the code examples are available only for the test data. To get the validation scores you have to repeat the same process on the holdout set. . Throughout this chapter, you will work with New York City Taxi competition data. The problem is to predict the fare amount for a taxi ride in New York City. The competition metric is the root mean squared error. . The first goal is to evaluate the Baseline model on the validation data. You will replicate the simplest Baseline based on the mean of &quot;fare_amount&quot;. Recall that as a validation strategy we used a 30% holdout split with validation_train as train and validation_test as holdout DataFrames. Both of them are available in your workspace. . Instructions . 100 XP . Calculate the mean of &quot;fare_amount&quot; over the whole validation_train DataFrame. | Assign this naive prediction value to all the holdout predictions. Store them in the &quot;pred&quot; column. | . import numpy as np from sklearn.metrics import mean_squared_error from math import sqrt # Calculate the mean fare_amount on the validation_train data naive_prediction = np.mean(validation_train[&#39;fare_amount&#39;]) # Assign naive prediction to all the holdout observations validation_test[&#39;pred&#39;] = naive_prediction # Measure the local RMSE rmse = sqrt(mean_squared_error(validation_test[&#39;fare_amount&#39;], validation_test[&#39;pred&#39;])) print(&#39;Validation RMSE for Baseline I model: {:.3f}&#39;.format(rmse)) . It&#39;s exactly the same number you&#39;ve seen in the slides, well done! So, to avoid overfitting you should fully replicate your models using the validation data. Go forward to create a couple of other baselines! . Baseline based on the date . We&#39;ve already built 3 different baseline models. To get more practice, let&#39;s build a couple more. The first model is based on the grouping variables. It&#39;s clear that the ride fare could depend on the part of the day. For example, prices could be higher during the rush hours. . Your goal is to build a baseline model that will assign the average &quot;fare_amount&quot; for the corresponding hour. For now, you will create the model for the whole train data and make predictions for the test dataset. . The train and test DataFrames are available in your workspace. Moreover, the &quot;pickup_datetime&quot; column in both DataFrames is already converted to a datetime object for you. . Instructions . 100 XP . Get the hour from the &quot;pickup_datetime&quot; column for the train and test DataFrames. | Calculate the mean &quot;fare_amount&quot; for each hour on the train data. | Make test predictions using pandas&#39; map() method and the grouping obtained. | Write predictions to the file. | . train[&#39;hour&#39;] = train[&#39;pickup_datetime&#39;].dt.hour test[&#39;hour&#39;] = test[&#39;pickup_datetime&#39;].dt.hour # Calculate average fare_amount grouped by pickup hour hour_groups = train.groupby(&#39;hour&#39;)[&#39;fare_amount&#39;].mean() # Make predictions on the test set test[&#39;fare_amount&#39;] = test.hour.map(hour_groups) # Write predictions test[[&#39;id&#39;,&#39;fare_amount&#39;]].to_csv(&#39;hour_mean_sub.csv&#39;, index=False) . Great! Such baseline achieves 1409th place on the Public Leaderboard which is slightly better than grouping by the number of passengers. Also, remember to replicate all the results for the validation set as it was done in the previous exercise. . Baseline based on the gradient boosting . Let&#39;s build a final baseline based on the Random Forest. You&#39;ve seen a huge score improvement moving from the grouping baseline to the Gradient Boosting in the video. Now, you will use sklearn&#39;s Random Forest to further improve this score. . The goal of this exercise is to take numeric features and train a Random Forest model without any tuning. After that, you could make test predictions and validate the result on the Public Leaderboard. Note that you&#39;ve already got an &quot;hour&quot; feature which could also be used as an input to the model. . Instructions . 100 XP . Add the &quot;hour&quot; feature to the list of numeric features. | Fit the RandomForestRegressor on the train data with numeric features and &quot;fare_amount&quot; as a target. | Use the trained Random Forest model to make predictions on the test data. | . from sklearn.ensemble import RandomForestRegressor # Select only numeric features features = [&#39;pickup_longitude&#39;, &#39;pickup_latitude&#39;, &#39;dropoff_longitude&#39;, &#39;dropoff_latitude&#39;, &#39;passenger_count&#39;, &#39;hour&#39;] # Train a Random Forest model rf = RandomForestRegressor() rf.fit(train[features], train.fare_amount) # Make predictions on the test data test[&#39;fare_amount&#39;] = rf.predict(test[features]) # Write predictions test[[&#39;id&#39;,&#39;fare_amount&#39;]].to_csv(&#39;rf_sub.csv&#39;, index=False) . Congratulations! This final baseline achieves the 1051st place on the Public Leaderboard which is slightly better than the Gradient Boosting from the video. So, now you know how to build fast and simple baseline models to validate your initial pipeline. . Hyperparameter tuning . Ridge Regression . Least squares linear regression $$ text{Loss} = sum_{i=1}^{N} (y_i - hat{y}_i)^2 rightarrow text{min} $$ . | Ridge Regression $$ text{Loss} = sum_{i=1}^{N} (y_i - hat{y}_{i})^2 + alpha sum_{j=1}^K w_j^2 rightarrow text{min}$$ . | $ alpha$ is hyperparameter . | . | . Hyperparameter optimization strategies . Grid Search - Choose the predefined grid of hyperparamter values | Random Search - Choose the search space of hyperparamter values | Bayesian optimization - Choose the search space of hyperparameter values | . | . Grid search . Recall that we&#39;ve created a baseline Gradient Boosting model in the previous lesson. Your goal now is to find the best max_depth hyperparameter value for this Gradient Boosting model. This hyperparameter limits the number of nodes in each individual tree. You will be using K-fold cross-validation to measure the local performance of the model for each hyperparameter value. . You&#39;re given a function get_cv_score(), which takes the train dataset and dictionary of the model parameters as arguments and returns the overall validation RMSE score over 3-fold cross-validation. . Instructions . 100 XP . Specify the grid for possible max_depth values with 3, 6, 9, 12 and 15. | Pass each hyperparameter candidate in the grid to the model params dictionary. | . max_depth_grid = [3, 6, 9, 12, 15] results = {} # For each value in the grid for max_depth_candidate in max_depth_grid: # Specify parameters for the model params = {&#39;max_depth&#39;: max_depth_candidate} # Calculate validation score for a particular hyperparameter validation_score = get_cv_score(train, params) # Save the results for each max depth value results[max_depth_candidate] = validation_score print(results) . Nice! We have a validation score for each value in the grid. It&#39;s clear that the optimal max depth value is located somewhere between 3 and 6. The next step could be to use a smaller grid, for example [3, 4, 5, 6] and repeat the same process. Moving from larger to smaller grids allows us to find the most optimal values. Keep going to try optimizing 2 hyperparameters simultaneously! . 2D grid search . The drawback of tuning each hyperparameter independently is a potential dependency between different hyperparameters. The better approach is to try all the possible hyperparameter combinations. However, in such cases, the grid search space is rapidly expanding. For example, if we have 2 parameters with 10 possible values, it will yield 100 experiment runs. . Your goal is to find the best hyperparameter couple of max_depth and subsample for the Gradient Boosting model. subsample is a fraction of observations to be used for fitting the individual trees. . You&#39;re given a function get_cv_score(), which takes the train dataset and dictionary of the model parameters as arguments and returns the overall validation RMSE score over 3-fold cross-validation. . Instructions . 100 XP . Specify the grids for possible max_depth and subsample values. For max_depth: 3, 5 and 7. For subsample: 0.8, 0.9 and 1.0. | Apply the product() function from the itertools package to the hyperparameter grids. It returns all possible combinations for these two grids. | Pass each hyperparameters candidate couple to the model params dictionary. | . import itertools # Hyperparameter grids max_depth_grid = [3, 5, 7] subsample_grid = [0.8, 0.9, 1.0] results = {} # For each couple in the grid for max_depth_candidate, subsample_candidate in itertools.product(max_depth_grid, subsample_grid): params = {&#39;max_depth&#39;: max_depth_candidate, &#39;subsample&#39;: subsample_candidate} validation_score = get_cv_score(train, params) # Save the results for each couple results[(max_depth_candidate, subsample_candidate)] = validation_score print(results) . Great! You can see that tuning multiple hyperparameters simultaneously achieves better results. In the previous exercise, tuning only the max_depth parameter gave the best RMSE of $6.50. With `max_depth` equal to 7 and `subsample` equal to 0.8, the best RMSE is now $6.16. However, do not spend too much time on the hyperparameter tuning at the beginning of the competition! Another approach that almost always improves your solution is model ensembling. Go on for it! . Model ensembling . Model blending | Model stacking Split train data into two parts | Train multiple models on Part 1 | Make predictions on Part 2 | Make predictions on the test data | Train a new model on Part 2 using predictions as features | Make predictions on the test data using the 2nd level model | | . Model blending . Model stacking I . Model stacking II . Final tips . Testing Kaggle forum ideas . Select final submissions . Final thoughts .",
            "url": "https://islamalam.github.io/blog/2021/12/26/winning-a-kaggle-competition-in-python.html",
            "relUrl": "/2021/12/26/winning-a-kaggle-competition-in-python.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "1. Introduction",
            "content": "Machine Learning with PySpark . Course Description . Spark is a powerful, general purpose tool for working with Big Data. Spark transparently handles the distribution of compute tasks across a cluster. This means that operations are fast, but it also allows you to focus on the analysis rather than worry about technical details. In this course you&#39;ll learn how to get data into Spark and then delve into the three fundamental Spark Machine Learning algorithms: Linear Regression, Logistic Regression/Classifiers, and creating pipelines. Along the way you&#39;ll analyse a large dataset of flight delays and spam text messages. With this background you&#39;ll be ready to harness the power of Spark and apply it on your own Machine Learning projects! . https://projector-video-pdf-converter.datacamp.com/14989/chapter1.pdf . import pyspark import numpy as np import pandas as pd . 1.1 Machine Learning &amp; Spark . Spark Compute accross a distributed cluster. | Data processed in memory | Well documented high level API | . | . . 1.2 Characteristics of Spark . Spark is currently the most popular technology for processing large quantities of data. Not only is it able to handle enormous data volumes, but it does so very efficiently too! Also, unlike some other distributed computing technologies, developing with Spark is a pleasure. . Which of these describe Spark? . Answer the question . 50XP . Possible Answers . Spark is a framework for cluster computing. . | Spark does most processing in memory. . | Spark has a high-level API, which conceals a lot of complexity. . | All of the above. . | . Spark has all of this and more! . 1.3 Components in a Spark Cluster . Spark is a distributed computing platform. It achieves efficiency by distributing data and computation across a cluster of computers. . A Spark cluster consists of a number of hardware and software components which work together. . Which of these is not part of a Spark cluster? . Answer the question . 50XP . Possible Answers . One or more nodes | . A cluster manager | . A load balancer | . Executors | . A load balancer distributes work across multiple resources, preventing overload on any one resource. In Spark this function is performed by the cluster manager. . 1.4 Connecting to Spark . Creating-a-SparkSession . In this exercise, you&#39;ll spin up a local Spark cluster using all available cores. The cluster will be accessible via a SparkSession object. . The SparkSession class has a builder attribute, which is an instance of the Builder class. The Builder class exposes three important methods that let you: . specify the location of the master node; | name the application (optional); and | retrieve an existing SparkSession or, if there is none, create a new one. | . The SparkSession class has a version attribute which gives the version of Spark. . Find out more about SparkSession here. . Once you are finished with the cluster, it&#39;s a good idea to shut it down, which will free up its resources, making them available for other processes. . from pyspark.sql import SparkSession # Create SparkSession object spark = SparkSession.builder.master(&#39;local[*]&#39;).appName(&#39;test&#39;).getOrCreate() # What version of Spark? print(spark.version) # Terminate the cluster spark.stop() . 1.5 Location of Spark master . Which of the following is not a valid way to specify the location of a Spark cluster? . Answer the question . 50XP . Possible Answers . spark://13.59.151.161:7077 | . spark://ec2-18-188-22-23.us-east-2.compute.amazonaws.com:7077 | . spark://18.188.22.23 | . local | . local[4] | . local[*] | . A Spark URL must always include a port number, so this URL is not valid. . 1.6 Creating a SparkSession . In this exercise, you&#39;ll spin up a local Spark cluster using all available cores. The cluster will be accessible via a SparkSession object. . The SparkSession class has a builder attribute, which is an instance of the Builder class. The Builder class exposes three important methods that let you: . specify the location of the master node; | name the application (optional); and | retrieve an existing SparkSession or, if there is none, create a new one. | . The SparkSession class has a version attribute which gives the version of Spark. . Find out more about SparkSession here. . Once you are finished with the cluster, it&#39;s a good idea to shut it down, which will free up its resources, making them available for other processes. . Note:: You might find it useful to review the slides from the lessons in the Slides panel next to the IPython Shell. . Instructions . 100 XP . Import the SparkSession class from pyspark.sql. | Create a SparkSession object connected to a local cluster. Use all available cores. Name the application &#39;test&#39;. | Use the SparkSession object to retrieve the version of Spark running on the cluster. Note: The version might be different to the one that&#39;s used in the presentation (it gets updated from time to time). | Shut down the cluster. | . from pyspark.sql import SparkSession # Create SparkSession object spark = SparkSession.builder .master(&#39;local[*]&#39;) .appName(&#39;test&#39;) .getOrCreate() # What version of Spark? print(spark.version) # Terminate the cluster spark.stop() . Nicely done! The session object will now allow us to load data into Spark. . 1.7 Loading Data . 1.8 Loading flights data . In this exercise you&#39;re going to load some airline flight data from a CSV file. To ensure that the exercise runs quickly these data have been trimmed down to only 50 000 records. You can get a larger dataset in the same format here. . Notes on CSV format: . fields are separated by a comma (this is the default separator) and | missing data are denoted by the string &#39;NA&#39;. | . Data dictionary: . mon — month (integer between 1 and 12) | dom — day of month (integer between 1 and 31) | dow — day of week (integer; 1 = Monday and 7 = Sunday) | org — origin airport (IATA code) | mile — distance (miles) | carrier — carrier (IATA code) | depart — departure time (decimal hour) | duration — expected duration (minutes) | delay — delay (minutes) | . pyspark has been imported for you and the session has been initialized. . Note: The data have been aggressively down-sampled. . Instructions . 100 XP . Instructions . 100 XP . Read data from a CSV file called &#39;flights.csv&#39;. Assign data types to columns automatically. Deal with missing data. | How many records are in the data? | Take a look at the first five records. | What data types have been assigned to the columns? Do these look correct? | . flights = spark.read.csv(&#39;flights.csv&#39;, sep=&#39;,&#39;, header=True, inferSchema=True, nullValue=&#39;NA&#39;) # Get number of records print(&quot;The data contain %d records.&quot; % flights.count()) # View the first five records flights.show(5) # Check column data types print(flights.dtypes) . The correct data types have been inferred for all of the columns. . 1.9 Loading SMS spam data . You&#39;ve seen that it&#39;s possible to infer data types directly from the data. Sometimes it&#39;s convenient to have direct control over the column types. You do this by defining an explicit schema. . The file sms.csv contains a selection of SMS messages which have been classified as either &#39;spam&#39; or &#39;ham&#39;. These data have been adapted from the UCI Machine Learning Repository. There are a total of 5574 SMS, of which 747 have been labelled as spam. . Notes on CSV format: . no header record and | fields are separated by a semicolon (this is not the default separator). | . Data dictionary: . id — record identifier | text — content of SMS message | label — spam or ham (integer; 0 = ham and 1 = spam) | . Instructions . 100 XP . Specify the data schema, giving columns names (&quot;id&quot;, &quot;text&quot;, and &quot;label&quot;) and column types. | Read data from a delimited file called &quot;sms.csv&quot;. | Print the schema for the resulting DataFrame. | . from pyspark.sql.types import StructType, StructField, IntegerType, StringType # Specify column names and types schema = StructType([ StructField(&quot;id&quot;, IntegerType()), StructField(&quot;text&quot;, StringType()), StructField(&quot;label&quot;, IntegerType()) ]) # Load data from a delimited file sms = spark.read.csv(&quot;sms.csv&quot;, sep=&#39;;&#39;, header=False, schema=schema) # Print schema of DataFrame sms.printSchema() . Excellent! You now know how to initiate a Spark session and load data. In the next chapter you&#39;ll use the data you&#39;ve just loaded to build a classification model. . 2. Classification . Now that you are familiar with getting data into Spark, you&#39;ll move onto building two types of classification model: Decision Trees and Logistic Regression. You&#39;ll also find out about a few approaches to data preparation. . 2.1 Data Preparation . 2.2 Removing columns and rows . You previously loaded airline flight data from a CSV file. You&#39;re going to develop a model which will predict whether or not a given flight will be delayed. . In this exercise you need to trim those data down by: . removing an uninformative column and | removing rows which do not have information about whether or not a flight was delayed. | The data are available as flights. . Note:: You might find it useful to revise the slides from the lessons in the Slides panel next to the IPython Shell. . Instructions . 100 XP . Instructions . 100 XP . Remove the flight column. | Find out how many records have missing values in the delay column. | Remove records with missing values in the delay column. | Remove records with missing values in any column and get the number of remaining rows. | . flights_drop_column = flights.drop(&#39;flight&#39;) # Number of records with missing &#39;delay&#39; values flights_drop_column.filter(&#39;delay IS NULL&#39;).count() # Remove records with missing &#39;delay&#39; values flights_valid_delay = flights_drop_column.filter(&#39;delay IS NOT NULL&#39;) # Remove records with missing values in any column and get the number of remaining rows flights_none_missing = flights_valid_delay.dropna() print(flights_none_missing.count()) . You&#39;ve discarded the columns and rows which will certainly not contribute to a model. . 2.3 Column manipulation . The Federal Aviation Administration (FAA) considers a flight to be &quot;delayed&quot; when it arrives 15 minutes or more after its scheduled time. . The next step of preparing the flight data has two parts: . convert the units of distance, replacing the mile column with a kmcolumn; and | create a Boolean column indicating whether or not a flight was delayed. | Instructions . 100 XP . Instructions . 100 XP . Import a function which will allow you to round a number to a specific number of decimal places. | Derive a new km column from the mile column, rounding to zero decimal places. One mile is 1.60934 km. | Remove the mile column. | Create a label column with a value of 1 indicating the delay was 15 minutes or more and 0 otherwise. | . from pyspark.sql.functions import round # Convert &#39;mile&#39; to &#39;km&#39; and drop &#39;mile&#39; column flights_km = flights.withColumn(&#39;km&#39;, round(flights.mile * 1.60934, 0)).drop(&#39;mile&#39;) # Create &#39;label&#39; column indicating whether flight delayed (1) or not (0) flights_km = flights_km.withColumn(&#39;label&#39;, (flights_km.delay &gt;= 15).cast(&#39;integer&#39;)) # Check first five records flights_km.show(5) . Fifteen minutes seems like quite a wide margin, but who are you to argue with the FAA? . 2.4 Categorical columns . In the flights data there are two columns, carrier and org, which hold categorical data. You need to transform those columns into indexed numerical values. . Instructions . 100 XP . Import the appropriate class and create an indexer object to transform the carrier column from a string to an numeric index. | Prepare the indexer object on the flight data. | Use the prepared indexer to create the numeric index column. | Repeat the process for the org column. | . from pyspark.ml.feature import StringIndexer # Create an indexer indexer = StringIndexer(inputCol=&#39;carrier&#39;, outputCol=&#39;carrier_idx&#39;) # Indexer identifies categories in the data indexer_model = indexer.fit(flights) # Indexer creates a new column with numeric index values flights_indexed = indexer_model.transform(flights) # Repeat the process for the other categorical feature flights_indexed = StringIndexer(inputCol=&#39;org&#39;, outputCol=&#39;org_idx&#39;).fit(flights_indexed).transform(flights_indexed) . Our Machine Learning model needs numbers not strings, so these transformations are vital! . 2.5 Assembling columns . The final stage of data preparation is to consolidate all of the predictor columns into a single column. . An updated version of the flights data, which takes into account all of the changes from the previous few exercises, has the following predictor columns: . mon, dom and dow | carrier_idx (indexed value from carrier) | org_idx (indexed value from org) | km | depart | duration | . Instructions . 100 XP . Import the class which will assemble the predictors. | Create an assembler object that will allow you to merge the predictors columns into a single column. | Use the assembler to generate a new consolidated column. | . from pyspark.ml.feature import VectorAssembler # Create an assembler object assembler = VectorAssembler(inputCols=[ &#39;mon&#39;, &#39;dom&#39;, &#39;dow&#39;, &#39;carrier_idx&#39;, &#39;org_idx&#39;, &#39;km&#39;, &#39;depart&#39;, &#39;duration&#39; ], outputCol=&#39;features&#39;) # Consolidate predictor columns flights_assembled = assembler.transform(flights) # Check the resulting column flights_assembled.select(&#39;features&#39;, &#39;delay&#39;).show(5, truncate=False) . The data are now ready for building our first Machine Learning model. You&#39;ve worked hard to get this sorted: well done! . 2.6 Decision Tree . 2.7 Train/test split . To objectively assess a Machine Learning model you need to be able to test it on an independent set of data. You can&#39;t use the same data that you used to train the model: of course the model will perform (relatively) well on those data! . You will split the data into two components: . training data (used to train the model) and | testing data (used to test the model). | . Instructions . 100 XP . Randomly split the flights data into two sets with 80:20 proportions. For repeatability set a random number seed of 17 for the split. | Check that the training data has roughly 80% of the records from the original data. | . flights_train, flights_test = flights.randomSplit([0.8, 0.2], seed=17) # Check that training set has around 80% of records training_ratio = flights_train.count() / flights_test.count() print(training_ratio) . The ratio looks as expected. You&#39;re ready to train and test a Decision Tree model! . 2.8 Build a Decision Tree . Now that you&#39;ve split the flights data into training and testing sets, you can use the training set to fit a Decision Tree model. . The data are available as flights_train and flights_test. . NOTE: It will take a few seconds for the model to train… please be patient! . Instructions . 100 XP . Import the class for creating a Decision Tree classifier. | Create a classifier object and fit it to the training data. | Make predictions for the testing data and take a look at the predictions. | . from pyspark.ml.classification import DecisionTreeClassifier # Create a classifier object and fit to the training data tree = DecisionTreeClassifier() tree_model = tree.fit(flights_train) # Create predictions for the testing data and take a look at the predictions prediction = tree_model.transform(flights_test) prediction.select(&#39;label&#39;, &#39;prediction&#39;, &#39;probability&#39;).show(5, False) . Congratulations! You&#39;ve built your first Machine Learning model with PySpark. Now to test! . 2.9 Evaluate the Decision Tree . You can assess the quality of your model by evaluating how well it performs on the testing data. Because the model was not trained on these data, this represents an objective assessment of the model. . A confusion matrix gives a useful breakdown of predictions versus known values. It has four cells which represent the counts of: . True Negatives (TN) — model predicts negative outcome &amp; known outcome is negative | True Positives (TP) — model predicts positive outcome &amp; known outcome is positive | False Negatives (FN) — model predicts negative outcome but known outcome is positive | False Positives (FP) — model predicts positive outcome but known outcome is negative. | . Instructions . 100 XP . Create a confusion matrix by counting the combinations of label and prediction. Display the result. | Count the number of True Negatives, True Positives, False Negatives and False Positives. | Calculate the accuracy. | . prediction.groupBy(&#39;label&#39;, &#39;prediction&#39;).count().show() # Calculate the elements of the confusion matrix TN = prediction.filter(&#39;prediction = 0 AND label = prediction&#39;).count() TP = prediction.filter(&#39;prediction = 1 AND label = prediction&#39;).count() FN = prediction.filter(&#39;prediction = 0 AND label = 1&#39;).count() FP = prediction.filter(&#39;prediction = 1 AND label = 0&#39;).count() # Accuracy measures the proportion of correct predictions accuracy = (TN + TP) / (TN + TP + FN + FP) print(accuracy) . The accuracy is decent but there are a lot of false predictions. We can make this model better! . 2.10 Logistic Regression . 2.11 Build a Logistic Regression model . You&#39;ve already built a Decision Tree model using the flights data. Now you&#39;re going to create a Logistic Regression model on the same data. . The objective is to predict whether a flight is likely to be delayed by at least 15 minutes (label 1) or not (label 0). . Although you have a variety of predictors at your disposal, you&#39;ll only use the mon, depart and duration columns for the moment. These are numerical features which can immediately be used for a Logistic Regression model. You&#39;ll need to do a little more work before you can include categorical features. Stay tuned! . The data have been split into training and testing sets and are available as flights_train and flights_test. . Instructions . 100 XP . Import the class for creating a Logistic Regression classifier. | Create a classifier object and train it on the training data. | Make predictions for the testing data and create a confusion matrix. | . from pyspark.ml.classification import LogisticRegression # Create a classifier object and train on training data logistic = LogisticRegression().fit(flights_train) # Create predictions for the testing data and show confusion matrix prediction = logistic.transform(flights_test) prediction.groupBy(&#39;label&#39;, &#39;prediction&#39;).count().show() . Now let&#39;s unpack that confusion matrix. . 2.12 Evaluate the Logistic Regression model . Accuracy is generally not a very reliable metric because it can be biased by the most common target class. . There are two other useful metrics: . precision and | recall. | . Check the slides for this lesson to get the relevant expressions. . Precision is the proportion of positive predictions which are correct. For all flights which are predicted to be delayed, what proportion is actually delayed? . Recall is the proportion of positives outcomes which are correctly predicted. For all delayed flights, what proportion is correctly predicted by the model? . The precision and recall are generally formulated in terms of the positive target class. But it&#39;s also possible to calculate weighted versions of these metrics which look at both target classes. . The components of the confusion matrix are available as TN, TP, FN and FP, as well as the object prediction. . Instructions . 100 XP . Find the precision and recall. | Create a multi-class evaluator and evaluate weighted precision. | Create a binary evaluator and evaluate AUC using the &quot;areaUnderROC&quot; metric. | . from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator # Calculate precision and recall precision = TP / (TP + FP) recall = TP / (TP + FN) print(&#39;precision = {:.2f} nrecall = {:.2f}&#39;.format(precision, recall)) # Find weighted precision multi_evaluator = MulticlassClassificationEvaluator() weighted_precision = multi_evaluator.evaluate(prediction, {multi_evaluator.metricName: &quot;weightedPrecision&quot;}) # Find AUC binary_evaluator = BinaryClassificationEvaluator() auc = binary_evaluator.evaluate(prediction, {binary_evaluator.metricName: &quot;areaUnderROC&quot;}) . The weighted precision indicates what proportion of predictions (positive and negative) are correct. . 2.13 Turning Text into Tables . 2.14 Punctuation, numbers and tokens . At the end of the previous chapter you loaded a dataset of SMS messages which had been labeled as either &quot;spam&quot; (label 1) or &quot;ham&quot; (label 0). You&#39;re now going to use those data to build a classifier model. . But first you&#39;ll need to prepare the SMS messages as follows: . remove punctuation and numbers | tokenize (split into individual words) | remove stop words | apply the hashing trick | convert to TF-IDF representation. | . In this exercise you&#39;ll remove punctuation and numbers, then tokenize the messages. . The SMS data are available as sms. . Instructions . 100 XP . Instructions . 100 XP . Import the function to replace regular expressions and the feature to tokenize. | Replace all punctuation characters from the text column with a space. Do the same for all numbers in the text column. | Split the text column into tokens. Name the output column words. | . from pyspark.sql.functions import regexp_replace from pyspark.ml.feature import Tokenizer # Remove punctuation (REGEX provided) and numbers wrangled = sms.withColumn(&#39;text&#39;, regexp_replace(sms.text, &#39;[_():;,.!? -]&#39;, &#39; &#39;)) wrangled = wrangled.withColumn(&#39;text&#39;, regexp_replace(wrangled.text, &#39;[0-9]&#39;, &#39; &#39;)) # Merge multiple spaces wrangled = wrangled.withColumn(&#39;text&#39;, regexp_replace(wrangled.text, &#39; +&#39;, &#39; &#39;)) # Split the text into words wrangled = Tokenizer(inputCol=&#39;text&#39;, outputCol=&#39;words&#39;).transform(wrangled) wrangled.show(4, truncate=False) . Well done! Next you&#39;ll remove stop words and apply the hashing trick. . 2.15 Stop words and hashing . The next steps will be to remove stop words and then apply the hashing trick, converting the results into a TF-IDF. . A quick reminder about these concepts: . The hashing trick provides a fast and space-efficient way to map a very large (possibly infinite) set of items (in this case, all words contained in the SMS messages) onto a smaller, finite number of values. | The TF-IDF matrix reflects how important a word is to each document. It takes into account both the frequency of the word within each document but also the frequency of the word across all of the documents in the collection. | . The tokenized SMS data are stored in sms in a column named words. You&#39;ve cleaned up the handling of spaces in the data so that the tokenized text is neater. . Instructions . 100 XP . Instructions . 100 XP . Import the StopWordsRemover, HashingTF and IDF classes. | Create a StopWordsRemover object (input column words, output column terms). Apply to sms. | Create a HashingTF object (input results from previous step, output column hash). Apply to wrangled. | Create an IDF object (input results from previous step, output column features). Apply to wrangled. | . from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF # Remove stop words. wrangled = StopWordsRemover(inputCol=&#39;words&#39;, outputCol=&#39;terms&#39;) .transform(sms) # Apply the hashing trick wrangled = HashingTF(inputCol=&#39;terms&#39;, outputCol=&#39;hash&#39;, numFeatures=1024) .transform(wrangled) # Convert hashed symbols to TF-IDF tf_idf = IDF(inputCol=&#39;hash&#39;, outputCol=&#39;features&#39;) .fit(wrangled).transform(wrangled) tf_idf.select(&#39;terms&#39;, &#39;features&#39;).show(4, truncate=False) . Great! Now you&#39;re ready to build a spam classifier. . 2.16 Training a spam classifier . The SMS data have now been prepared for building a classifier. Specifically, this is what you have done: . removed numbers and punctuation | split the messages into words (or &quot;tokens&quot;) | removed stop words | applied the hashing trick and | converted to a TF-IDF representation. | . Next you&#39;ll need to split the TF-IDF data into training and testing sets. Then you&#39;ll use the training data to fit a Logistic Regression model and finally evaluate the performance of that model on the testing data. . The data are stored in sms and LogisticRegression has been imported for you. . Instructions . 100 XP . Split the data into training and testing sets in a 4:1 ratio. Set the random number seed to 13 to ensure repeatability. | Create a LogisticRegression object and fit it to the training data. | Generate predictions on the testing data. | Use the predictions to form a confusion matrix. | . sms_train, sms_test = sms.randomSplit([0.8, 0.2], seed=13) # Fit a Logistic Regression model to the training data logistic = LogisticRegression(regParam=0.2).fit(sms_train) # Make predictions on the testing data prediction = logistic.transform(sms_test) # Create a confusion matrix, comparing predictions to known labels prediction.groupBy(&#39;label&#39;, &#39;prediction&#39;).count().show() . Well played! Your classifier won&#39;t be fooled by spam SMS. . 3. Regression . Next you&#39;ll learn to create Linear Regression models. You&#39;ll also find out how to augment your data by engineering new predictors as well as a robust approach to selecting only the most relevant predictors. . 3.1 One-Hot Encoding . 3.2 Encoding flight origin . The org column in the flights data is a categorical variable giving the airport from which a flight departs. . ORD — O&#39;Hare International Airport (Chicago) | SFO — San Francisco International Airport | JFK — John F Kennedy International Airport (New York) | LGA — La Guardia Airport (New York) | SMF — Sacramento | SJC — San Jose | TUS — Tucson International Airport | OGG — Kahului (Hawaii) | . Obviously this is only a small subset of airports. Nevertheless, since this is a categorical variable, it needs to be one-hot encoded before it can be used in a regression model. . The data are in a variable called flights. You have already used a string indexer to create a column of indexed values corresponding to the strings in org. . Note:: You might find it useful to revise the slides from the lessons in the Slides panel next to the IPython Shell. . Instructions . 100 XP . Instructions . 100 XP . Import the one-hot encoder class. | Create an one-hot encoder instance, naming the output column &#39;org_dummy&#39;. | Apply the one-hot encoder to the flights data. | Generate a summary of the mapping from categorical values to binary encoded dummy variables. Include only unique values and order by org_idx. | . from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator # Create an instance of the one hot encoder onehot = OneHotEncoderEstimator(inputCols=[&#39;org_idx&#39;], outputCols=[&#39;org_dummy&#39;]) # Apply the one hot encoder to the flights data onehot = onehot.fit(flights) flights_onehot = onehot.transform(flights) # Check the results flights_onehot.select(&#39;org&#39;, &#39;org_idx&#39;, &#39;org_dummy&#39;).distinct().sort(&#39;org_idx&#39;).show() . Excellent! Note that one of the category levels, OGG, does not get a dummy variable. . 3.3 Encoding shirt sizes . You have data for a consignment of t-shirts. The data includes the size of the shirt, which is given as either S, M, L or XL. . Here are the counts for the different sizes: . +-+--+ |size|count| +-+--+ | S| 8| | M| 15| | L| 20| | XL| 7| +-+--+ . The sizes are first converted to an index using StringIndexer and then one-hot encoded using OneHotEncoderEstimator. . Which of the following is not true: . Answer the question . 50XP . Possible Answers . S shirts get index 2.0 and are one-hot encoded as (3,[2],[1.0]) | . M shirts get index 1.0 and are one-hot encoded as (3,[1],[1.0]) | . L shirts get index 0.0 and are one-hot encoded as (3,[0],[1.0]) | . XL shirts get index 3.0 and are one-hot encoded as (3,[3],[1.0]) | . Correct! This statement is false: XL is the least frequent size, so it receives an index of 3. However, it is one-hot encoded to (3,[],[]) because it does not get it&#39;s own dummy variable. If none of the other dummy variables are true, then this one must be true. So to make a separate dummy variable would be redundant! . 3.4 Regression . 3.5 Flight duration model: Just distance . In this exercise you&#39;ll build a regression model to predict flight duration (the duration column). . For the moment you&#39;ll keep the model simple, including only the distance of the flight (the km column) as a predictor. . The data are in flights. The first few records are displayed in the terminal. These data have also been split into training and testing sets and are available as flights_train and flights_test. . Instructions . 100 XP . Create a linear regression object. Specify the name of the label column. Fit it to the training data. | Make predictions on the testing data. | Create a regression evaluator object and use it to evaluate RMSE on the testing data. | . from pyspark.ml.regression import LinearRegression from pyspark.ml.evaluation import RegressionEvaluator # Create a regression object and train on training data regression = LinearRegression(featuresCol=&#39;features&#39;, labelCol=&#39;duration&#39;).fit(flights_train) # Create predictions for the testing data and take a look at the predictions predictions = regression.transform(flights_test) predictions.select(&#39;duration&#39;, &#39;prediction&#39;).show(5, False) # Calculate the RMSE RegressionEvaluator(labelCol=&#39;duration&#39;, metricName=&#39;rmse&#39;).evaluate(predictions) . You&#39;ve built a simple regression model. Let&#39;s make sense of the coefficients! . 3.6 Interpreting the coefficients . The linear regression model for flight duration as a function of distance takes the form . $$ duration = α + β × distance $$ . where . α — intercept (component of duration which does not depend on distance) and | β — coefficient (rate at which duration increases as a function of distance; also called the slope). | . By looking at the coefficients of your model you will be able to infer . how much of the average flight duration is actually spent on the ground and | what the average speed is during a flight. | . The linear regression model is available as regression. . Instructions . 100 XP . Instructions . 100 XP . What&#39;s the intercept? | What are the coefficients? This is a vector. | Extract the element from the vector which corresponds to the slope for distance. | Find the average speed in km per hour. | . inter = regression.intercept print(inter) # Coefficients coefs = regression.coefficients print(coefs) # Average minutes per km minutes_per_km = regression.coefficients[0] print(minutes_per_km) # Average speed in km per hour avg_speed = 60 / minutes_per_km print(avg_speed) . The average speed of a commercial jet is around 850 km/hour. But you got that already from the data! . 3.7 Flight duration model: Adding origin airport . Some airports are busier than others. Some airports are bigger than others too. Flights departing from large or busy airports are likely to spend more time taxiing or waiting for their takeoff slot. So it stands to reason that the duration of a flight might depend not only on the distance being covered but also the airport from which the flight departs. . You are going to make the regression model a little more sophisticated by including the departure airport as a predictor. . These data have been split into training and testing sets and are available as flights_train and flights_test. The origin airport, stored in the org column, has been indexed into org_idx, which in turn has been one-hot encoded into org_dummy. The first few records are displayed in the terminal. . Instructions . 100 XP . Fit a linear regression model to the training data. | Make predictions for the testing data. | Calculate the RMSE for predictions on the testing data. | . from pyspark.ml.regression import LinearRegression from pyspark.ml.evaluation import RegressionEvaluator # Create a regression object and train on training data regression = LinearRegression(featuresCol=&#39;features&#39;, labelCol=&#39;duration&#39;).fit(flights_train) # Create predictions for the testing data predictions = regression.transform(flights_test) # Calculate the RMSE on testing data RegressionEvaluator(labelCol=&#39;duration&#39;, metricName=&#39;rmse&#39;).evaluate(predictions) . Looking good! Let&#39;s try to make sense of the coefficients. . 3.8 Interpreting coefficients . Remember that origin airport, org, has eight possible values (ORD, SFO, JFK, LGA, SMF, SJC, TUS and OGG) which have been one-hot encoded to seven dummy variables in org_dummy. . The values for km and org_dummy have been assembled into features, which has eight columns with sparse representation. Column indices in features are as follows: . 0 — km | 1 — ORD | 2 — SFO | 3 — JFK | 4 — LGA | 5 — SMF | 6 — SJC and | 7 — TUS. | . Note that OGG does not appear in this list because it is the reference level for the origin airport category. . In this exercise you&#39;ll be using the intercept and coefficients attributes to interpret the model. . The coefficients attribute is a list, where the first element indicates how flight duration changes with flight distance. . Instructions . 100 XP . Instructions . 100 XP . Find the average speed in km per hour. This will be different to the value that you got earlier because your model is now more sophisticated. | What&#39;s the average time on the ground at OGG? | What&#39;s the average time on the ground at JFK? | What&#39;s the average time on the ground at LGA? | . avg_speed_hour = 60 / regression.coefficients[0] print(avg_speed_hour) # Average minutes on ground at OGG inter = regression.intercept print(inter) # Average minutes on ground at JFK avg_ground_jfk = inter + regression.coefficients[3] print(avg_ground_jfk) # Average minutes on ground at LGA avg_ground_lga = inter + regression.coefficients[4] print(avg_ground_lga) . You&#39;re going to spend over an hour on the ground at JFK or LGA but only around 15 minutes at OGG. . 3.9 Bucketing &amp; Engineering . 3.10 Bucketing departure time . Time of day data are a challenge with regression models. They are also a great candidate for bucketing. . In this lesson you will convert the flight departure times from numeric values between 0 (corresponding to 00:00) and 24 (corresponding to 24:00) to binned values. You&#39;ll then take those binned values and one-hot encode them. . Instructions . 100 XP . Create a bucketizer object with bin boundaries which correspond to 0:00, 03:00, 06:00, …, 24:00. Specify input column as depart and output column as depart_bucket. | Bucket the departure times. Show the first five values for depart and depart_bucket. | Create a one-hot encoder object. Specify output column as depart_dummy. | Train the encoder on the data and then use it to convert the bucketed departure times to dummy variables. Show the first five values for depart, depart_bucket and depart_dummy. | . from pyspark.ml.feature import Bucketizer, OneHotEncoderEstimator # Create buckets at 3 hour intervals through the day buckets = Bucketizer(splits=[3 * x for x in range(9)], inputCol=&#39;depart&#39;, outputCol=&#39;depart_bucket&#39;) # Bucket the departure times bucketed = buckets.transform(flights) bucketed.select(&#39;depart&#39;, &#39;depart_bucket&#39;).show(5) # Create a one-hot encoder onehot = OneHotEncoderEstimator(inputCols=[&#39;depart_bucket&#39;], outputCols=[&#39;depart_dummy&#39;]) # One-hot encode the bucketed departure times flights_onehot = onehot.fit(bucketed).transform(bucketed) flights_onehot.select(&#39;depart&#39;, &#39;depart_bucket&#39;, &#39;depart_dummy&#39;).show(5) . Now you can add departure time to your regression model. . 3.11 Flight duration model: Adding departure time . In the previous exercise the departure time was bucketed and converted to dummy variables. Now you&#39;re going to include those dummy variables in a regression model for flight duration. . The data are in flights. The km, org_dummy and depart_dummy columns have been assembled into features, where km is index 0, org_dummy runs from index 1 to 7 and depart_dummy from index 8 to 14. . The data have been split into training and testing sets and a linear regression model, regression, has been built on the training data. Predictions have been made on the testing data and are available as predictions. . Instructions . 100 XP . Instructions . 100 XP . Find the RMSE for predictions on the testing data. | Find the average time spent on the ground for flights departing from OGG between 21:00 and 24:00. | Find the average time spent on the ground for flights departing from OGG between 00:00 and 03:00. | Find the average time spent on the ground for flights departing from JFK between 00:00 and 03:00. | . from pyspark.ml.evaluation import RegressionEvaluator RegressionEvaluator(labelCol=&#39;duration&#39;, metricName=&#39;rmse&#39;).evaluate(predictions) # Average minutes on ground at OGG for flights departing between 21:00 and 24:00 avg_eve_ogg = regression.intercept print(avg_eve_ogg) # Average minutes on ground at OGG for flights departing between 00:00 and 03:00 avg_night_ogg = regression.intercept + regression.coefficients[8] print(avg_night_ogg) # Average minutes on ground at JFK for flights departing between 00:00 and 03:00 avg_night_jfk = regression.intercept + regression.coefficients[3] + regression.coefficients[8] print(avg_night_jfk) . Adding departure time resulted in a smaller RMSE. Nice! . 3.12 Regularization . 3.13 Flight duration model: More features! . Let&#39;s add more features to our model. This will not necessarily result in a better model. Adding some features might improve the model. Adding other features might make it worse. . More features will always make the model more complicated and difficult to interpret. . These are the features you&#39;ll include in the next model: . km | org (origin airport, one-hot encoded, 8 levels) | depart (departure time, binned in 3 hour intervals, one-hot encoded, 8 levels) | dow (departure day of week, one-hot encoded, 7 levels) and | mon (departure month, one-hot encoded, 12 levels). | . These have been assembled into the features column, which is a sparse representation of 32 columns (remember one-hot encoding produces a number of columns which is one fewer than the number of levels). . The data are available as flights, randomly split into flights_train and flights_test. The object predictions is also available. . Instructions . 100 XP . Instructions . 100 XP . Fit a linear regression model to the training data. | Generate predictions for the testing data. | Calculate the RMSE on the testing data. | Look at the model coefficients. Are any of them zero? | . from pyspark.ml.regression import LinearRegression from pyspark.ml.evaluation import RegressionEvaluator # Fit linear regression model to training data regression = LinearRegression(labelCol=&#39;duration&#39;).fit(flights_train) # Make predictions on testing data predictions = regression.transform(flights_test) # Calculate the RMSE on testing data rmse = RegressionEvaluator(labelCol=&#39;duration&#39;, metricName=&#39;rmse&#39;).evaluate(predictions) print(&quot;The test RMSE is&quot;, rmse) # Look at the model coefficients coeffs = regression.coefficients print(coeffs) . With all those non-zero coefficients the model is a little hard to interpret! . 3.14 Flight duration model: Regularisation! . In the previous exercise you added more predictors to the flight duration model. The model performed well on testing data, but with so many coefficients it was difficult to interpret. . In this exercise you&#39;ll use Lasso regression (regularized with a L1 penalty) to create a more parsimonious model. Many of the coefficients in the resulting model will be set to zero. This means that only a subset of the predictors actually contribute to the model. Despite the simpler model, it still produces a good RMSE on the testing data. . You&#39;ll use a specific value for the regularization strength. Later you&#39;ll learn how to find the best value using cross validation. . The data (same as previous exercise) are available as flights, randomly split into flights_train and flights_test. . Instructions . 100 XP . Fit a linear regression model to the training data. | Calculate the RMSE on the testing data. | Look at the model coefficients. | How many of the coefficients are equal to zero? | . from pyspark.ml.regression import LinearRegression from pyspark.ml.evaluation import RegressionEvaluator # Fit Lasso model (α = 1) to training data regression = LinearRegression(labelCol=&#39;duration&#39;, regParam=1, elasticNetParam=1).fit(flights_train) # Calculate the RMSE on testing data rmse = RegressionEvaluator(labelCol=&#39;duration&#39;, metricName=&#39;rmse&#39;).evaluate(regression.transform(flights_test)) print(&quot;The test RMSE is&quot;, rmse) # Look at the model coefficients coeffs = regression.coefficients print(coeffs) # Number of zero coefficients zero_coeff = sum([beta == 0 for beta in regression.coefficients]) print(&quot;Number of coefficients equal to 0:&quot;, zero_coeff) . Regularisation produced a far simpler model with similar test performance. . 4. Ensembles &amp; Pipelines . Finally you&#39;ll learn how to make your models more efficient. You&#39;ll find out how to use pipelines to make your code clearer and easier to maintain. Then you&#39;ll use cross-validation to better test your models and select good model parameters. Finally you&#39;ll dabble in two types of ensemble model. . Pipeline . Flight duration model: Pipeline stages . You&#39;re going to create the stages for the flights duration model pipeline. You will use these in the next exercise to build a pipeline and to create a regression model. . Instructions . 100 XP . Create an indexer to convert the &#39;org&#39; column into an indexed column called &#39;org_idx&#39;. | Create a one-hot encoder to convert the &#39;org_idx&#39; and &#39;dow&#39; columns into dummy variable columns called &#39;org_dummy&#39; and &#39;dow_dummy&#39;. | Create an assembler which will combine the &#39;km&#39; column with the two dummy variable columns. The output column should be called &#39;features&#39;. | Create a linear regression object to predict flight duration. | . Note:: You might find it useful to revisit the slides from the lessons in the Slides panel next to the IPython Shell. . indexer = StringIndexer(inputCol=&#39;org&#39;, outputCol=&#39;org_idx&#39;) # One-hot encode index values onehot = OneHotEncoderEstimator( inputCols=[&#39;org_idx&#39;, &#39;dow&#39;], outputCols=[&#39;org_dummy&#39;, &#39;dow_dummy&#39;] ) # Assemble predictors into a single column assembler = VectorAssembler(inputCols=[&#39;km&#39;, &#39;org_dummy&#39;, &#39;dow_dummy&#39;], outputCol=&#39;features&#39;) # A linear regression object regression = LinearRegression(labelCol=&#39;duration&#39;) . Flight duration model: Pipeline model . You&#39;re now ready to put those stages together in a pipeline. . You&#39;ll construct the pipeline and then train the pipeline on the training data. This will apply each of the individual stages in the pipeline to the training data in turn. None of the stages will be exposed to the testing data at all: there will be no leakage! . Once the entire pipeline has been trained it will then be used to make predictions on the testing data. . The data are available as flights, which has been randomly split into flights_train and flights_test. . Instructions . 100 XP . Import the class for creating a pipeline. | Create a pipeline object and specify the indexer, onehot, assembler and regression stages, in this order. | Train the pipeline on the training data. | Make predictions on the testing data. | . from pyspark.ml import Pipeline # Construct a pipeline pipeline = Pipeline(stages=[indexer, onehot, assembler, regression]) # Train the pipeline on the training data pipeline = pipeline.fit(flights_train) # Make predictions on the testing data predictions = pipeline.transform(flights_test) . SMS spam pipeline . You haven&#39;t looked at the SMS data for quite a while. Last time we did the following: . split the text into tokens | removed stop words | applied the hashing trick | converted the data from counts to IDF and | trained a logistic regression model. | . Each of these steps was done independently. This seems like a great application for a pipeline! . Note: The Pipeline and LogisticRegression classes have already been imported into the session, so you don&#39;t need to worry about that! . Instructions . 100 XP . Instructions . 100 XP . Create an object for splitting text into tokens. | Create an object to remove stop words. Rather than explicitly giving the input column name, use the getOutputCol() method on the previous object. | Create objects for applying the hashing trick and transforming the data into a TF-IDF. Use the getOutputCol() method again. | Create a pipeline which wraps all of the above steps as well as an object to create a Logistic Regression model. | . from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF # Break text into tokens at non-word characters tokenizer = Tokenizer(inputCol=&#39;text&#39;, outputCol=&#39;words&#39;) # Remove stop words remover = StopWordsRemover(inputCol=&#39;words&#39;, outputCol=&#39;terms&#39;) # Apply the hashing trick and transform to TF-IDF hasher = HashingTF(inputCol=&#39;terms&#39;, outputCol=&quot;hash&quot;) idf = IDF(inputCol=&quot;hash&quot;, outputCol=&quot;features&quot;) # Create a logistic regression object and add everything to a pipeline logistic = LogisticRegression() pipeline = Pipeline(stages=[tokenizer, remover, hasher, idf, logistic]) . Cross-Validation . Cross validating simple flight duration model . You&#39;ve already built a few models for predicting flight duration and evaluated them with a simple train/test split. However, cross-validation provides a much better way to evaluate model performance. . In this exercise you&#39;re going to train a simple model for flight duration using cross-validation. Travel time is usually strongly correlated with distance, so using the km column alone should give a decent model. . The data have been randomly split into flights_train and flights_test. . The following classes have already been imported: LinearRegression, RegressionEvaluator, ParamGridBuilder and CrossValidator. . Instructions . 100 XP . Create an empty parameter grid. | Create objects for building and evaluating a linear regression model. The model should predict the &quot;duration&quot; field. | Create a cross-validator object. Provide values for the estimator, estimatorParamMaps and evaluator arguments. Choose 5-fold cross validation. | Train and test the model across multiple folds of the training data. | . params = ParamGridBuilder().build() # Create objects for building and evaluating a regression model regression = LinearRegression(labelCol=&#39;duration&#39;) evaluator = RegressionEvaluator(labelCol=&#39;duration&#39;) # Create a cross validator cv = CrossValidator(estimator=regression, estimatorParamMaps=params, evaluator=evaluator, numFolds=5) # Train and test model on multiple folds of the training data cv = cv.fit(flights_train) # NOTE: Since cross-valdiation builds multiple models, the fit() method can take a little while to complete. . Cross validating flight duration model pipeline . The cross-validated model that you just built was simple, using km alone to predict duration. . Another important predictor of flight duration is the origin airport. Flights generally take longer to get into the air from busy airports. Let&#39;s see if adding this predictor improves the model! . In this exercise you&#39;ll add the org field to the model. However, since org is categorical, there&#39;s more work to be done before it can be included: it must first be transformed to an index and then one-hot encoded before being assembled with km and used to build the regression model. We&#39;ll wrap these operations up in a pipeline. . The following objects have already been created: . params — an empty parameter grid | evaluator — a regression evaluator | regression — a LinearRegression object with labelCol=&#39;duration&#39;. | . All of the required classes have already been imported. . Instructions . 100 XP . Create a string indexer. Specify the input and output fields as org and org_idx. | Create a one-hot encoder. Name the output field org_dummy. | Assemble the km and org_dummy fields into a single field called features. | Create a pipeline using the following operations: string indexer, one-hot encoder, assembler and linear regression. Use this to create a cross-validator. | . indexer = StringIndexer(inputCol=&#39;org&#39;, outputCol=&#39;org_idx&#39;) # Create an one-hot encoder for the indexed org field onehot = OneHotEncoderEstimator(inputCols=[&#39;org_idx&#39;], outputCols=[&#39;org_dummy&#39;]) # Assemble the km and one-hot encoded fields assembler = VectorAssembler(inputCols=[&#39;km&#39;, &#39;org_dummy&#39;], outputCol=&#39;features&#39;) # Create a pipeline and cross-validator. pipeline = Pipeline(stages=[indexer, onehot, assembler, regression]) cv = CrossValidator(estimator=pipeline, estimatorParamMaps=params, evaluator=evaluator) . Wrapping operations in a pipeline makes cross validating the entire workflow easy! . Grid Search . Optimizing flights linear regression . Up until now you&#39;ve been using the default hyper-parameters when building your models. In this exercise you&#39;ll use cross validation to choose an optimal (or close to optimal) set of model hyper-parameters. . The following have already been created: . regression — a LinearRegression object | pipeline — a pipeline with string indexer, one-hot encoder, vector assembler and linear regression and | evaluator — a RegressionEvaluator object. | . Instructions . 100 XP . Create a parameter grid builder. | Add grids for with regression.regParam (values 0.01, 0.1, 1.0, and 10.0) and regression.elasticNetParam (values 0.0, 0.5, and 1.0). | Build the grid. | Create a cross validator, specifying five folds. | . params = ParamGridBuilder() # Add grids for two parameters params = params.addGrid(regression.regParam, [0.01, 0.1, 1.0, 10.0]) .addGrid(regression.elasticNetParam, [0.0, 0.5, 1.0]) # Build the parameter grid params = params.build() print(&#39;Number of models to be tested: &#39;, len(params)) # Create cross-validator cv = CrossValidator(estimator=pipeline, estimatorParamMaps=params, evaluator=evaluator, numFolds=5) . Nice! Multiple models are built effortlessly using grid search. . Dissecting the best flight duration model . You just set up a CrossValidator to find good parameters for the linear regression model predicting flight duration. . The model pipeline has multiple stages (objects of type StringIndexer, OneHotEncoderEstimator, VectorAssembler and LinearRegression), which operate in sequence. . Now you&#39;re going to take a closer look at the pipeline, split out the stages and use it to make predictions on the testing data. . The following have already been created: . cv — a trained CrossValidatorModel object and | evaluator — a RegressionEvaluator object. | . The flights data have been randomly split into flights_train and flights_test. . Instructions . 100 XP . Retrieve the best model. | Look at the stages in the best model. | Isolate the linear regression stage and extract its parameters. | Use the best model to generate predictions on the testing data and calculate the RMSE. | . best_model = cv.bestModel # Look at the stages in the best model print(best_model.stages) # Get the parameters for the LinearRegression object in the best model best_model.stages[3].extractParamMap() # Generate predictions on testing data using the best model then calculate RMSE predictions = best_model.transform(flights_test) evaluator.evaluate(predictions) . The best model performs pretty well on the testing data! . SMS spam optimised . The pipeline you built earlier for the SMS spam model used the default parameters for all of the elements in the pipeline. It&#39;s very unlikely that these parameters will give a particularly good model though. . In this exercise you&#39;ll set up a parameter grid which can be used with cross validation to choose a good set of parameters for the SMS spam classifier. . The following are already defined: . hasher — a HashingTF object and | logistic — a LogisticRegression object. | . Instructions . 100 XP . Instructions . 100 XP . Create a parameter grid builder object. | Add grid points for numFeatures and binary parameters to the HashingTF object, giving values 1024, 4096 and 16384, and True and False, respectively. | Add grid points for regParam and elasticNetParam parameters to the LogisticRegression object, giving values of 0.01, 0.1, 1.0 and 10.0, and 0.0, 0.5, and 1.0 respectively. | Build the parameter grid. | . params = ParamGridBuilder() # Add grid for hashing trick parameters params = params.addGrid(hasher.numFeatures, (1024, 4096, 16384)) .addGrid(hasher.binary, (True, False)) # Add grid for logistic regression parameters params = params.addGrid(logistic.regParam, (0.01, 0.1, 1.0, 10.0)) .addGrid(logistic.elasticNetParam, (0.0, 0.5, 1.0)) # Build parameter grid params = params.build() . Using cross-validation on a pipeline makes it possible to optimise each stage in the workflow. . How many models for grid search? . How many models will be built when the cross-validator below is fit to data? . params = ParamGridBuilder().addGrid(hasher.numFeatures, [1024, 4096, 16384]) .addGrid(hasher.binary, [True, False]) .addGrid(logistic.regParam, [0.01, 0.1, 1.0, 10.0]) .addGrid(logistic.elasticNetParam, [0.0, 0.5, 1.0]) .build() cv = CrossValidator(..., estimatorParamMaps=params, numFolds=5) . Answer the question . 50XP . Possible Answers . 3 | . 5 | . 72 | . 360 | . Correct! There are 72 points in the parameter grid and 5 folds in the cross-validator. The product is 360. It takes time to build all of those models, which is why we&#39;re not doing it here! . Ensemble . Delayed flights with Gradient-Boosted Trees . You&#39;ve previously built a classifier for flights likely to be delayed using a Decision Tree. In this exercise you&#39;ll compare a Decision Tree model to a Gradient-Boosted Trees model. . The flights data have been randomly split into flights_train and flights_test. . Instructions . 100 XP . Import the classes required to create Decision Tree and Gradient-Boosted Tree classifiers. | Create Decision Tree and Gradient-Boosted Tree classifiers. Train on the training data. | Create an evaluator and calculate AUC on testing data for both classifiers. Which model performs better? | For the Gradient-Boosted Tree classifier print the number of trees and the relative importance of features. | . from pyspark.ml.classification import DecisionTreeClassifier, GBTClassifier from pyspark.ml.evaluation import BinaryClassificationEvaluator # Create model objects and train on training data tree = DecisionTreeClassifier().fit(flights_train) gbt = GBTClassifier().fit(flights_train) # Compare AUC on testing data evaluator = BinaryClassificationEvaluator() evaluator.evaluate(tree.transform(flights_test)) evaluator.evaluate(gbt.transform(flights_test)) # Find the number of trees and the relative importance of features print(gbt.trees) print(gbt.featureImportances) . Good job! A Gradient-Boosted Tree almost always provides better performance than a plain Decision Tree. . Delayed flights with a Random Forest . In this exercise you&#39;ll bring together cross validation and ensemble methods. You&#39;ll be training a Random Forest classifier to predict delayed flights, using cross validation to choose the best values for model parameters. . You&#39;ll find good values for the following parameters: . featureSubsetStrategy — the number of features to consider for splitting at each node and | maxDepth — the maximum number of splits along any branch. | . Unfortunately building this model takes too long, so we won&#39;t be running the .fit() method on the pipeline. . Instructions . 100 XP . Create a random forest classifier object. | Create a parameter grid builder object. Add grid points for the featureSubsetStrategy and maxDepth parameters. | Create binary classification evaluator. | Create a cross-validator object, specifying the estimator, parameter grid and evaluator. Choose 5-fold cross validation. | . forest = RandomForestClassifier() # Create a parameter grid params = ParamGridBuilder() .addGrid(forest.featureSubsetStrategy, [&#39;all&#39;, &#39;onethird&#39;, &#39;sqrt&#39;, &#39;log2&#39;]) .addGrid(forest.maxDepth, [2, 5, 10]) .build() # Create a binary classification evaluator evaluator = BinaryClassificationEvaluator() # Create a cross-validator cv = CrossValidator(estimator=forest, estimatorParamMaps=params, evaluator=evaluator, numFolds=5) . Excellent! A grid search can be used to optimize all of the parameters in a model pipeline. . Evaluating Random Forest . In this final exercise you&#39;ll be evaluating the results of cross-validation on a Random Forest model. . The following have already been created: . cv - a cross-validator which has already been fit to the training data | evaluator — a BinaryClassificationEvaluator object and | flights_test — the testing data. | . Instructions . 100 XP . Print a list of average AUC metrics across all models in the parameter grid. | Display the average AUC for the best model. This will be the largest AUC in the list. | Print an explanation of the maxDepth and featureSubsetStrategy parameters for the best model. | Display the AUC for the best model predictions on the testing data. | . print(cv.avgMetrics) # Average AUC for the best model print(max(cv.avgMetrics)) # What&#39;s the optimal parameter value for maxDepth? print(cv.bestModel.explainParam(&#39;maxDepth&#39;)) # What&#39;s the optimal parameter value for featureSubsetStrategy? print(cv.bestModel.explainParam(&#39;featureSubsetStrategy&#39;)) # AUC for best model on testing data print(evaluator.evaluate(cv.transform(flights_test))) . Fantastic! Optimized Random Forest &gt; Random Forest &gt; Decision Tree . Closing thoughts .",
            "url": "https://islamalam.github.io/blog/2021/12/26/machine-learning-with-pyspark.html",
            "relUrl": "/2021/12/26/machine-learning-with-pyspark.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "1. Getting to know PySpark",
            "content": "Introduction to PySpark . Course Description: . In this course, you&#39;ll learn how to use Spark from Python! Spark is a tool for doing parallel computation with large datasets and it integrates well with Python. PySpark is the Python package that makes the magic happen. You&#39;ll use this package to work with data about flights from Portland and Seattle. You&#39;ll learn to wrangle this data and build a whole machine learning pipeline to predict whether or not flights will be delayed. Get ready to put some Spark in your Python code and dive into the world of high-performance machine learning! . ! pip install pyspark . Collecting pyspark Downloading pyspark-3.2.0.tar.gz (281.3 MB) |████████████████████████████████| 281.3 MB 38 kB/s Collecting py4j==0.10.9.2 Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB) |████████████████████████████████| 198 kB 40.8 MB/s Building wheels for collected packages: pyspark Building wheel for pyspark (setup.py) ... done Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=fab35528b694432280a1206b41c03efeef4945f96ea5965e75c2cf01dba9f583 Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718 Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9.2 pyspark-3.2.0 . import pyspark import numpy as np import pandas as pd . 1.1 What is Spark, anyway? . Spark is a platform for cluster computing. Spark lets you spread data and computations over clusters with multiple nodes (think of each node as a separate computer). Splitting up your data makes it easier to work with very large datasets because each node only works with a small amount of data. . As each node works on its own subset of the total data, it also carries out a part of the total calculations required, so that both data processing and computation are performed in parallel over the nodes in the cluster. It is a fact that parallel computation can make certain types of programming tasks much faster. . However, with greater computing power comes greater complexity. . Deciding whether or not Spark is the best solution for your problem takes some experience, but you can consider questions like: . Is my data too big to work with on a single machine? | Can my calculations be easily parallelized? | . . Are you excited to learn more about Spark? . Answer the question . 50XP . Possible Answers . No way! . | Yes way! . | . 1.2 Using Spark in Python . The first step in using Spark is connecting to a cluster. . In practice, the cluster will be hosted on a remote machine that&#39;s connected to all other nodes. There will be one computer, called the master that manages splitting up the data and the computations. The master is connected to the rest of the computers in the cluster, which are called worker. The master sends the workers data and calculations to run, and they send their results back to the master. . When you&#39;re just getting started with Spark it&#39;s simpler to just run a cluster locally. Thus, for this course, instead of connecting to another computer, all computations will be run on DataCamp&#39;s servers in a simulated cluster. . Creating the connection is as simple as creating an instance of the SparkContext class. The class constructor takes a few optional arguments that allow you to specify the attributes of the cluster you&#39;re connecting to. . An object holding all these attributes can be created with the SparkConf() constructor. Take a look at the documentation for all the details! . For the rest of this course you&#39;ll have a SparkContext called sc already available in your workspace. . . How do you connect to a Spark cluster from PySpark? . Answer the question . 50XP . Possible Answers . Create an instance of the SparkContext class. . | Import the pyspark library. . | Plug your computer into the cluster. . | . Great job! I knew you were paying attention. . 1.3 Examining The SparkContext . In this exercise you&#39;ll get familiar with the SparkContext. . You&#39;ll probably notice that code takes longer to run than you might expect. This is because Spark is some serious software. It takes more time to start up than you might be used to. You may also find that running simpler computations might take longer than expected. That&#39;s because all the optimizations that Spark has under its hood are designed for complicated operations with big data sets. That means that for simple or small problems Spark may actually perform worse than some other solutions! . Instructions . 100 XP . Get to know the SparkContext. . Call print() on sc to verify there&#39;s a SparkContext in your environment. | print() sc.version to see what version of Spark is running on your cluster. | . sc = pyspark.SparkContext() . print(sc) # Print Spark version print(sc.version) . &lt;SparkContext master=local[*] appName=pyspark-shell&gt; 3.2.0 . Awesome! You&#39;re up and running with Spark. . 1.4 Using DataFrames . Spark&#39;s core data structure is the Resilient Distributed Dataset (RDD). This is a low level object that lets Spark work its magic by splitting data across multiple nodes in the cluster. However, RDDs are hard to work with directly, so in this course you&#39;ll be using the Spark DataFrame abstraction built on top of RDDs. . The Spark DataFrame was designed to behave a lot like a SQL table (a table with variables in the columns and observations in the rows). Not only are they easier to understand, DataFrames are also more optimized for complicated operations than RDDs. . When you start modifying and combining columns and rows of data, there are many ways to arrive at the same result, but some often take much longer than others. When using RDDs, it&#39;s up to the data scientist to figure out the right way to optimize the query, but the DataFrame implementation has much of this optimization built in! . To start working with Spark DataFrames, you first have to create a SparkSession object from your SparkContext. You can think of the SparkContext as your connection to the cluster and the SparkSession as your interface with that connection. . Remember, for the rest of this course you&#39;ll have a SparkSession called spark available in your workspace! . . Which of the following is an advantage of Spark DataFrames over RDDs? . Answer the question . 50XP . Possible Answers . Operations using DataFrames are automatically optimized. . | They are smaller. . | They can perform more kinds of operations. . | They can hold more kinds of data. . | . Exactly! This is another way DataFrames are like SQL tables. . 1.5 Creating a SparkSession . from pyspark.sql import SparkSession # Create my_spark my_spark = SparkSession.builder.getOrCreate() # Print my_spark print(my_spark) . &lt;pyspark.sql.session.SparkSession object at 0x7f537909d3d0&gt; . Great work! You did that like a PySpark Pro! . 1.6 Viewing tables . Once you&#39;ve created a SparkSession, you can start poking around to see what data is in your cluster! . Your SparkSession has an attribute called catalog which lists all the data inside the cluster. This attribute has a few methods for extracting different pieces of information. . One of the most useful is the .listTables() method, which returns the names of all the tables in your cluster as a list. . Instructions . 100 XP . See what tables are in your cluster by calling spark.catalog.listTables() and printing the result! | . spark = (SparkSession .builder .appName(&quot;flights&quot;) .getOrCreate()) # Path to data set csv_file = &quot;./dataset/flights_small.csv&quot; # Read and create a temporary view # Infer schema (note that for larger files you # may want to specify the schema) flights = (spark.read.format(&quot;csv&quot;) .option(&quot;inferSchema&quot;, &quot;true&quot;) .option(&quot;header&quot;, &quot;true&quot;) .load(csv_file)) flights.createOrReplaceTempView(&quot;flights&quot;) . print(spark.catalog.listTables()) . Fantastic! What kind of data do you think is in that table? . 1.7 Are you query-ious? . One of the advantages of the DataFrame interface is that you can run SQL queries on the tables in your Spark cluster. If you don&#39;t have any experience with SQL, don&#39;t worry, we&#39;ll provide you with queries! (To learn more SQL, start with our Introduction to SQL course.) . As you saw in the last exercise, one of the tables in your cluster is the flights table. This table contains a row for every flight that left Portland International Airport (PDX) or Seattle-Tacoma International Airport (SEA) in 2014 and 2015. . Running a query on this table is as easy as using the .sql() method on your SparkSession. This method takes a string containing the query and returns a DataFrame with the results! . If you look closely, you&#39;ll notice that the table flights is only mentioned in the query, not as an argument to any of the methods. This is because there isn&#39;t a local object in your environment that holds that data, so it wouldn&#39;t make sense to pass the table as an argument. . Remember, we&#39;ve already created a SparkSession called spark in your workspace. (It&#39;s no longer called my_spark because we created it for you!) . Instructions . 100 XP . Instructions . 100 XP . Use the .sql() method to get the first 10 rows of the flights table and save the result to flights10. The variable query contains the appropriate SQL query. | Use the DataFrame method .show() to print flights10. | . query = &quot;FROM flights SELECT * LIMIT 10&quot; # Get the first 10 rows of flights flights10 = spark.sql(query) # Show the results flights10.show() . Awesome work! You&#39;ve got queries down! . 1.8 Pandafy a Spark DataFrame . Suppose you&#39;ve run a query on your huge dataset and aggregated it down to something a little more manageable. . Sometimes it makes sense to then take that table and work with it locally using a tool like pandas. Spark DataFrames make that easy with the .toPandas() method. Calling this method on a Spark DataFrame returns the corresponding pandas DataFrame. It&#39;s as simple as that! . This time the query counts the number of flights to each airport from SEA and PDX. . Remember, there&#39;s already a SparkSession called spark in your workspace! . Instructions . 100 XP . Run the query using the .sql() method. Save the result in flight_counts. | Use the .toPandas() method on flight_counts to create a pandas DataFrame called pd_counts. | Print the .head() of pd_counts to the console. | . query = &quot;SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest&quot; # Run the query flight_counts = spark.sql(query) # Convert the results to a pandas DataFrame pd_counts = flight_counts.toPandas() # Print the head of pd_counts print(pd_counts.head()) . Great job! You did it! . 1.9 Put some Spark in your data . In the last exercise, you saw how to move data from Spark to pandas. However, maybe you want to go the other direction, and put a pandas DataFrame into a Spark cluster! The SparkSession class has a method for this as well. . The .createDataFrame() method takes a pandas DataFrame and returns a Spark DataFrame. . The output of this method is stored locally, not in the SparkSession catalog. This means that you can use all the Spark DataFrame methods on it, but you can&#39;t access the data in other contexts. . For example, a SQL query (using the .sql() method) that references your DataFrame will throw an error. To access the data in this way, you have to save it as a temporary table. . You can do this using the .createTempView() Spark DataFrame method, which takes as its only argument the name of the temporary table you&#39;d like to register. This method registers the DataFrame as a table in the catalog, but as this table is temporary, it can only be accessed from the specific SparkSession used to create the Spark DataFrame. . There is also the method .createOrReplaceTempView(). This safely creates a new temporary table if nothing was there before, or updates an existing table if one was already defined. You&#39;ll use this method to avoid running into problems with duplicate tables. . Check out the diagram to see all the different ways your Spark data structures interact with each other. . . . There&#39;s already a SparkSession called spark in your workspace, numpy has been imported as np, and pandas as pd. . Instructions . 100 XP . Instructions . 100 XP . The code to create a pandas DataFrame of random numbers has already been provided and saved under pd_temp. | Create a Spark DataFrame called spark_temp by calling the Spark method .createDataFrame() with pd_temp as the argument. | Examine the list of tables in your Spark cluster and verify that the new DataFrame is not present. Remember you can use spark.catalog.listTables() to do so. | Register the spark_temp DataFrame you just created as a temporary table using the .createOrReplaceTempView() method. THe temporary table should be named &quot;temp&quot;. Remember that the table name is set including it as the only argument to your method! | Examine the list of tables again. | . pd_temp = pd.DataFrame(np.random.random(10)) # Create spark_temp from pd_temp spark_temp = spark.createDataFrame(pd_temp) # Examine the tables in the catalog print(spark.catalog.listTables()) # Add spark_temp to the catalog spark_temp.createOrReplaceTempView(&#39;temp&#39;) # Examine the tables in the catalog again print(spark.catalog.listTables()) . Awesome! Now you can get your data in and out of Spark. . 1.10 Dropping the middle man . Now you know how to put data into Spark via pandas, but you&#39;re probably wondering why deal with pandas at all? Wouldn&#39;t it be easier to just read a text file straight into Spark? Of course it would! . Luckily, your SparkSession has a .read attribute which has several methods for reading different data sources into Spark DataFrames. Using these you can create a DataFrame from a .csv file just like with regular pandas DataFrames! . The variable file_path is a string with the path to the file airports.csv. This file contains information about different airports all over the world. . A SparkSession named spark is available in your workspace. . Instructions . 100 XP . Instructions . 100 XP . Use the .read.csv() method to create a Spark DataFrame called airports The first argument is file_path | Pass the argument header=True so that Spark knows to take the column names from the first line of the file. | . | Print out this DataFrame by calling .show(). | . file_path = &quot;/usr/local/share/datasets/airports.csv&quot; # Read in the airports data airports = spark.read.csv(file_path, header=True) # Show the data airports.show() . Awesome job! You&#39;ve got the basics of Spark under your belt! . 2. Manipulating data . In this chapter, you&#39;ll learn about the pyspark.sql module, which provides optimized data queries to your Spark session. . 2.1 Creating columns . In this chapter, you&#39;ll learn how to use the methods defined by Spark&#39;s DataFrame class to perform common data operations. . Let&#39;s look at performing column-wise operations. In Spark you can do this using the .withColumn() method, which takes two arguments. First, a string with the name of your new column, and second the new column itself. . The new column must be an object of class Column. Creating one of these is as easy as extracting a column from your DataFrame using df.colName. . Updating a Spark DataFrame is somewhat different than working in pandas because the Spark DataFrame is immutable. This means that it can&#39;t be changed, and so columns can&#39;t be updated in place. . Thus, all these methods return a new DataFrame. To overwrite the original DataFrame you must reassign the returned DataFrame using the method like so: . df = df.withColumn(&quot;newCol&quot;, df.oldCol + 1) . The above code creates a DataFrame with the same columns as df plus a new column, newCol, where every entry is equal to the corresponding entry from oldCol, plus one. . To overwrite an existing column, just pass the name of the column as the first argument! . Remember, a SparkSession called spark is already in your workspace. . Instructions . 100 XP . Use the spark.table() method with the argument &quot;flights&quot; to create a DataFrame containing the values of the flights table in the .catalog. Save it as flights. | Show the head of flights using flights.show(). Check the output: the column air_time contains the duration of the flight in minutes. | Update flights to include a new column called duration_hrs, that contains the duration of each flight in hours (you&#39;ll need to divide air_time by the number of minutes in an hour). | . flights = spark.table(&#39;flights&#39;) # Show the head flights.show() # Add duration_hrs flights = flights.withColumn(&#39;duration_hrs&#39;, flights.air_time / 60) . 2.2 SQL in a nutshell . As you move forward, it will help to have a basic understanding of SQL. A more in depth look can be found here. . A SQL query returns a table derived from one or more tables contained in a database. . Every SQL query is made up of commands that tell the database what you want to do with the data. The two commands that every query has to contain are SELECT and FROM. . The SELECT command is followed by the columns you want in the resulting table. . The FROM command is followed by the name of the table that contains those columns. The minimal SQL query is: . SELECT * FROM my_table; . The * selects all columns, so this returns the entire table named my_table. . Similar to .withColumn(), you can do column-wise computations within a SELECT statement. For example, . SELECT origin, dest, air_time / 60 FROM flights; . returns a table with the origin, destination, and duration in hours for each flight. . Another commonly used command is WHERE. This command filters the rows of the table based on some logical condition you specify. The resulting table contains the rows where your condition is true. For example, if you had a table of students and grades you could do: . SELECT * FROM students WHERE grade = &#39;A&#39;; . to select all the columns and the rows containing information about students who got As. . . Which of the following queries returns a table of tail numbers and destinations for flights that lasted more than 10 hours? . Answer the question . 50XP . Possible Answers . SELECT dest, tail_num FROM flights WHERE air_time &gt; 10; | . SELECT dest, tail_num FROM flights WHERE air_time &gt; 600; | . SELECT * FROM flights WHERE air_time &gt; 600; | . Great work! You&#39;re a SQL wizard! . 2.3 SQL in a nutshell (2) . Another common database task is aggregation. That is, reducing your data by breaking it into chunks and summarizing each chunk. . This is done in SQL using the GROUP BY command. This command breaks your data into groups and applies a function from your SELECT statement to each group. . For example, if you wanted to count the number of flights from each of two origin destinations, you could use the query . SELECT COUNT(*) FROM flights GROUP BY origin; . GROUP BY origin tells SQL that you want the output to have a row for each unique value of the origin column. The SELECT statement selects the values you want to populate each of the columns. Here, we want to COUNT() every row in each of the groups. . It&#39;s possible to GROUP BY more than one column. When you do this, the resulting table has a row for every combination of the unique values in each column. The following query counts the number of flights from SEA and PDX to every destination airport: . SELECT origin, dest, COUNT(*) FROM flights GROUP BY origin, dest; . The output will have a row for every combination of the values in origin and dest (i.e. a row listing each origin and destination that a flight flew to). There will also be a column with the COUNT() of all the rows in each group. . Remember, a more in depth look at SQL can be found here. . . What information would this query get? Remember the flights table holds information about flights that departed PDX and SEA in 2014 and 2015. Note that AVG() function gets the average value of a column! . SELECT AVG(air_time) / 60 FROM flights GROUP BY origin, carrier; . Answer the question . 50XP . Possible Answers . The average length of each airline&#39;s flights from SEA and from PDX in hours. | . The average length of each flight. | . The average length of each airline&#39;s flights. | . Awesome! You&#39;ve got this SQL stuff down! . 2.4 Filtering Data . Now that you have a bit of SQL know-how under your belt, it&#39;s easier to talk about the analogous operations using Spark DataFrames. . Let&#39;s take a look at the .filter() method. As you might suspect, this is the Spark counterpart of SQL&#39;s WHERE clause. The .filter() method takes either an expression that would follow the WHERE clause of a SQL expression as a string, or a Spark Column of boolean (True/False) values. . For example, the following two expressions will produce the same output: . flights.filter(&quot;air_time &gt; 120&quot;).show() flights.filter(flights.air_time &gt; 120).show() . Notice that in the first case, we pass a string to .filter(). In SQL, we would write this filtering task as SELECT * FROM flights WHERE air_time &gt; 120. Spark&#39;s .filter() can accept any expression that could go in the WHEREclause of a SQL query (in this case, &quot;air_time &gt; 120&quot;), as long as it is passed as a string. Notice that in this case, we do not reference the name of the table in the string -- as we wouldn&#39;t in the SQL request. . In the second case, we actually pass a column of boolean values to .filter(). Remember that flights.air_time &gt; 120 returns a column of boolean values that has True in place of those records in flights.air_time that are over 120, and False otherwise. . Remember, a SparkSession called spark is already in your workspace, along with the Spark DataFrame flights. . Instructions . 100 XP . Use the .filter() method to find all the flights that flew over 1000 miles two ways: First, pass a SQL string to .filter() that checks whether the distance is greater than 1000. Save this as long_flights1. | Then pass a column of boolean values to .filter() that checks the same thing. Save this as long_flights2. | . | Use .show() to print heads of both DataFrames and make sure they&#39;re actually equal! | . long_flights1 = flights.filter(&quot;distance &gt; 1000&quot;) # Filter flights by passing a column of boolean values long_flights2 = flights.filter(flights.distance &gt; 1000) # Print the data to check they&#39;re equal long_flights1.show() long_flights2.show() . Awesome! PySpark often provides a few different ways to get the same results. . 2.5 Selecting . The Spark variant of SQL&#39;s SELECT is the .select() method. This method takes multiple arguments - one for each column you want to select. These arguments can either be the column name as a string (one for each column) or a column object (using the df.colName syntax). When you pass a column object, you can perform operations like addition or subtraction on the column to change the data contained in it, much like inside .withColumn(). . The difference between .select() and .withColumn() methods is that .select() returns only the columns you specify, while .withColumn() returns all the columns of the DataFrame in addition to the one you defined. It&#39;s often a good idea to drop columns you don&#39;t need at the beginning of an operation so that you&#39;re not dragging around extra data as you&#39;re wrangling. In this case, you would use .select() and not .withColumn(). . Remember, a SparkSession called spark is already in your workspace, along with the Spark DataFrame flights. . Instructions . 100 XP . Select the columns &quot;tailnum&quot;, &quot;origin&quot;, and &quot;dest&quot; from flights by passing the column names as strings. Save this as selected1. | Select the columns &quot;origin&quot;, &quot;dest&quot;, and &quot;carrier&quot; using the df.colName syntax and then filter the result using both of the filters already defined for you (filterA and filterB) to only keep flights from SEA to PDX. Save this as selected2. | . selected1 = flights.select(&quot;tailnum&quot;, &quot;origin&quot;, &quot;dest&quot;) # Select the second set of columns temp = flights.select(flights.origin, flights.dest, flights.carrier) # Define first filter filterA = flights.origin == &quot;SEA&quot; # Define second filter filterB = flights.dest == &quot;PDX&quot; # Filter the data, first by filterA then by filterB selected2 = temp.filter(filterA).filter(filterB) . Great work! You&#39;re speeding right through this course! . 2.6 Selecting II . Similar to SQL, you can also use the .select() method to perform column-wise operations. When you&#39;re selecting a column using the df.colName notation, you can perform any column operation and the .select() method will return the transformed column. For example, . flights.select(flights.air_time/60) . returns a column of flight durations in hours instead of minutes. You can also use the .alias() method to rename a column you&#39;re selecting. So if you wanted to .select() the column duration_hrs (which isn&#39;t in your DataFrame) you could do . flights.select((flights.air_time/60).alias(&quot;duration_hrs&quot;)) . The equivalent Spark DataFrame method .selectExpr() takes SQL expressions as a string: . flights.selectExpr(&quot;air_time/60 as duration_hrs&quot;) . with the SQL as keyword being equivalent to the .alias() method. To select multiple columns, you can pass multiple strings. . Remember, a SparkSession called spark is already in your workspace, along with the Spark DataFrame flights. . Instructions . 100 XP . Instructions . 100 XP . Create a table of the average speed of each flight both ways. . Calculate average speed by dividing the distance by the air_time (converted to hours). Use the .alias() method name this column &quot;avg_speed&quot;. Save the output as the variable avg_speed. | Select the columns &quot;origin&quot;, &quot;dest&quot;, &quot;tailnum&quot;, and avg_speed (without quotes!). Save this as speed1. | Create the same table using .selectExpr() and a string containing a SQL expression. Save this as speed2. | . avg_speed = (flights.distance/(flights.air_time/60)).alias(&quot;avg_speed&quot;) # Select the correct columns speed1 = flights.select(&quot;origin&quot;, &quot;dest&quot;, &quot;tailnum&quot;, avg_speed) # Create the same table using a SQL expression speed2 = flights.selectExpr(&quot;origin&quot;, &quot;dest&quot;, &quot;tailnum&quot;, &quot;distance/(air_time/60) as avg_speed&quot;) . Wow! You&#39;re doing great! . 2.7 Aggregating . All of the common aggregation methods, like .min(), .max(), and .count() are GroupedData methods. These are created by calling the .groupBy() DataFrame method. You&#39;ll learn exactly what that means in a few exercises. For now, all you have to do to use these functions is call that method on your DataFrame. For example, to find the minimum value of a column, col, in a DataFrame, df, you could do . df.groupBy().min(&quot;col&quot;).show() . This creates a GroupedData object (so you can use the .min() method), then finds the minimum value in col, and returns it as a DataFrame. . Now you&#39;re ready to do some aggregating of your own! . A SparkSession called spark is already in your workspace, along with the Spark DataFrame flights. . Instructions . 100 XP . Find the length of the shortest (in terms of distance) flight that left PDX by first .filter()ing and using the .min() method. Perform the filtering by referencing the column directly, not passing a SQL string. | Find the length of the longest (in terms of time) flight that left SEA by filter()ing and using the .max() method. Perform the filtering by referencing the column directly, not passing a SQL string. | . flights.filter(flights.origin == &#39;PDX&#39;).groupBy().min(&#39;distance&#39;).show() # Find the longest flight from SEA in terms of air time flights.filter(flights.origin == &#39;SEA&#39;).groupBy().max(&#39;air_time&#39;).show() . Fantastic work! How do these methods help you learn about your data? . 2.8 Aggregating II . To get you familiar with more of the built in aggregation methods, here&#39;s a few more exercises involving the flights table! . Remember, a SparkSession called spark is already in your workspace, along with the Spark DataFrame flights. . Instructions . 100 XP . Use the .avg() method to get the average air time of Delta Airlines flights (where the carrier column has the value &quot;DL&quot;) that left SEA. The place of departure is stored in the column origin. show() the result. | Use the .sum() method to get the total number of hours all planes in this dataset spent in the air by creating a column called duration_hrs from the column air_time. show() the result. | . flights.filter(flights.carrier == &#39;DL&#39;).filter(flights.origin == &#39;SEA&#39;).groupBy().avg(&#39;air_time&#39;).show() # Total hours in the air flights.withColumn(&#39;duration_hrs&#39;, flights.air_time / 60).groupBy().sum(&#39;duration_hrs&#39;).show() . Stellar job! Now you can answer some interesting questions about the data. . &lt;script.py&gt; output: ++ | avg(air_time)| ++ |188.20689655172413| ++ ++ | sum(duration_hrs)| ++ |25289.600000000126| ++ . 2.9 Grouping and Aggregating I . Part of what makes aggregating so powerful is the addition of groups. PySpark has a whole class devoted to grouped data frames: pyspark.sql.GroupedData, which you saw in the last two exercises. . You&#39;ve learned how to create a grouped DataFrame by calling the .groupBy() method on a DataFrame with no arguments. . Now you&#39;ll see that when you pass the name of one or more columns in your DataFrame to the .groupBy() method, the aggregation methods behave like when you use a GROUP BY statement in a SQL query! . Remember, a SparkSession called spark is already in your workspace, along with the Spark DataFrame flights. . Instructions . 100 XP . Instructions . 100 XP . Create a DataFrame called by_plane that is grouped by the column tailnum. | Use the .count() method with no arguments to count the number of flights each plane made. | Create a DataFrame called by_origin that is grouped by the column origin. | Find the .avg() of the air_time column to find average duration of flights from PDX and SEA. | . by_plane = flights.groupBy(&#39;tailnum&#39;) # Number of flights each plane made by_plane.count().show() # Group by origin by_origin = flights.groupBy(&quot;origin&quot;) # Average duration of flights from PDX and SEA by_origin.avg(&#39;air_time&#39;).show() . Great work! You&#39;re passing with flying colors! . &lt;script.py&gt; output: +-+--+ |tailnum|count| +-+--+ | N442AS| 38| | N102UW| 2| | N36472| 4| | N38451| 4| | N73283| 4| | N513UA| 2| | N954WN| 5| | N388DA| 3| | N567AA| 1| | N516UA| 2| | N927DN| 1| | N8322X| 1| | N466SW| 1| | N6700| 1| | N607AS| 45| | N622SW| 4| | N584AS| 31| | N914WN| 4| | N654AW| 2| | N336NW| 1| +-+--+ only showing top 20 rows +++ |origin| avg(air_time)| +++ | SEA| 160.4361496051259| | PDX|137.11543248288737| +++ . 2.10 Grouping and Aggregating II . In addition to the GroupedData methods you&#39;ve already seen, there is also the .agg() method. This method lets you pass an aggregate column expression that uses any of the aggregate functions from the pyspark.sql.functions submodule. . This submodule contains many useful functions for computing things like standard deviations. All the aggregation functions in this submodule take the name of a column in a GroupedData table. . Remember, a SparkSession called spark is already in your workspace, along with the Spark DataFrame flights. The grouped DataFrames you created in the last exercise are also in your workspace. . Instructions . 100 XP . Instructions . 100 XP . Import the submodule pyspark.sql.functions as F. | Create a GroupedData table called by_month_dest that&#39;s grouped by both the month and dest columns. Refer to the two columns by passing both strings as separate arguments. | Use the .avg() method on the by_month_dest DataFrame to get the average dep_delay in each month for each destination. | Find the standard deviation of dep_delay by using the .agg() method with the function F.stddev(). | . import pyspark.sql.functions as F # Group by month and est by_month_dest = flights.groupBy(&#39;month&#39;, &#39;dest&#39;) # Average departure delay by month and destination by_month_dest.avg(&#39;dep_delay&#39;).show() # Standard deviation of departure delay by_month_dest.agg(F.stddev(&#39;dep_delay&#39;)).show() . Amazing! You&#39;re learning so much from just a few simple methods! . &lt;script.py&gt; output: +--+-+--+ |month|dest| avg(dep_delay)| +--+-+--+ | 11| TUS| -2.3333333333333335| | 11| ANC| 7.529411764705882| | 1| BUR| -1.45| | 1| PDX| -5.6923076923076925| | 6| SBA| -2.5| | 5| LAX|-0.15789473684210525| | 10| DTW| 2.6| | 6| SIT| -1.0| | 10| DFW| 18.176470588235293| | 3| FAI| -2.2| | 10| SEA| -0.8| | 2| TUS| -0.6666666666666666| | 12| OGG| 25.181818181818183| | 9| DFW| 4.066666666666666| | 5| EWR| 14.25| | 3| RDM| -6.2| | 8| DCA| 2.6| | 7| ATL| 4.675675675675675| | 4| JFK| 0.07142857142857142| | 10| SNA| -1.1333333333333333| +--+-+--+ only showing top 20 rows +--+-+-+ |month|dest|stddev_samp(dep_delay)| +--+-+-+ | 11| TUS| 3.0550504633038935| | 11| ANC| 18.604716401245316| | 1| BUR| 15.22627576540667| | 1| PDX| 5.677214918493858| | 6| SBA| 2.380476142847617| | 5| LAX| 13.36268698685904| | 10| DTW| 5.639148871948674| | 6| SIT| NaN| | 10| DFW| 45.53019017606675| | 3| FAI| 3.1144823004794873| | 10| SEA| 18.70523227029577| | 2| TUS| 14.468356276140469| | 12| OGG| 82.64480404939947| | 9| DFW| 21.728629347782924| | 5| EWR| 42.41595968929191| | 3| RDM| 2.16794833886788| | 8| DCA| 9.946523680831074| | 7| ATL| 22.767001039582183| | 4| JFK| 8.156774303176903| | 10| SNA| 13.726234873756304| +--+-+-+ only showing top 20 rows . 2.11 Joining . Another very common data operation is the join. Joins are a whole topic unto themselves, so in this course we&#39;ll just look at simple joins. If you&#39;d like to learn more about joins, you can take a look here. . A join will combine two different tables along a column that they share. This column is called the key. Examples of keys here include the tailnum and carrier columns from the flights table. . For example, suppose that you want to know more information about the plane that flew a flight than just the tail number. This information isn&#39;t in the flights table because the same plane flies many different flights over the course of two years, so including this information in every row would result in a lot of duplication. To avoid this, you&#39;d have a second table that has only one row for each plane and whose columns list all the information about the plane, including its tail number. You could call this table planes . When you join the flights table to this table of airplane information, you&#39;re adding all the columns from the planes table to the flights table. To fill these columns with information, you&#39;ll look at the tail number from the flights table and find the matching one in the planes table, and then use that row to fill out all the new columns. . Now you&#39;ll have a much bigger table than before, but now every row has all information about the plane that flew that flight! . . Which of the following is not true? . Answer the question . 50XP . Possible Answers . Joins combine tables. | . Joins add information to a table. | . Storing information in separate tables can reduce repetition. | . There is only one kind of join. | . Great job! If there were only one kind of join, it would be tough to create some more complicated kinds of tables. . 2.12 Joining II . In PySpark, joins are performed using the DataFrame method .join(). This method takes three arguments. The first is the second DataFrame that you want to join with the first one. The second argument, on, is the name of the key column(s) as a string. The names of the key column(s) must be the same in each table. The third argument, how, specifies the kind of join to perform. In this course we&#39;ll always use the value how=&quot;leftouter&quot;. . The flights dataset and a new dataset called airports are already in your workspace. . Instructions . 100 XP . Instructions . 100 XP . Examine the airports DataFrame by calling .show(). Note which key column will let you join airports to the flights table. | Rename the faa column in airports to dest by re-assigning the result of airports.withColumnRenamed(&quot;faa&quot;, &quot;dest&quot;) to airports. | Join the flights with the airports DataFrame on the dest column by calling the .join() method on flights. Save the result as flights_with_airports. The first argument should be the other DataFrame, airports. | The argument on should be the key column. | The argument how should be &quot;leftouter&quot;. | . | Call .show() on flights_with_airports to examine the data again. Note the new information that has been added. | . print(airports.show()) # rename the faa column airports = airports.withColumnRenamed(&#39;faa&#39;, &#39;dest&#39;) # Joint the DataFrames flights_with_airports = flights.join(airports, on=&#39;dest&#39;, how=&#39;leftouter&#39;) # Examine the new DataFrame print(flights_with_airports.show()) . Fantastic work! You&#39;re a data manipulation pro! . &lt;script.py&gt; output: ++--+-+--+-+++ |faa| name| lat| lon| alt| tz|dst| ++--+-+--+-+++ |04G| Lansdowne Airport| 41.1304722| -80.6195833|1044| -5| A| |06A|Moton Field Munic...| 32.4605722| -85.6800278| 264| -5| A| |06C| Schaumburg Regional| 41.9893408| -88.1012428| 801| -6| A| |06N| Randall Airport| 41.431912| -74.3915611| 523| -5| A| |09J|Jekyll Island Air...| 31.0744722| -81.4277778| 11| -4| A| |0A9|Elizabethton Muni...| 36.3712222| -82.1734167|1593| -4| A| |0G6|Williams County A...| 41.4673056| -84.5067778| 730| -5| A| |0G7|Finger Lakes Regi...| 42.8835647| -76.7812318| 492| -5| A| |0P2|Shoestring Aviati...| 39.7948244| -76.6471914|1000| -5| U| |0S9|Jefferson County ...| 48.0538086| -122.8106436| 108| -8| A| |0W3|Harford County Ai...| 39.5668378| -76.2024028| 409| -5| A| |10C| Galt Field Airport| 42.4028889| -88.3751111| 875| -6| U| |17G|Port Bucyrus-Craw...| 40.7815556| -82.9748056|1003| -5| A| |19A|Jackson County Ai...| 34.1758638| -83.5615972| 951| -4| U| |1A3|Martin Campbell F...| 35.0158056| -84.3468333|1789| -4| A| |1B9| Mansfield Municipal| 42.0001331| -71.1967714| 122| -5| A| |1C9|Frazier Lake Airpark|54.0133333333333|-124.768333333333| 152| -8| A| |1CS|Clow Internationa...| 41.6959744| -88.1292306| 670| -6| U| |1G3| Kent State Airport| 41.1513889| -81.4151111|1134| -4| A| |1OH| Fortman Airport| 40.5553253| -84.3866186| 885| -5| U| ++--+-+--+-+++ only showing top 20 rows None +-+-+--++--++--++-+-+++--+--+-++--++--+-+++ |dest|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|air_time|distance|hour|minute| name| lat| lon| alt| tz|dst| +-+-+--++--++--++-+-+++--+--+-++--++--+-+++ | LAX|2014| 12| 8| 658| -7| 935| -5| VX| N846VA| 1780| SEA| 132| 954| 6| 58| Los Angeles Intl|33.942536|-118.408075| 126| -8| A| | HNL|2014| 1| 22| 1040| 5| 1505| 5| AS| N559AS| 851| SEA| 360| 2677| 10| 40| Honolulu Intl|21.318681|-157.922428| 13|-10| N| | SFO|2014| 3| 9| 1443| -2| 1652| 2| VX| N847VA| 755| SEA| 111| 679| 14| 43| San Francisco Intl|37.618972|-122.374889| 13| -8| A| | SJC|2014| 4| 9| 1705| 45| 1839| 34| WN| N360SW| 344| PDX| 83| 569| 17| 5|Norman Y Mineta S...| 37.3626|-121.929022| 62| -8| A| | BUR|2014| 3| 9| 754| -1| 1015| 1| AS| N612AS| 522| SEA| 127| 937| 7| 54| Bob Hope|34.200667|-118.358667| 778| -8| A| | DEN|2014| 1| 15| 1037| 7| 1352| 2| WN| N646SW| 48| PDX| 121| 991| 10| 37| Denver Intl|39.861656|-104.673178|5431| -7| A| | OAK|2014| 7| 2| 847| 42| 1041| 51| WN| N422WN| 1520| PDX| 90| 543| 8| 47|Metropolitan Oakl...|37.721278|-122.220722| 9| -8| A| | SFO|2014| 5| 12| 1655| -5| 1842| -18| VX| N361VA| 755| SEA| 98| 679| 16| 55| San Francisco Intl|37.618972|-122.374889| 13| -8| A| | SAN|2014| 4| 19| 1236| -4| 1508| -7| AS| N309AS| 490| SEA| 135| 1050| 12| 36| San Diego Intl|32.733556|-117.189667| 17| -8| A| | ORD|2014| 11| 19| 1812| -3| 2352| -4| AS| N564AS| 26| SEA| 198| 1721| 18| 12| Chicago Ohare Intl|41.978603| -87.904842| 668| -6| A| | LAX|2014| 11| 8| 1653| -2| 1924| -1| AS| N323AS| 448| SEA| 130| 954| 16| 53| Los Angeles Intl|33.942536|-118.408075| 126| -8| A| | PHX|2014| 8| 3| 1120| 0| 1415| 2| AS| N305AS| 656| SEA| 154| 1107| 11| 20|Phoenix Sky Harbo...|33.434278|-112.011583|1135| -7| N| | LAS|2014| 10| 30| 811| 21| 1038| 29| AS| N433AS| 608| SEA| 127| 867| 8| 11| Mc Carran Intl|36.080056| -115.15225|2141| -8| A| | ANC|2014| 11| 12| 2346| -4| 217| -28| AS| N765AS| 121| SEA| 183| 1448| 23| 46|Ted Stevens Ancho...|61.174361|-149.996361| 152| -9| A| | SFO|2014| 10| 31| 1314| 89| 1544| 111| AS| N713AS| 306| SEA| 129| 679| 13| 14| San Francisco Intl|37.618972|-122.374889| 13| -8| A| | SFO|2014| 1| 29| 2009| 3| 2159| 9| UA| N27205| 1458| PDX| 90| 550| 20| 9| San Francisco Intl|37.618972|-122.374889| 13| -8| A| | SMF|2014| 12| 17| 2015| 50| 2150| 41| AS| N626AS| 368| SEA| 76| 605| 20| 15| Sacramento Intl|38.695417|-121.590778| 27| -8| A| | MDW|2014| 8| 11| 1017| -3| 1613| -7| WN| N8634A| 827| SEA| 216| 1733| 10| 17| Chicago Midway Intl|41.785972| -87.752417| 620| -6| A| | BOS|2014| 1| 13| 2156| -9| 607| -15| AS| N597AS| 24| SEA| 290| 2496| 21| 56|General Edward La...|42.364347| -71.005181| 19| -5| A| | BUR|2014| 6| 5| 1733| -12| 1945| -10| OO| N215AG| 3488| PDX| 111| 817| 17| 33| Bob Hope|34.200667|-118.358667| 778| -8| A| +-+-+--++--++--++-+-+++--+--+-++--++--+-+++ only showing top 20 rows None . 3. Getting started with machine learning pipelines . PySpark has built-in, cutting-edge machine learning routines, along with utilities to create full machine learning pipelines. You&#39;ll learn about them in this chapter. . 3.1 Machine Learning Pipelines . In the next two chapters you&#39;ll step through every stage of the machine learning pipeline, from data intake to model evaluation. Let&#39;s get to it! . At the core of the pyspark.ml module are the Transformer and Estimator classes. Almost every other class in the module behaves similarly to these two basic classes. . Transformer classes have a .transform() method that takes a DataFrame and returns a new DataFrame; usually the original one with a new column appended. For example, you might use the class Bucketizer to create discrete bins from a continuous feature or the class PCA to reduce the dimensionality of your dataset using principal component analysis. . Estimator classes all implement a .fit() method. These methods also take a DataFrame, but instead of returning another DataFrame they return a model object. This can be something like a StringIndexerModel for including categorical data saved as strings in your models, or a RandomForestModel that uses the random forest algorithm for classification or regression. . . Which of the following is not true about machine learning in Spark? . Answer the question . 50XP . Possible Answers . Spark&#39;s algorithms give better results than other algorithms. . | Working in Spark allows you to create reproducible machine learning pipelines. . | Machine learning pipelines in Spark are made up of Transformers and Estimators. . | PySpark uses the pyspark.ml submodule to interface with Spark&#39;s machine learning routines. . | . That&#39;s right! Spark is just a platform that implements the same algorithms that can be found elsewhere. . 3.2 Join the DataFrames . In the next two chapters you&#39;ll be working to build a model that predicts whether or not a flight will be delayed based on the flights data we&#39;ve been working with. This model will also include information about the plane that flew the route, so the first step is to join the two tables: flights and planes! . Instructions . 100 XP . First, rename the year column of planes to plane_year to avoid duplicate column names. | Create a new DataFrame called model_data by joining the flights table with planes using the tailnum column as the key. | . planes = planes.withColumnRenamed(&#39;year&#39;, &#39;plane_year&#39;) # Join the DataFrame model_data = flights.join(planes, on=&#39;tailnum&#39;, how=&#39;leftouter&#39;) . Awesome work! You&#39;re one step closer to a model! . 3.3 Data types . Good work! Before you get started modeling, it&#39;s important to know that Spark only handles numeric data. That means all of the columns in your DataFrame must be either integers or decimals (called &#39;doubles&#39; in Spark). . When we imported our data, we let Spark guess what kind of information each column held. Unfortunately, Spark doesn&#39;t always guess right and you can see that some of the columns in our DataFrame are strings containing numbers as opposed to actual numeric values. . To remedy this, you can use the .cast() method in combination with the .withColumn() method. It&#39;s important to note that .cast() works on columns, while .withColumn() works on DataFrames. . The only argument you need to pass to .cast() is the kind of value you want to create, in string form. For example, to create integers, you&#39;ll pass the argument &quot;integer&quot; and for decimal numbers you&#39;ll use &quot;double&quot;. . You can put this call to .cast() inside a call to .withColumn() to overwrite the already existing column, just like you did in the previous chapter! . . What kind of data does Spark need for modeling? . Answer the question . 50XP . Possible Answers . Doubles | . Integers | . Decimals | . Numeric | . Strings | . Great job! Spark needs numeric values (doubles or integers) to do machine learning. . 3.4 String to integer . Now you&#39;ll use the .cast() method you learned in the previous exercise to convert all the appropriate columns from your DataFrame model_data to integers! . To convert the type of a column using the .cast() method, you can write code like this: . dataframe = dataframe.withColumn(&quot;col&quot;, dataframe.col.cast(&quot;new_type&quot;)) . Instructions . 100 XP . Use the method .withColumn() to .cast() the following columns to type &quot;integer&quot;. Access the columns using the df.col notation: model_data.arr_delay | model_data.air_time | model_data.month | model_data.plane_year | . | . model_data = model_data.withColumn(&quot;arr_delay&quot;, model_data.arr_delay.cast(&#39;integer&#39;)) model_data = model_data.withColumn(&#39;air_time&#39;, model_data.air_time.cast(&#39;integer&#39;)) model_data = model_data.withColumn(&#39;month&#39;, model_data.month.cast(&#39;integer&#39;)) model_data = model_data.withColumn(&#39;plane_year&#39;, model_data.plane_year.cast(&#39;integer&#39;)) . Awesome! You&#39;re a pro at converting columns! . 3.5 Create a new column . In the last exercise, you converted the column plane_year to an integer. This column holds the year each plane was manufactured. However, your model will use the planes&#39; age, which is slightly different from the year it was made! . Instructions . 100 XP . Create the column plane_age using the .withColumn() method and subtracting the year of manufacture (column plane_year) from the year (column year) of the flight. | . model_data = model_data.withColumn(&#39;plane_age&#39;, model_data.year - model_data.plane_year) . Great work! Now you have one more variable to include in your model. . 3.6 Making a Boolean . Consider that you&#39;re modeling a yes or no question: is the flight late? However, your data contains the arrival delay in minutes for each flight. Thus, you&#39;ll need to create a boolean column which indicates whether the flight was late or not! . Instructions . 100 XP . Use the .withColumn() method to create the column is_late. This column is equal to model_data.arr_delay &gt; 0. | Convert this column to an integer column so that you can use it in your model and name it label (this is the default name for the response variable in Spark&#39;s machine learning routines). | Filter out missing values (this has been done for you). | . model_data = model_data.withColumn(&#39;is_late&#39;, model_data.arr_delay &gt; 0) # Convert to an integer model_data = model_data.withColumn(&#39;label&#39;, model_data.is_late.cast(&#39;integer&#39;)) # Remove missing values model_data = model_data.filter(&quot;arr_delay is not NULL and dep_delay is not NULL and air_time is not NULL and plane_year is not NULL&quot;) . Awesome! Now you&#39;ve defined the column that you&#39;re going to use as the outcome in your model. . 3.7 Strings and factors . As you know, Spark requires numeric data for modeling. So far this hasn&#39;t been an issue; even boolean columns can easily be converted to integers without any trouble. But you&#39;ll also be using the airline and the plane&#39;s destination as features in your model. These are coded as strings and there isn&#39;t any obvious way to convert them to a numeric data type. . Fortunately, PySpark has functions for handling this built into the pyspark.ml.features submodule. You can create what are called &#39;one-hot vectors&#39; to represent the carrier and the destination of each flight. A one-hot vector is a way of representing a categorical feature where every observation has a vector in which all elements are zero except for at most one element, which has a value of one (1). . Each element in the vector corresponds to a level of the feature, so it&#39;s possible to tell what the right level is by seeing which element of the vector is equal to one (1). . The first step to encoding your categorical feature is to create a StringIndexer. Members of this class are Estimators that take a DataFrame with a column of strings and map each unique string to a number. Then, the Estimator returns a Transformer that takes a DataFrame, attaches the mapping to it as metadata, and returns a new DataFrame with a numeric column corresponding to the string column. . The second step is to encode this numeric column as a one-hot vector using a OneHotEncoder. This works exactly the same way as the StringIndexer by creating an Estimator and then a Transformer. The end result is a column that encodes your categorical feature as a vector that&#39;s suitable for machine learning routines! . This may seem complicated, but don&#39;t worry! All you have to remember is that you need to create a StringIndexer and a OneHotEncoder, and the Pipeline will take care of the rest. . . Why do you have to encode a categorical feature as a one-hot vector? . Answer the question . 50XP . Possible Answers . It makes fitting the model faster. | . Spark can only model numeric features. | . For compatibility with scikit-learn. | . Awesome! You remembered that Spark can only model numeric features. . 3.8 Carrier . In this exercise you&#39;ll create a StringIndexer and a OneHotEncoder to code the carrier column. To do this, you&#39;ll call the class constructors with the arguments inputCol and outputCol. . The inputCol is the name of the column you want to index or encode, and the outputCol is the name of the new column that the Transformer should create. . Instructions . 100 XP . Create a StringIndexer called carr_indexer by calling StringIndexer() with inputCol=&quot;carrier&quot; and outputCol=&quot;carrier_index&quot;. | Create a OneHotEncoder called carr_encoder by calling OneHotEncoder() with inputCol=&quot;carrier_index&quot; and outputCol=&quot;carrier_fact&quot;. | . carr_indexer = StringIndexer(inputCol=&#39;carrier&#39;, outputCol=&#39;carrier_index&#39;) # Create a OneHotEncoder carr_encoder = OneHotEncoder(inputCol=&#39;carrier_index&#39;, outputCol=&#39;carrier_fact&#39;) . 3.9 Destination . Now you&#39;ll encode the dest column just like you did in the previous exercise. . Instructions . 100 XP . Create a StringIndexer called dest_indexer by calling StringIndexer() with inputCol=&quot;dest&quot; and outputCol=&quot;dest_index&quot;. | Create a OneHotEncoder called dest_encoder by calling OneHotEncoder() with inputCol=&quot;dest_index&quot; and outputCol=&quot;dest_fact&quot;. | . dest_indexer = StringIndexer(inputCol=&#39;dest&#39;, outputCol=&#39;dest_index&#39;) # Create a OneHotEncoder dest_encoder = OneHotEncoder(inputCol=&#39;dest_index&#39;, outputCol=&#39;dest_fact&#39;) . Perfect! You&#39;re all done messing with factors. . 3.10 Assemble a vector . The last step in the Pipeline is to combine all of the columns containing our features into a single column. This has to be done before modeling can take place because every Spark modeling routine expects the data to be in this form. You can do this by storing each of the values from a column as an entry in a vector. Then, from the model&#39;s point of view, every observation is a vector that contains all of the information about it and a label that tells the modeler what value that observation corresponds to. . Because of this, the pyspark.ml.feature submodule contains a class called VectorAssembler. This Transformer takes all of the columns you specify and combines them into a new vector column. . Instructions . 100 XP . Instructions . 100 XP . Create a VectorAssembler by calling VectorAssembler() with the inputCols names as a list and the outputCol name &quot;features&quot;. The list of columns should be [&quot;month&quot;, &quot;air_time&quot;, &quot;carrier_fact&quot;, &quot;dest_fact&quot;, &quot;plane_age&quot;]. | . | . vec_assembler = VectorAssembler(inputCols=[&quot;month&quot;, &quot;air_time&quot;, &quot;carrier_fact&quot;, &quot;dest_fact&quot;, &quot;plane_age&quot;], outputCol=&#39;features&#39;) . Good job! Your data is all assembled now. . 3.11 Create the pipeline . You&#39;re finally ready to create a Pipeline! . Pipeline is a class in the pyspark.ml module that combines all the Estimators and Transformers that you&#39;ve already created. This lets you reuse the same modeling process over and over again by wrapping it up in one simple object. Neat, right? . Instructions . 100 XP . Import Pipeline from pyspark.ml. | Call the Pipeline() constructor with the keyword argument stages to create a Pipeline called flights_pipe. stages should be a list holding all the stages you want your data to go through in the pipeline. Here this is just: [dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler] | . | . from pyspark.ml import Pipeline # Make the pipeline flights_pipe = Pipeline(stages=[dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler]) . Fantastic! You&#39;ve made a fully reproducible machine learning pipeline! . 3.12 Test vs Train . After you&#39;ve cleaned your data and gotten it ready for modeling, one of the most important steps is to split the data into a test set and a train set. After that, don&#39;t touch your test data until you think you have a good model! As you&#39;re building models and forming hypotheses, you can test them on your training data to get an idea of their performance. . Once you&#39;ve got your favorite model, you can see how well it predicts the new data in your test set. This never-before-seen data will give you a much more realistic idea of your model&#39;s performance in the real world when you&#39;re trying to predict or classify new data. . In Spark it&#39;s important to make sure you split the data after all the transformations. This is because operations like StringIndexer don&#39;t always produce the same index even when given the same list of strings. . . Why is it important to use a test set in model evaluation? . Answer the question . 50XP . Possible Answers . Evaluating your model improves its accuracy. . | By evaluating your model with a test set you can get a good idea of performance on new data. . | Using a test set lets you check your code for errors. . | . Exactly! A test set approximates the &#39;real world error&#39; of your model. . 3.13 Transform the data . Hooray, now you&#39;re finally ready to pass your data through the Pipeline you created! . Instructions . 100 XP . Create the DataFrame piped_data by calling the Pipeline methods .fit() and .transform() in a chain. Both of these methods take model_data as their only argument. | . piped_data = flights_pipe.fit(model_data).transform(model_data) . Great work! Your pipeline chewed right through that data! . 3.14 Split the data . Now that you&#39;ve done all your manipulations, the last step before modeling is to split the data! . Instructions . 100 XP . Use the DataFrame method .randomSplit() to split piped_data into two pieces, training with 60% of the data, and test with 40% of the data by passing the list [.6, .4] to the .randomSplit() method. | . training, test = piped_data.randomSplit([.6, .4]) . Awesome! Now you&#39;re ready to start fitting a model! . 4. Model tuning and selection . In this last chapter, you&#39;ll apply what you&#39;ve learned to create a model that predicts which flights will be delayed. . 4.1 What is logistic regression? . The model you&#39;ll be fitting in this chapter is called a logistic regression. This model is very similar to a linear regression, but instead of predicting a numeric variable, it predicts the probability (between 0 and 1) of an event. . To use this as a classification algorithm, all you have to do is assign a cutoff point to these probabilities. If the predicted probability is above the cutoff point, you classify that observation as a &#39;yes&#39; (in this case, the flight being late), if it&#39;s below, you classify it as a &#39;no&#39;! . You&#39;ll tune this model by testing different values for several hyperparameters. A hyperparameter is just a value in the model that&#39;s not estimated from the data, but rather is supplied by the user to maximize performance. For this course it&#39;s not necessary to understand the mathematics behind all of these values - what&#39;s important is that you&#39;ll try out a few different choices and pick the best one. . . Why do you supply hyperparameters? . Answer the question . 50XP . Possible Answers . They explain information about the data. | . They improve model performance. . | They improve model fitting speed. . | . Great job! You supply hyperparameters to optimize your model. . 4.2 Create the modeler . The Estimator you&#39;ll be using is a LogisticRegression from the pyspark.ml.classification submodule. . Instructions . 100 XP . Import the LogisticRegression class from pyspark.ml.classification. | Create a LogisticRegression called lr by calling LogisticRegression() with no arguments. | . from pyspark.ml.classification import LogisticRegression # Create a LogisticRegression Estimator lr = LogisticRegression() . Great work! That&#39;s the first step to any modeling in PySpark. . 4.3 Cross validation . In the next few exercises you&#39;ll be tuning your logistic regression model using a procedure called k-fold cross validation. This is a method of estimating the model&#39;s performance on unseen data (like your test DataFrame). . It works by splitting the training data into a few different partitions. The exact number is up to you, but in this course you&#39;ll be using PySpark&#39;s default value of three. Once the data is split up, one of the partitions is set aside, and the model is fit to the others. Then the error is measured against the held out partition. This is repeated for each of the partitions, so that every block of data is held out and used as a test set exactly once. Then the error on each of the partitions is averaged. This is called the cross validation error of the model, and is a good estimate of the actual error on the held out data. . You&#39;ll be using cross validation to choose the hyperparameters by creating a grid of the possible pairs of values for the two hyperparameters, elasticNetParam and regParam, and using the cross validation error to compare all the different models so you can choose the best one! . What does cross validation allow you to estimate? . . Answer the question . 50XP . Possible Answers . The model&#39;s error on held out data. . | The model&#39;s error on data used for fitting. . | The time it will take to fit the model. . | . Exactly! The cross validation error is an estimate of the model&#39;s error on the test set. . 4.4 Create the evaluator . The first thing you need when doing cross validation for model selection is a way to compare different models. Luckily, the pyspark.ml.evaluation submodule has classes for evaluating different kinds of models. Your model is a binary classification model, so you&#39;ll be using the BinaryClassificationEvaluator from the pyspark.ml.evaluation module. . This evaluator calculates the area under the ROC. This is a metric that combines the two kinds of errors a binary classifier can make (false positives and false negatives) into a simple number. You&#39;ll learn more about this towards the end of the chapter! . Instructions . 100 XP . Import the submodule pyspark.ml.evaluation as evals. | Create evaluator by calling evals.BinaryClassificationEvaluator() with the argument metricName=&quot;areaUnderROC&quot;. | . import pyspark.ml.evaluation as evals # Create a BinaryClassificationEvaluator evaluator = evals.BinaryClassificationEvaluator(metricName=&quot;areaUnderROC&quot;) . Perfect! Now you can compare models using the metric output by your evaluator! . 4.5 Make a grid . Next, you need to create a grid of values to search over when looking for the optimal hyperparameters. The submodule pyspark.ml.tuning includes a class called ParamGridBuilder that does just that (maybe you&#39;re starting to notice a pattern here; PySpark has a submodule for just about everything!). . You&#39;ll need to use the .addGrid() and .build() methods to create a grid that you can use for cross validation. The .addGrid() method takes a model parameter (an attribute of the model Estimator, lr, that you created a few exercises ago) and a list of values that you want to try. The .build() method takes no arguments, it just returns the grid that you&#39;ll use later. . Instructions . 100 XP . Instructions . 100 XP . Import the submodule pyspark.ml.tuning under the alias tune. | Call the class constructor ParamGridBuilder() with no arguments. Save this as grid. | Call the .addGrid() method on grid with lr.regParam as the first argument and np.arange(0, .1, .01) as the second argument. This second call is a function from the numpy module (imported as np) that creates a list of numbers from 0 to .1, incrementing by .01. Overwrite grid with the result. | Update grid again by calling the .addGrid() method a second time create a grid for lr.elasticNetParam that includes only the values [0, 1]. | Call the .build() method on grid and overwrite it with the output. | . import pyspark.ml.tuning as tune # Create the parameter grid grid = tune.ParamGridBuilder() # Add the hyperparameter grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01)) grid = grid.addGrid(lr.elasticNetParam, [0, 1]) # Build the grid grid = grid.build() . Awesome! That&#39;s the last ingredient in your cross validation recipe! . 4.6 Make the validator . The submodule pyspark.ml.tuning also has a class called CrossValidator for performing cross validation. This Estimator takes the modeler you want to fit, the grid of hyperparameters you created, and the evaluator you want to use to compare your models. . The submodule pyspark.ml.tune has already been imported as tune. You&#39;ll create the CrossValidator by passing it the logistic regression Estimator lr, the parameter grid, and the evaluator you created in the previous exercises. . Instructions . 100 XP . Create a CrossValidator by calling tune.CrossValidator() with the arguments: estimator=lr | estimatorParamMaps=grid | evaluator=evaluator | . | Name this object cv. | . cv = tune.CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator ) . Great job! You&#39;re almost a machine learning pro! . 4.7 Fit the model(s) . You&#39;re finally ready to fit the models and select the best one! . Unfortunately, cross validation is a very computationally intensive procedure. Fitting all the models would take too long on DataCamp. . To do this locally you would use the code: . # Fit cross validation models models = cv.fit(training) # Extract the best model best_lr = models.bestModel . Remember, the training data is called training and you&#39;re using lr to fit a logistic regression model. Cross validation selected the parameter values regParam=0 and elasticNetParam=0 as being the best. These are the default values, so you don&#39;t need to do anything else with lr before fitting the model. . Instructions . 100 XP . Create best_lr by calling lr.fit() on the training data. | Print best_lr to verify that it&#39;s an object of the LogisticRegressionModel class. | . best_lr = lr.fit(training) # Print best_lr print(best_lr) . Wow! You fit your first Spark model! . 4.8 Evaluating binary classifiers . For this course we&#39;ll be using a common metric for binary classification algorithms call the AUC, or area under the curve. In this case, the curve is the ROC, or receiver operating curve. The details of what these things actually measure isn&#39;t important for this course. All you need to know is that for our purposes, the closer the AUC is to one (1), the better the model is! . . If you&#39;ve created a perfect binary classification model, what would the AUC be? . Answer the question . 50XP . Possible Answers . -1 . | 1 . | 0 . | .5 . | . Great job! An AUC of one represents a model that always perfectly classifies observations. . 4.9 Evaluate the model . Remember the test data that you set aside waaaaaay back in chapter 3? It&#39;s finally time to test your model on it! You can use the same evaluator you made to fit the model. . Instructions . 100 XP . Use your model to generate predictions by applying best_lr.transform() to the test data. Save this as test_results. | Call evaluator.evaluate() on test_results to compute the AUC. Print the output. | . test_results = best_lr.transform(test) # Evaluate the predictions print(evaluator.evaluate(test_results)) . Congratulations! What do you think of the AUC? Your model isn&#39;t half bad! You went from knowing nothing about Spark to doing advanced machine learning. Great job on making it to the end of the course! The next steps are learning how to create large scale Spark clusters and manage and submit jobs so that you can use models in the real world. Check out some of the other DataCamp courses that use Spark! And remember, Spark is still being actively developed, so there&#39;s new features coming all the time! .",
            "url": "https://islamalam.github.io/blog/2021/12/26/introduction-to-pyspark.html",
            "relUrl": "/2021/12/26/introduction-to-pyspark.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "1. Image Processing With Neural Networks",
            "content": ". Image Processing with Keras in Python . Course Description Deep learning methods use data to train neural network algorithms to do a variety of machine learning tasks, such as classification of different classes of objects. Convolutional neural networks are deep learning algorithms that are particularly powerful for analysis of images. This course will teach you how to construct, train and evaluate convolutional neural networks. You will also learn how to improve their ability to learn from data, and how to interpret the results of the training. . 1.1 Introducing convolutional neural networks . 1.2 Images as data: visualizations . To display image data, you will rely on Python&#39;s Matplotlib library, and specifically use matplotlib&#39;s pyplot sub-module, that contains many plotting commands. Some of these commands allow you to display the content of images stored in arrays. . Instructions . 100 XP . Import the image from the file bricks.png into data. | Display the image in data on the screen. | . import matplotlib.pyplot as plt # Load the image data = plt.imread(&#39;bricks.png&#39;) # Display the image plt.imshow(data) plt.show() . . Nicely done! Visit Matplotlib&#39;s website to learn more about plotting commands you could use. . 1.3 Images as data: changing images . To modify an image, you can modify the existing numbers in the array. In a color image, you can change the values in one of the color channels without affecting the other colors, by indexing on the last dimension of the array. . The image you imported in the previous exercise is available in data. . Instructions . 100 XP . Modify the bricks image to replace the top left corner of the image (10 by 10 pixels) into a red square. | Visualize the resulting image. | . data[0:10, 0:10, 0] = 1 # Set the green channel in this part of the image to 0 data[0:10, 0:10, 1] = 0 # Set the blue channel in this part of the image to 0 data[0:10, 0:10, 2] = 0 # Visualize the result plt.imshow(data) plt.show() . . Great job! You now know how to manipulate images. By the way, if you set both the green and red channels to 1, that part of the image would be yellow. . 1.4 Classifying images . 1.5 Using one-hot encoding to represent images . Neural networks expect the labels of classes in a dataset to be organized in a one-hot encoded manner: each row in the array contains zeros in all columns, except the column corresponding to a unique label, which is set to 1. . The fashion dataset contains three categories: . Shirts | Dresses | Shoes | In this exercise, you will create a one-hot encoding of a small sample of these labels. . Instructions . 100 XP . Initialize the ohe_labels variable to hold the one-hot encoded array. | Use np.where() to find the location of the category of the item in each iteration in categories. | Assign a 1 into the correct row/column combination in every iteration. | . n_categories = 3 # The unique values of categories in the data categories = np.array([&quot;shirt&quot;, &quot;dress&quot;, &quot;shoe&quot;]) # Initialize ohe_labels as all zeros ohe_labels = np.zeros((len(labels), n_categories)) # Loop over the labels for ii in range(len(labels)): # Find the location of this label in the categories variable jj = np.where(categories == labels[ii]) # Set the corresponding zero to one ohe_labels[ii, jj] = 1 . Nice! You can use this array to test classification performance. . 1.6 Evaluating a classifier . To evaluate a classifier, we need to test it on images that were not used during training. This is called &quot;cross-validation&quot;: a prediction of the class (e.g., t-shirt, dress or shoe) is made from each of the test images, and these predictions are compared with the true labels of these images. . The results of cross-validation are provided as one-hot encoded arrays: test_labels and predictions. . Instructions . 100 XP . Multiply the arrays with each other and sum the result to find the total number of correct predictions. | Divide the number of correct answers (the sum) by the length of predictions array to calculate the proportion of correct predictions. | . number_correct = (test_labels * predictions).sum() print(number_correct) # Calculate the proportion of correct predictions proportion_correct = number_correct / len(predictions) print(proportion_correct) . Great job! Let&#39;s talk about fitting classification models using Keras. . 1.7 Classification with Keras . 1.8 Build a neural network . We will use the Keras library to create neural networks and to train these neural networks to classify images. These models will all be of the Sequential type, meaning that the outputs of one layer are provided as inputs only to the next layer. . In this exercise, you will create a neural network with Dense layers, meaning that each unit in each layer is connected to all of the units in the previous layer. For example, each unit in the first layer is connected to all of the pixels in the input images. The Dense layer object receives as arguments the number of units in that layer, and the activation function for the units. For the first layer in the network, it also receives an input_shape keyword argument. . This course touches on a lot of concepts you may have forgotten, so if you ever need a quick refresher, download the Keras Cheat Sheet and keep it handy! . Instructions . 100 XP . The first layer receives images as input, has 10 units and &#39;relu&#39; activation. | The second input layer has 10 units and &#39;relu&#39; activation. | The output layer has one unit for each category (3 categories) and &#39;softmax&#39; activation. | . from keras.models import Sequential from keras.layers import Dense # Initializes a sequential model model = Sequential() # First layer model.add(Dense(10, activation=&#39;relu&#39;, input_shape=(784,))) # Second layer model.add(Dense(10, activation=&#39;relu&#39;)) # Output layer model.add(Dense(3, activation=&#39;softmax&#39;)) . Congratulations! You&#39;ve built a neural network! . 1.9 Compile a neural network . Once you have constructed a model in Keras, the model needs to be compiled before you can fit it to data. This means that you need to specify the optimizer that will be used to fit the model and the loss function that will be used in optimization. Optionally, you can also specify a list of metrics that the model will keep track of. For example, if you want to know the classification accuracy, you will provide the list [&#39;accuracy&#39;] to the metrics keyword argument. . Instructions . 100 XP . Write code to compile the model with the &#39;adam&#39; optimizer and &#39;categorical_crossentropy&#39; as the loss function. . model.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . This neural network model is now ready to be fit to data. . 1.10 Fitting a neural network model to clothing data . In this exercise, you will fit the fully connected neural network that you constructed in the previous exercise to image data. The training data is provided as two variables: train_data that contains the pixel data for 50 images of the three clothing classes and train_labels, which contains one-hot encoded representations of the labels for each one of these 50 images. Transform the data into the network&#39;s expected input and then fit the model on training data and training labels. . The model you compiled in the previous exercise, and train_data and train_labels are available in your workspace. . Instructions . 100 XP . Prepare the data for fitting by reshaping it. | Fit the model by passing the input training data and training labels to the model&#39;s .fit() method. | . train_data = train_data.reshape(50, 28*28) # Fit the model model.fit(train_data, train_labels, validation_split=0.2, epochs=3) . This model works well on the training data, but does it work well on test data? . 1.11 Cross-validation for neural network evaluation . To evaluate the model, we use a separate test data-set. As in the train data, the images in the test data also need to be reshaped before they can be provided to the fully-connected network because the network expects one column per pixel in the input. . The model you fit in the previous exercise, and test_data and test_labels are available in your workspace. . Instructions . 100 XP . Reshape the test_data so that it can be used to evaluate the model. | Evaluate the model on test_data using test_labels. | . test_data = test_data.reshape(10, 28*28) # Evaluate the model model.evaluate(test_data, test_labels) . Not too bad! The model cross-validates rather accurately. . 2. Using Convolutions . Convolutions are the fundamental building blocks of convolutional neural networks. In this chapter, you will be introducted to convolutions and learn how they operate on image data. You will also see how you incorporate convolutions into Keras neural networks. . 2.1 Convolutions . 2.2 One dimensional convolutions . A convolution of an one-dimensional array with a kernel comprises of taking the kernel, sliding it along the array, multiplying it with the items in the array that overlap with the kernel in that location and summing this product. . Instructions 100 XP Multiply each window in the input array with the kernel and sum the multiplied result and allocate the result into the correct entry in the output array (conv). . array = np.array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0]) kernel = np.array([1, -1, 0]) conv = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # Output array for ii in range(8): conv[ii] = (kernel * array[ii:ii+3]).sum() # Print conv print(conv) . Nicely done. Notice that we&#39;ve only multiplied the kernel with eight different positions . 2.3 Image convolutions . The convolution of an image with a kernel summarizes a part of the image as the sum of the multiplication of that part of the image with the kernel. In this exercise, you will write the code that executes a convolution of an image with a kernel using Numpy. Given a black and white image that is stored in the variable im, write the operations inside the loop that would execute the convolution with the provided kernel. . Instructions . 100 XP . Select the right window from the image in each iteration and multiply this part of the image with the kernel. | Sum the result and allocate the sum to the correct entry in the output array (results). | . kernel = np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]]) result = np.zeros(im.shape) # Output array for ii in range(im.shape[0] - 3): for jj in range(im.shape[1] - 3): result[ii, jj] = (im[ii:ii+3, jj:jj+3] * kernel).sum() # Print result print(result) . In a future exercise, you will see how to use Keras to implement a convolution like this one. . 2.4 Defining image convolution kernels . In the previous exercise, you wrote code that performs a convolution given an image and a kernel. This code is now stored in a function called convolution() that takes two inputs: image and kernel and produces the convolved image. In this exercise, you will be asked to define the kernel that finds a particular feature in the image. . For example, the following kernel finds a vertical line in images: . np.array([[-1, 1, -1], [-1, 1, -1], [-1, 1, -1]]) . Instructions 1/3 . 33 XP . Define a kernel that finds horizontal lines in images. . kernel = np.array([[-1, -1, -1], [1, 1, 1], [-1, -1, -1]]) . Instructions 2/3 . Define a kernel that finds a light spot surrounded by dark pixels. . kernel = np.array([[-1, -1, -1], [-1, 1, -1], [-1, -1, -1]]) . Instructions 3/3 . Define a kernel that finds a dark spot surrounded by bright pixels. . kernel = np.array([[1, 1, 1], [1, -1, 1], [1, 1, 1]]) . 2.5 Implementing image convolutions in Keras . 2.6 Convolutional network for image classification . Convolutional networks for classification are constructed from a sequence of convolutional layers (for image processing) and fully connected (Dense) layers (for readout). In this exercise, you will construct a small convolutional network for classification of the data from the fashion dataset. . Instructions . 100 XP . Add a Conv2D layer to construct the input layer of the network. Use a kernel size of 3 by 3. You can use the img_rows and img_cols objects available in your workspace to define the input_shape of this layer. | Add a Flatten layer to translate between the image processing and classification part of your network. | Add a Dense layer to classify the 3 different categories of clothing in the dataset. | . from keras.models import Sequential from keras.layers import Dense, Conv2D, Flatten # Initialize the model object model = Sequential() # Add a convolutional layer model.add(Conv2D(10, kernel_size=3, activation=&#39;relu&#39;, input_shape=(img_rows, img_cols, 1))) # Flatten the output of the convolutional layer model.add(Flatten()) # Add an output layer for the 3 categories model.add(Dense(3, activation=&#39;softmax&#39;)) . Congratulations! You just built a model with one convolutional layer. . 2.7 Training a CNN to classify clothing types . Before training a neural network it needs to be compiled with the right cost function, using the right optimizer. During compilation, you can also define metrics that the network calculates and reports in every epoch. Model fitting requires a training data set, together with the training labels to the network. . The Conv2D model you built in the previous exercise is available in your workspace. . Instructions . 100 XP . Compile the network using the &#39;adam&#39; optimizer and the &#39;categorical_crossentropy&#39; cost function. In the metrics list define that the network to report &#39;accuracy&#39;. | Fit the network on train_data and train_labels. Train for 3 epochs with a batch size of 10 images. In training, set aside 20% of the data as a validation set, using the validation_split keyword argument. | . model.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) # Fit the model on a training set model.fit(train_data, train_labels, validation_split=0.2, epochs=3, batch_size=10) . Validation accuracy converges to 100%! . 2.8 Evaluating a CNN with test data . To evaluate a trained neural network, you should provide a separate testing data set of labeled images. The model you fit in the previous exercise is available in your workspace. . Instructions . 100 XP . Evaluate the data on a separate test set: test_data and test_labels. | Use the same batch size that was used for fitting (10 images per batch). | . model.evaluate(test_data, test_labels, batch_size=10) . The first number in the output is the value of the cross-entropy loss, the second is the value of the accuracy. For this model, it&#39;s 100%! . 2.9 Tweaking your convolutions . Padding allows a convolutional layer to retain the resolution of the input into this layer. This is done by adding zeros around the edges of the input image, so that the convolution kernel can overlap with the pixels on the edge of the image. . Instructions . 100 XP . Add a Conv2D layer and choose a padding such that the output has the same size as the input. . model = Sequential() # Add the convolutional layer model.add(Conv2D(10, kernel_size=3, activation=&#39;relu&#39;, input_shape=(img_rows, img_cols, 1), padding=&#39;same&#39;)) # Feed into output layer model.add(Flatten()) model.add(Dense(3, activation=&#39;softmax&#39;)) . Great job! With padding set to &#39;same&#39;, the output layer will have the same size as the input layer! . 2.10 Add padding to a CNN . The size of the strides of the convolution kernel determines whether the kernel will skip over some of the pixels as it slides along the image. This affects the size of the output because when strides are larger than one, the kernel will be centered on only some of the pixels. . Instructions . 100 XP . Construct a neural network with a Conv2D layer with strided convolutions that skips every other pixel. . model = Sequential() # Add the convolutional layer model.add(Conv2D(10, kernel_size=3, activation=&#39;relu&#39;, input_shape=(img_rows, img_cols, 1), strides=2)) # Feed into output layer model.add(Flatten()) model.add(Dense(3, activation=&#39;softmax&#39;)) . With strides set to 2, the network skips every other pixel. . 2.11 Add strides to a convolutional network . The size of the strides of the convolution kernel determines whether the kernel will skip over some of the pixels as it slides along the image. This affects the size of the output because when strides are larger than one, the kernel will be centered on only some of the pixels. . Instructions . 100 XP . Construct a neural network with a Conv2D layer with strided convolutions that skips every other pixel. . model = Sequential() # Add the convolutional layer model.add(Conv2D(10, kernel_size=3, activation=&#39;relu&#39;, input_shape=(img_rows, img_cols, 1), strides=2)) # Feed into output layer model.add(Flatten()) model.add(Dense(3, activation=&#39;softmax&#39;)) . 2.12 Calculate the size of convolutional layer output . Zero padding and strides affect the size of the output of a convolution. . What is the size of the output for an input of size 256 by 256, with a kernel of size 4 by 4, padding of 1 and strides of 2? . Instructions . 50 XP . Possible Answers . 127 . | 255 . | 128 . | 256 . | . . . 3. Going Deeper . Convolutional neural networks gain a lot of power when they are constructed with multiple layers (deep networks). In this chapter, you will learn how to stack multiple convolutional layers into a deep network. You will also learn how to keep track of the number of parameters, as the network grows, and how to control this number. . 3.1 Going deeper . 3.2 Creating a deep learning network . A deep convolutional neural network is a network that has more than one layer. Each layer in a deep network receives its input from the preceding layer, with the very first layer receiving its input from the images used as training or test data. . Here, you will create a network that has two convolutional layers. . Instructions . 100 XP . The first convolutional layer is the input layer of the network. This should have 15 units with kernels of 2 by 2 pixels. It should have a &#39;relu&#39; activation function. It can use the variables img_rows and img_cols to define its input_shape. | The second convolutional layer receives its inputs from the first layer. It should have 5 units with kernels of 2 by 2 pixels. It should also have a &#39;relu&#39; activation function. | . from keras.models import Sequential from keras.layers import Dense, Conv2D, Flatten model = Sequential() # Add a convolutional layer (15 units) model.add(Conv2D(15, kernel_size=2, activation=&#39;relu&#39;, input_shape=(img_rows, img_cols, 1), strides=2)) # Add another convolutional layer (5 units) model.add(Conv2D(5, kernel_size=2, activation=&#39;relu&#39;, input_shape=(img_rows, img_cols, 1), strides=2)) # Flatten and feed to output layer model.add(Flatten()) model.add(Dense(3, activation=&#39;softmax&#39;)) . Congratulations!! You built a network with multiple convolution layers. . 3.3 Train a deep CNN to classify clothing images . Training a deep learning model is very similar to training a single layer network. Once the model is constructed (as you have done in the previous exercise), the model needs to be compiled with the right set of parameters. Then, the model is fit by providing it with training data, as well as training labels. After training is done, the model can be evaluated on test data. . The model you built in the previous exercise is available in your workspace. . Instructions . 100 XP . Compile the model to use the categorical cross-entropy loss function and the Adam optimizer. | Train the network with train_data for 3 epochs with batches of 10 images each. | Use randomly selected 20% of the training data as validation data during training. | Evaluate the model with test_data, use a batch size of 10. | . model.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) # Fit the model to training data model.fit(train_data, train_labels, validation_split=0.2, epochs=3, batch_size=10) # Evaluate the model on test data model.evaluate(test_data, test_labels, batch_size=10) . &lt;script.py&gt; output: Epoch 1/3 1/4 [======&gt;.......................] - ETA: 3s - loss: 1.1531 - accuracy: 0.3000 4/4 [==============================] - 2s 172ms/step - loss: 1.0844 - accuracy: 0.5000 - val_loss: 0.9340 - val_accuracy: 1.0000 Epoch 2/3 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.9334 - accuracy: 0.9000 4/4 [==============================] - 0s 10ms/step - loss: 0.8868 - accuracy: 0.9250 - val_loss: 0.7414 - val_accuracy: 1.0000 Epoch 3/3 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.7673 - accuracy: 0.9000 4/4 [==============================] - 0s 9ms/step - loss: 0.7358 - accuracy: 0.9750 - val_loss: 0.5658 - val_accuracy: 1.0000 1/1 [==============================] - ETA: 0s - loss: 0.8316 - accuracy: 0.7000 1/1 [==============================] - 0s 15ms/step - loss: 0.8316 - accuracy: 0.7000 . Accuracy calculated on the test data is not subject to overfitting. . 3.4 What is special about a deep network? . Networks with more convolution layers are called &quot;deep&quot; networks, and they may have more power to fit complex data, because of their ability to create hierarchical representations of the data that they fit. . What is a major difference between a deep CNN and a CNN with only one convolutional layer? . Answer the question . 50XP . Possible Answers . A deep network is inspired by the human visual system. . | A deep network requires more data and more computation to fit. . | A deep network has more dense layers. . | A deep network has larger convolutions. . | . 3.5 How many parameters? . 3.6 How many parameters in a CNN? . We need to know how many parameters a CNN has, so we can adjust the model architecture, to reduce this number or shift parameters from one part of the network to another. How many parameters would a network have if its inputs are images with 28-by-28 pixels, there is one convolutional layer with 10 units kernels of 3-by-3 pixels, using zero padding (input has the same size as the output), and one densely connected layer with 2 units? . Instructions . 50 XP . Possible Answers . 100 . | 1668 . | 15,782 . | 15,682 . | . In [3]: from keras.models import Sequential from keras.layers import Dense, Conv2D, Flatten model = Sequential() # Add a convolutional layer (10 units) model.add(Conv2D(10, kernel_size=3, activation=&#39;relu&#39;, input_shape=(28, 28, 1), strides=1, padding=&#39;same&#39;)) # Add another convolutional layer (5 units) model.add(Flatten()) model.add(Dense(2, activation=&#39;softmax&#39;)) model.summary() Model: &quot;sequential&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 28, 28, 10) 100 _________________________________________________________________ flatten (Flatten) (None, 7840) 0 _________________________________________________________________ dense (Dense) (None, 2) 15682 ================================================================= Total params: 15,782 Trainable params: 15,782 Non-trainable params: 0 _________________________________________________________________ . 3.7 How many parameters in a deep CNN? . n this exercise, you will use Keras to calculate the total number of parameters along with the number of parameters in each layer of the network. . We have already provided code that builds a deep CNN for you. . Instructions . 100 XP . Summarize the network, providing a count of the number of parameters. . model = Sequential() model.add(Conv2D(10, kernel_size=2, activation=&#39;relu&#39;, input_shape=(28, 28, 1))) model.add(Conv2D(10, kernel_size=2, activation=&#39;relu&#39;)) model.add(Flatten()) model.add(Dense(3, activation=&#39;softmax&#39;)) # Summarize the model model.summary() . This model has 20,743 parameters! . &lt;script.py&gt; output: Model: &quot;sequential&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 27, 27, 10) 50 _________________________________________________________________ conv2d_1 (Conv2D) (None, 26, 26, 10) 410 _________________________________________________________________ flatten (Flatten) (None, 6760) 0 _________________________________________________________________ dense (Dense) (None, 3) 20283 ================================================================= Total params: 20,743 Trainable params: 20,743 Non-trainable params: 0 _________________________________________________________________ . 3.8 Pooling operations . 3.9 Write your own pooling operation . As we have seen before, CNNs can have a lot of parameters. Pooling layers are often added between the convolutional layers of a neural network to summarize their outputs in a condensed manner, and reduce the number of parameters in the next layer in the network. This can help us if we want to train the network more rapidly, or if we don&#39;t have enough data to learn a very large number of parameters. . A pooling layer can be described as a particular kind of convolution. For every window in the input it finds the maximal pixel value and passes only this pixel through. In this exercise, you will write your own max pooling operation, based on the code that you previously used to write a two-dimensional convolution operation. . Instructions . 100 XP . Index into the input array (im) and select the right window. | Find the maximum in this window. | Allocate this into the right entry in the output array (result). | . result = np.zeros((im.shape[0]//2, im.shape[1]//2)) # Pooling operation for ii in range(result.shape[0]): for jj in range(result.shape[1]): result[ii, jj] = np.max(im[ii*2:ii*2+2, jj*2:jj*2+2]) . The resulting image is smaller, but retains the salient features in every location . 3.10 Keras pooling layers . Keras implements a pooling operation as a layer that can be added to CNNs between other layers. In this exercise, you will construct a convolutional neural network similar to the one you have constructed before: . Convolution =&gt; Convolution =&gt; Flatten =&gt; Dense . However, you will also add a pooling layer. The architecture will add a single max-pooling layer between the convolutional layer and the dense layer with a pooling of 2x2: . Convolution =&gt; Max pooling =&gt; Convolution =&gt; Flatten =&gt; Dense . A Sequential model along with Dense, Conv2D, Flatten, and MaxPool2D objects are available in your workspace. . Instructions . 100 XP . Add an input convolutional layer (15 units, kernel size of 2, relu activation). | Add a maximum pooling operation (pooling over windows of size 2x2). | Add another convolution layer (5 units, kernel size of 2, relu activation). | Flatten the output of the second convolution and add a Dense layer for output (3 categories, softmax activation). | . model.add(Conv2D(15, kernel_size=2, activation=&#39;relu&#39;, input_shape=(img_rows, img_cols, 1))) # Add a pooling operation model.add(MaxPool2D(2)) # Add another convolutional layer model.add(Conv2D(5, kernel_size=2, activation=&#39;relu&#39;)) # Flatten and feed to output layer model.add(Flatten()) model.add(Dense(3, activation=&#39;softmax&#39;)) model.summary() . This model is even deeper, but has fewer parameters. . &lt;script.py&gt; output: Model: &quot;sequential&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 27, 27, 15) 75 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 13, 13, 15) 0 _________________________________________________________________ conv2d_1 (Conv2D) (None, 12, 12, 5) 305 _________________________________________________________________ flatten (Flatten) (None, 720) 0 _________________________________________________________________ dense (Dense) (None, 3) 2163 ================================================================= Total params: 2,543 Trainable params: 2,543 Non-trainable params: 0 _________________________________________________________________ . 3.11 Train a deep CNN with pooling to classify images . Training a CNN with pooling layers is very similar to training of the deep networks that y have seen before. Once the network is constructed (as you did in the previous exercise), the model needs to be appropriately compiled, and then training data needs to be provided, together with the other arguments that control the fitting procedure. . The following model from the previous exercise is available in your workspace: . Convolution =&gt; Max pooling =&gt; Convolution =&gt; Flatten =&gt; Dense . Instructions . 100 XP . Compile this model to use the categorical cross-entropy loss function and the Adam optimizer. | Train the model for 3 epochs with batches of size 10. | Use 20% of the data as validation data. | Evaluate the model on test_data with test_labels (also batches of size 10). | . model.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) # Fit to training data model.fit(train_data, train_labels, validation_split=0.2, epochs=3, batch_size=10) # Evaluate on test data model.evaluate(test_data, test_labels, batch_size=10) . This model is also very accurate! . &lt;script.py&gt; output: Epoch 1/3 1/4 [======&gt;.......................] - ETA: 3s - loss: 1.1283 - accuracy: 0.2000 4/4 [==============================] - 2s 223ms/step - loss: 1.0961 - accuracy: 0.3750 - val_loss: 1.0882 - val_accuracy: 0.5000 Epoch 2/3 1/4 [======&gt;.......................] - ETA: 0s - loss: 1.0671 - accuracy: 0.5000 4/4 [==============================] - 0s 10ms/step - loss: 1.0618 - accuracy: 0.7000 - val_loss: 1.0560 - val_accuracy: 0.6000 Epoch 3/3 1/4 [======&gt;.......................] - ETA: 0s - loss: 1.0397 - accuracy: 0.9000 4/4 [==============================] - 0s 12ms/step - loss: 1.0248 - accuracy: 0.8500 - val_loss: 1.0239 - val_accuracy: 0.6000 1/1 [==============================] - ETA: 0s - loss: 1.0101 - accuracy: 0.6000 1/1 [==============================] - 0s 19ms/step - loss: 1.0101 - accuracy: 0.6000 . 4. Understanding and Improving Deep Convolutional Networks . There are many ways to improve training by neural networks. In this chapter, we will focus on our ability to track how well a network is doing, and explore approaches towards improving convolutional neural networks. . 4.1 Tracking learning . 4.2 Plot the learning curves . During learning, the model will store the loss function evaluated in each epoch. Looking at the learning curves can tell us quite a bit about the learning process. In this exercise, you will plot the learning and validation loss curves for a model that you will train. . Instructions . 100 XP . Fit the model to the training data (train_data). | Use a validation split of 20%, 3 epochs and batch size of 10. | Plot the training loss. | Plot the validation loss. | . import matplotlib.pyplot as plt # Train the model and store the training object training = model.fit(train_data, train_labels, validation_split=0.2, epochs=3, batch_size=10 ) # Extract the history from the training object history = training.history # Plot the training loss plt.plot(history[&#39;loss&#39;]) # Plot the validation loss plt.plot(history[&#39;val_loss&#39;]) # Show the figure plt.show() . That&#39;s great! If you continue for many epochs, the validation loss will start going back up. . . 4.3 Using stored weights to predict in a test set . Model weights stored in an hdf5 file can be reused to populate an untrained model. Once the weights are loaded into this model, it behaves just like a model that has been trained to reach these weights. For example, you can use this model to make predictions from an unseen data set (e.g. test_data). . Instructions . 100 XP . Load the weights from a file called &#39;weights.hdf5&#39;. | Predict the classes of the first three images from test_data. | . model.load_weights(&#39;weights.hdf5&#39;) # Predict from the first three images in the test data model.predict(test_data[0:3,:]) . Nicely done! How would you use these weights to evaluate the model instead? . 4.4 Regularization . 4.5 Adding dropout to your network . Dropout is a form of regularization that removes a different random subset of the units in a layer in each round of training. In this exercise, we will add dropout to the convolutional neural network that we have used in previous exercises: . Convolution (15 units, kernel size 2, &#39;relu&#39; activation) | Dropout (20%) | Convolution (5 units, kernel size 2, &#39;relu&#39; activation) | Flatten | Dense (3 units, &#39;softmax&#39; activation) | A Sequential model along with Dense, Conv2D, Flatten, and Dropout objects are available in your workspace. . Instructions . 100 XP . Add dropout applied to the first layer with 20%. | Add a flattening layer. | . model.add(Conv2D(15, kernel_size=2, activation=&#39;relu&#39;, input_shape=(img_rows, img_cols, 1))) # Add a dropout layer model.add(Dropout(0.20)) # Add another convolutional layer model.add(Conv2D(5, kernel_size=2, activation=&#39;relu&#39;)) # Flatten and feed to output layer model.add(Flatten()) model.add(Dense(3, activation=&#39;softmax&#39;)) . Great! Now the kernels will be more different from each other. . 4.6 Add batch normalization to your network . Batch normalization is another form of regularization that rescales the outputs of a layer to make sure that they have mean 0 and standard deviation 1. In this exercise, we will add batch normalization to the convolutional neural network that we have used in previous exercises: . Convolution (15 units, kernel size 2, &#39;relu&#39; activation) | Batch normalization | Convolution (5 unites, kernel size 2, &#39;relu&#39; activation) | Flatten | Dense (3 units, &#39;softmax&#39; activation) | A Sequential model along with Dense, Conv2D, Flatten, and Dropout objects are available in your workspace. . Instructions . 100 XP . Add the first convolutional layer. You can use the img_rows and img_cols objects available in your workspace to define the input_shape of this layer. | Add batch normalization applied to the outputs of the first layer. | . model.add(Conv2D(15, kernel_size=2, activation=&#39;relu&#39;, input_shape=(img_rows, img_cols, 1))) # Add batch normalization layer model.add(BatchNormalization()) # Add another convolutional layer model.add(Conv2D(5, kernel_size=2, activation=&#39;relu&#39;)) # Flatten and feed to output layer model.add(Flatten()) model.add(Dense(3, activation=&#39;softmax&#39;)) . That&#39;s useful! That should improve training. . 4.7 Interpreting the model . 4.8 Extracting a kernel from a trained network . One way to interpret models is to examine the properties of the kernels in the convolutional layers. In this exercise, you will extract one of the kernels from a convolutional neural network with weights that you saved in a hdf5 file. . Instructions . 100 XP . Load the weights into the model from the file weights.hdf5. | Get the first convolutional layer in the model from the layers attribute. | Use the .get_weights() method to extract the weights from this layer. | . model.load_weights(&#39;weights.hdf5&#39;) # Get the first convolutional layer from the model c1 = model.layers[0] # Get the weights of the first convolutional layer weights1 = c1.get_weights() # Pull out the first channel of the first kernel in the first layer kernel = weights1[0][:, :, 0, 0] print(kernel) . That&#39;s great! You can extract the weights from other layers too. . 4.9 Shape of the weights . A Keras neural network stores its layers in a list called model.layers. For the convolutional layers, you can get the weights using the .get_weights() method. This returns a list, and the first item in this list is an array representing the weights of the convolutional kernels. If the shape of this array is (2, 2, 1, 5), what does the first number (2) represent? . Instructions . 50 XP . Possible Answers . There are 2 channels in black and white images. . | The kernel size is 2 by 2. bold text . | The model used a convolutional unit with 2 dimensions. . | There are 2 convolutional layers in the network. . | . That&#39;s correct, each of the 2s in this shape is one of the dimensions of the kernel. . 4.10 Visualizing kernel responses . One of the ways to interpret the weights of a neural network is to see how the kernels stored in these weights &quot;see&quot; the world. That is, what properties of an image are emphasized by this kernel. In this exercise, we will do that by convolving an image with the kernel and visualizing the result. Given images in the test_data variable, a function called extract_kernel() that extracts a kernel from the provided network, and the function called convolution() that we defined in the first chapter, extract the kernel, load the data from a file and visualize it with matplotlib. . A deep CNN model, a function convolution(), along with the kernel you extracted in an earlier exercise is available in your workspace. . Ready to take your deep learning to the next level? Check out Advanced Deep Learning with Keras to see how the Keras functional API lets you build domain knowledge to solve new types of problems. . Instructions . 100 XP . Use the convolution() function to convolve the extracted kernel with the first channel of the fourth item in the image array. | Visualize the resulting convolution with imshow(). | . import matplotlib.pyplot as plt # Convolve with the fourth image in test_data out = convolution(test_data[3, :, :, 0], kernel) # Visualize the result plt.imshow(out) plt.show() . That&#39;s nice. You can keep going and visualize the kernel responses for all the kernels in this layer! . . 4.11 Next steps .",
            "url": "https://islamalam.github.io/blog/2021/12/26/image-processing-with-keras-in-python.html",
            "relUrl": "/2021/12/26/image-processing-with-keras-in-python.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Hyperparameter Tuning in Python",
            "content": "1. Hyperparameters and Parameters . In this introductory chapter you will learn the difference between hyperparameters and parameters. You will practice extracting and analyzing parameters, setting hyperparameter values for several popular machine learning algorithms. Along the way you will learn some best practice tips &amp; tricks for choosing which hyperparameters to tune and what values to set &amp; build learning curves to analyze your hyperparameter choices. . link text . 1.1 Introduction &amp; &#39;Parameters&#39; . 1.2 Parameters in Logistic Regression . Now that you have had a chance to explore what a parameter is, let us apply this knowledge. It is important to be able to review any new algorithm and identify which elements are parameters and hyperparameters. . Which of the following is a parameter for the Scikit Learn logistic regression model? Here we mean conceptually based on the theory introduced in this course. NOT what the Scikit Learn documentation calls a parameter or attribute. . Answer the question . 50XP . Possible Answers . n_jobs . | coef_ . | class_weight . | LogisticRegression() . | . Yes! coef_ contains the important information about coefficients on our variables in the model. We do not set this, it is learned by the algorithm through the modeling process. . 1.3 Extracting a Logistic Regression parameter . You are now going to practice extracting an important parameter of the logistic regression model. The logistic regression has a few other parameters you will not explore here but you can review them in the scikit-learn.org documentation for the LogisticRegression() module under &#39;Attributes&#39;. . This parameter is important for understanding the direction and magnitude of the effect the variables have on the target. . In this exercise we will extract the coefficient parameter (found in the coef_ attribute), zip it up with the original column names, and see which variables had the largest positive effect on the target variable. . You will have available: . A logistic regression model object named log_reg_clf | The X_train DataFrame | . sklearn and pandas have been imported for you. . Instructions . 100 XP . Create a list of the original column names used in the training DataFrame. | Extract the coefficients of the logistic regression estimator. | Create a DataFrame of coefficients and variable names &amp; view it. | Print out the top 3 &#39;positive&#39; variables based on the coefficient size. | . original_variables = list(X_train.columns) # Extract the coefficients of the logistic regression estimator model_coefficients = log_reg_clf.coef_[0] # Create a dataframe of the variables and coefficients &amp; print it out coefficient_df = pd.DataFrame({&quot;Variable&quot; : original_variables, &quot;Coefficient&quot;: model_coefficients}) print(coefficient_df) # Print out the top 3 positive variables top_three_df = coefficient_df.sort_values(by=&#39;Coefficient&#39;, axis=0, ascending=False)[0:3] print(top_three_df) . Nice! You have succesfully extracted and reviewed a very important parameter for the Logistic Regression Model. The coefficients of the model allow you to see which variables are having a larger or smaller impact on the outcome. Additionally the sign lets you know if it is a positive or negative relationship. . 1.4 Extracting a Random Forest parameter . You will now translate the work previously undertaken on the logistic regression model to a random forest model. A parameter of this model is, for a given tree, how it decided to split at each level. . This analysis is not as useful as the coefficients of logistic regression as you will be unlikely to ever explore every split and every tree in a random forest model. However, it is a very useful exercise to peak under the hood at what the model is doing. . In this exercise we will extract a single tree from our random forest model, visualize it and programmatically extract one of the splits. . You have available: . A random forest model object, rf_clf | An image of the top of the chosen decision tree, tree_viz_image | The X_train DataFrame &amp; the original_variables list | . Instructions . 100 XP . Instructions . 100 XP . Extract the 7th tree (6th index) from the random forest model. | Visualize this tree (tree_viz_image) to see the split decisions. | Extract the feature &amp; level of the top split. | Print out the feature and level together. | . chosen_tree = rf_clf.estimators_[6] # Visualize the graph using the provided image imgplot = plt.imshow(tree_viz_image) plt.show() # Extract the parameters and level of the top (index 0) node split_column = chosen_tree.tree_.feature[0] split_column_name = X_train.columns[split_column] split_value = chosen_tree.tree_.threshold[0] # Print out the feature and level print(&quot;This node split on feature {}, at a value of {}&quot;.format(split_column_name, split_value)) . Excellent! You visualized and extracted some of the parameters of a random forest model. . . 1.5 Introducing Hyperparameters . 1.6 Hyperparameters in Random Forests . As you saw, there are many different hyperparameters available in a Random Forest model using Scikit Learn. Here you can remind yourself how to differentiate between a hyperparameter and a parameter, and easily check whether something is a hyperparameter. . You can create a random forest estimator yourself from the imported Scikit Learn package. Then print this estimator out to see the hyperparameters and their values. . Which of the following is a hyperparameter for the Scikit Learn random forest model? . Instructions . 50 XP . Possible Answers . oob_score . | classes_ . | trees . | random_level . | . That&#39;s correct! oob_score set to True or False decides whether to use out-of-bag samples to estimate the generalization accuracy. . 1.7 Exploring Random Forest Hyperparameters . Understanding what hyperparameters are available and the impact of different hyperparameters is a core skill for any data scientist. As models become more complex, there are many different settings you can set, but only some will have a large impact on your model. . You will now assess an existing random forest model (it has some bad choices for hyperparameters!) and then make better choices for a new random forest model and assess its performance. . You will have available: . X_train, X_test, y_train, y_test DataFrames | An existing pre-trained random forest estimator, rf_clf_old | The predictions of the existing random forest estimator on the test set, rf_old_predictions | . Instructions 1/3 . 35 XP . Print out the hyperparameters of the existing random forest classifier by printing the estimator and then create a confusion matrix and accuracy score from it. The test set y_test and the old predictions rf_old_predictions will be quite useful! | . print(rf_clf_old) # Get confusion matrix &amp; accuracy for the old rf_model print(&quot;Confusion Matrix: n n {} n Accuracy Score: n n {}&quot;.format( confusion_matrix(y_test, rf_old_predictions), accuracy_score(y_test, rf_old_predictions))) . Instructions 2/3 . 35 XP . Create a new random forest classifier with a better n_estimators (try 500) then fit this to the data and obtain predictions. | . print(rf_clf_old) # Get confusion matrix &amp; accuracy for the old rf_model print(&quot;Confusion Matrix: n n {} n Accuracy Score: n n {}&quot;.format( confusion_matrix(y_test, rf_old_predictions), accuracy_score(y_test, rf_old_predictions))) # Create a new random forest classifier with better hyperparamaters rf_clf_new = RandomForestClassifier(n_estimators=500) # Fit this to the data and obtain predictions rf_new_predictions = rf_clf_new.fit(X_train, y_train).predict(X_test) . Instructions 3/3 . 30 XP . Assess the performance of the new random forest classifier. Create the confusion matrix and accuracy score and print them out. How does this compare to the first model you were given? | . print(rf_clf_old) # Get confusion matrix &amp; accuracy for the old rf_model print(&quot;Confusion Matrix: n n {} n Accuracy Score: n n {}&quot;.format( confusion_matrix(y_test, rf_old_predictions), accuracy_score(y_test, rf_old_predictions))) # Create a new random forest classifier with better hyperparamaters rf_clf_new = RandomForestClassifier(n_estimators=500) # Fit this to the data and obtain predictions rf_new_predictions = rf_clf_new.fit(X_train, y_train).predict(X_test) # Assess the new model (using new predictions!) print(&quot;Confusion Matrix: n n&quot;, confusion_matrix(y_test, rf_new_predictions)) print(&quot;Accuracy Score: n n&quot;, accuracy_score(y_test, rf_new_predictions)) . Nice! We got a nice 5% accuracy boost just from changing the n_estimators. You have had your first taste of hyperparameter tuning for a random forest model. . 1.8 Hyperparameters of KNN . To apply the concepts learned in the prior exercise, it is good practice to try out learnings on a new algorithm. The k-nearest-neighbors algorithm is not as popular as it used to be but can still be an excellent choice for data that has groups of data that behave similarly. Could this be the case for our credit card users? . In this case you will try out several different values for one of the core hyperparameters for the knn algorithm and compare performance. . You will have available: . X_train, X_test, y_train, y_test DataFrames | . Instructions . 100 XP . Build a knn estimator for the following values of n_neighbors [5,10,20]. | Fit each to the training data and produce predictions. | Get an accuracy score for each model and print them out. | . knn_5 = KNeighborsClassifier(n_neighbors=5) knn_10 = KNeighborsClassifier(n_neighbors=10) knn_20 = KNeighborsClassifier(n_neighbors=20) # Fit each to the training data &amp; produce predictions knn_5_predictions = knn_5.fit(X_train, y_train).predict(X_test) knn_10_predictions = knn_10.fit(X_train, y_train).predict(X_test) knn_20_predictions = knn_20.fit(X_train, y_train).predict(X_test) # Get an accuracy score for each of the models knn_5_accuracy = accuracy_score(y_test, knn_5_predictions) knn_10_accuracy = accuracy_score(y_test, knn_10_predictions) knn_20_accuracy = accuracy_score(y_test, knn_20_predictions) print(&quot;The accuracy of 5, 10, 20 neighbours was {}, {}, {}&quot;.format(knn_5_accuracy, knn_10_accuracy, knn_20_accuracy)) . Phew! You succesfully tested 3 different options for 1 hyperparameter, but it was pretty exhausting. Next, we will try to find a way to make this easier. . 1.9 Setting &amp; Analyzing Hyperparameter Values . 1.10 Automating Hyperparameter Choice . Finding the best hyperparameter of interest without writing hundreds of lines of code for hundreds of models is an important efficiency gain that will greatly assist your future machine learning model building. . An important hyperparameter for the GBM algorithm is the learning rate. But which learning rate is best for this problem? By writing a loop to search through a number of possibilities, collating these and viewing them you can find the best one. . Possible learning rates to try include 0.001, 0.01, 0.05, 0.1, 0.2 and 0.5 . You will have available X_train, X_test, y_train &amp; y_test datasets, and GradientBoostingClassifier has been imported for you. . Instructions . 100 XP . Create a learning_rates list for the learning rates, and a results_list to hold the accuracy score of your predictions. | Write a loop to create a GBM model for each learning rate mentioned and create predictions for each model. | Save the learning rate and accuracy score to a results_list. | Turn the results list into a DataFrame and print it out. | . learning_rates = [0.001, 0.01, 0.05, 0.1, 0.2, 0.5] results_list = [] # Create the for loop to evaluate model predictions for each learning rate for learning_rate in learning_rates: model = GradientBoostingClassifier(learning_rate=learning_rate) predictions = model.fit(X_train, y_train).predict(X_test) # Save the learning rate and accuracy score results_list.append([learning_rate, accuracy_score(y_test, predictions)]) # Gather everything into a DataFrame results_df = pd.DataFrame(results_list, columns=[&#39;learning_rate&#39;, &#39;accuracy&#39;]) print(results_df) . Nice! You efficiently tested a few different values for a single hyperparameter and can easily see which learning rate value was the best. Here, it seems that a learning rate of 0.05 yields the best accuracy. . 1.11 Building Learning Curves . If we want to test many different values for a single hyperparameter it can be difficult to easily view that in the form of a DataFrame. Previously you learned about a nice trick to analyze this. A graph called a &#39;learning curve&#39; can nicely demonstrate the effect of increasing or decreasing a particular hyperparameter on the final result. . Instead of testing only a few values for the learning rate, you will test many to easily see the effect of this hyperparameter across a large range of values. A useful function from NumPy is np.linspace(start, end, num) which allows you to create a number of values (num) evenly spread within an interval (start, end) that you specify. . You will have available X_train, X_test, y_train &amp; y_test datasets. . Instructions . 100 XP . Create a list of 30 learning rates evenly spread between 0.01 and 2. | Create a similar loop to last exercise but just save out accuracy scores to a list. | Plot the learning rates against the accuracy score. | . learn_rates = np.linspace(0.01, 2, num=30) accuracies = [] # Create the for loop for learn_rate in learn_rates: # Create the model, predictions &amp; save the accuracies as before model = GradientBoostingClassifier(learning_rate=learn_rate) predictions = model.fit(X_train, y_train).predict(X_test) accuracies.append(accuracy_score(y_test, predictions)) # Plot results plt.plot(learn_rates, accuracies); plt.gca().set(xlabel=&#39;learning_rate&#39;, ylabel=&#39;Accuracy&#39;, title=&#39;Accuracy for different learning_rates&#39;) plt.show() . Excellent work! You can see that for low values, you get a pretty good accuracy. However once the learning rate pushes much above 1.5, the accuracy starts to drop. You have learned and practiced a useful skill for visualizing large amounts of results for a single hyperparameter. . . 2. Grid search . This chapter introduces you to a popular automated hyperparameter tuning methodology called Grid Search. You will learn what it is, how it works and practice undertaking a Grid Search using Scikit Learn. You will then learn how to analyze the output of a Grid Search &amp; gain practical experience doing this. . 2.1 Introducing Grid Search . 2.2 Build Grid Search functions . In data science it is a great idea to try building algorithms, models and processes &#39;from scratch&#39; so you can really understand what is happening at a deeper level. Of course there are great packages and libraries for this work (and we will get to that very soon!) but building from scratch will give you a great edge in your data science work. . In this exercise, you will create a function to take in 2 hyperparameters, build models and return results. You will use this function in a future exercise. . You will have available the X_train, X_test, y_train and y_test datasets available. . Instructions . 100 XP . Build a function that takes two parameters called learning_rate and max_depth for the learning rate and maximum depth. | Add capability in the function to build a GBM model and fit it to the data with the input hyperparameters. | Have the function return the results of that model and the chosen hyperparameters (learning_rate and max_depth). | . def gbm_grid_search(learning_rate, max_depth): # Create the model model = GradientBoostingClassifier(learning_rate=learning_rate, max_depth=max_depth) # Use the model to make predictions predictions = model.fit(X_train, y_train).predict(X_test) # Return the hyperparameters and score return([learning_rate, max_depth, accuracy_score(y_test, predictions)]) . Nice! You now have a function you can call to test different combinations of two hyperparameters for the GBM algorithm. In the next exercise we will use it to test some values and analyze the results. . 2.3 Iteratively tune multiple hyperparameters . In this exercise, you will build on the function you previously created to take in 2 hyperparameters, build a model and return the results. You will now use that to loop through some values and then extend this function and loop with another hyperparameter. . The function gbm_grid_search(learn_rate, max_depth) is available in this exercise. . If you need to remind yourself of the function you can run the function print_func() that has been created for you . Instructions 1/3 . 35 XP . Write a for-loop to test the values (0.01, 0.1, 0.5) for the learning_rate and (2, 4, 6) for the max_depth using the function you created gbm_grid_search and print the results. | . results_list = [] learn_rate_list = [0.01, 0.1, 0.5] max_depth_list = [2, 4, 6] # Create the for loop for learn_rate in learn_rate_list: for max_depth in max_depth_list: results_list.append(gbm_grid_search(learn_rate,max_depth)) # Print the results print(results_list) . Instructions 2/3 . 35 XP . Extend the gbm_grid_search function to include the hyperparameter subsample. Name this new function gbm_grid_search_extended. | . results_list = [] learn_rate_list = [0.01, 0.1, 0.5] max_depth_list = [2,4,6] # Extend the function input def gbm_grid_search_extended(learn_rate, max_depth, subsample): # Extend the model creation section model = GradientBoostingClassifier(learning_rate=learn_rate, max_depth=max_depth, subsample=subsample) predictions = model.fit(X_train, y_train).predict(X_test) # Extend the return part return([learn_rate, max_depth, subsample, accuracy_score(y_test, predictions)]) . Instructions 3/3 . 30 XP . Extend your loop to call gbm_grid_search (available in your console), then test the values [0.4 , 0.6] for the subsample hyperparameter and print the results. max_depth_list &amp; learn_rate_list are available in your environment. | . results_list = [] # Create the new list to test subsample_list = [0.4, 0.6] for learn_rate in learn_rate_list: for max_depth in max_depth_list: # Extend the for loop for subsample in subsample_list: # Extend the results to include the new hyperparameter results_list.append(gbm_grid_search_extended(learn_rate, max_depth, subsample)) # Print results print(results_list) . Congratulations. You have effectively built your own grid search! You went from 2 to 3 hyperparameters and can see how you could extend that to even more values and hyperparameters. That was a lot of effort though. Be warned - we are now entering a world that can get very computationally expensive very fast! . 2.4 How Many Models? . Adding more hyperparameters or values, you increase the amount of models created but the increases is not linear it is proportional to how many values and hyperparameters you already have. . How many models would be created when running a grid search over the following hyperparameters and values for a GBM algorithm? . learning_rate = [0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 1, 2] | max_depth = [4,6,8,10,12,14,16,18, 20] | subsample = [0.4, 0.6, 0.7, 0.8, 0.9] | max_features = [&#39;auto&#39;, &#39;sqrt&#39;, &#39;log2&#39;] | . These lists are in your console so you can utilize properties of them to help you! . Instructions . 50 XP . Possible Answers . 26 . | 9 of one model, 9 of another . | 1 large model . | 1215 . | . In [2]: len(learning_rate) * len(max_depth) * len(subsample) * len(max_features) Out[2]: 1215 . Excellent! For every value of one hyperparameter, we test EVERY value of EVERY other hyperparameter. So you correctly multiplied the number of values (the lengths of the lists). . 2.5 Grid Search with Scikit Learn . 2.6 GridSearchCV inputs . Let&#39;s test your knowledge of GridSeachCV inputs by answering the question below. . Three GridSearchCV objects are available in the console, named model_1, model_2, model_3. Note that there is no data available to fit these models. Instead, you must answer by looking at their construct. . Which of these GridSearchCV objects would not work when we try to fit it? . Instructions . 50 XP . Possible Answers . model_1 would not work when we try to fit it. . | model_2 would not work when we try to fit it. . | model_3 would not work when we try to fit it. . | None - they will all work when we try to fit them. . | . Correct! By looking at the Scikit Learn documentation (or your excellent memory!) you know that number_attempts is not a valid hyperparameter. This GridSearchCV will not fit to our data. . Model #1: GridSearchCV( estimator = RandomForestClassifier(), param_grid = {&#39;max_depth&#39;: [2, 4, 8, 15], &#39;max_features&#39;: [&#39;auto&#39;, &#39;sqrt&#39;]}, scoring=&#39;roc_auc&#39;, n_jobs=4, cv=5, refit=True, return_train_score=True) Model #2: GridSearchCV( estimator = KNeighborsClassifier(), param_grid = {&#39;n_neighbors&#39;: [5, 10, 20], &#39;algorithm&#39;: [&#39;ball_tree&#39;, &#39;brute&#39;]}, scoring=&#39;accuracy&#39;, n_jobs=8, cv=10, refit=False) Model #3: GridSearchCV( estimator = GradientBoostingClassifier(), param_grid = {&#39;number_attempts&#39;: [2, 4, 6], &#39;max_depth&#39;: [3, 6, 9, 12]}, scoring=&#39;accuracy&#39;, n_jobs=2, cv=7, refit=True) . 2.7 GridSearchCV with Scikit Learn . The GridSearchCV module from Scikit Learn provides many useful features to assist with efficiently undertaking a grid search. You will now put your learning into practice by creating a GridSearchCV object with certain parameters. . The desired options are: . A Random Forest Estimator, with the split criterion as &#39;entropy&#39; | 5-fold cross validation | The hyperparameters max_depth (2, 4, 8, 15) and max_features (&#39;auto&#39; vs &#39;sqrt&#39;) | Use roc_auc to score the models | Use 4 cores for processing in parallel | Ensure you refit the best model and return training scores | . You will have available X_train, X_test, y_train &amp; y_test datasets. . Instructions . 100 XP . Create a Random Forest estimator as specified in the context above. | Create a parameter grid as specified in the context above. | Create a GridSearchCV object as outlined in the context above, using the two elements created in the previous two instructions. | . rf_class = RandomForestClassifier(criterion=&#39;entropy&#39;) # Create the parameter grid param_grid = { &#39;max_depth&#39;:[2, 4, 8, 15], &#39;max_features&#39;:[&#39;auto&#39;, &#39;sqrt&#39;]} # Create a GridSearchCV object grid_rf_class = GridSearchCV( estimator=rf_class, param_grid=param_grid, scoring=&#39;roc_auc&#39;, n_jobs=4, cv=5, refit=True, return_train_score=True) print(grid_rf_class) . Excellent work! You now understand all the inputs to a GridSearchCV object and can tune many different hyperparameters and many different values for each on a chosen algorithm! . 2.8 Understanding a grid search output . 2.9 Using the best outputs . Which of the following parameters must be set in order to be able to directly use the best_estimator_ property for predictions? . Answer the question . 50XP . Possible Answers . return_train_score = True . | refit = True . | refit = False . | verbose = 1 . | . Correct! When we set this to true, the creation of the grid search object automatically refits the best parameters on the whole training set and creates the best_estimator_ property. . 2.10 Exploring the grid search results . You will now explore the cv_results_ property of the GridSearchCV object defined in the video. This is a dictionary that we can read into a pandas DataFrame and contains a lot of useful information about the grid search we just undertook. . A reminder of the different column types in this property: . time_ columns | param_ columns (one for each hyperparameter) and the singular params column (with all hyperparameter settings) | a train_score column for each cv fold including the mean_train_score and std_train_score columns | a test_score column for each cv fold including the mean_test_score and std_test_score columns | a rank_test_score column with a number from 1 to n (number of iterations) ranking the rows based on their mean_test_score | . Instructions . 100 XP . Read the cv_results_ property of the grid_rf_class GridSearchCV object into a data frame &amp; print the whole thing out to inspect. | Extract &amp; print the singular column containing a dictionary of all hyperparameters used in each iteration of the grid search. | Extract &amp; print the row that had the best mean test score by indexing using the rank_test_score column. | . cv_results_df = pd.DataFrame(grid_rf_class.cv_results_) print(cv_results_df) # Extract and print the column with a dictionary of hyperparameters used column = cv_results_df.loc[:, [&quot;params&quot;]] print(column) # Extract and print the row that had the best mean test score best_row = cv_results_df[cv_results_df[&#39;rank_test_score&#39;] == 1] print(best_row) . Great work! You have built invaluable skills in looking &#39;under the hood&#39; at what your grid search is doing by extracting and analysing the cv_results_ property. . 2.11 Analyzing the best results . At the end of the day, we primarily care about the best performing &#39;square&#39; in a grid search. Luckily Scikit Learn&#39;s gridSearchCv objects have a number of parameters that provide key information on just the best square (or row in cv_results_). . Three properties you will explore are: . best_score_ – The score (here ROC_AUC) from the best-performing square. | best_index_ – The index of the row in cv_results_ containing information on the best-performing square. | best_params_ – A dictionary of the parameters that gave the best score, for example &#39;max_depth&#39;: 10 | . The grid search object grid_rf_class is available. . A dataframe (cv_results_df) has been created from the cv_results_ for you on line 6. This will help you index into the results. . Instructions . 100 XP . Extract and print out the ROC_AUC score from the best performing square in grid_rf_class. | Create a variable from the best-performing row by indexing into cv_results_df. | Create a variable, best_n_estimators by extracting the n_estimators parameter from the best-performing square in grid_rf_class and print it out. | . best_score = grid_rf_class.best_score_ print(best_score) # Create a variable from the row related to the best-performing square cv_results_df = pd.DataFrame(grid_rf_class.cv_results_) best_row = cv_results_df.loc[[grid_rf_class.best_index_]] print(best_row) # Get the n_estimators parameter from the best-performing square and print best_n_estimators = grid_rf_class.best_params_[&#39;n_estimators&#39;] print(best_n_estimators) . Nice stuff! Being able to quickly find and prioritize the huge volume of information given back from machine learning modeling output is a great skill. Here you had great practice doing that with cv_results_ by quickly isolating the key information on the best performing square. This will be very important when your grids grow from 12 squares to many more! . 2.12 Using the best results . While it is interesting to analyze the results of our grid search, our final goal is practical in nature; we want to make predictions on our test set using our estimator object. . We can access this object through the best_estimator_ property of our grid search object. . Let&#39;s take a look inside the best_estimator_ property, make predictions, and generate evaluation scores. We will firstly use the default predict (giving class predictions), but then we will need to use predict_proba rather than predict to generate the roc-auc score as roc-auc needs probability scores for its calculation. We use a slice [:,1] to get probabilities of the positive class. . You have available the X_test and y_test datasets to use and the grid_rf_class object from previous exercises. . Instructions . 100 XP . Check the type of the best_estimator_ property. | Use the best_estimator_ property to make predictions on our test set. | Generate a confusion matrix and ROC_AUC score from our predictions. | . print(type(grid_rf_class.best_estimator_)) # Create an array of predictions directly using the best_estimator_ property predictions = grid_rf_class.best_estimator_.predict(X_test) # Take a look to confirm it worked, this should be an array of 1&#39;s and 0&#39;s print(predictions[0:5]) # Now create a confusion matrix print(&quot;Confusion Matrix n&quot;, confusion_matrix(y_test, predictions)) # Get the ROC-AUC score predictions_proba = grid_rf_class.best_estimator_.predict_proba(X_test)[:,1] print(&quot;ROC-AUC Score n&quot;, roc_auc_score(y_test, predictions_proba)) . Nice stuff! The .best_estimator_ property is a really powerful property to understand for streamlining your machine learning model building process. You now can run a grid search and seamlessly use the best model from that search to make predictions. Piece of cake! . 3. Random Search . In this chapter you will be introduced to another popular automated hyperparameter tuning methodology called Random Search. You will learn what it is, how it works and importantly how it differs from grid search. You will learn some advantages and disadvantages of this method and when to choose this method compared to Grid Search. You will practice undertaking a Random Search with Scikit Learn as well as visualizing &amp; interpreting the output. . 3.1 Introducing Random Search . 3.2 Randomly Sample Hyperparameters . To undertake a random search, we firstly need to undertake a random sampling of our hyperparameter space. . In this exercise, you will firstly create some lists of hyperparameters that can be zipped up to a list of lists. Then you will randomly sample hyperparameter combinations preparation for running a random search. . You will use just the hyperparameters learning_rate and min_samples_leaf of the GBM algorithm to keep the example illustrative and not overly complicated. . Instructions . 100 XP . Create a list of 200 values for the learning_rate hyperparameter between 0.01 and 1.5 and assign to the list learn_rate_list. | Create a list of values between 10 and 40 inclusive for the hyperparameter min_samples_leaf and assign to the list min_samples_list. | Combine these lists into a list of lists to sample from. | Randomly sample 250 models from these hyperparameter combinations and print the result. | . learn_rate_list = list(np.linspace(0.01, 1.5, 200)) # Create a list of values for the min_samples_leaf hyperparameter min_samples_list = list(range(10, 41)) # Combination list combinations_list = [list(x) for x in product(learn_rate_list, min_samples_list)] # Sample hyperparameter combinations for a random search. random_combinations_index = np.random.choice(range(0, len(combinations_list)), 250, replace=False) combinations_random_chosen = [combinations_list[x] for x in random_combinations_index] # Print the result print(combinations_random_chosen) . Excellent work! You generated some hyperparameter combinations and randomly sampled in that space. The output was not too nice though, in the next lesson we will use a much more efficient method for this. In a future lesson we will also make this output look much nicer! . 3.3 Randomly Search with Random Forest . To solidify your knowledge of random sampling, let&#39;s try a similar exercise but using different hyperparameters and a different algorithm. . As before, create some lists of hyperparameters that can be zipped up to a list of lists. You will use the hyperparameters criterion, max_depth and max_features of the random forest algorithm. Then you will randomly sample hyperparameter combinations in preparation for running a random search. . You will use a slightly different package for sampling in this task, random.sample(). . Instructions . 100 XP . Create lists of the values &#39;gini&#39; and &#39;entropy&#39; for criterion &amp; &quot;auto&quot;, &quot;sqrt&quot;, &quot;log2&quot;, None for max_features. | Create a list of values between 3 and 55 inclusive for the hyperparameter max_depth and assign to the list max_depth_list. Remember that range(N,M) will create a list from N to M-1. | Combine these lists into a list of lists to sample from using product(). | Randomly sample 150 models from the combined list and print the result. | . criterion_list = [&#39;gini&#39;, &#39;entropy&#39;] max_feature_list = [&#39;auto&#39;, &#39;sqrt&#39;, &#39;log2&#39;, None] # Create a list of values for the max_depth hyperparameter max_depth_list = list(range(3, 56)) # Combination list combinations_list = [list(x) for x in product(criterion_list, max_feature_list, max_depth_list)] # Sample hyperparameter combinations for a random search combinations_random_chosen = random.sample(combinations_list, 150) # Print the result print(combinations_random_chosen) . Excellent work! This one was a bit harder but you managed to sample using text options and learned a new function to sample your lists. . 3.4 Visualizing a Random Search . Visualizing the search space of random search allows you to easily see the coverage of this technique and therefore allows you to see the effect of your sampling on the search space. . In this exercise you will use several different samples of hyperparameter combinations and produce visualizations of the search space. . The function sample_and_visualize_hyperparameters() takes a single argument (number of combinations to sample) and then randomly samples hyperparameter combinations, just like you did in the last exercise! The function will then visualize the combinations. . If you want to see the function definition, you can use Python&#39;s handy inspect library, like so: . print(inspect.getsource(sample_and_visualize_hyperparameters)) . Instructions . 100 XP . Instructions . 100 XP . Confirm how many possible hyperparameter combinations there are in combinations_list by assigning to the variable number_combs and print this out. | Sample and visualize 50, 500 and 1500 combinations. You will use a loop for succinctness. What do you notice about the visualization? | Now sample and visualize the entire set of combinations. You have already made a variable to assist with this. What does this look like? | . # Extract the 7th (index 6) tree from the random forest chosen_tree = rf_clf.estimators_[6] # Visualize the graph using the provided image imgplot = plt.imshow(tree_viz_image) plt.show() # Extract the parameters and level of the top (index 0) node split_column = chosen_tree.tree_.feature[0] split_column_name = X_train.columns[split_column] split_value = chosen_tree.tree_.threshold[1] # Print out the feature and level print(&quot;This node split on feature {}, at a value of {}&quot;.format(split_column_name, split_value)) . import matplotlib.pyplot as plt def sample_and_visualize_hyperparameters(n_samples): # If asking for all combinations, just return the entire list. if n_samples == len(combinations_list): combinations_random_chosen = combinations_list else: combinations_random_chosen = [] random_combinations_index = np.random.choice(range(0, len(combinations_list)), n_samples, replace=False) combinations_random_chosen = [combinations_list[x] for x in random_combinations_index] # Pull out the X and Y to plot rand_y, rand_x = [x[0] for x in combinations_random_chosen], [x[1] for x in combinations_random_chosen] # Plot plt.clf() plt.scatter(rand_y, rand_x, c=[&#39;blue&#39;]*len(combinations_random_chosen)) plt.gca().set(xlabel=&#39;learn_rate&#39;, ylabel=&#39;min_samples_leaf&#39;, title=&#39;Random Search Hyperparameters&#39;) plt.gca().set_xlim([0.01, 1.5]) plt.gca().set_ylim([10, 29]) . learn_rate_list = list(np.linspace(0.01, 1.5, 200)) # Create a list of values for the min_samples_leaf hyperparameter min_samples_list = list(range(10, 30)) . number_combs = len(combinations_list) print(number_combs) # Sample and visualise specified combinations for x in [50, 500, 1500]: sample_and_visualize_hyperparameters(x) # Sample all the hyperparameter combinations &amp; visualise sample_and_visualize_hyperparameters(number_combs) . Those were some great viz you produced! Notice how the bigger your sample space of a random search the more it looks like a grid search? In a later lesson we will look closer at comparing these two methods side by side. . 3.5 Random Search in Scikit Learn . 3.6 RandomSearchCV inputs . Let&#39;s test your knowledge of how RandomizedSearchCV differs from GridSearchCV. . You can check the documentation on Scitkit Learn&#39;s website to compare these two functions. . Which of these parameters is only for a RandomizedSearchCV? . Instructions . 50 XP . Possible Answers . param_grid . | n_jobs . | best_estimator_ . | n_iter . | . # Confirm how many hyperparameter combinations &amp; print number_combs = len(combinations_list) print(number_combs) # Sample and visualise specified combinations for x in [50, 500, 1500]: sample_and_visualize_hyperparameters(x) # Sample all the hyperparameter combinations &amp; visualise sample_and_visualize_hyperparameters(____) . Correct! RandomizedSearchCV asks you for how many models to sample from the grid you set. . 3.7 The RandomizedSearchCV Object . Just like the GridSearchCV library from Scikit Learn, RandomizedSearchCV provides many useful features to assist with efficiently undertaking a random search. You&#39;re going to create a RandomizedSearchCV object, making the small adjustment needed from the GridSearchCV object. . The desired options are: . A default Gradient Boosting Classifier Estimator | 5-fold cross validation | Use accuracy to score the models | Use 4 cores for processing in parallel | Ensure you refit the best model and return training scores | Randomly sample 10 models | . The hyperparameter grid should be for learning_rate (150 values between 0.1 and 2) and min_samples_leaf (all values between and including 20 and 64). . You will have available X_train &amp; y_train datasets. . Instructions . 100 XP . Create a parameter grid as specified in the context above. | Create a RandomizedSearchCV object as outlined in the context above. | Fit the RandomizedSearchCV object to the training data. | Print the values chosen by the modeling process for both hyperparameters. | . from sklearn.model_selection import train_test_split credit_card = pd.read_csv(&#39;./dataset/credit-card-full.csv&#39;) # To change categorical variable with dummy variables credit_card = pd.get_dummies(credit_card, columns=[&#39;SEX&#39;, &#39;EDUCATION&#39;, &#39;MARRIAGE&#39;], drop_first=True) X = credit_card.drop([&#39;ID&#39;, &#39;default payment next month&#39;], axis=1) y = credit_card[&#39;default payment next month&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True) from sklearn.ensemble import GradientBoostingClassifier from sklearn.model_selection import RandomizedSearchCV . param_grid = {&#39;learning_rate&#39;: np.linspace(0.1, 2, 150), &#39;min_samples_leaf&#39;: list(range(20, 65))} # Create a random search object random_GBM_class = RandomizedSearchCV( estimator=GradientBoostingClassifier(), param_distributions=param_grid, n_iter=10, scoring=&#39;accuracy&#39;, n_jobs=4, cv=5, refit=True, return_train_score=True ) # Fit to the training data random_GBM_class.fit(X_train, y_train) # Print the values used for both hyperparameters print(random_GBM_class.cv_results_[&#39;param_learning_rate&#39;]) print(random_GBM_class.cv_results_[&#39;param_min_samples_leaf&#39;]) . Great work! You have succesfully taken the knowledge gained from the grid search section and adjusted it to be able to run a random search. This is a very valuable tool to add to your Machine Learning Toolkit! . 3.8 RandomSearchCV in Scikit Learn . Let&#39;s practice building a RandomizedSearchCV object using Scikit Learn. . The hyperparameter grid should be for max_depth (all values between and including 5 and 25) and max_features (&#39;auto&#39; and &#39;sqrt&#39;). . The desired options for the RandomizedSearchCV object are: . A RandomForestClassifier Estimator with n_estimators of 80. | 3-fold cross validation (cv) | Use roc_auc to score the models | Use 4 cores for processing in parallel (n_jobs) | Ensure you refit the best model and return training scores | Only sample 5 models for efficiency (n_iter) | . X_train &amp; y_train datasets are loaded for you. . Remember, to extract the chosen hyperparameters these are found in cv_results_ with a column per hyperparameter. For example, the column for the hyperparameter criterion would be param_criterion. . Instructions . 100 XP . Create a hyperparameter grid as specified in the context above. | Create a RandomizedSearchCV object as outlined in the context above. | Fit the RandomizedSearchCV object to the training data. | Index into the cv_results_ object to print the values chosen by the modeling process for both hyperparameters (max_depth and max_features). | . param_grid = {&#39;max_depth&#39;: list(range(5, 26)), &#39;max_features&#39;: [&#39;auto&#39;, &#39;sqrt&#39;]} # Create a random search object random_rf_class = RandomizedSearchCV( estimator = RandomForestClassifier(n_estimators=80), param_distributions = param_grid, n_iter = 5, scoring=&#39;roc_auc&#39;, n_jobs=4, cv = 3, refit=True, return_train_score = True ) # Fit to the training data random_rf_class.fit(X_train, y_train) # Print the values used for both hyperparameters print(random_rf_class.cv_results_[&#39;param_max_depth&#39;]) print(random_rf_class.cv_results_[&#39;param_max_features&#39;]) . Excellent stuff! You adapted your knowledge to a new algorithm and set of hyperparameters and values. Being able to transpose your knowledge to new situations is an invaluable skill - excellent! . 3.9 Comparing Grid and Random Search . 3.10 Comparing Random &amp; Grid Search . In the video, you just studied some of the advantages and disadvantages of random search as compared to grid search. . . Which of the following is an advantage of random search? . Answer the question . 50XP . Possible Answers . It exhaustively searches all possible hyperparameter combinations, so is guaranteed to find the best model within the specified grid. . | It doesn&#39;t matter what grid you sample from, it will still find the best model. . | . There are no advantages, it is worse than Grid Search. | . It is more computationally efficient than Grid Search. | . Yes! As you saw in the slides, random search tests a larger space of values so is more likely to get close to the best score, given the same computational resources as Grid Search. . 3.11 Grid and Random Search Side by Side . Visualizing the search space of random and grid search together allows you to easily see the coverage that each technique has and therefore brings to life their specific advantages and disadvantages. . In this exercise, you will sample hyperparameter combinations in a grid search way as well as a random search way, then plot these to see the difference. . You will have available: . combinations_list which is a list of combinations of learn_rate and min_samples_leaf for this algorithm | The function visualize_search() which will make your hyperparameter combinations into X and Y coordinates and plot both grid and random search combinations on the same graph. It takes as input two lists of hyperparameter combinations. | . If you wish to view the visualize_search() function definition, you can run this code: . import inspect print(inspect.getsource(visualize_search)) . Instructions 1/4 . 35 XP . Sample (by slicing) 300 hyperparameter combinations for a grid search from combinations_list into two lists and print the result. | . grid_combinations_chosen = combinations_list[0:300] # Print result print(grid_combinations_chosen) . Instructions 2/4 . 35 XP . Let&#39;s randomly sample too. Create a list of every index in combinations_list to sample from using range() | Use np.random.choice() to sample 300 combinations. The first two arguments are a list to sample from and the number of samples. | . grid_combinations_chosen = combinations_list[0:300] # Create a list of sample indexes sample_indexes = list(range(0, len(combinations_list))) # Randomly sample 300 indexes random_indexes = np.random.choice(sample_indexes, 300, replace=False) . Instructions 3/4 . 0 XP . Now use the list of random indexes to index into combinations_list using a list comprehension. | . grid_combinations_chosen = combinations_list[0:300] # Create a list of sample indexes sample_indexes = list(range(0,len(combinations_list))) # Randomly sample 300 indexes random_indexes = np.random.choice(sample_indexes, 300, replace=False) # Use indexes to create random sample random_combinations_chosen = [combinations_list[index] for index in random_indexes] . Instructions 4/4 . 30 XP . Use the provided visualize_search() function to visualize the two sampling methodologies. The first argument is your grid combinations, the second argument is the random combinations you created. | . grid_combinations_chosen = combinations_list[0:300] # Create a list of sample indexes sample_indexes = list(range(0,len(combinations_list))) # Randomly sample 300 indexes random_indexes = np.random.choice(sample_indexes, 300, replace=False) # Use indexes to create random sample random_combinations_chosen = [combinations_list[index] for index in random_indexes] # Call the function to produce the visualization visualize_search(grid_combinations_chosen, random_combinations_chosen) . That is a great viz you produced! You can really see how a grid search will cover a small area completely whilst random search will cover a much larger area but not completely. . 4. Informed Search . In this final chapter you will be given a taste of more advanced hyperparameter tuning methodologies known as &#39;&#39;informed search&#39;&#39;. This includes a methodology known as Coarse To Fine as well as Bayesian &amp; Genetic hyperparameter tuning algorithms. You will learn how informed search differs from uninformed search and gain practical skills with each of the mentioned methodologies, comparing and contrasting them as you go. . 4.1 Informed Search: Coarse to Fine . 4.2 Visualizing Coarse to Fine . You&#39;re going to undertake the first part of a Coarse to Fine search. This involves analyzing the results of an initial random search that took place over a large search space, then deciding what would be the next logical step to make your hyperparameter search finer. . You have available: . combinations_list - a list of the possible hyperparameter combinations the random search was undertaken on. | results_df - a DataFrame that has each hyperparameter combination and the resulting accuracy of all 500 trials. Each hyperparameter is a column, with the header the hyperparameter name. | visualize_hyperparameter() - a function that takes in a column of the DataFrame (as a string) and produces a scatter plot of this column&#39;s values compared to the accuracy scores. An example call of the function would be visualize_hyperparameter(&#39;accuracy&#39;) | . If you wish to view the visualize_hyperparameter() function definition, you can run this code: . import inspect print(inspect.getsource(visualize_hyperparameter)) . Instructions . 100 XP . Confirm (by printing out) the size of the combinations_list, justifying the need to start with a random search. | Sort the results_df by accuracy values and print the top 10 rows. Are there clear insights? Beware a small sample size! | Confirm (by printing out) which hyperparameters were used in this search. These are the column names in results_df. | Call visualize_hyperparameter() with each hyperparameter in turn (max_depth, min_samples_leaf, learn_rate). Are there any trends? | . def visualize_hyperparameter(name): plt.clf() plt.scatter(results_df[name],results_df[&#39;accuracy&#39;], c=[&#39;blue&#39;]*500) plt.gca().set(xlabel=&#39;{}&#39;.format(name), ylabel=&#39;accuracy&#39;, title=&#39;Accuracy for different {}s&#39;.format(name)) plt.gca().set_ylim([0,100]) . from itertools import product max_depth_list = range(1, 6) min_samples_leaf_list = range(3, 14) learn_rate_list = np.linspace(0.01, 1.33, 200) combinations_list = [list(x) for x in product(max_depth_list, min_samples_leaf_list, learn_rate_list)] results_df = pd.read_csv(&#39;./dataset/results_df.csv&#39;) . print(len(combinations_list)) # Sort the results_df by accuracy and print the top 10 rows print(results_df.sort_values(by=&#39;accuracy&#39;, ascending=False).head(10)) # Confirm which hyperparameters were used in this search print(results_df.columns) # Call visualize_hyperparameter() with each hyperparameter in turn visualize_hyperparameter(&#39;max_depth&#39;) visualize_hyperparameter(&#39;min_samples_leaf&#39;) visualize_hyperparameter(&#39;learn_rate&#39;) . Great stuff! We have undertaken the first step of a Coarse to Fine search. Results clearly seem better when max_depth is below 20. learn_rates smaller than 1 seem to perform well too. There is not a strong trend for min_samples leaf though. Let&#39;s use this in the next exercise! . 4.3 Coarse to Fine Iterations . You will now visualize the first random search undertaken, construct a tighter grid and check the results. You will have available: . results_df - a DataFrame that has the hyperparameter combination and the resulting accuracy of all 500 trials. Only the hyperparameters that had the strongest visualizations from the previous exercise are included (max_depth and learn_rate) | visualize_first() - This function takes no arguments but will visualize each of your hyperparameters against accuracy for your first random search. | . If you wish to view the visualize_first() (or the visualize_second()) function definition, you can run this code: . import inspect print(inspect.getsource(visualize_first)) . Instructions 1/3 . 35 XP . Use the visualize_first() function to check the values of max_depth and learn_rate that tend to perform better. A convenient red line will be added to make this explicit. | . def visualize_first(): for name in results_df.columns[0:2]: plt.clf() plt.scatter(results_df[name],results_df[&#39;accuracy&#39;], c=[&#39;blue&#39;]*500) plt.gca().set(xlabel=&#39;{}&#39;.format(name), ylabel=&#39;accuracy&#39;, title=&#39;Accuracy for different {}s&#39;.format(name)) plt.gca().set_ylim([0,100]) x_line = 20 if name == &quot;learn_rate&quot;: x_line = 1 plt.axvline(x=x_line, color=&quot;red&quot;, linewidth=4) def visualize_second(): for name in results_df2.columns[0:2]: plt.clf() plt.scatter(results_df[name],results_df[&#39;accuracy&#39;], c=[&#39;blue&#39;]*500) plt.gca().set(xlabel=&#39;{}&#39;.format(name), ylabel=&#39;accuracy&#39;, title=&#39;Accuracy for different {}s&#39;.format(name)) plt.gca().set_ylim([0,100]) x_line = 20 if name == &quot;learn_rate&quot;: x_line = 1 plt.axvline(x=x_line, color=&quot;red&quot;, linewidth=4) . visualize_first() . Instructions 2/3 . 35 XP . Now create a more narrow grid search, testing for max_depth values between 1 and 20 and for 50 learning rates between 0.001 and 1. | . # visualize_first() # Create some combinations lists &amp; combine: max_depth_list = list(range(1,21)) learn_rate_list = np.linspace(0.001,1,50) . Instructions 3/3 . 30 XP . We ran the 1,000 model grid search in the background based on those new combinations. Now use the visualize_second() function to visualize the second iteration (grid search) and see if there is any improved results. This function takes no arguments, just run it in-place to generate the plots! | . # visualize_first() # Create some combinations lists &amp; combine: max_depth_list = list(range(1,21)) learn_rate_list = np.linspace(0.001,1,50) # Call the function to visualize the second results visualize_second() . Excellent! You can see in the second example our results are all generally higher. There also appears to be a bump around max_depths between 5 and 10 as well as learn_rate less than 0.2 so perhaps there is even more room for improvement! . 4.4 Informed Search: Bayesian Statistics . Bayes rule . A statistical method of using new evidence to iteratively update our beliefs about some outcome $$ P(A vert B) = frac{P(B vert A) P(A)}{P(B)}$$ . | LHS = the probability of A given B has occurred. B is some new evidence (Posterior) . | RHS = how to calculate LHS . $P(A)$ is the &#39;prior&#39;. The initial hypothesis about the event. | $P(A∣B)$ is the probability given new evidence | $P(B)$ is the &#39;marginal likelihood&#39;. It is the probability of observing this new evidence | $P(B vert A)$ is the likelihood which is the probability of observing the evidence, given the event we care about | . | . | Bayes in Hyperparameter Tuning . Pick a hyperparameter combination | Build a model | Get new evidence (the score of the model) | Update our belief and chose better hyperparamters next round | . | . 4.5 Bayes Rule in Python . In this exercise you will undertake a practical example of setting up Bayes formula, obtaining new evidence and updating your &#39;beliefs&#39; in order to get a more accurate result. The example will relate to the likelihood that someone will close their account for your online software product. . These are the probabilities we know: . 7% (0.07) of people are likely to close their account next month | 15% (0.15) of people with accounts are unhappy with your product (you don&#39;t know who though!) | 35% (0.35) of people who are likely to close their account are unhappy with your product | . Instructions 1/3 . 35 XP . Assign the different probabilities (as decimals) to variables. p_unhappy is the likelihood someone is unhappy, p_unhappy_close is the probability that someone is unhappy with the product, given they are going to close their account. | . p_unhappy = 0.15 p_unhappy_close = 0.35 . Instructions 2/3 . 35 XP . Assign the probability that someone will close their account next month to the variable p_close as a decimal. | . p_unhappy = 0.15 p_unhappy_close = 0.35 # Probabiliy someone will close p_close = 0.07 . Instructions 3/3 . 30 XP . You interview one of your customers and discover they are unhappy. What is the probability they will close their account, now that you know this evidence? Assign the result to p_close_unhappy and print it. | . p_unhappy = 0.15 p_unhappy_close = 0.35 # Probabiliy someone will close p_close = 0.07 # Probability unhappy person will close p_close_unhappy = (p_unhappy_close * p_close) / p_unhappy print(p_close_unhappy) . Nice work! You correctly were able to frame this problem in a Bayesian way, and update your beliefs using new evidence. There&#39;s a 16.3% chance that a customer, given that they are unhappy, will close their account. Next we&#39;ll use a package which uses this methodology to automatically tune hyperparameters for us. . 4.6 Bayesian Hyperparameter tuning with Hyperopt . In this example you will set up and run a Bayesian hyperparameter optimization process using the package Hyperopt (already imported as hp for you). You will set up the domain (which is similar to setting up the grid for a grid search), then set up the objective function. Finally, you will run the optimizer over 20 iterations. . You will need to set up the domain using values: . max_depth using quniform distribution (between 2 and 10, increasing by 2) | learning_rate using uniform distribution (0.001 to 0.9) | . Note that for the purpose of this exercise, this process was reduced in data sample size and hyperopt &amp; GBM iterations. If you are trying out this method by yourself on your own machine, try a larger search space, more trials, more cvs and a larger dataset size to really see this in action! . Instructions . 100 XP . Set up a space dictionary using the domain mentioned above. | Set up the objective function using a gradient boosting classifier. | Run the algorithm for 20 evaluations (just use the default, suggested algorithm from the slides). | . from sklearn.model_selection import train_test_split credit_card = pd.read_csv(&#39;./dataset/credit-card-full.csv&#39;) # To change categorical variable with dummy variables credit_card = pd.get_dummies(credit_card, columns=[&#39;SEX&#39;, &#39;EDUCATION&#39;, &#39;MARRIAGE&#39;], drop_first=True) X = credit_card.drop([&#39;ID&#39;, &#39;default payment next month&#39;], axis=1) y = credit_card[&#39;default payment next month&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True) import hyperopt as hp from hyperopt import tpe, hp, fmin from sklearn.ensemble import GradientBoostingClassifier from sklearn.model_selection import cross_val_score . space = {&#39;max_depth&#39;: hp.quniform(&#39;max_depth&#39;, 2, 10, 2), &#39;learning_rate&#39;: hp.uniform(&#39;learning_rate&#39;, 0.001, 0.9)} # Set up objective function def objective(params): params = {&#39;max_depth&#39;: int(params[&#39;max_depth&#39;]), &#39;learning_rate&#39;: params[&#39;learning_rate&#39;]} gbm_clf = GradientBoostingClassifier(n_estimators=100, **params) best_score = cross_val_score(gbm_clf, X_train, y_train, scoring=&#39;accuracy&#39;, cv=2, n_jobs=4).mean() loss = 1 - best_score return loss # Run the algorithm best = fmin(fn=objective, space=space, max_evals=20, rstate=np.random.RandomState(42), algo=tpe.suggest) print(best) . Excellent! You succesfully built your first Bayesian hyperparameter tuning algorithm. This will be a very powerful tool for your machine learning modeling in future. Bayesian hyperparameter tuning is a new and popular method so this first taster is a valuable thing to gain experience in. You are highly encouraged to extend this example on your own! . &lt;script.py&gt; output: 0%| | 0/20 [00:00&lt;?, ?it/s, best loss: ?] 5%|5 | 1/20 [00:00&lt;00:06, 2.91it/s, best loss: 0.26759418985474637] 10%|# | 2/20 [00:00&lt;00:06, 2.79it/s, best loss: 0.2549063726593165] 15%|#5 | 3/20 [00:01&lt;00:05, 2.92it/s, best loss: 0.2549063726593165] 20%|## | 4/20 [00:01&lt;00:05, 3.00it/s, best loss: 0.2549063726593165] 25%|##5 | 5/20 [00:02&lt;00:07, 1.98it/s, best loss: 0.2549063726593165] 30%|### | 6/20 [00:02&lt;00:06, 2.08it/s, best loss: 0.2549063726593165] 35%|###5 | 7/20 [00:02&lt;00:05, 2.35it/s, best loss: 0.2549063726593165] 40%|#### | 8/20 [00:03&lt;00:04, 2.47it/s, best loss: 0.2549063726593165] 45%|####5 | 9/20 [00:03&lt;00:04, 2.64it/s, best loss: 0.2549063726593165] 50%|##### | 10/20 [00:03&lt;00:03, 2.82it/s, best loss: 0.2549063726593165] 55%|#####5 | 11/20 [00:04&lt;00:02, 3.03it/s, best loss: 0.2549063726593165] 60%|###### | 12/20 [00:04&lt;00:02, 2.96it/s, best loss: 0.2549063726593165] 65%|######5 | 13/20 [00:04&lt;00:02, 2.74it/s, best loss: 0.2549063726593165] 70%|####### | 14/20 [00:05&lt;00:03, 1.87it/s, best loss: 0.2525688142203555] 75%|#######5 | 15/20 [00:06&lt;00:02, 2.23it/s, best loss: 0.2525688142203555] 80%|######## | 16/20 [00:06&lt;00:01, 2.48it/s, best loss: 0.2525688142203555] 85%|########5 | 17/20 [00:07&lt;00:01, 1.83it/s, best loss: 0.24246856171404285] 90%|######### | 18/20 [00:07&lt;00:00, 2.12it/s, best loss: 0.24246856171404285] 95%|#########5| 19/20 [00:07&lt;00:00, 2.45it/s, best loss: 0.24246856171404285] 100%|##########| 20/20 [00:08&lt;00:00, 2.80it/s, best loss: 0.24246856171404285] 100%|##########| 20/20 [00:08&lt;00:00, 2.47it/s, best loss: 0.24246856171404285] {&#39;learning_rate&#39;: 0.11310589268581149, &#39;max_depth&#39;: 6.0} . 4.7 Informed Search: Genetic Algorithms . Genetics in Machine Learning Create some models (that have hyperparameter settings) | Pick the best (by our scoring function) : these are the ones that &quot;survive&quot; | Create new models that are similar to the best ones | Add in some randomness so we don&#39;t reach a local optimum | Repeat until we are happy! | | Advantages It allows us to learn from previous iterations, just like bayesian hyperparameter tuning | It has the additional advantage of some randomness | Takes care of many tedious aspects of machine learning | . | . 4.8 Genetic Hyperparameter Tuning with TPOT . You&#39;re going to undertake a simple example of genetic hyperparameter tuning. TPOT is a very powerful library that has a lot of features. You&#39;re just scratching the surface in this lesson, but you are highly encouraged to explore in your own time. . This is a very small example. In real life, TPOT is designed to be run for many hours to find the best model. You would have a much larger population and offspring size as well as hundreds more generations to find a good model. . You will create the estimator, fit the estimator to the training data and then score this on the test data. . For this example we wish to use: . 3 generations | 4 in the population size | 3 offspring in each generation | accuracy for scoring | . A random_state of 2 has been set for consistency of results. . Instructions . 100 XP . Assign the values outlined in the context to the inputs for tpot_clf. | Create the tpot_clf classifier with the correct inputs. | Fit the classifier to the training data (X_train &amp; y_train are available in your workspace). | Use the fitted classifier to score on the test set (X_test &amp; y_test are available in your workspace). | . from tpot import TPOTClassifier . number_generations = 3 population_size = 4 offspring_size = 3 scoring_function = &#39;accuracy&#39; # Create the tpot classifier tpot_clf = TPOTClassifier(generations=number_generations, population_size=population_size, offspring_size=offspring_size, scoring=scoring_function, verbosity=2, random_state=2, cv=2) # Fit the classifier to the training data tpot_clf.fit(X_train, y_train) # Score on the test set print(tpot_clf.score(X_test, y_test)) . Nice work! You can see in the output the score produced by the chosen model (in this case a version of Naive Bayes) over each generation, and then the final accuracy score with the hyperparameters chosen for the final model. This is a great first example of using TPOT for automated hyperparameter tuning. You can now extend on this on your own and build great machine learning models! . 4.9 Analysing TPOT&#39;s stability . You will now see the random nature of TPOT by constructing the classifier with different random states and seeing what model is found to be best by the algorithm. This assists to see that TPOT is quite unstable when not run for a reasonable amount of time. . Instructions 1/3 . 35 XP . Create the TPOT classifier, fit to the data and score using a random_state of 42. | . tpot_clf = TPOTClassifier(generations=2, population_size=4, offspring_size=3, scoring=&#39;accuracy&#39;, cv=2, verbosity=2, random_state=42) # Fit the classifier to the training data tpot_clf.fit(X_train, y_train) # Score on the test set print(tpot_clf.score(X_test, y_test)) . Instructions 2/3 . Now try using a random_state of 122. The numbers don&#39;t mean anything special, but should produce different results. | . tpot_clf = TPOTClassifier(generations=2, population_size=4, offspring_size=3, scoring=&#39;accuracy&#39;, cv=2, verbosity=2, random_state=122) # Fit the classifier to the training data tpot_clf.fit(X_train, y_train) # Score on the test set print(tpot_clf.score(X_test, y_test)) . Instructions 3/3 . Finally try using the random_state of 99. See how there is a different result again? | . tpot_clf = TPOTClassifier(generations=2, population_size=4, offspring_size=3, scoring=&#39;accuracy&#39;, cv=2, verbosity=2, random_state=99) # Fit the classifier to the training data tpot_clf.fit(X_train, y_train) # Score on the test set print(tpot_clf.score(X_test, y_test)) . Well done! You can see that TPOT is quite unstable when only running with low generations, population size and offspring. The first model chosen was a Decision Tree, then a K-nearest Neighbor model and finally a Random Forest. Increasing the generations, population size and offspring and running this for a long time will assist to produce better models and more stable results. Don&#39;t hesitate to try it yourself on your own machine! . 4.10 Congratulations! .",
            "url": "https://islamalam.github.io/blog/2021/12/26/hyperparameter-tuning-in-python.html",
            "relUrl": "/2021/12/26/hyperparameter-tuning-in-python.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Image Processing in Python",
            "content": ". 1. Introducing Image Processing and scikit-image . Jump into digital image structures and learn to process them! Extract data, transform and analyze images using NumPy and Scikit-image. With just a few lines of code, you will convert RGB images to grayscale, get data from them, obtain histograms containing very useful information, and separate objects from the background! . 1.1 Make images come alive with scikit-image . Whats the main difference between the images shown below? . Image of coffee next to coins image . These images have been preloaded as coffee_image and coins_image from the scikit-image data module using: . coffee_image = data.coffee() coins_image = data.coins() . Choose the right answer that best describes the main difference related to color and dimensional structure. . In the console, use the function shape() from NumPy, to obtain the image shape (Height, Width, Dimensions) and find out. NumPy is already imported as np. . Instructions . 50 XP . Possible Answers . Both have 3 channels for RGB-3 color representation. . | coffee_image has a shape of (303, 384), grayscale. And coins_image (400, 600, 3), RGB-3. . | coins_image has a shape of (303, 384), grayscale. And coffee_image (400, 600, 3), RGB-3. . | Both are grayscale, with single color dimension. . | . 1.2 Is this gray or full of color? . 1.3 RGB to grayscale . In this exercise you will load an image from scikit-image module data and make it grayscale, then compare both of them in the output. . We have preloaded a function show_image(image, title=&#39;Image&#39;) that displays the image using Matplotlib. You can check more about its parameters using ?show_image() or help(show_image) in the console. . . Instructions . 100 XP . Import the data and color modules from Scikit image. The first module provides example images, and the second, color transformation functions. | Load the rocket image. | Convert the RGB-3 rocket image to grayscale. | . from skimage import data, color # Load the rocket image rocket = data.rocket() # Convert the image to grayscale gray_scaled_rocket = color.rgb2gray(rocket) # Show the original image show_image(rocket, &#39;Original RGB image&#39;) # Show the grayscale image show_image(gray_scaled_rocket, &#39;Grayscale image&#39;) . 1.4 NumPy for images . 1.5 Flipping out . As a prank, someone has turned an image from a photo album of a trip to Seville upside-down and back-to-front! Now, we need to straighten the image, by flipping it. . . _Image loaded as flipped_seville._ . Using the NumPy methods learned in the course, flip the image horizontally and vertically. Then display the corrected image using the show_image() function. . NumPy is already imported as np. . Instructions 1/3 . Flip the image vertically. | Now, flip the vertically-flipped image horizontally. | Show the, now fixed, image. | . seville_vertical_flip = np.flipud(flipped_seville) # Flip the image horizontally seville_horizontal_flip = np.fliplr(seville_vertical_flip) # Show the resulting image show_image(seville_horizontal_flip, &#39;Seville&#39;) . 1.6 Histograms . In this exercise, you will analyze the amount of red in the image. To do this, the histogram of the red channel will be computed for the image shown below: . . Image loaded as image. . Extracting information from images is a fundamental part of image enhancement. This way you can balance the red and blue to make the image look colder or warmer. . You will use hist() to display the 256 different intensities of the red color. And ravel() to make these color values an array of one flat dimension. . Matplotlib is preloaded as plt and Numpy as np. . Remember that if we want to obtain the green color of an image we would do the following: . green = image[:, :, 1] . Instructions . 100 XP . Obtain the red channel using slicing. | Plot the histogram and bins in a range of 256. Don&#39;t forget .ravel() for the color channel. | . red_channel = image[:, :, 0] # Plot the red histogram with bins in a range of 256 plt.hist(red_channel.ravel(), bins=256) # Set title and show plt.title(&#39;Red Histogram&#39;) plt.show() . 1.7 Getting started with thresholding . In this exercise, you&#39;ll transform a photograph to binary so you can separate the foreground from the background. . To do so, you need to import the required modules, load the image, obtain the optimal thresh value using threshold_otsu() and apply it to the image. . You&#39;ll see the resulting binarized image when using the show_image() function, previously explained. . . _Image loaded as chess_pieces_image._ . Remember we have to turn colored images to grayscale. For that we will use the rgb2gray() function learned in previous video. Which has already been imported for you. . Instructions . 100 XP . Instructions . 100 XP . Import the otsu threshold function. | Turn the image to grayscale. | Obtain the optimal threshold value of the image. | Apply thresholding to the image. | . from skimage.filters import threshold_otsu # Make the image grayscale using rgb2gray chess_pieces_image_gray = rgb2gray(chess_pieces_image) # Obtain the optimal threshold value with otsu thresh = threshold_otsu(chess_pieces_image_gray) # Apply thresholding to the image binary = chess_pieces_image_gray &gt; thresh # Show the image show_image(binary, &#39;Binary image&#39;) . 1.8 Apply global thresholding . Sometimes, it isn&#39;t that obvious to identify the background. If the image background is relatively uniform, then you can use a global threshold value as we practiced before, using threshold_otsu(). However, if there&#39;s uneven background illumination, adaptive thresholding threshold_local() (a.k.a. local thresholding) may produce better results. . In this exercise, you will compare both types of thresholding methods (global and local), to find the optimal way to obtain the binary image we need. . . _Image loaded as page_image._ . Instructions 1/2 . 50 XP . Import the otsu threshold function, obtain the optimal global thresh value of the image, and apply global thresholding. | . from skimage.filters import threshold_otsu # Obtain the optimal otsu global thresh value global_thresh = threshold_otsu(page_image) # Obtain the binary image by applying global thresholding binary_global = page_image &gt; global_thresh # Show the binary image obtained show_image(binary_global, &#39;Global thresholding&#39;) . Import the local threshold function, set block size to 35, obtain the local thresh value, and apply local thresholding. | . from skimage.filters import threshold_local # Set the block size to 35 block_size = 35 # Obtain the optimal local thresholding local_thresh = threshold_local(page_image, block_size, offset=10) # Obtain the binary image by applying local thresholding binary_local = page_image &gt; local_thresh # Show the binary image show_image(binary_local, &#39;Local thresholding&#39;) . 1.9 When the background isn&#39;t that obvious . As we saw in the video, not being sure about what thresholding method to use isn&#39;t a problem. In fact, scikit-image provides us with a function to check multiple methods and see for ourselves what the best option is. It returns a figure comparing the outputs of different global thresholding methods. . _Image loaded as fruits_image._ . You will apply this function to this image, matplotlib.pyplot has been loaded as plt. Remember that you can use try_all_threshold() to try multiple global algorithms. . Instructions . 100 XP . Import the try all function. | Import the rgb to gray convertor function. | Turn the fruits image to grayscale. | Use the try all method on the resulting grayscale image. | . from skimage.filters import try_all_threshold # Import the rgb to gray convertor function from skimage.color import rgb2gray # Turn the fruits_image to grayscale grayscale = rgb2gray(fruits_image) # Use the try all method on the resulting grayscale image fig, ax = try_all_threshold(grayscale, verbose=False) # Show the resulting plots plt.show() . 1.10 Trying other methods . In this exercise, you will decide what type of thresholding is best used to binarize an image of knitting and craft tools. In doing so, you will be able to see the shapes of the objects, from paper hearts to scissors more clearly. . . _Image loaded as tools_image._ . What type of thresholding would you use judging by the characteristics of the image? Is the background illumination and intensity even or uneven? . Instructions . 100 XP . Instructions . 100 XP . Import the appropriate thresholding and rgb2gray() functions. | Turn the image to grayscale. | Obtain the optimal thresh. | Obtain the binary image by applying thresholding. | . from skimage.filters import threshold_otsu from skimage.color import rgb2gray # Turn the image grayscale gray_tools_image = rgb2gray(tools_image) # Obtain the optimal thresh thresh = threshold_otsu(gray_tools_image) # Obtain the binary image by applying thresholding binary_image = gray_tools_image &gt; thresh # Show the resulting binary image show_image(binary_image, &#39;Binarized image&#39;) . 1.11 Apply thresholding . 2. Filters, Contrast, Transformation and Morphology . You will learn to detect object shapes using edge detection filters, improve medical images with contrast enhancement and even enlarge pictures to five times its original size! You will also apply morphology to make thresholding more accurate when segmenting images and go to the next level of processing images with Python. . 2.1 Jump into filtering . 2.2 Edge detection . In this exercise, you&#39;ll detect edges in an image by applying the Sobel filter. . . Image preloaded as soaps_image. . Theshow_image() function has been already loaded for you. . Let&#39;s see if it spots all the figures in the image. . Instructions . 100 XP . Import the color module so you can convert the image to grayscale. | Import the sobel() function from filters module. | Make soaps_image grayscale using the appropriate method from the color module. | Apply the sobel edge detection filter on the obtained grayscale image soaps_image_gray. | . from skimage import color # Import the filters module and sobel function from skimage.filters import sobel # Make the image grayscale soaps_image_gray = color.rgb2gray(soaps_image) # Apply edge detection filter edge_sobel = sobel(soaps_image_gray) # Show original and resulting image to compare show_image(soaps_image, &quot;Original&quot;) show_image(edge_sobel, &quot;Edges with Sobel&quot;) . 2.3 Blurring to reduce noise . In this exercise you will reduce the sharpness of an image of a building taken during a London trip, through filtering. . . Image loaded as building_image. . Instructions . 100 XP . Import the Gaussian filter. | Apply the filter to the building_image, set the multichannel parameter to the correct value. | Show the original building_image and resulting gaussian_image. | . from skimage.filters import gaussian # Apply filter gaussian_image = gaussian(building_image, multichannel=True) # Show original and resulting image to compare show_image(building_image, &quot;Original&quot;) show_image(gaussian_image, &quot;Reduced sharpness Gaussian&quot;) . 2.4 Contrast enhancement . . The histogram tell us. . Just as we saw previously, you can calculate the contrast by calculating the range of the pixel intensities i.e. by subtracting the minimum pixel intensity value from the histogram to the maximum one. . You can obtain the maximum pixel intensity of the image by using the np.max() method from NumPy and the minimum with np.min() in the console. . The image has already been loaded as clock_image, NumPy as np and the show_image() function. . Instructions . 50 XP . Possible Answers . The contrast is 255 (high contrast). . | The contrast is 148. . | The contrast is 189. . | The contrast is 49 (low contrast). . | . 2.5 What&#39;s the contrast of this image? . You are trying to improve the tools of a hospital by pre-processing the X-ray images so that doctors have a higher chance of spotting relevant details. You&#39;ll test our code on a chest X-ray image from the National Institutes of Health Chest X-Ray Dataset . . _Image loaded as chest_xray_image._ . First, you&#39;ll check the histogram of the image and then apply standard histogram equalization to improve the contrast. Remember we obtain the histogram by using the hist() function from Matplotlib, which has been already imported as plt. . Instructions 1/4 . 25 XP . Import the required Scikit-image module for contrast. | . from skimage import exposure . Instructions 2/4 . 25 XP . Show the histogram from the original x-ray image chest_xray_image, using the hist() function. | . from skimage import exposure # Show original x-ray image and its histogram show_image(chest_xray_image, &#39;Original x-ray&#39;) plt.title(&#39;Histogram of image&#39;) plt.hist(chest_xray_image.ravel(), bins=256) plt.show() . Instructions 3/4 . 25 XP . Use histogram equalization on chest_xray_image to obtain the improved image and load it as xray_image_eq. | . from skimage import exposure # Show original x-ray image and its histogram show_image(chest_xray_image, &#39;Original x-ray&#39;) plt.title(&#39;Histogram of image&#39;) plt.hist(chest_xray_image.ravel(), bins=256) plt.show() # Use histogram equalization to improve the contrast xray_image_eq = exposure.equalize_hist(chest_xray_image) . Instructions 4/4 . 25 XP . Show the resulting improved image xray_image_eq. | . from skimage import exposure # Show original x-ray image and its histogram show_image(chest_xray_image, &#39;Original x-ray&#39;) plt.title(&#39;Histogram of image&#39;) plt.hist(chest_xray_image.ravel(), bins=256) plt.show() # Use histogram equalization to improve the contrast xray_image_eq = exposure.equalize_hist(chest_xray_image) # Show the resulting image show_image(xray_image_eq, &#39;Resulting image&#39;) . 2.6 Medical images . 2.7 Aerial image . In this exercise, we will improve the quality of an aerial image of a city. The image has low contrast and therefore we can not distinguish all the elements in it. . _Image loaded as image_aerial._ . For this we will use the normal or standard technique of Histogram Equalization. . Instructions . 100 XP . Import the required module from scikit-image. | Use the histogram equalization function from the module previously imported. | Show the resulting image. | . from skimage import exposure # Use histogram equalization to improve the contrast image_eq = exposure.equalize_hist(image_aerial) # Show the original and resulting image show_image(image_aerial, &#39;Original&#39;) show_image(image_eq, &#39;Resulting image&#39;) . 2.8 Let&#39;s add some impact and contrast . Have you ever wanted to enhance the contrast of your photos so that they appear more dramatic? . In this exercise, you&#39;ll increase the contrast of a cup of coffee. Something you could share with your friends on social media. Don&#39;t forget to use #ImageProcessingDatacamp as hashtag! . Even though this is not our Sunday morning coffee cup, you can still apply the same methods to any of our photos. . . A function called show_image(), that displays an image using Matplotlib, has already been defined. It has the arguments image and title, with title being &#39;Original&#39; by default. . Instructions . 100 XP . Instructions . 100 XP . Import the module that includes the Contrast Limited Adaptive Histogram Equalization (CLAHE) function. | Obtain the image you&#39;ll work on, with a cup of coffee in it, from the module that holds all the images for testing purposes. | From the previously imported module, call the function to apply the adaptive equalization method on the original image and set the clip limit to 0.03. | . from skimage import data, exposure # Load the image original_image = data.coffee() # Apply the adaptive equalization on the original image adapthist_eq_image = exposure.equalize_adapthist(original_image, clip_limit=0.03) # Compare the original image to the equalized show_image(original_image) show_image(adapthist_eq_image, &#39;#ImageProcessingDatacamp&#39;) . 2.9 Transformations . 2.10 Aliasing, rotating and rescaling . Let&#39;s look at the impact of aliasing on images. . Remember that aliasing is an effect that causes different signals, in this case pixels, to become indistinguishable or distorted. . You&#39;ll make this cat image upright by rotating it 90 degrees and then rescaling it two times. Once with the anti aliasing filter applied before rescaling and a second time without it, so you can compare them. . _Image preloaded as image_cat._ . Instructions 1/4 . 25 XP . Import the module and the rotating and rescaling functions. | . from skimage.transform import rotate, rescale . Instructions 2/4 . 25 XP . Rotate the image 90 degrees clockwise. | . from skimage.transform import rotate, rescale # Rotate the image 90 degrees clockwise rotated_cat_image = rotate(image_cat, -90) . Instructions 3/4 . 25 XP . Rescale the cat_image to be 4 times smaller and apply the anti aliasing filter. Set whether or not the image should be treated as multichannel (colored). | . from skimage.transform import rotate, rescale # Rotate the image 90 degrees clockwise rotated_cat_image = rotate(image_cat, -90) # Rescale with anti aliasing rescaled_with_aa = rescale(rotated_cat_image, 1/4, anti_aliasing=True, multichannel=True) . Instructions 4/4 . 25 XP . Rescale the rotated_cat_image to be 4 times smaller without applying an anti aliasing filter. | . from skimage.transform import rotate, rescale # Rotate the image 90 degrees clockwise rotated_cat_image = rotate(image_cat, -90) # Rescale with anti aliasing rescaled_with_aa = rescale(rotated_cat_image, 1/4, anti_aliasing=True, multichannel=True) # Rescale without anti aliasing rescaled_without_aa = rescale(rotated_cat_image, 1/4, anti_aliasing=False, multichannel=True) # Show the resulting images show_image(rescaled_with_aa, &quot;Transformed with anti aliasing&quot;) show_image(rescaled_without_aa, &quot;Transformed without anti aliasing&quot;) . 2.11 Enlarging images . Have you ever tried resizing an image to make it larger? This usually results in loss of quality, with the enlarged image looking blurry. . The good news is that the algorithm used by scikit-image works very well for enlarging images up to a certain point. . In this exercise you&#39;ll enlarge an image three times!! . You&#39;ll do this by rescaling the image of a rocket, that will be loaded from the data module. . . Instructions . 100 XP . Instructions . 100 XP . Import the module and function needed to enlarge images, you&#39;ll do this by rescaling. | Import the data module. | Load the rocket() image from data. | Enlarge the rocket_image so it is 3 times bigger, with the anti aliasing filter applied. Make sure to set multichannel to True or you risk your session timing out! | . from skimage.transform import rescale # Import the data module from skimage import data # Load the image from data rocket_image = data.rocket() # Enlarge the image so it is 3 times bigger enlarged_rocket_image = rescale(rocket_image, 3, anti_aliasing=True, multichannel=True) # Show original and resulting image show_image(rocket_image) show_image(enlarged_rocket_image, &quot;3 times enlarged image&quot;) . 2.12 Proportionally resizing . We want to downscale the images of a veterinary blog website so all of them have the same compressed size. . It&#39;s important that you do this proportionally, meaning that these are not distorted. . First, you&#39;ll try it out for one image so you know what code to test later in the rest of the pictures. . _The image preloaded as dogs_banner._ . Remember that by looking at the shape of the image, you can know its width and height. . Instructions . 100 XP . Import the module and function to resize. | Set the proportional height and width so it is half the image&#39;s height size. | Resize using the calculated proportional height and width. | . from skimage.transform import resize # Set proportional height so its half its size height = int(dogs_banner.shape[0] / 2) width = int(dogs_banner.shape[1] / 2) # Resize using the calculated proportional height and width image_resized = resize(dogs_banner, (height, width), anti_aliasing=True) # Show the original and resized image show_image(dogs_banner, &#39;Original&#39;) show_image(image_resized, &#39;Resized image&#39;) . 2.13 Morphology . 2.14 Handwritten letters . A very interesting use of computer vision in real-life solutions is performing Optical Character Recognition (OCR) to distinguish printed or handwritten text characters inside digital images of physical documents. . Let&#39;s try to improve the definition of this handwritten letter so that it&#39;s easier to classify. . . As we can see it&#39;s the letter R, already binary, with some noise in it. It&#39;s already loaded as upper_r_image. . Apply the morphological operation that will discard the pixels near the letter boundaries. . Instructions . 100 XP . Instructions . 100 XP . Import the module from scikit-image. | Apply the morphological operation for eroding away the boundaries of regions of foreground pixels. | . from skimage import morphology # Obtain the eroded shape eroded_image_shape = morphology.binary_erosion(upper_r_image) # See results show_image(upper_r_image, &#39;Original&#39;) show_image(eroded_image_shape, &#39;Eroded image&#39;) . 2.15 Improving thresholded image . In this exercise, we&#39;ll try to reduce the noise of a thresholded image using the dilation morphological operation. . _Image already loaded as world_image._ . This operation, in a way, expands the objects in the image. . Instructions . 100 XP . Import the module. | Obtain the binarized and dilated image, from the original image world_image. | . from skimage import morphology # Obtain the dilated image dilated_image = morphology.binary_dilation(world_image) # See results show_image(world_image, &#39;Original&#39;) show_image(dilated_image, &#39;Dilated image&#39;) . 3. Image restoration, Noise, Segmentation and Contours . So far, you have done some very cool things with your image processing skills! In this chapter, you will apply image restoration to remove objects, logos, text, or damaged areas in pictures! You will also learn how to apply noise, use segmentation to speed up processing, and find elements in images by their contours. . https://projector-video-pdf-converter.datacamp.com/16921/chapter4.pdf#pdfjs.action=download . 3.1 Image restoration . 3.2 Let&#39;s restore a damaged image . In this exercise, we&#39;ll restore an image that has missing parts in it, using the inpaint_biharmonic() function. . . Loaded as defect_image. . We&#39;ll work on an image from the data module, obtained by data.astronaut(). Some of the pixels have been replaced with 0s using a binary mask, on purpose, to simulate a damaged image. Replacing pixels with 0s turns them totally black. The defective image is saved as an array called defect_image. . The mask is a black and white image with patches that have the position of the image bits that have been corrupted. We can apply the restoration function on these areas. This mask is preloaded as mask. . Remember that inpainting is the process of reconstructing lost or deteriorated parts of images and videos. . Instructions 1/3 . 35 XP . Import the inpaint function in the restoration module in scikit-image (skimage). | . from skimage.restoration import inpaint . Instructions 2/3 . 35 XP . Show the defective image using show_image(). | . from skimage.restoration import inpaint # Show the defective image show_image(defect_image, &#39;Image to restore&#39;) . Instructions 3/3 . 30 XP . Call the correct function from inpaint. Use the corrupted image as the first parameter, then the mask and multichannel boolean. | . from skimage.restoration import inpaint # Show the defective image show_image(defect_image, &#39;Image to restore&#39;) # Apply the restoration function to the image using the mask restored_image = inpaint.inpaint_biharmonic(defect_image, mask, multichannel=True) show_image(restored_image) . 3.3 Removing logos . As we saw in the video, another use of image restoration is removing objects from an scene. In this exercise, we&#39;ll remove the Datacamp logo from an image. . _Image loaded as image_with_logo._ . You will create and set the mask to be able to erase the logo by inpainting this area. . Remember that when you want to remove an object from an image you can either manually delineate that object or run some image analysis algorithm to find it. . Instructions . 100 XP . Instructions . 100 XP . Initialize a mask with the same shape as the image, using np.zeros(). | In the mask, set the region that will be inpainted to 1 . | Apply inpainting to image_with_logo using the mask. | . mask = np.zeros(image_with_logo.shape[:-1]) # Set the pixels where the logo is to 1 mask[210:290, 360:425] = 1 # Apply inpainting to remove the logo image_logo_removed = inpaint.inpaint_biharmonic(image_with_logo, mask, multichannel=True) # Show the original and logo removed images show_image(image_with_logo, &#39;Image with logo&#39;) show_image(image_logo_removed, &#39;Image with logo removed&#39;) . 3.4 Noise . In this exercise, we&#39;ll practice adding noise to a fruit image. . . Image preloaded as fruit_image. . Instructions . 100 XP . Import the util module and the random noise function. | Add noise to the image. | Show the original and resulting image. | . from skimage.util import random_noise # Add noise to the image noisy_image = random_noise(fruit_image) # Show original and resulting image show_image(fruit_image, &#39;Original&#39;) show_image(noisy_image, &#39;Noisy image&#39;) . 3.5 Let&#39;s make some noise! . We have a noisy image that we want to improve by removing the noise in it. . . Preloaded as noisy_image. . Use total variation filter denoising to accomplish this. . Instructions . 100 XP . Instructions . 100 XP . Import the denoise_tv_chambolle function from its module. | Apply total variation filter denoising. | Show the original noisy and the resulting denoised image. | . from skimage.restoration import denoise_tv_chambolle # Apply total variation filter denoising denoised_image = denoise_tv_chambolle(noisy_image, multichannel=True) # Show the noisy and denoised images show_image(noisy_image, &#39;Noisy&#39;) show_image(denoised_image, &#39;Denoised image&#39;) . 3.6 Reducing noise . 3.7 Reducing noise while preserving edges . In this exercise, you will reduce the noise in this landscape picture. . . Preloaded as landscape_image. . Since we prefer to preserve the edges in the image, we&#39;ll use the bilateral denoising filter. . Instructions . 100 XP . Import the denoise_bilateral function from its module. | Apply bilateral filter denoising. | Show the original noisy and the resulting denoised image. | . from skimage.restoration import denoise_bilateral # Apply bilateral filter denoising denoised_image = denoise_bilateral(landscape_image, multichannel=True) # Show original and resulting images show_image(landscape_image, &#39;Noisy image&#39;) show_image(denoised_image, &#39;Denoised image&#39;) . Great! You denoised the image without losing sharpness. In this case denoise_bilateral() worked well with the default optional parameters. . 3.8 Superpixels &amp; segmentation . 3.9 Number of pixels . Let&#39;s calculate the total number of pixels in this image. . . Image preloaded as face_image . The total amount of pixel is its resolution. Given by $ Height × Width $ . Use .shape from NumPy which is preloaded as np, in the console to check the width and height of the image. . Instructions . 50 XP . Possible Answers . face_image is 191 * 191 = 36,481 pixels . | face_image is 265 * 191 = 50,615 pixels . | face_image is 1265 * 1191 = 1,506,615 pixels . | face_image is 2265 * 2191 = 4,962,615 pixels . | . In [2]: face_image.shape Out[2]: (265, 191, 3) . Yes! The image is 50,615 pixels in total. . 3.10 Superpixel segmentation . In this exercise, you will apply unsupervised segmentation to the same image, before it&#39;s passed to a face detection machine learning model. . So you will reduce this image from $265 × 191 = 50,615 pixels$ pixels down to pixels down to $400$ regions. . . Already preloaded as face_image. . The show_image() function has been preloaded for you as well. . Instructions . 100 XP . Import the slic() function from the segmentation module. | Import the label2rgb() function from the color module. | Obtain the segmentation with 400 regions using slic(). | Put segments on top of original image to compare with label2rgb(). | . from skimage.segmentation import slic # Import the label2rgb function from color module from skimage.color import label2rgb # Obtain the segmentation with 400 regions segments = slic(face_image, n_segments= 400) # Put segments on top of original image to compare segmented_image = label2rgb(segments, face_image, kind=&#39;avg&#39;) # Show the segmented image show_image(segmented_image, &quot;Segmented image, 400 superpixels&quot;) . Awesome work! You reduced the image from 50,615 pixels to 400 regions! Much more computationally efficient for, for example, face detection machine learning models. . . 3.11 Finding contours . 3.12 Contouring shapes . In this exercise we&#39;ll find the contour of a horse. . For that we will make use of a binarized image provided by scikit-image in its data module. Binarized images are easier to process when finding contours with this algorithm. Remember that contour finding only supports 2D image arrays. . Once the contour is detected, we will display it together with the original image. That way we can check if our analysis was correct! . show_image_contour(image, contours) is a preloaded function that displays the image with all contours found using Matplotlib. . . Remember you can use the find_contours() function from the measure module, by passing the thresholded image and a constant value. . Instructions . 100 XP . Instructions . 100 XP . Import the data and the module needed for contouring detection. | Obtain the horse image shown in the context area. | Find the contours of the horse image using a constant level value of 0.8. | . In this exercise we&#39;ll find the contour of a horse. . For that we will make use of a binarized image provided by scikit-image in its data module. Binarized images are easier to process when finding contours with this algorithm. Remember that contour finding only supports 2D image arrays. . Once the contour is detected, we will display it together with the original image. That way we can check if our analysis was correct! . show_image_contour(image, contours) is a preloaded function that displays the image with all contours found using Matplotlib. . . Remember you can use the find_contours() function from the measure module, by passing the thresholded image and a constant value. . Instructions . 100 XP . Import the data and the module needed for contouring detection. | Obtain the horse image shown in the context area. | Find the contours of the horse image using a constant level value of 0.8. | . from skimage import measure, data # Obtain the horse image horse_image = data.horse() # Find the contours with a constant level value of 0.8 contours = measure.find_contours(horse_image, 0.8) # Shows the image with contours found show_image_contour(horse_image, contours) . Awesome job! You were able to find the horse contours! In the next exercise you will do some image preparation first and binarize the image yourself before finding the contours. . . 3.13 Find contours of an image that is not binary . Let&#39;s work a bit more on how to prepare an image to be able to find its contours and extract information from it. . We&#39;ll process an image of two purple dice loaded as image_dice and determine what number was rolled for each dice. . . In this case, the image is not grayscale or binary yet. This means we need to perform some image pre-processing steps before looking for the contours. First, we&#39;ll transform the image to a 2D array grayscale image and next apply thresholding. Finally, the contours are displayed together with the original image. . color, measure and filters modules are already imported so you can use the functions to find contours and apply thresholding. . We also import the io module to load the image_dice from local memory, using imread. Read more here. . Instructions 1/4 . 35 XP . Transform the image to grayscale using rgb2gray(). | . image_dice = color.rgb2gray(image_dice) . Instructions 2/4 . 35 XP . Obtain the optimal threshold value for the image and set it as thresh. | . image_dice = color.rgb2gray(image_dice) # Obtain the optimal thresh value thresh = filters.threshold_otsu(image_dice) . Instructions 3/4 . 0 XP . Apply thresholding to the image once you have the optimal threshold value thresh, using the corresponding operator. | . image_dice = color.rgb2gray(image_dice) # Obtain the optimal thresh value thresh = filters.threshold_otsu(image_dice) # Apply thresholding binary = image_dice &gt; thresh . Instructions 4/4 . 30 XP . Apply the corresponding function to obtain the contours and use a value level of 0.8. | . image_dice = color.rgb2gray(image_dice) # Obtain the optimal thresh value thresh = filters.threshold_otsu(image_dice) # Apply thresholding binary = image_dice &gt; thresh # Find contours at a constant value of 0.8 contours = measure.find_contours(binary, 0.8) # Show the image show_image_contour(image_dice, contours) . . Great work! You made the image a 2D array by slicing, applied thresholding and succesfully found the contour. Now you can apply it to any image you work on in the future. . 3.14 Count the dots in a dice&#39;s image . Now we have found the contours, we can extract information from it. . In the previous exercise, we prepared a purple dices image to find its contours: . . This time we&#39;ll determine what number was rolled for the dice, by counting the dots in the image. . The contours found in the previous exercise are preloaded as contours. . Create a list with all contour&#39;s shapes as shape_contours. You can see all the contours shapes by calling shape_contours in the console, once you have created it. . Check that most of the contours aren&#39;t bigger in size than 50. If you count them, they are the exact number of dots in the image. . show_image_contour(image, contours) is a preloaded function that displays the image with all contours found using Matplotlib. . Instructions . 100 XP . Make shape_contours be a list with all contour shapes of contours. | Set max_dots_shape to 50. | Set the shape condition of the contours to be the maximum shape size of the dots max_dots_shape. | Print the dice&#39;s number. | . shape_contours = [cnt.shape[0] for cnt in contours] # Set 50 as the maximum size of the dots shape max_dots_shape = 50 # Count dots in contours excluding bigger than dots size dots_contours = [cnt for cnt in contours if np.shape(cnt)[0] &lt; max_dots_shape] # Shows all contours found show_image_contour(binary, contours) # Print the dice&#39;s number print(&quot;Dice&#39;s dots number: {}. &quot;.format(len(dots_contours))) . &lt;script.py&gt; output: Dice&#39;s dots number: 9. . Great work! You calculated the dice&#39;s number in the image by classifing its contours. . . 4. Advanced Operations, Detecting Faces and Features . After completing this chapter, you will have a deeper knowledge of image processing as you will be able to detect edges, corners, and even faces! You will learn how to detect not just front faces but also face profiles, cat, or dogs. You will apply your skills to more complex real-world applications. Learn to master several widely used image processing techniques with very few lines of code! . link PDF . 4.1 Finding the edges with Canny . 4.2 Edges . In this exercise you will identify the shapes in a grapefruit image by detecting the edges, using the Canny algorithm. . . Image preloaded as grapefruit. . The color module has already been preloaded for you. . Instructions . 100 XP . Import the canny edge detector from the feature module. | Convert the image to grayscale, using the method from the color module used in previous chapters. | Apply the canny edge detector to the grapefruit image. | . from skimage.feature import canny # Convert image to grayscale grapefruit = color.rgb2gray(grapefruit) # Apply canny edge detector canny_edges = canny(grapefruit) # Show resulting image show_image(canny_edges, &quot;Edges with Canny&quot;) . You can see the shapes and details of the grapefruits of the original image being highlighted. . . 4.3 Less edgy . Let&#39;s now try to spot just the outer shape of the grapefruits, the circles. You can do this by applying a more intense Gaussian filter to first make the image smoother. This can be achieved by specifying a bigger sigma in the canny function. . In this exercise, you&#39;ll experiment with sigma values of the canny() function. . . Image preloaded as grapefruit. . The show_image has already been preloaded. . Instructions 1/3 . 35 XP . Instructions 1/3 . 35 XP . Apply the canny edge detector to the grapefruit image with a sigma of 1.8. | . canny_edges = canny(grapefruit, sigma=1.8) . Apply the canny edge detector to the grapefruit image with a sigma of 2.2. . edges_1_8 = canny(grapefruit, sigma=1.8) # Apply canny edge detector with a sigma of 2.2 edges_2_2 = canny(grapefruit, 2.2) . Show the resulting images. . edges_1_8 = canny(grapefruit, sigma=1.8) # Apply canny edge detector with a sigma of 2.2 edges_2_2 = canny(grapefruit, sigma=2.2) # Show resulting images show_image(edges_1_8, &quot;Sigma of 1.8&quot;) show_image(edges_2_2, &quot;Sigma of 2.2&quot;) . The bigger the sigma value, the less edges are detected because of the gaussian filter pre applied. . . . 4.4 Right around the corner . In this exercise, you will detect the corners of a building using the Harris corner detector. . . Image preloaded as building_image. . The functions show_image() and show_image_with_corners() have already been preloaded for you. As well as the color module for converting images to grayscale. . Instructions . 100 XP . Import the corner_harris() function from the feature module. | Convert the building_image to grayscale. | Apply the harris detector to obtain the measure response image with the possible corners. | Find the peaks of the corners. | . from skimage.feature import corner_harris, corner_peaks # Convert image from RGB-3 to grayscale building_image_gray = color.rgb2gray(building_image) # Apply the detector to measure the possible corners measure_image = corner_harris(building_image_gray) # Find the peaks of the corners using the Harris detector coords = corner_peaks(measure_image, min_distance=2) # Show original and resulting image with corners detected show_image(building_image, &quot;Original&quot;) show_image_with_corners(building_image, coords) . Great! You made the Harris algorithm work fine. . . . 4.5 Perspective . In this exercise, you will test what happens when you set the minimum distance between corner peaks to be a higher number. Remember you do this with the min_distance attribute parameter of the corner_peaks() function. . . Image preloaded as building_image. . The functions show_image(), show_image_with_corners() and required packages have already been preloaded for you. As well as all the previous code for finding the corners. The Harris measure response image obtained with corner_harris() is preloaded as measure_image. . Instructions 1/3 . 35 XP . Find the peaks of the corners with a minimum distance of 2 pixels. . Take Hint (-10 XP) . | Find the peaks of the corners with a minimum distance of 40 pixels. . | Show original and resulting image with corners detected. . | def show_image_with_corners(image, coords, title=&quot;Corners detected&quot;): plt.imshow(image, interpolation=&#39;nearest&#39;, cmap=&#39;gray&#39;) plt.title(title) plt.plot(coords[:, 1], coords[:, 0], &#39;+r&#39;, markersize=15) plt.axis(&#39;off&#39;) plt.show() plt.close() . coords_w_min_2 = corner_peaks(measure_image, min_distance=2) print(&quot;With a min_distance set to 2, we detect a total&quot;, len(coords_w_min_2), &quot;corners in the image.&quot;) # Find the peaks with a min distance of 40 pixels coords_w_min_40 = corner_peaks(measure_image, min_distance=40) print(&quot;With a min_distance set to 40, we detect a total&quot;, len(coords_w_min_40), &quot;corners in the image.&quot;) # Show original and resulting image with corners detected show_image_with_corners(building_image, coords_w_min_2, &quot;Corners detected with 2 px of min_distance&quot;) show_image_with_corners(building_image, coords_w_min_40, &quot;Corners detected with 40 px of min_distance&quot;) . Well done! With a 40-pixel distance between the corners there are a lot less corners than with 2 pixels. . . . 4.6 Less corners . 4.7 Face detection . 4.8 Is someone there? . In this exercise, you will check whether or not there is a person present in an image taken at night. . . Image preloaded as night_image. . The Cascade of classifiers class from feature module has been already imported. The same is true for the show_detected_face() function, that is used to display the face marked in the image and crop so it can be shown separately. . Instructions . 100 XP . Instructions . 100 XP . Load the trained file from the data module. | Initialize the detector cascade with the trained file. | Detect the faces in the image, setting the minimum size of the searching window to 10 pixels and 200 pixels for the maximum. | . def show_detected_face(result, detected, title=&quot;Face image&quot;): plt.figure() plt.imshow(result) img_desc = plt.gca() plt.set_cmap(&#39;gray&#39;) plt.title(title) plt.axis(&#39;off&#39;) for patch in detected: img_desc.add_patch( patches.Rectangle( (patch[&#39;c&#39;], patch[&#39;r&#39;]), patch[&#39;width&#39;], patch[&#39;height&#39;], fill=False, color=&#39;r&#39;, linewidth=2) ) plt.show() crop_face(result, detected) def crop_face(result, detected, title=&quot;Face detected&quot;): for d in detected: print(d) rostro= result[d[&#39;r&#39;]:d[&#39;r&#39;]+d[&#39;width&#39;], d[&#39;c&#39;]:d[&#39;c&#39;]+d[&#39;height&#39;]] plt.figure(figsize=(8, 6)) plt.imshow(rostro) plt.title(title) plt.axis(&#39;off&#39;) plt.show() . trained_file = data.lbp_frontal_face_cascade_filename() # Initialize the detector cascade detector = Cascade(trained_file) # Detect faces with min and max size of searching window detected = detector.detect_multi_scale(img = night_image, scale_factor=1.2, step_ratio=1, min_size=(10,10), max_size=(200,200)) # Show the detected faces show_detected_face(night_image, detected) . The detector found the face even when it&#39;s very small and pixelated. Note though that you would ideally want a well-illuminated image for detecting faces. . . . 4.9 Multiple faces . In this exercise, you will detect multiple faces in an image and show them individually. Think of this as a way to create a dataset of your own friends&#39; faces! . . Image preloaded as friends_image. . The Cascade of classifiers class from feature module has already been imported, as well as the show_detected_face() function which is used to display the face marked in the image and crop it so it can be shown separately. . Instructions . 100 XP . Load the trained file .lbp_frontal_face_cascade_filename(). from the data module. | Initialize the detector cascade with trained file. | Detect the faces in the image, setting a scale_factor of 1.2 and step_ratio of 1. | . trained_file = data.lbp_frontal_face_cascade_filename() # Initialize the detector cascade detector = Cascade(trained_file) # Detect faces with scale factor to 1.2 and step ratio to 1 detected = detector.detect_multi_scale(img=friends_image, scale_factor=1.2, step_ratio=1, min_size=(10, 10), max_size=(200, 200)) # Show the detected faces show_detected_face(friends_image, detected) . Wow! The detector gave you a list with all the detected faces. Can you think about what you can use this for? . . . . 4.10 Segmentation and face detection . Previously, you learned how to make processes more computationally efficient with unsupervised superpixel segmentation. In this exercise, you&#39;ll do just that! . Using the slic() function for segmentation, pre-process the image before passing it to the face detector. . . Image preloaded as profile_image. . The Cascade class, the slic() function from segmentation module, and the show_detected_face() function for visualization have already been imported. The detector is already initialized and ready to use as detector. . Instructions . 100 XP . Apply superpixel segmentation and obtain the segments a.k.a. labels using slic(). | Obtain the segmented image using label2rgb(), passing the segments and profile_image. | Detect the faces, using the detector with multi scale method. | . segments = slic(profile_image, n_segments= 100) # Obtain segmented image using label2rgb segmented_image = label2rgb(segments, profile_image, kind=&#39;avg&#39;) # Detect the faces with multi scale method detected = detector.detect_multi_scale(img=segmented_image, scale_factor=1.2, step_ratio=1, min_size=(10, 10), max_size=(1000, 1000)) # Show the detected faces show_detected_face(segmented_image, detected) . Hurray! You applied segementation to the image before passing it to the face detector and it&#39;s finding the face even when the image is relatively large. This time you used 1000 by 1000 pixels as the maximum size of the searching window because the face in this case was indeed rather larger in comparison to the image. . . 4.11 Real-world applications . 4.12 Privacy protection . Let&#39;s look at a real-world application of what you have learned in the course. . In this exercise, you will detect human faces in the image and for the sake of privacy, you will anonymize data by blurring people&#39;s faces in the image automatically. . . Image preloaded as group_image. . You can use the gaussian filter for the blurriness. . The face detector is ready to use as detector and all packages needed have been imported. . Instructions . 100 XP . Detect the faces in the image using the detector, set the minimum size of the searching window to 10 by 10 pixels. | Go through each detected face with a for loop. | Apply a gaussian filter to detect and blur faces, using a sigma of 8. | . detected = detector.detect_multi_scale(img=group_image, scale_factor=1.2, step_ratio=1, min_size=(10, 10), max_size=(100, 100)) # For each detected face for d in detected: # Obtain the face rectangle from detected coordinates face = getFaceRectangle(d) # Apply gaussian filter to extracted face blurred_face = gaussian(face, multichannel=True, sigma = 8) # Merge this blurry face to our final image and show it resulting_image = mergeBlurryFace(group_image, blurred_face) show_image(resulting_image, &quot;Blurred faces&quot;) . Awesome work! You solved this important issue by applying what you have learned in the course. . . 4.13 Help Sally restore her graduation photo . You are going to combine all the knowledge you acquired throughout the course to complete a final challenge: reconstructing a very damaged photo. . Help Sally restore her favorite portrait which was damaged by noise, distortion, and missing information due to a breach in her laptop. . . Sally&#39;s damaged portrait is already loaded as damaged_image. . You will be fixing the problems of this image by: . Rotating it to be uprightusing rotate() | Applying noise reduction with denoise_tv_chambolle() | Reconstructing the damaged parts with inpaint_biharmonic() from the inpaint module. | . show_image() is already preloaded. . Instructions . 100 XP . Instructions . 100 XP . Import the necessary module to apply restoration on the image. | Rotate the image by calling the function rotate(). | Use the chambolle algorithm to remove the noise from the image. | With the mask provided, use the biharmonic method to restore the missing parts of the image and obtain the final image. | . def get_mask(image): # Create mask with three defect regions: left, middle, right respectively mask_for_solution = np.zeros(image.shape[:-1]) mask_for_solution[450:475, 470:495] = 1 mask_for_solution[320:355, 140:175] = 1 mask_for_solution[130:155, 345:370] = 1 return mask_for_solution . from skimage.restoration import denoise_tv_chambolle, inpaint from skimage import transform # Transform the image so it&#39;s not rotated upright_img = transform.rotate(damaged_image, 20) # Remove noise from the image, using the chambolle method upright_img_without_noise = denoise_tv_chambolle(upright_img,weight=0.1, multichannel=True) # Reconstruct the image missing parts mask = get_mask(upright_img) result = inpaint.inpaint_biharmonic(upright_img_without_noise, mask, multichannel=True) show_image(result) . Great work! You have learned a lot about image processing methods and algorithms: You performed rotation, removed annoying noise, and fixed the missing pixels of the damaged image. Sally is happy and proud of you! . . 4.14 Amazing work! .",
            "url": "https://islamalam.github.io/blog/python/datacamp/machine%20learning%20scientist%20with%20python/image%20processing%20with%20python/2021/12/18/image-processing-in-python.html",
            "relUrl": "/python/datacamp/machine%20learning%20scientist%20with%20python/image%20processing%20with%20python/2021/12/18/image-processing-in-python.html",
            "date": " • Dec 18, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Advanced Deep Learning with Keras",
            "content": "Advanced Deep Learning with Keras . 1. The Keras Functional API . In this chapter, you’ll become familiar with the basics of the Keras functional API. You’ll build a simple functional network using functional building blocks, fit it to data, and make predictions. . Keras input and dense layers | Input layers | Dense layers | Output layers | Build and compile a model | Build a model | Compile a model | Visualize a model | Fit and evaluate a model5 | Fit the model to the tournament basketball data | Evaluate the model on a test set | . 2. Two Input Networks Using Categorical Embeddings, Shared Layers, and Merge Layers . In this chapter, you will build two-input networks that use categorical embeddings to represent high-cardinality data, shared layers to specify re-usable building blocks, and merge layers to join multiple inputs to a single output. By the end of this chapter, you will have the foundational building blocks for designing neural networks with complex data flows. . Category embeddings | Define team lookup | Define team model | Shared layers | Defining two inputs | Lookup both inputs in the same model | Merge layers | Output layer using shared layer | Model using two inputs and one output | Predict from your model | Fit the model to the regular season training data | Evaluate the model on the tournament test data | . 3. Multiple Inputs: 3 Inputs (and Beyond!) . In this chapter, you will extend your 2-input model to 3 inputs, and learn how to use Keras’ summary and plot functions to understand the parameters and topology of your neural networks. By the end of the chapter, you will understand how to extend a 2-input model to 3 inputs and beyond. . Three-input models | Make an input layer for home vs. away | Make a model and compile it | Fit the model and evaluate | Summarizing and plotting models | Model summaries | Plotting models | Stacking models | Add the model predictions to the tournament data | Create an input layer with multiple columns | Fit the model | Evaluate the model | . 4. Multiple Outputs . In this chapter, you will build neural networks with multiple outputs, which can be used to solve regression problems with multiple targets. You will also build a model that solves a regression problem and a classification problem simultaneously. . Two-output models | Simple two-output model | Fit a model with two outputs | Inspect the model (I) | Evaluate the model | Single model for classification and regression | Classification and regression in one model | Compile and fit the model | Inspect the model (II) | Evaluate on new data with two metrics | Wrap-up | .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/08/advanced-deep-learning-with-keras.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/08/advanced-deep-learning-with-keras.html",
            "date": " • Dec 8, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Working with Geospatial Data in Python",
            "content": "Working with Geospatial Data in Python . This is the memo of Working with Geospatial Data in Python from DataCamp . . You can find the original course HERE . . ### Course Description . A good proportion of the data out there in the real world is inherently spatial. From the population recorded in the national census, to every shop in your neighborhood, the majority of datasets have a location aspect that you can exploit to make the most of what they have to offer. This course will show you how to integrate spatial data into your Python Data Science workflow. You will learn how to interact with, manipulate and augment real-world data using their geographic dimension. You will learn to read tabular spatial data in the most common formats (e.g. GeoJSON, shapefile, geopackage) and visualize them in maps. You will then combine different sources using their location as the bridge that puts them in relation to each other. And, by the end of the course, you will be able to understand what makes geographic data unique, allowing you to transform and repurpose them in different contexts. . ### . Introduction to geospatial vector data | Spatial relationships | Projecting and transforming geometries | Putting it all together – Artisanal mining sites case study | 1. Introduction to geospatial vector data . . In this chapter, you will be introduced to the concepts of geospatial data, and more specifically of vector data. You will then learn how to represent such data in Python using the GeoPandas library, and the basics to read, explore and visualize such data. And you will exercise all this with some datasets about the city of Paris. . 1.1 Geospatial data . . 1.1.1 Restaurants in Paris . Throughout the exercises in this course, we will work with several datasets about the city of Paris. . In this exercise, we will start with exploring a dataset about the restaurants in the center of Paris (compiled from a Paris Data open dataset ). The data contains the coordinates of the point locations of the restaurants and a description of the type of restaurant. . We expect that you are familiar with the basics of the pandas library to work with tabular data ( DataFrame objects) in Python. Here, we will use pandas to read the provided csv file, and then use matplotlib to make a visualization of the points. With matplotlib, we first create a figure and axes object with fig, ax = plt.subplots() , and then use this axes object ax to create the plot. . # Import pandas and matplotlib import pandas as pd import matplotlib.pyplot as plt # Read the restaurants csv file restaurants = pd.read_csv(&quot;paris_restaurants.csv&quot;) # Inspect the first rows of restaurants print(restaurants.head()) # Make a plot of all points fig, ax = plt.subplots() ax.plot(restaurants.x, restaurants.y, &#39;o&#39;) plt.show() . type x y 0 Restaurant européen 259641.691646 6.251867e+06 1 Restaurant traditionnel français 259572.339603 6.252030e+06 2 Restaurant traditionnel français 259657.276374 6.252143e+06 3 Restaurant indien, pakistanais et Moyen Orient 259684.438330 6.252203e+06 4 Restaurant traditionnel français 259597.943086 6.252230e+06 . . 1.1.2 Adding a background map . A plot with just some points can be hard to interpret without any spatial context. Therefore, in this exercise we will learn how to add a background map. . We are going to make use of the contextily package. The add_basemap() function of this package makes it easy to add a background web map to our plot. We begin by plotting our data first, and then pass the matplotlib axes object to the add_basemap() function. contextily will then download the web tiles needed for the geographical extent of your plot. . To set the size of the plotted points, we can use the markersize keyword of the plot() method. . Pandas has been imported as pd and matplotlib’s pyplot functionality as plt . . # Read the restaurants csv file restaurants = pd.read_csv(&quot;paris_restaurants.csv&quot;) # Import contextily import contextily # A figure of all restaurants with background fig, ax = plt.subplots() ax.plot(restaurants.x, restaurants.y, &#39;o&#39;, markersize=1) contextily.add_basemap(ax) plt.show() . . . 1.2 Introduction to GeoPandas . 1.2.1 Explore the Paris districts (I) . In this exercise, we introduce a next dataset about Paris: the administrative districts of Paris (compiled from a Paris Data open dataset ). . The dataset is available as a GeoPackage file, a specialised format to store geospatial vector data, and such a file can be read by GeoPandas using the geopandas.read_file() function. . To get a first idea of the dataset, we can inspect the first rows with head() and do a quick visualization with `plot(). The attribute information about the districts included in the dataset is the district name and the population (total number of inhabitants of each district). . # Import GeoPandas import geopandas # Read the Paris districts dataset districts = geopandas.read_file(&#39;paris_districts.gpkg&#39;) # Inspect the first rows print(districts.head()) # Make a quick visualization of the districts districts.plot() plt.show() . id district_name population geometry 0 1 St-Germain-l&#39;Auxerrois 1672 POLYGON ((451922.1333912524 5411438.484355546,... 1 2 Halles 8984 POLYGON ((452278.4194036503 5412160.89282334, ... 2 3 Palais-Royal 3195 POLYGON ((451553.8057660239 5412340.522224233,... 3 4 Place-Vendôme 3044 POLYGON ((451004.907944323 5412654.094913081, ... 4 5 Gaillon 1345 POLYGON ((451328.7522686935 5412991.278156867,... . . 1.2.2 Explore the Paris districts (II) . In the previous exercise, we used the customized plot() method of the GeoDataFrame, which produces a simple visualization of the geometries in the dataset. The GeoDataFrame and GeoSeries objects can be seen as “spatial-aware” DataFrame and Series objects, and compared to their pandas counterparts, they expose additional spatial-specific methods and attributes. . The .geometry attribute of a GeoDataFrame always returns the column with the geometry objects as a GeoSeries , whichever the actual name of the column (in the default case it will also be called ‘geometry’). . Another example of extra spatial functionality is the area attribute, giving the area of the polygons. . GeoPandas has been imported as geopandas and the districts dataset is available as the districts variable. . # Check what kind of object districts is print(type(districts)) # Check the type of the geometry attribute print(type(districts.geometry)) # Inspect the first rows of the geometry print(districts.geometry.head()) # Inspect the area of the districts print(districts.geometry.area) . &lt;class &#39;geopandas.geodataframe.GeoDataFrame&#39;&gt; &lt;class &#39;geopandas.geoseries.GeoSeries&#39;&gt; 0 POLYGON ((451922.1333912524 5411438.484355546,... 1 POLYGON ((452278.4194036503 5412160.89282334, ... 2 POLYGON ((451553.8057660239 5412340.522224233,... 3 POLYGON ((451004.907944323 5412654.094913081, ... 4 POLYGON ((451328.7522686935 5412991.278156867,... Name: geometry, dtype: object 0 8.685379e+05 1 4.122371e+05 2 2.735494e+05 ... 78 1.598127e+06 79 2.089783e+06 Length: 80, dtype: float64 . 1.2.3 The Paris restaurants as a GeoDataFrame . In the first coding exercise of this chapter, we imported the locations of the restaurants in Paris from a csv file. To enable the geospatial functionality of GeoPandas, we want to convert the pandas DataFrame to a GeoDataFrame. This can be done with the GeoDataFrame() constructor and the geopandas.points_from_xy() function, and is done for you. . Now we have a GeoDataFrame, all spatial functionality becomes available, such as plotting the geometries. In this exercise we will make the same figure as in the first exercise with the restaurants dataset, but now using the GeoDataFrame’s plot() method. . Pandas has been imported as pd , GeoPandas as geopandas and matplotlib’s pyplot functionality as plt . . # Read the restaurants csv file into a DataFrame df = pd.read_csv(&quot;paris_restaurants.csv&quot;) # Convert it to a GeoDataFrame restaurants = geopandas.GeoDataFrame(df, geometry=geopandas.points_from_xy(df.x, df.y)) # Inspect the first rows of the restaurants GeoDataFrame print(restaurants.head()) # Make a plot of the restaurants ax = restaurants.plot(markersize=1) import contextily contextily.add_basemap(ax) plt.show() . type x y 0 European restuarant 259641.691646 6.251867e+06 1 Traditional French restaurant 259572.339603 6.252030e+06 2 Traditional French restaurant 259657.276374 6.252143e+06 3 Indian / Middle Eastern restaurant 259684.438330 6.252203e+06 4 Traditional French restaurant 259597.943086 6.252230e+06 geometry 0 POINT (259641.6916457232 6251867.062617987) 1 POINT (259572.3396029567 6252029.683163137) 2 POINT (259657.2763744336 6252143.400946028) 3 POINT (259684.4383301869 6252203.137238394) 4 POINT (259597.9430858413 6252230.044091299) . . . 1.3 Exploring and visualizing spatial data . 1.3.1 Visualizing the population density . Let’s get back to the districts dataset. In a previous exercise we visualized the districts with a uniform column. But often we want to show the spatial variation of a variable, and color the polygons accordingly. . In this exercise we will visualize the spatial variation of the population density within the center of Paris. For this, we will first calculate the population density by dividing the population number with the area, and add it as a new column to the dataframe. . The districts dataset is already loaded as districts , GeoPandas has been imported as geopandas and matplotlib.pyplot as plt . . # Inspect the first rows of the districts dataset print(districts.head()) # Inspect the area of the districts print(districts.area) # Add a population density column districts[&#39;population_density&#39;] = districts.population / districts.area * 10**6 # Make a plot of the districts colored by the population density districts.plot(column=&#39;population_density&#39;, legend=True) plt.show() . . id district_name population geometry 0 1 St-Germain-l&#39;Auxerrois 1672 POLYGON ((451922.1333912524 5411438.484355546,... 1 2 Halles 8984 POLYGON ((452278.4194036503 5412160.89282334, ... 2 3 Palais-Royal 3195 POLYGON ((451553.8057660239 5412340.522224233,... 3 4 Place-Vendôme 3044 POLYGON ((451004.907944323 5412654.094913081, ... 4 5 Gaillon 1345 POLYGON ((451328.7522686935 5412991.278156867,... 0 8.685379e+05 1 4.122371e+05 2 2.735494e+05 3 2.693111e+05 ... 78 1.598127e+06 79 2.089783e+06 Length: 80, dtype: float64 . 1.3.2 Using pandas functionality: groupby . This course will focus on the spatial functionality of GeoPandas, but don’t forget that we still have a dataframe, and all functionality you know from Pandas is still applicable. . In this exercise, we will recap a common functionality: the groupby operation. You may want to use this operation when you have a column containing groups, and you want to calculate a statistic for each group. In the groupby() method, you pass the column that contains the groups. On the resulting object, you can then call the method you want to calculate for each group. In this exercise, we want to know the size of each group of type of restaurants. . We refer to the course on Manipulating DataFrames with pandas for more information and exercises on this groupby operation. . # Load the restaurants data restaurants = geopandas.read_file(&quot;paris_restaurants.geosjon&quot;) # Calculate the number of restaurants of each type type_counts = restaurants.groupby(&#39;type&#39;).size() # Print the result print(type_counts) . type African restaurant 138 Asian restaurant 1642 Caribbean restaurant 27 Central and South American restuarant 97 European restuarant 1178 Indian / Middle Eastern restaurant 394 Maghrebian restaurant 207 Other world restaurant 107 Traditional French restaurant 1945 dtype: int64 . 1.3.3 Plotting multiple layers . Another typical pandas functionality is filtering a dataframe: taking a subset of the rows based on a condition (which generates a boolean mask). . In this exercise, we will take the subset of all African restaurants, and then make a multi-layered plot. In such a plot, we combine the visualization of several GeoDataFrames on a single figure. To add one layer, we can use the ax keyword of the plot() method of a GeoDataFrame to pass it a matplotlib axes object. . The restaurants data is already loaded as the restaurants GeoDataFrame. GeoPandas is imported as geopandas and matplotlib.pyplot as plt . . # Load the restaurants dataset restaurants = geopandas.read_file(&quot;paris_restaurants.geosjon&quot;) # Take a subset of the African restaurants african_restaurants = restaurants[restaurants[&#39;type&#39;]==&#39;African restaurant&#39;] # Make a multi-layered plot fig, ax = plt.subplots(figsize=(10, 10)) restaurants.plot(ax=ax, color=&#39;grey&#39;) african_restaurants.plot(ax=ax, color=&#39;red&#39;) # Remove the box, ticks and labels ax.set_axis_off() plt.show() . . 2. Spatial relationships . . 2.1 Shapely geometries and spatial relationships . 2.1.1 Creating a Point geometry . The Eiffel Tower is an iron lattice tower built in the 19th century, and is probably the most iconic view of Paris. . . (By couscouschocolat [ CC BY 2.0 ], via Wikimedia Commons) . The location of the Eiffel Tower is: x of 255422.6 and y of 6250868.9. . # Import the Point geometry from shapely.geometry import Point # Construct a point object for the Eiffel Tower eiffel_tower = Point(255422.6, 6250868.9) # Print the result print(eiffel_tower) # POINT (255422.6 6250868.9) . 2.1.2 Shapely’s spatial methods . Now we have a shapely Point object for the Eiffel Tower, we can use the different methods available on such a geometry object to perform spatial operations, such as calculating a distance or checking a spatial relationship. . We repeated the construction of eiffel_tower , and also provide the code that extracts one of the neighbourhoods (the Montparnasse district), as well as one of the restaurants located within Paris. . # Construct a point object for the Eiffel Tower eiffel_tower = Point(255422.6, 6250868.9) # Accessing the Montparnasse geometry (Polygon) and restaurant district_montparnasse = districts.loc[52, &#39;geometry&#39;] resto = restaurants.loc[956, &#39;geometry&#39;] # Is the Eiffel Tower located within the Montparnasse district? print(eiffel_tower.within(district_montparnasse)) # False # Does the Montparnasse district contains the restaurant? print(district_montparnasse.contains(resto)) # True # The distance between the Eiffel Tower and the restaurant? print(eiffel_tower.distance(resto)) # 4431.459825587039 . Note that the contains() and within() methods are the opposite of each other: if geom1.contains(geom2) is True , then also geom2.within(geom1) will be True . . . 2.2 Spatial relationships with GeoPandas . 2.2.1 In which district in the Eiffel Tower located? . Let’s return to the Eiffel Tower example. In previous exercises, we constructed a Point geometry for its location, and we checked that it was not located in the Montparnasse district. Let’s now determine in which of the districts of Paris it is located. . The districts GeoDataFrame has been loaded, and the Shapely and GeoPandas libraries are imported. . # Construct a point object for the Eiffel Tower eiffel_tower = Point(255422.6, 6250868.9) # Create a boolean Series mask = districts.contains(eiffel_tower) # Print the boolean Series print(mask.head()) # Filter the districts with the boolean mask print(districts[mask]) . 0 False 1 False 2 False 3 False 4 False dtype: bool id district_name population geometry 27 28 Gros-Caillou 25156 POLYGON ((257097.2898896902 6250116.967139574,... . 2.2.2 How far is the closest restaurant? . Now, we might be interested in the restaurants nearby the Eiffel Tower. To explore them, let’s visualize the Eiffel Tower itself as well as the restaurants within 1km. . To do this, we can calculate the distance to the Eiffel Tower for each of the restaurants. Based on this result, we can then create a mask that takes True if the restaurant is within 1km, and False otherwise, and use it to filter the restaurants GeoDataFrame. Finally, we make a visualization of this subset. . The restaurants GeoDataFrame has been loaded, and the eiffel_tower object created. Further, matplotlib, GeoPandas and contextily have been imported. . # The distance from each restaurant to the Eiffel Tower dist_eiffel = restaurants.distance(eiffel_tower) # The distance to the closest restaurant print(dist_eiffel.min()) # 460.6976028277898 # Filter the restaurants for closer than 1 km restaurants_eiffel = restaurants[dist_eiffel&lt;1000] # Make a plot of the close-by restaurants ax = restaurants_eiffel.plot() geopandas.GeoSeries([eiffel_tower]).plot(ax=ax, color=&#39;red&#39;) contextily.add_basemap(ax) ax.set_axis_off() plt.show() . . . 2.3 The spatial join operation . . 2.3.1 Paris: spatial join of districts and bike stations . Let’s return to the Paris data on districts and bike stations. We will now use the spatial join operation to identify the district in which each station is located. . The districts and bike sharing stations datasets are already pre-loaded for you as the districts and stations GeoDataFrames, and GeoPandas has been imported as geopandas . # Join the districts and stations datasets joined = geopandas.sjoin(stations, districts, op=&#39;within&#39;) # Inspect the first five rows of the result print(joined.head()) . name bike_stands available_bikes 0 14002 - RASPAIL QUINET 44 4 143 14112 - FAUBOURG SAINT JACQUES CASSINI 16 0 293 14033 - DAGUERRE GASSENDI 38 1 346 14006 - SAINT JACQUES TOMBE ISSOIRE 22 0 429 14111 - DENFERT-ROCHEREAU CASSINI 24 8 geometry index_right id district_name 0 POINT (450804.448740735 5409797.268203795) 52 53 Montparnasse 143 POINT (451419.446715647 5409421.528587255) 52 53 Montparnasse 293 POINT (450708.2275807534 5409406.941172979) 52 53 Montparnasse 346 POINT (451340.0264470892 5409124.574548723) 52 53 Montparnasse 429 POINT (451274.5111513372 5409609.730783217) 52 53 Montparnasse . 2.3.2 Map of tree density by district (1) . Using a dataset of all trees in public spaces in Paris, the goal is to make a map of the tree density by district. For this, we first need to find out how many trees each district contains, which we will do in this exercise. In the following exercise, we will then use this result to calculate the density and create a map. . To obtain the tree count by district, we first need to know in which district each tree is located, which we can do with a spatial join. Then, using the result of the spatial join, we will calculate the number of trees located in each district using the pandas ‘group-by’ functionality. . GeoPandas has been imported as geopandas . . # trees species location_type geometry 0 Marronnier Alignement POINT (455834.1224756146 5410780.605718749) 1 Marronnier Alignement POINT (446546.2841757428 5412574.696813397) 2 Marronnier Alignement POINT (449768.283096671 5409876.55691999) 3 Marronnier Alignement POINT (451779.7079508423 5409292.07146508) 4 Sophora Alignement POINT (447041.3613609616 5409756.711514045) . # Read the trees and districts data trees = geopandas.read_file(&quot;paris_trees.gpkg&quot;) districts = geopandas.read_file(&quot;paris_districts_utm.geojson&quot;) # Spatial join of the trees and districts datasets joined = geopandas.sjoin(trees, districts, op=&#39;within&#39;) # Calculate the number of trees in each district trees_by_district = joined.groupby(&#39;district_name&#39;).size() # Convert the series to a DataFrame and specify column name trees_by_district = trees_by_district.to_frame(name=&#39;n_trees&#39;) # Inspect the result print(trees_by_district.head()) . n_trees district_name Amérique 183 Archives 8 Arsenal 60 Arts-et-Metiers 20 Auteuil 392 . 2.3.3 Map of tree density by district (2) . Now we have obtained the number of trees by district, we can make the map of the districts colored by the tree density. . For this, we first need to merge the number of trees in each district we calculated in the previous step ( trees_by_district ) back to the districts dataset. We will use the pd.merge() function to join two dataframes based on a common column. . Since not all districts have the same size, it is a fairer comparison to visualize the tree density: the number of trees relative to the area. . The district dataset has been pre-loaded as districts , and the final result of the previous exercise (a DataFrame with the number of trees for each district) is available as trees_by_district . GeoPandas has been imported as geopandas and Pandas as pd . . # Print the first rows of the result of the previous exercise print(trees_by_district.head()) # Merge the &#39;districts&#39; and &#39;trees_by_district&#39; dataframes districts_trees = pd.merge(districts, trees_by_district, on=&#39;district_name&#39;) # Inspect the result print(districts_trees.head()) . district_name n_trees 0 Amérique 728 1 Archives 34 2 Arsenal 213 3 Arts-et-Metiers 79 4 Auteuil 1474 id district_name geometry n_trees 0 1 St-Germain-l&#39;Auxerrois POLYGON ((451922.1333912524 5411438.484355546,... 152 1 2 Halles POLYGON ((452278.4194036503 5412160.89282334, ... 149 2 3 Palais-Royal POLYGON ((451553.8057660239 5412340.522224233,... 6 3 4 Place-Vendôme POLYGON ((451004.907944323 5412654.094913081, ... 17 4 5 Gaillon POLYGON ((451328.7522686935 5412991.278156867,... 18 . # Merge the &#39;districts&#39; and &#39;trees_by_district&#39; dataframes districts_trees = pd.merge(districts, trees_by_district, on=&#39;district_name&#39;) # Add a column with the tree density districts_trees[&#39;n_trees_per_area&#39;] = districts_trees[&#39;n_trees&#39;] / districts_trees.geometry.area # Make of map of the districts colored by &#39;n_trees_per_area&#39; districts_trees.plot(column=&#39;n_trees_per_area&#39;) plt.show() . . . 2.4 Choropleths . . 2.4.1 Equal interval choropleth . In the last exercise, we created a map of the tree density. Now we know more about choropleths, we will explore this visualisation in more detail. . First, let’s visualize the effect of just using the number of trees versus the number of trees normalized by the area of the district (the tree density). Second, we will create an equal interval version of this map instead of using a continuous color scale. This classification algorithm will split the value space in equal bins and assign a color to each. . The district_trees GeoDataFrame, the final result of the previous exercise is already loaded. It includes the variable n_trees_per_area , measuring tree density by district (note the variable has been multiplied by 10,000). . # Print the first rows of the tree density dataset print(districts_trees.head()) # Make a choropleth of the number of trees districts_trees.plot(column=&#39;n_trees&#39;, legend=True) plt.show() # Make a choropleth of the number of trees per area districts_trees.plot(column=&#39;n_trees_per_area&#39;, legend=True) plt.show() # Make a choropleth of the number of trees districts_trees.plot(column=&#39;n_trees_per_area&#39;, scheme=&#39;equal_interval&#39;, legend=True) plt.show() . . 2.4.2 Quantiles choropleth . In this exercise we will create a quantile version of the tree density map. Remember that the quantile algorithm will rank and split the values into groups with the same number of elements to assign a color to each. This time, we will create seven groups that allocate the colors of the YlGn colormap across the entire set of values. . The district_trees GeoDataFrame is again already loaded. It includes the variable n_trees_per_area , measuring tree density by district (note the variable has been multiplied by 10,000). . # Generate the choropleth and store the axis ax = districts_trees.plot(column=&#39;n_trees_per_area&#39;, scheme=&#39;quantiles&#39;, k=7, cmap=&#39;YlGn&#39;, legend=True) # Remove frames, ticks and tick labels from the axis ax.set_axis_off() plt.show() . . 2.4.3 Compare classification algorithms . In this final exercise, you will build a multi map figure that will allow you to compare the two approaches to map variables we have seen. . You will rely on standard matplotlib patterns to build a figure with two subplots (Axes axes[0] and axes[1] ) and display in each of them, respectively, an equal interval and quantile based choropleth. Once created, compare them visually to explore the differences that the classification algorithm can have on the final result. . This exercise comes with a GeoDataFrame object loaded under the name district_trees that includes the variable n_trees_per_area , measuring tree density by district. . # Set up figure and subplots fig, axes = plt.subplots(nrows=2) # Plot equal interval map districts_trees.plot(&#39;n_trees_per_area&#39;, scheme=&#39;equal_interval&#39;, k=5, legend=True, ax=axes[0]) axes[0].set_title(&#39;Equal Interval&#39;) axes[0].set_axis_off() # Plot quantiles map districts_trees.plot(&#39;n_trees_per_area&#39;, scheme=&#39;quantiles&#39;, k=5, legend=True, ax=axes[1]) axes[1].set_title(&#39;Quantiles&#39;) axes[1].set_axis_off() # Display maps plt.show() . . 3. Projecting and transforming geometries . . 3.1 Coordinate Reference Systems . . 3.1.1 Geographic vs projected coordinates . The CRS attribute stores the information about the Coordinate Reference System in which the data is represented. In this exercises, we will explore the CRS and the coordinates of the districts dataset about the districts of Paris. . # Import the districts dataset districts = geopandas.read_file(&quot;paris_districts.geojson&quot;) # Print the CRS information print(districts.crs) # {&#39;init&#39;: &#39;epsg:4326&#39;} # Print the first rows of the GeoDataFrame print(districts.head()) . id district_name population geometry 0 1 St-Germain-l&#39;Auxerrois 1672 POLYGON ((2.344593389828428 48.85404991486192,... 1 2 Halles 8984 POLYGON ((2.349365804803003 48.86057567227663,... 2 3 Palais-Royal 3195 POLYGON ((2.339465868602756 48.86213531210705,... 3 4 Place-Vendôme 3044 POLYGON ((2.331944969393234 48.86491285292422,... 4 5 Gaillon 1345 POLYGON ((2.336320212305949 48.8679713890312, ... . Indeed, this dataset is using geographic coordinates: longitude and latitude in degrees. We could see that the crs attribute referenced the EPSG:4326 (the code for WGS84, the most common used geographic coordinate system). A further rule of thumb is that the coordinates were in a small range (&lt;180) and cannot be expressing meters. . . 3.2 Working with coordinate systems in GeoPandas . . 3.2.1 Projecting a GeoDataFrame . The Paris districts dataset is provided in geographical coordinates (longitude/latitude in WGS84). To see the result of naively using the data as is for plotting or doing calculations, we will first plot the data as is, and then plot a projected version. . The standard projected CRS for France is the RGF93 / Lambert-93 reference system (referenced by the EPSG:2154 number). . GeoPandas and matplotlib have already been imported, and the districts dataset is read and assigned to the districts variable. . # Print the CRS information print(districts.crs) # {&#39;init&#39;: &#39;epsg:4326&#39;} # Plot the districts dataset districts.plot() plt.show() # Convert the districts to the RGF93 reference system districts_RGF93 = districts.to_crs(epsg=2154) # Plot the districts dataset again districts_RGF93.plot() plt.show() . . The plot using longitude/latitude degrees distorted the shape of Paris quite a bit. . 3.2.2 Projecting a Point . In the previous chapter, we worked with the Eiffel Tower location. Again, we provided you the coordinates in a projected coordinate system, so you could, for example, calculate distances. Let’s return to this iconic landmark, and express its location in geographical coordinates: 48°51′29.6″N, 2°17′40.2″E. Or, in decimals: latitude of 48.8584 and longitude of 2.2945. . Shapely geometry objects have no notion of a CRS, and thus cannot be directly converted to another CRS. Therefore, we are going to use the GeoPandas to transform the Eiffel Tower point location to an alternative CRS. We will put the single point in a GeoSeries, use the to_crs() method, and extract the point again. . # Construct a Point object for the Eiffel Tower from shapely.geometry import Point eiffel_tower = Point(2.2945, 48.8584) # Put the point in a GeoSeries with the correct CRS s_eiffel_tower = geopandas.GeoSeries([eiffel_tower], crs={&#39;init&#39;: &#39;EPSG:4326&#39;}) # Convert to other CRS s_eiffel_tower_projected = s_eiffel_tower.to_crs(epsg=2154) # Print the projected point print(s_eiffel_tower_projected) 0 POINT (648237.3015492002 6862271.681553576) dtype: object . #### 3.2.3 Calculating distance in a projected CRS . Now we have the Eiffel Tower location in a projected coordinate system, we can calculate the distance to other points. . The final s_eiffel_tower_projected of the previous exercise containing the projected Point is already provided, and we extract the single point into the eiffel_tower variable. Further, the restaurants dataframe (using WGS84 coordinates) is also loaded. . # Extract the single Point eiffel_tower = s_eiffel_tower_projected[0] # Ensure the restaurants use the same CRS restaurants = restaurants.to_crs(s_eiffel_tower_projected.crs) # The distance from each restaurant to the Eiffel Tower dist_eiffel = restaurants.distance(eiffel_tower) # The distance to the closest restaurant print(min(dist_eiffel)) # 303.56255387894674 . Because our data was now in a projected coordinate reference system that used meters as unit, we know that the result of 303 is actually 303 meter. . 3.2.4 Projecting to Web Mercator for using web tiles . In the first chapter, we did an exercise on plotting the restaurant locations in Paris and adding a background map to it using the contextily package. . Currently, contextily assumes that your data is in the Web Mercator projection, the system used by most web tile services. And in that first exercise, we provided the data in the appropriate CRS so you didn’t need to care about this aspect. . However, typically, your data will not come in Web Mercator ( EPSG:3857 ) and you will have to align them with web tiles on your own. . GeoPandas, matplotlib and contextily are already imported. . # Convert to the Web Mercator projection restaurants_webmercator = restaurants.to_crs(epsg=3857) # Plot the restaurants with a background map ax = restaurants_webmercator.plot(markersize=1) contextily.add_basemap(ax) plt.show() . . . 3.3 Spatial operations: creating new geometries . . 3.3.1 Exploring a Land Use dataset . For the following exercises, we first introduce a new dataset: a dataset about the land use of Paris (a simplified version based on the open European Urban Atlas ). The land use indicates for what kind of activity a certain area is used, such as residential area or for recreation. It is a polygon dataset, with a label representing the land use class for different areas in Paris. . In this exercise, we will read the data, explore it visually, and calculate the total area of the different classes of land use in the area of Paris. . # Import the land use dataset land_use = geopandas.read_file(&#39;paris_land_use.shp&#39;) print(land_use.head()) # Make a plot of the land use with &#39;class&#39; as the color land_use.plot(column=&#39;class&#39;, legend=True, figsize=(15, 10)) plt.show() # Add the area as a new column land_use[&#39;area&#39;] = land_use.area # Calculate the total area for each land use class total_area = land_use.groupby(&#39;class&#39;)[&#39;area&#39;].sum() / 1000**2 print(total_area) . class geometry 0 Water bodies POLYGON ((3751386.280643055 2890064.32259039, ... 1 Roads and associated land POLYGON ((3751390.345445618 2886000, 3751390.3... 2 Roads and associated land POLYGON ((3751390.345445618 2886898.191588611,... 3 Roads and associated land POLYGON ((3751390.345445618 2887500, 3751390.3... 4 Roads and associated land POLYGON ((3751390.345445618 2888647.356784857,... class Continuous Urban Fabric 45.943090 Discontinuous Dense Urban Fabric 3.657343 Green urban areas 9.858438 Industrial, commercial, public 13.295042 Railways and associated land 1.935793 Roads and associated land 7.401574 Sports and leisure facilities 3.578509 Water bodies 3.189706 Name: area, dtype: float64 . . 3.3.2 Intersection of two polygons . For this exercise, we are going to use 2 individual polygons: the district of Muette extracted from the districts dataset, and the green urban area of Boulogne, a large public park in the west of Paris, extracted from the land_use dataset. The two polygons have already been assigned to the muette and park_boulogne variables. . We first visualize the two polygons. You will see that they overlap, but the park is not fully located in the district of Muette. Let’s determine the overlapping part. . # Plot the two polygons geopandas.GeoSeries([park_boulogne, muette]).plot(alpha=0.5, color=[&#39;green&#39;, &#39;blue&#39;]) plt.show() # Calculate the intersection of both polygons intersection = park_boulogne.intersection(muette) # Plot the intersection geopandas.GeoSeries([intersection]).plot() plt.show() # Print proportion of district area that occupied park print(intersection.area / muette.area) # 0.4352082235641065 . . 3.3.3 Intersecting a GeoDataFrame with a Polygon . Combining the land use dataset and the districts dataset, we can now investigate what the land use is in a certain district. . For that, we first need to determine the intersection of the land use dataset with a given district. Let’s take again the Muette district as example case. . The land use and districts datasets have already been imported as land_use and districts , and the Muette district has been extracted into the muette shapely polygon. Further, GeoPandas and matplotlib are imported. . # Print the land use datset and Notre-Dame district polygon print(land_use.head()) print(type(muette)) # Calculate the intersection of the land use polygons with Notre Dame land_use_muette = land_use.geometry.intersection(muette) # Plot the intersection land_use_muette.plot(edgecolor=&#39;black&#39;) plt.show() # Print the first five rows of the intersection print(land_use_muette.head()) . . class geometry 0 Industrial, commercial, public POLYGON ((3751385.614444552 2895114.54542058, ... 1 Water bodies POLYGON ((3751386.280643055 2890064.32259039, ... 2 Roads and associated land POLYGON ((3751390.345445618 2886000, 3751390.3... 3 Roads and associated land POLYGON ((3751390.345445618 2886898.191588611,... 4 Roads and associated land POLYGON ((3751390.345445618 2887500, 3751390.3... &lt;class &#39;shapely.geometry.polygon.Polygon&#39;&gt; 0 () 1 () 2 () 3 () 4 () dtype: object . . 3.4 Overlaying spatial datasets . . 3.4.1 Overlay of two polygon layers . Going back to the land use and districts datasets, we will now combine both datasets in an overlay operation. Create a new GeoDataFrame consisting of the intersection of the land use polygons wich each of the districts, but make sure to bring the attribute data from both source layers. . # Print the first five rows of both datasets print(land_use.head()) print(districts.head()) # Overlay both datasets based on the intersection combined = geopandas.overlay(land_use, districts, how=&#39;intersection&#39;) # Print the first five rows of the result print(combined.head()) . class geometry 0 Industrial, commercial, public POLYGON ((3751385.614444552 2895114.54542058, ... 1 Water bodies POLYGON ((3751386.280643055 2890064.32259039, ... 2 Roads and associated land POLYGON ((3751390.345445618 2886000, 3751390.3... 3 Roads and associated land POLYGON ((3751390.345445618 2886898.191588611,... 4 Roads and associated land POLYGON ((3751390.345445618 2887500, 3751390.3... id district_name population geometry 0 1 St-Germain-l&#39;Auxerrois 1672 POLYGON ((3760188.134760949 2889260.456597198,... 1 2 Halles 8984 POLYGON ((3760610.022313007 2889946.421907361,... 2 3 Palais-Royal 3195 POLYGON ((3759905.524344832 2890194.453753149,... 3 4 Place-Vendôme 3044 POLYGON ((3759388.396359455 2890559.229067303,... 4 5 Gaillon 1345 POLYGON ((3759742.125111854 2890864.393745991,... class id district_name population 0 Water bodies 61 Auteuil 67967 1 Continuous Urban Fabric 61 Auteuil 67967 2 Roads and associated land 61 Auteuil 67967 3 Green urban areas 61 Auteuil 67967 4 Roads and associated land 61 Auteuil 67967 geometry 0 POLYGON ((3751395.345451574 2890118.001377039,... 1 (POLYGON ((3753253.104067317 2888254.529208081... 2 POLYGON ((3751519.830145844 2890061.508628568,... 3 (POLYGON ((3754314.716711559 2890283.121013219... 4 POLYGON ((3751619.112743544 2890500, 3751626.6... . 3.4.2 Inspecting the overlay result . Now that we created the overlay of the land use and districts datasets, we can more easily inspect the land use for the different districts. Let’s get back to the example district of Muette, and inspect the land use of that district. . GeoPandas and Matplotlib are already imported. The result of the overlay() function from the previous exercises is available as combined . . # Print the first rows of the overlay result print(combined.head()) # Add the area as a column combined[&#39;area&#39;] = combined.area # Take a subset for the Muette district land_use_muette = combined[combined.district_name==&#39;Muette&#39;] # Visualize the land use of the Muette district land_use_muette.plot(column=&#39;class&#39;) plt.show() # Calculate the total area for each land use class print(land_use_muette.groupby(&#39;class&#39;)[&#39;area&#39;].sum() / 1000**2) . class id district_name population 0 Water bodies 61 Auteuil 67967 1 Continuous Urban Fabric 61 Auteuil 67967 2 Roads and associated land 61 Auteuil 67967 3 Green urban areas 61 Auteuil 67967 4 Roads and associated land 61 Auteuil 67967 geometry 0 POLYGON ((3751395.345451574 2890118.001377039,... 1 (POLYGON ((3753253.104067317 2888254.529208081... 2 POLYGON ((3751519.830145844 2890061.508628568,... 3 (POLYGON ((3754314.716711559 2890283.121013219... 4 POLYGON ((3751619.112743544 2890500, 3751626.6... class Continuous Urban Fabric 1.275297 Discontinuous Dense Urban Fabric 0.088289 Green urban areas 2.624229 Industrial, commercial, public 0.362990 Railways and associated land 0.005424 Roads and associated land 0.226271 Sports and leisure facilities 0.603989 Water bodies 0.292194 Name: area, dtype: float64 . . 4. Putting it all together – Artisanal mining sites case study . . 4.1 Introduction to the dataset . . 4.1.1 Import and explore the data . In this exercise, we will start with reading and exploring two new datasets: . First, a dataset on artisanal mining sites in Eastern Congo (adapted from IPIS open data ). | Second, a dataset on the national parks in Congo (adapted from the World Resources Institute ). | . For each of those datasets, the exercise consists of importing the necessary packages, reading the data with geopandas.read_file() , inspecting the first 5 rows and the Coordinate Reference System (CRS) of the data, and making a quick visualization. . # Import GeoPandas and Matplotlib import geopandas import matplotlib.pyplot as plt # Read the mining site data mining_sites = geopandas.read_file(&#39;ipis_cod_mines.geojson&#39;) # Print the first rows and the CRS information print(mining_sites.head()) print(mining_sites.crs) # Make a quick visualisation mining_sites.plot() plt.show() . visit_date name n_workers mineral geometry 0 2013-03-27 Mayi-Tatu 150.0 Gold POINT (29.66033 1.01089) 1 2013-03-27 Mabanga 115.0 Gold POINT (29.65862 1.00308) 2 2013-03-27 Molende 130.0 Gold POINT (29.65629 0.98563) 3 2013-03-27 Embouchure 135.0 Gold POINT (29.64494 0.99976) 4 2013-03-27 Apumu-Atandele-Jerusalem-Luka Yayo 270.0 Gold POINT (29.66 0.956) {&#39;init&#39;: &#39;epsg:4326&#39;} . . # Import GeoPandas and Matplotlib import geopandas import matplotlib.pyplot as plt # Read the mining site data national_parks = geopandas.read_file(&quot;cod_conservation.shp&quot;) # Print the first rows and the CRS information print(national_parks.head()) print(national_parks.crs) # Make a quick visualisation national_parks.plot() plt.show() . Type Name geometry 0 Nature Reserve Luki Biosphere Reserve POLYGON ((1469015.469222862 -605537.8418950802... 1 Nature Reserve Itombwe Nature Reserve POLYGON ((3132067.8539 -408115.0111999996, 313... 2 Nature Reserve Okapi Faunal Reserve POLYGON ((3197982.926399998 148235.506099999, ... 3 National park Salonga National park POLYGON ((2384337.1864 -280729.9739000015, 238... 4 National park Salonga National park POLYGON ((2399938.984200001 -152211.4943000004... {&#39;no_defs&#39;: True, &#39;y_0&#39;: 0, &#39;x_0&#39;: 0, &#39;proj&#39;: &#39;merc&#39;, &#39;units&#39;: &#39;m&#39;, &#39;datum&#39;: &#39;WGS84&#39;, &#39;lon_0&#39;: 0, &#39;lat_ts&#39;: 5} . . For the mining sites, it indicated EPSG:4326, so the dataset is expressed in geographical longitude/latitude. The last dataset, the national parks, is in projected coordinates. So we will need to make sure both datasets are in the same CRS to be able to use them together. . 4.1.2 Convert to common CRS and save to a file . As we have seen in the previous exercises, both datasets are using a different Coordinate Reference System (CRS). This is also illustrated by the first plot in this exercise (for which the code is already provided in the script): both datasets are about the same region, so they should normally overlap in their coordinates; but they don’t. . For further analyses in the rest of this chapter, we will convert both datasets to the same CRS, and save both to a new file. To ensure we can do distance-based calculations, we will convert them to a projected CRS: the local UTM zone 35, which is identified by EPSG:32735 ( https://epsg.io/32735 ). . The mining sites ( mining_sites ) and national parks ( national_parks ) datasets are already loaded, and GeoPandas and matplotlib are imported. . # Plot the natural parks and mining site data ax = national_parks.plot() mining_sites.plot(ax=ax, color=&#39;red&#39;) plt.show() # Convert both datasets to UTM projection mining_sites_utm = mining_sites.to_crs(epsg=32735) national_parks_utm = national_parks.to_crs(epsg=32735) # Plot the converted data again ax = national_parks_utm.plot() mining_sites_utm.plot(ax=ax, color=&#39;red&#39;) plt.show() . . # Read the mining site data mining_sites = geopandas.read_file(&quot;ipis_cod_mines.geojson&quot;) national_parks = geopandas.read_file(&quot;cod_conservation.shp&quot;) # Convert both datasets to UTM projection mining_sites_utm = mining_sites.to_crs(epsg=32735) national_parks_utm = national_parks.to_crs(epsg=32735) # Write converted data to a file mining_sites_utm.to_file(&#39;ipis_cod_mines_utm.gpkg&#39;, driver=&#39;GPKG&#39;) national_parks_utm.to_file(&quot;cod_conservation_utm.shp&quot;, driver=&#39;ESRI Shapefile&#39;) . 4.1.3 Styling a multi-layered plot . Now we have converted both datasets to the same Coordinate Reference System, let’s make a nicer plot combining the two. . The datasets in UTM coordinates as we saved them to files in the last exercise are read back in and made available as the mining_sites and national_parks variables. GeoPandas and matplotlib are already imported. . # Plot of the parks and mining sites ax = national_parks.plot(color=&#39;green&#39;) mining_sites.plot(ax=ax, markersize=5) plt.show() . . # Plot of the parks and mining sites ax = national_parks.plot(color=&#39;green&#39;) mining_sites.plot(ax=ax, markersize=5, alpha=0.5) ax.set_axis_off() plt.show() . . # Plot of the parks and mining sites ax = national_parks.plot(color=&#39;green&#39;) mining_sites.plot(ax=ax, column = &#39;mineral&#39;, markersize=5, legend=True) ax.set_axis_off() plt.show() . . . 4.2 Additional spatial operations . . 4.2.1 Buffer around a point . Consider the city of Goma, the capital of the North Kivu province of Congo, close to the border with Rwanda. Its coordinates are 1.66°S 29.22°E (the Point is already provided in UTM coordinates as the goma variable). . How many mining sites are located within 50 km of Goma? And how much area of national park? Let’s determine that using the buffer operation. Remember that distances should be expressed in the unit of the CRS (i.e. in meter in this case). . Note: if you have a boolean Series (for example as result of a spatial relationship method), then you can calculate how many True values (ie. how many geometries passed the check) by taking the sum of those booleans because in that case the True and False values will be seen as ones and zeros. . # goma is a Point print(type(goma)) # Create a buffer of 50km around Goma goma_buffer = goma.buffer(50000) # The buffer is a polygon print(type(goma_buffer)) # Check how many sites are located within the buffer mask = mining_sites.within(goma_buffer) print(mask.sum()) # Calculate the area of national park within the buffer print(national_parks.intersection(goma_buffer).area.sum() / (1000**2)) . 4.2.2 Mining sites within national parks . For this exercise, let’s start with one of the national parks, the Kahuzi-Biega National park (which was extracted from the national_parks dataset and is provided as the kahuzi variable). . Which of the mining sites are located within this national park? . And as a second step: can we determine all mining sites that are located within one of the national parks and in which park? . The mining sites ( mining_sites ) and national parks ( national_parks ) datasets are already loaded, and GeoPandas is already imported. . # Extract the single polygon for the Kahuzi-Biega National park kahuzi = national_parks[national_parks[&#39;Name&#39;] == &quot;Kahuzi-Biega National park&quot;].geometry.squeeze() # Take a subset of the mining sites located within Kahuzi sites_kahuzi = mining_sites[mining_sites.within(kahuzi)] print(sites_kahuzi) # Determine in which national park a mining site is located sites_within_park = geopandas.sjoin(mining_sites, national_parks, op=&#39;within&#39;, how=&#39;inner&#39;) print(sites_within_park.head()) # The number of mining sites in each national park print(sites_within_park[&#39;name&#39;].value_counts()) . visit_date name n_workers mineral 661 2013-08-28Z Ibozia/Kalumé 80.0 Cassiterite 662 2013-08-26Z Matamba 150.0 Cassiterite 663 2013-08-27Z Mutete/Mukina 170.0 Cassiterite 664 2013-08-28Z Mutete 100.0 Cassiterite 760 2014-02-25Z Mazankala 120.0 Cassiterite 813 2015-07-28Z Kitendebwa 50.0 Gold 869 2013-09-28Z Sebwa-Lukoma 130.0 Cassiterite 870 2013-10-30Z Rwamakaza 160.0 Cassiterite 1481 2009-01-01Z Mugaba I 50.0 Gold 1482 2009-01-01Z Mugaba Ouest 46.0 Gold 1676 2015-08-02Z Nguba(Nkuba) kamisoke 122.0 Cassiterite geometry 661 POINT (567832.7086093378 9759143.339360647) 662 POINT (598323.5389475008 9758688.142411157) 663 POINT (570733.4369126211 9761871.114227083) 664 POINT (569881.0930415759 9762219.110778008) 760 POINT (613075.5326777868 9722956.979837928) 813 POINT (693078.9282059025 9770107.517721133) 869 POINT (660406.3452248175 9715261.717041001) 870 POINT (661266.834456568 9716072.198784607) 1481 POINT (685167.3714990132 9744069.967416598) 1482 POINT (683156.6865782175 9746324.416321497) 1676 POINT (622151.3489110788 9808363.111073116) visit_date name n_workers mineral geometry 253 2013-09-05Z Kiviri/Tayna 244.0 Gold POINT (709734.912568812 9961013.720415946) 578 2015-09-02Z Lubondozi 3 30.0 Gold POINT (578464.3150203574 9555456.293453641) 579 2015-09-02Z Katamu 180.0 Gold POINT (576249.9033853477 9554313.725408439) 580 2015-09-02Z Kimabwe 1 120.0 Gold POINT (576425.7766608761 9556329.633628448) 581 2015-09-02Z Lubondozi 1 300.0 Gold POINT (579164.711161439 9554722.924142597) index_right Type Name 253 23 Nature Reserve Tayna Nature Reserve 578 15 Hunting Domain Luama-Kivu Hunting Domain 579 15 Hunting Domain Luama-Kivu Hunting Domain 580 15 Hunting Domain Luama-Kivu Hunting Domain 581 15 Hunting Domain Luama-Kivu Hunting Domain Colline 7 1 Mutete 1 .. Kimabwe 1 1 Muchacha 1 Name: name, Length: 64, dtype: int64 . . 4.3 Applying custom spatial operations . #### 4.3.1 Finding the name of the closest National Park . Let’s start with a custom query for a single mining site. Here, we will determine the name of the national park that is the closest to the specific mining site. . The datasets on the mining sites ( mining_sites ) and national parks ( national_parks ) are already loaded. . mining_sites.head(1) visit_date name n_workers mineral geometry 0 2013-03-27Z Mayi-Tatu 150.0 Gold POINT (796089.4159891906 10111855.17426374) national_parks.head(1) Type Name geometry 0 Nature Reserve Luki Biosphere Reserve POLYGON ((-1038121.47250213 9375412.18990065, ... . # Get the geometry of the first row single_mine = mining_sites.geometry[0] # Calculate the distance from each national park to this mine dist = national_parks.distance(single_mine) # The index of the minimal distance idx = dist.idxmin() # Access the name of the corresponding national park closest_park = national_parks.loc[idx, &#39;Name&#39;] print(closest_park) . 4.3.2 Applying a custom operation to each geometry . Now we know how to get the closest national park for a single point, let’s do this for all points. For this, we are first going to write a function, taking a single point as argument and returning the desired result. Then we can use this function to apply it to all points. . The datasets on the mining sites ( mining_sites ) and national parks ( national_parks ) are already loaded. The single mining site from the previous exercises is already defined as single_mine . . # Define a function that returns the closest national park def closest_national_park(geom, national_parks): dist = national_parks.distance(geom) idx = dist.idxmin() closest_park = national_parks.loc[idx, &#39;Name&#39;] return closest_park # Call the function on single_mine print(closest_national_park(single_mine, national_parks)) # Apply the function to all mining sites mining_sites[&#39;closest_park&#39;] = mining_sites.geometry.apply(closest_national_park, national_parks=national_parks) print(mining_sites.head()) . Virunga National park visit_date name n_workers mineral 0 2013-03-27Z Mayi-Tatu 150.0 Gold 1 2013-03-27Z Mabanga 115.0 Gold 2 2013-03-27Z Molende 130.0 Gold 3 2013-03-27Z Embouchure 135.0 Gold 4 2013-03-27Z Apumu-Atandele-Jerusalem-Luka Yayo 270.0 Gold geometry closest_park 0 POINT (796089.4159891906 10111855.17426374) Virunga National park 1 POINT (795899.6640655082 10110990.83998195) Virunga National park 2 POINT (795641.7066578076 10109059.78659637) Virunga National park 3 POINT (794376.3093052682 10110622.24995522) Virunga National park 4 POINT (796057.5042468573 10105781.54751797) Virunga National park . . 4.4 Working with raster data . . 4.4.1 Import and plot raster data . In this exercise, we are going to use a raster dataset of the vegetation types map (available from http://www.wri.org ). The raster values take a set of discrete values indicating the type of vegetation. Let’s start with reading the data and plotting it together with the mining site data. . The mining sites dataset ( mining_sites ) is already loaded, and GeoPandas and matplotlib are already imported. . # Import the rasterio package import rasterio # Open the raster dataset src = rasterio.open(&quot;central_africa_vegetation_map_foraf.tif&quot;) # Import the plotting functionality of rasterio import rasterio.plot # Plot the raster layer with the mining sites ax = rasterio.plot.show(src) mining_sites.plot(ax=ax, color=&#39;red&#39;, markersize=1) plt.show() . . 4.4.2 Extract information from raster layer . Let’s now extract information from the raster layer, based on a vector file. This functionality is provided by the rasterstats package. Specifically for this exercise, we will determine the vegetation type at all mining sites, by getting the nearest raster pixel value at each point of the mining site dataset. . A subset of the mining sites dataset ( mining_sites ) is already loaded, and GeoPandas and matplotlib are already imported. . # Import the rasterstats package import rasterstats # Extract the nearest value in the raster for all mining sites vegetation_raster = &quot;central_africa_vegetation_map_foraf.tif&quot; mining_sites[&#39;vegetation&#39;] = rasterstats.point_query(mining_sites.geometry, vegetation_raster, interpolate=&#39;nearest&#39;) print(mining_sites.head()) # Replace numeric vegation types codes with description mining_sites[&#39;vegetation&#39;] = mining_sites[&#39;vegetation&#39;].replace(vegetation_types) # Make a plot indicating the vegetation type mining_sites.plot(column=&#39;vegetation&#39;, legend=True) plt.show() . visit_date name n_workers mineral geometry vegetation 350 2013-05-30 Kunguo 154.0 Gold POINT (28.274 -1.00941) 1 2056 2017-08-10 Masange 4.0 Wolframite POINT (27.329875 -1.08929) 1 686 2013-09-09 Kabusangala 120.0 Gold POINT (27.136147 -3.425685) 1 602 2013-12-23 Simunofu 120.0 Cassiterite POINT (26.541903 -1.540585) 1 571 2013-09-07 Kigali 26.0 Cassiterite POINT (26.601226 -1.398051) 7 . . 4.4.3 Further reference . . . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/working-with-geospatial-data-in-python.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/working-with-geospatial-data-in-python.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Visualizing Geospatial Data in Python",
            "content": "Visualizing Geospatial Data in Python . This is the memo of the 5th course (5 courses in all) of ‘Data Visualization with Python’ skill track. . You can find the original course HERE . . ### Course Description . One of the most important tasks of a data scientist is to understand the relationships between their data’s physical location and their geographical context. In this course you’ll be learning to make attractive visualizations of geospatial data with the GeoPandas package. You will learn to spatially join datasets, linking data to context. Finally you will learn to overlay geospatial data to maps to add even more spatial cues to your work. You will use several datasets from the City of Nashville’s open data portal to find out where the chickens are in Nashville, which neighborhood has the most public art, and more! . ### . Building 2-layer maps : combining polygons and scatterplots | Creating and joining GeoDataFrames | GeoSeries and folium | Creating a choropleth building permit density in Nashville | 1. Building 2-layer maps : combining polygons and scatterplots . . 1.1 Introduction . . 1.1.1 Plotting a scatterplot from longitude and latitude . When using latitude and longitude to create a scatterplot, which value is plotted along the horizontal axis (as x)? . Longitude is plotted as x on the horizontal axis. . 1.1.2 Styling a scatterplot . In this exercise, you’ll be using plt.scatter() to plot the father and son height data from the video. The father_son DataFrame is available in your workspace. In each scatterplot, plot father_son.fheight as x-axis and father_son.sheight as y-axis. . # Import matplotlib.pyplot import matplotlib.pyplot as plt # Scatterplot 3 plt.scatter(father_son.fheight, father_son.sheight, c = &#39;yellow&#39;, edgecolor = &#39;darkblue&#39;) plt.show() plt.xlabel(&#39;father height (inches)&#39;) plt.ylabel(&#39;son height (inches)&#39;) plt.title(&#39;Son Height as a Function of Father Height&#39;) plt.grid() # Show your plot plt.show() . . 1.1.2 Extracting longitude and latitude . A DataFrame named df has been loaded to your workspace. Complete the code to extract longitude and latitude to new, separate columns. . df.head() StopID StopName Location 0 4431 MUSIC CITY CENTRAL 5TH - BAY 11 (36.16659, -86.781996) 1 588 CHARLOTTE AVE &amp; 7TH AVE N WB (36.165, -86.78406) 2 590 CHARLOTTE AVE &amp; 8TH AVE N WB (36.164393, -86.785451) 3 541 11TH AVE / N GULCH STATION OUTBOUND (36.162249, -86.790464) type(df.Location[0]) tuple . # extract latitude to a new column: lat df[&#39;lat&#39;] = [loc[0] for loc in df.Location] # extract longitude to a new column: lng df[&#39;lng&#39;] = [loc[1] for loc in df.Location] # print the first few rows of df again print(df.head()) . StopID StopName Location lat lng 0 4431 MUSIC CITY CENTRAL 5TH - BAY 11 (36.16659, -86.781996) 36.166590 -86.781996 1 588 CHARLOTTE AVE &amp; 7TH AVE N WB (36.165, -86.78406) 36.165000 -86.784060 2 590 CHARLOTTE AVE &amp; 8TH AVE N WB (36.164393, -86.785451) 36.164393 -86.785451 3 541 11TH AVE / N GULCH STATION OUTBOUND (36.162249, -86.790464) 36.162249 -86.790464 . #### Plotting chicken locations . Now you will create a scatterplot that shows where the Nashville chickens are! . # Import pandas and matplotlib.pyplot using their customary aliases import pandas as pd import matplotlib.pyplot as plt # Load the dataset chickens = pd.read_csv(chickens_path) # Look at the first few rows of the chickens DataFrame print(chickens.head()) # Plot the locations of all Nashville chicken permits plt.scatter(x = chickens.lng, y = chickens.lat) # Show the plot plt.show() . . . 1.2 Geometries and shapefiles . . 1.2.1 Creating a GeoDataFrame &amp; examining the geometry . Let’s see where service districts are in Nashville. The path to the service district shapefile has been stored in the variable shapefile_path . . # Import geopandas import geopandas as gpd # Read in the services district shapefile and look at the first few rows. service_district = gpd.read_file(shapefile_path) print(service_district.head()) # Print the contents of the service districts geometry in the first row print(service_district.loc[0, &#39;geometry&#39;]) . area_sq_mi objectid name geometry 0 198.0 0.0 Urban Services District POLYGON ((-86.68680500011935 36.28670500013504... 1 327.0 4.0 General Services District (POLYGON ((-86.56776164301485 36.0342383159728... POLYGON ((-86.68680500011935 36.28670500013504, -86.68706099969657 36.28550299967364, -86.68709498823965 36.28511683351293, -86.68712691935902 36.28475404474551, -86.6871549990252 36.28443499969863, -86.68715025108719 36.28438104319917, -86.68708600011215 36.2836510002216, ... . 1.2.2 Plotting shapefile polygons . The next step is to show the map of polygons. We have imported matplotlib.pyplot as plt and geopandas as gpd , A GeoDataFrame of the service districts called service_district is in your workspace. . # Import packages import geopandas as gpd import matplotlib.pyplot as plt # Plot the Service Districts without any additional arguments service_district.head() service_district.plot() plt.show() # Plot the Service Districts, color them according to name, and show a legend service_district.plot(column = &#39;name&#39;, legend = True) plt.show() . . . 1.3 Scatterplots over polygons . 1.3.1 Geometry . Geometry is a special type of data structure. What types of geometries might be stored in a geometry field? . lines, points, and polygons . 1.3.2 Plotting points over polygons – part 1 . Make a basic plot of the service districts with the chicken locations. The packages needed have already been imported for you. The chickens DataFrame and service_district GeoDataFrame are in your workspace. . # Plot the service district shapefile service_district.plot(column=&#39;name&#39;) # Add the chicken locations plt.scatter(x=chickens.lng, y=chickens.lat, c = &#39;black&#39;) # Show the plot plt.show() . . #### 1.3.3 Plotting points over polygons – part 2 . We have loaded the usual libraries as pd , plt , and gpd , the chickens dataset as chickens , and the service districts as service_district . Plot the service districts and chicken permits together to see what story your visualization tells. . # Plot the service district shapefile service_district.plot(column=&#39;name&#39;, legend=True) # Add the chicken locations plt.scatter(x=chickens.lng, y=chickens.lat, c=&#39;black&#39;, edgecolor = &#39;white&#39;) # Add labels and title plt.title(&#39;Nashville Chicken Permits&#39;) plt.xlabel(&#39;longitude&#39;) plt.ylabel(&#39;latitude&#39;) # Add grid lines and show the plot plt.grid() plt.show() . . 2. Creating and joining GeoDataFrames . . 2.1 GeoJSON and plotting with geopandas . . 2.1.1 Working with GeoJSON . The advantage of GeoJSON over shapefiles is: . The file is human readable, so you can open it in a text editor and understand the contents. | The file stands alone and doesn’t rely on other files. | GeoJSON supports multi-part geometries. | . 2.1.2 Colormaps . When you want to differentiate regions, but not imply any type of relationship between the regions, a qualitative colormap is the best choice. In this exercise you’ll compare a qualitative colormap to a sequential (quantitative) colormap using the school districts GeoDataFrame. It is available in your workspace as school_districts . . school_districts.head() first_name city zip email state ... position term_expir district phone geometry 0 Dr. Sharon Nashville 37218 gentryfordistrict1@comcast.net TN ... Member 2016 1 615-268-5269 (POLYGON ((-86.77136400034288 36.3835669997190... 1 Jill Madison 37115 jill.speering@mnps.org TN ... Vice-Chair 2016 3 615-562-5234 (POLYGON ((-86.75364713283636 36.4042760799855... 2 Dr. Jo Ann Nashville 37220 joann.brannon@mnps.org TN ... Member 2018 2 615-833-5976 (POLYGON ((-86.76696199971282 36.0833250002130... 3 Anna Hermitage 37076 anna.shepherd@mnps.org TN ... Chair 2018 4 615-210-3768 (POLYGON ((-86.5809831462547 36.20934685360503... 4 Amy Nashville 37221 amy.frogge@mnps.org TN ... Member 2016 9 615-521-5650 (POLYGON ((-86.97287099971373 36.2082789997189... [5 rows x 12 columns] . # Set legend style lgnd_kwds = {&#39;title&#39;: &#39;School Districts&#39;, &#39;loc&#39;: &#39;upper left&#39;, &#39;bbox_to_anchor&#39;: (1, 1.03), &#39;ncol&#39;: 1} # Plot the school districts using the tab20 colormap (qualitative) school_districts.plot(column = &#39;district&#39;, cmap = &#39;tab20&#39;, legend = True, legend_kwds = lgnd_kwds) plt.xlabel(&#39;Latitude&#39;) plt.ylabel(&#39;Longitude&#39;) plt.title(&#39;Nashville School Districts&#39;) plt.show(); . . # Set legend style lgnd_kwds = {&#39;title&#39;: &#39;School Districts&#39;, &#39;loc&#39;: &#39;upper left&#39;, &#39;bbox_to_anchor&#39;: (1, 1.03), &#39;ncol&#39;: 1} # Plot the school districts using the summer colormap (sequential) school_districts.plot(column = &#39;district&#39;, cmap = &#39;summer&#39;, legend = True, legend_kwds = lgnd_kwds) plt.xlabel(&#39;Latitude&#39;) plt.ylabel(&#39;Longitude&#39;) plt.title(&#39;Nashville School Districts&#39;) plt.show(); . . # Set legend style lgnd_kwds = {&#39;title&#39;: &#39;School Districts&#39;, &#39;loc&#39;: &#39;upper left&#39;, &#39;bbox_to_anchor&#39;: (1, 1.03), &#39;ncol&#39;: 1} # Plot the school districts using Set3 colormap without the column argument school_districts.plot(cmap = &#39;Set3&#39;, legend = True, legend_kwds = lgnd_kwds) plt.xlabel(&#39;Latitude&#39;) plt.ylabel(&#39;Longitude&#39;) plt.title(&#39;Nashville School Districts&#39;) plt.show(); . . There is no legend when the column argument is not supplied even if you set legend to True ! . 2.1.3 Map Nashville neighborhoods . This time you’ll read a GeoJSON file in to a GeoDataFrame to take a quick peek at where Nashville neighborhoods are. . name geometry 0 Historic Buena Vista (POLYGON ((-86.79511056795417 36.1757596496334... 1 Charlotte Park (POLYGON ((-86.87459668651866 36.1575770268129... 2 Hillwood (POLYGON ((-86.87613708067906 36.1355409894979... 3 West Meade (POLYGON ((-86.9038380396094 36.1255414807897,... 4 White Bridge (POLYGON ((-86.86321427797685 36.1288622289404... . import geopandas as gpd import matplotlib.pyplot as plt # Read in the neighborhoods geojson file neighborhoods = gpd.read_file(neighborhoods_path) # Print the first few rows of neighborhoods print(neighborhoods.head()) # Plot the neighborhoods, color according to name and use the Dark2 colormap neighborhoods.plot(column = &#39;name&#39;, cmap = &#39;Dark2&#39;) # Show the plot. plt.show() . . . ### 2.2 Projections and coordinate reference systems . . 2.2.1 Changing coordinate reference systems . In this exercise you will learn how to find a GeoDataFrame’s coordinate reference system and how to change it. The school districts GeoDataFrame is available in your workspace as school_districts . . # Print the first row of school districts GeoDataFrame and the crs print(school_districts.head(1)) print(school_districts.crs) # Convert the crs to epsg:3857 school_districts.geometry = school_districts.geometry.to_crs(epsg = 3857) # Print the first row of school districts GeoDataFrame and the crs again print(school_districts.head(1)) print(school_districts.crs) . first_name city zip email state ... position term_expir district phone geometry 0 Dr. Sharon Nashville 37218 gentryfordistrict1@comcast.net TN ... Member 2016 1 615-268-5269 (POLYGON ((-86.77136400034288 36.3835669997190... [1 rows x 12 columns] {&#39;init&#39;: &#39;epsg:4326&#39;} first_name city zip email state ... position term_expir district phone geometry 0 Dr. Sharon Nashville 37218 gentryfordistrict1@comcast.net TN ... Member 2016 1 615-268-5269 (POLYGON ((-9659344.055955959 4353528.76657080... [1 rows x 12 columns] {&#39;init&#39;: &#39;epsg:3857&#39;, &#39;no_defs&#39;: True} . You can change the coordinate reference system of a GeoDataFrame by changing the crs property of the GeoDataFrame. Notice that the units for geometry change when you change the CRS. You always need to ensure two GeoDataFrames share the same crs before you spatially join them. . 2.2.2 Construct a GeoDataFrame from a DataFrame . In this exercise, you will construct a geopandas GeoDataFrame from the Nashville Public Art DataFrame. You will need to import the Point constructor from the shapely.geometry module to create a geometry column in art before you can create a GeoDataFrame from art . This will get you ready to spatially join the art data and the neighborhoods data in order to discover which neighborhood has the most art. . The Nashville Public Art data has been loaded for you as art . . import pandas as pd import geopandas as gpd from shapely.geometry import Point import matplotlib.pyplot as plt # Print the first few rows of the art DataFrame print(art.head()) # Create a geometry column from lng &amp; lat art[&#39;geometry&#39;] = art.apply(lambda x: Point(float(x.lng), float(x.lat)), axis=1) # Create a GeoDataFrame from art and verify the type art_geo = gpd.GeoDataFrame(art, crs = neighborhoods.crs, geometry = art.geometry) print(type(art_geo)) . title last_name first_name address medium 0 [Cross Country Runners] Frost Miley 4001 Harding Rd., Nashville TN Bronze 1 [Fourth and Commerce Sculpture] Walker Lin 333 Commerce Street, Nashville TN NaN 2 12th &amp; Porter Mural Kennedy Kim 114 12th Avenue N Porter all-weather outdoor paint 3 A Splash of Color Stevenson and Stanley and ROFF (Harroff) Doug and Ronnica and Lynn 616 17th Ave. N. Steel, brick, wood, and fabric on frostproof c... 4 A Story of Nashville Ridley Greg 615 Church Street, Nashville TN Hammered copper repousse type desc lat lng loc 0 Sculpture NaN 36.12856 -86.83660 (36.12856, -86.8366) 1 Sculpture NaN 36.16234 -86.77774 (36.16234, -86.77774) 2 Mural Kim Kennedy is a musician and visual artist wh... 36.15790 -86.78817 (36.1579, -86.78817) 3 Mural Painted wooden hoop dancer on a twenty foot po... 36.16202 -86.79975 (36.16202, -86.79975) 4 Frieze Inside the Grand Reading Room, this is a serie... 36.16215 -86.78205 (36.16215, -86.78205) &lt;class &#39;geopandas.geodataframe.GeoDataFrame&#39;&gt; . Now that the public art data is in a GeoDataFrame we can join it to the neighborhoods with a special kind of join called a spatial join. . . 2.3 Spatial joins . . 2.3.1 Spatial join practice . Is there a difference between art (point data) that intersects with neighborhoods (polygon data) and art (point data) within neighborhoods (polygon data)? Explore different spatial joins with the art_geo and neighborhoods GeoDataFrames, which are available in your workspace. . # Spatially join art_geo and neighborhoods art_intersect_neighborhoods = gpd.sjoin(art_geo, neighborhoods, op = &#39;intersects&#39;) # Print the shape property of art_intersect_neighborhoods print(art_intersect_neighborhoods.shape) # (40, 13) art_geo.head(1) title last_name first_name address medium ... desc lat lng loc geometry 0 [Cross Country Runners] Frost Miley 4001 Harding Rd., Nashville TN Bronze ... NaN 36.12856 -86.8366 (36.12856, -86.8366) POINT (-86.8366 36.12856) [1 rows x 11 columns] neighborhoods.head(1) name geometry 0 Historic Buena Vista (POLYGON ((-86.79511056795417 36.1757596496334... art_intersect_neighborhoods.head(1) title last_name first_name address medium ... lng loc geometry index_right 1 [Fourth and Commerce Sculpture] Walker Lin 333 Commerce Street, Nashville TN NaN ... -86.77774 (36.16234, -86.77774) POINT (-86.77774000000001 36.16234) 41 name 1 Urban Residents [1 rows x 13 columns] . # Create art_within_neighborhoods by spatially joining art_geo and neighborhoods art_within_neighborhoods = gpd.sjoin(art_geo, neighborhoods, op = &#39;within&#39;) # Print the shape property of art_within_neighborhoods print(art_within_neighborhoods.shape) # (40, 13) . # Spatially join art_geo and neighborhoods and using the contains op art_containing_neighborhoods = gpd.sjoin(art_geo, neighborhoods, op = &#39;contains&#39;) # Print the shape property of art_containing_neighborhoods print(art_containing_neighborhoods.shape) # (0, 13) . 2.3.2 Finding the neighborhood with the most public art . Now that you have created art_geo , a GeoDataFrame, from the art DataFrame, you can join it spatially to the neighborhoods data to see what art is in each neighborhood. . # import packages import geopandas as gpd import pandas as pd # Spatially join neighborhoods with art_geo neighborhood_art = gpd.sjoin(art_geo, neighborhoods, op = &quot;within&quot;) # Print the first few rows print(neighborhood_art.head()) neighborhood_art.head(1) title last_name first_name address medium ... lng loc geometry index_right 1 [Fourth and Commerce Sculpture] Walker Lin 333 Commerce Street, Nashville TN NaN ... -86.77774 (36.16234, -86.77774) POINT (-86.77774000000001 36.16234) 41 name 1 Urban Residents [1 rows x 13 columns] . 2.3.3 Aggregating points within polygons . Now that you have spatially joined art and neighborhoods , you can group, aggregate, and sort the data to find which neighborhood has the most public art. You can count artwork titles to see how many artworks are in each neighborhood. . # Get name and title from neighborhood_art and group by name neighborhood_art_grouped = neighborhood_art[[&#39;name&#39;, &#39;title&#39;]].groupby(&#39;name&#39;) # Aggregate the grouped data and count the artworks within each polygon print(neighborhood_art_grouped.agg(&#39;count&#39;).sort_values(by = &#39;title&#39;, ascending = False)) . title name Urban Residents 22 Lockeland Springs 3 Edgehill (ONE) 2 Germantown 2 Hillsboro-West End 2 Inglewood 2 Sunnyside 2 Chestnut Hill (TAG) 1 Historic Edgefield 1 McFerrin Park 1 Renraw 1 Wedgewood Houston (SNAP) 1 . 2.3.4 Plotting the Urban Residents neighborhood and art . Now you know that most art is in the Urban Residents neighborhood. In this exercise, you’ll create a plot of art in that neighborhood. First you will subset just the urban_art from neighborhood_art and you’ll subset the urban_polygon from neighborhoods . Then you will create a plot of the polygon as ax before adding a plot of the art. . # Create urban_art from neighborhood_art where the neighborhood name is Urban Residents urban_art = neighborhood_art.loc[neighborhood_art.name == &#39;Urban Residents&#39;] # Get just the Urban Residents neighborhood polygon and save it as urban_polygon urban_polygon = neighborhoods.loc[neighborhoods.name == &quot;Urban Residents&quot;] # Plot the urban_polygon as ax ax = urban_polygon.plot(color = &#39;lightgreen&#39;) # Add a plot of the urban_art and show it urban_art.plot( ax = ax, column = &#39;type&#39;, legend = True); plt.show() . . 3. GeoSeries and folium . 3.1 GeoSeries attributes and methods I . . 3.1.1 Find the area of the Urban Residents neighborhood . How big is the Urban Residents neighborhood? . # Print the head of the urban polygon print(urban_polygon.head()) # Create a copy of the urban_polygon using EPSG:3857 and print the head urban_poly_3857 = urban_polygon.to_crs(epsg = 3857) print(urban_poly_3857.head()) # Print the area of urban_poly_3857 in kilometers squared area = urban_poly_3857.geometry.area / 10**6 print(&#39;The area of the Urban Residents neighborhood is &#39;, area[0], &#39; km squared&#39;) &#39;&#39;&#39; index name geometry 0 41 Urban Residents (POLYGON ((-86.78122053774267 36.1645653773768... index name geometry 0 41 Urban Residents (POLYGON ((-9660441.280680289 4323289.00479539... The area of the Urban Residents neighborhood is 1.1289896057984288 km squared &#39;&#39;&#39; . . 3.2 GeoSeries attributes and methods II . . 3.2.1 The center of the Urban Residents neighborhood . Now you’ll find the center point of the urban_poly_3857 and plot it over the polygon. . # Create downtown_center from urban_poly_3857 downtown_center = urban_poly_3857.geometry.centroid # Print the type of downtown_center print(type(downtown_center)) # Plot the urban_poly_3857 as ax and add the center point ax = urban_poly_3857.plot(color = &#39;lightgreen&#39;) downtown_center.plot(ax = ax, color = &#39;black&#39;) plt.xticks(rotation = 45) # Show the plot plt.show() . . 3.2.2 Prepare to calculate distances . In this exercise you will prepare a GeoDataFrame called art_dist_meters with the locations of downtown art converted to meters using EPSG:3857. You will use art_dist_meters in the next exercise to calculate the distance of each artwork from the center of the Urban Residents neighborhood in meters. . The art data is in your workspace, along with urban_poly_3857 and center_point , the center point of the Urban Residents neighborhood. A geometry column called geometry that uses degrees has already been created in the art DataFrame. . art.columns Index([&#39;title&#39;, &#39;last_name&#39;, &#39;first_name&#39;, &#39;address&#39;, &#39;medium&#39;, &#39;type&#39;, &#39;desc&#39;, &#39;lat&#39;, &#39;lng&#39;, &#39;loc&#39;, &#39;geometry&#39;, &#39;center&#39;], dtype=&#39;object&#39;) type(art) pandas.core.frame.DataFrame art.head(1) title last_name first_name address medium ... lat lng loc geometry 0 [Cross Country Runners] Frost Miley 4001 Harding Rd., Nashville TN Bronze ... 36.12856 -86.8366 (36.12856, -86.8366) POINT (-9666606.09421918 4318325.479300267) center 0 POINT (-9660034.312198792 4322835.782813124) [1 rows x 12 columns] . # Import packages from shapely.geometry import Point import geopandas as gpd import pandas as pd # Create art_dist_meters using art and the geometry from art art_dist_meters = gpd.GeoDataFrame(art, geometry = art.geometry, crs = {&#39;init&#39;: &#39;epsg:4326&#39;}) print(art_dist_meters.head(2)) # Set the crs of art_dist_meters to use EPSG:3857 art_dist_meters.geometry = art_dist_meters.geometry.to_crs(epsg = 3857) print(art_dist_meters.head(2)) # Add a column to art_meters, center art_dist_meters[&#39;center&#39;] = center_point . title last_name first_name address medium ... desc lat lng loc geometry 0 [Cross Country Runners] Frost Miley 4001 Harding Rd., Nashville TN Bronze ... NaN 36.12856 -86.83660 (36.12856, -86.8366) POINT (-86.8366 36.12856) 1 [Fourth and Commerce Sculpture] Walker Lin 333 Commerce Street, Nashville TN NaN ... NaN 36.16234 -86.77774 (36.16234, -86.77774) POINT (-86.77774000000001 36.16234) [2 rows x 11 columns] title last_name first_name address medium ... desc lat lng loc geometry 0 [Cross Country Runners] Frost Miley 4001 Harding Rd., Nashville TN Bronze ... NaN 36.12856 -86.83660 (36.12856, -86.8366) POINT (-9666606.09421918 4318325.479300267) 1 [Fourth and Commerce Sculpture] Walker Lin 333 Commerce Street, Nashville TN NaN ... NaN 36.16234 -86.77774 (36.16234, -86.77774) POINT (-9660053.828991087 4322982.159062029) [2 rows x 11 columns] . 3.2.3 Art distances from neighborhood center . Now that you have the center point and the art locations in the units we need to calculate distances in meters, it’s time to perform that step. . # Import package for pretty printing import pprint # Build a dictionary of titles and distances for Urban Residents art art_distances = {} for row in art_dist_meters.iterrows(): vals = row[1] key = vals[&#39;title&#39;] ctr = vals[&#39;center&#39;] art_distances[key] = vals[&#39;geometry&#39;].distance(other=ctr) # Pretty print the art_distances pprint.pprint(art_distances) . {&#39;12th &amp; Porter Mural&#39;: 1269.1502879119878, &#39;A Splash of Color&#39;: 2471.774738455904, &#39;A Story of Nashville&#39;: 513.5632030470281, &#39;Aerial Innovations Mural&#39;: 4516.755210408422, &#39;Airport Sun Project&#39;: 12797.594229783645, &#39;Andrew Jackson&#39;: 948.9812821640502, &#39;Angel&#39;: 10202.565989739454, &#39;Anticipation&#39;: 688.8349105273556, ...... . . 3.3 Street maps with folium . 3.3.1 Create a folium location from the urban centroid . In order to construct a folium map of the Urban Residents neighborhood, you need to build a coordinate pair location that is formatted for folium. . # Print the head of the urban_polygon print(urban_polygon.head()) # Create urban_center from the urban_polygon center urban_center = urban_polygon.center[0] # Print urban_center print(urban_center) # Create array for folium called urban_location urban_location = [urban_center.y, urban_center.x] # Print urban_location print(urban_location) . index name geometry center 0 41 Urban Residents (POLYGON ((-86.78122053774267 36.1645653773768... POINT (-86.77756457127047 36.16127820928791) POINT (-86.77756457127047 36.16127820928791) [36.161278209287914, -86.77756457127047] . 3.3.2 Create a folium map of downtown Nashville . In this exercise you will create a street map of downtown Nashville using folium. . # Construct a folium map with urban_location downtown_map = folium.Map(location = urban_location, zoom_start = 15) # Display the map display(downtown_map) . . 3.3.3 Folium street map of the downtown neighborhood . This time you will create the folium map of downtown and add the Urban Residents neighborhood area from urban_polygon . The urban_polygon has been printed to your console. . # Create array for called folium_loc from the urban_polygon center point point = urban_polygon.centroid[0] folium_loc = [point.y, point.x] # Construct a map from folium_loc: downtown_map downtown_map = folium.Map(location = folium_loc, zoom_start = 15) # Draw our neighborhood: Urban Residents folium.GeoJson(urban_polygon.geometry).add_to(downtown_map) # Display the map display(downtown_map) . . . 3.4 Creating markers and popups in folium . 3.4.1 Adding markers for the public art . Now that you have added the polygon for the Urban Residents neighborhood to your folium street map, it’s time to add the locations of the art within the neighborhood. You can do that by creating folium markers. Each marker needs a location assigned. Use iterrows() to loop through the data to grab the values you need. . # Iterate through the urban_art and print each part of tuple returned for row in urban_art.iterrows(): print(&#39;first part: &#39;, row[0]) print(&#39;second part: &#39;, row[1]) # Create a location and marker with each iteration for the downtown_map for row in urban_art.iterrows(): row_values = row[1] location = [row_values[&#39;lat&#39;], row_values[&#39;lng&#39;]] marker = folium.Marker(location = location) marker.add_to(downtown_map) # Display the map display(downtown_map) . first part: 1 second part: title [Fourth and Commerce Sculpture] last_name Walker first_name Lin address 333 Commerce Street, Nashville TN medium NaN type Sculpture desc NaN lat 36.1623 lng -86.7777 loc (36.16234, -86.77774) geometry POINT (-86.77774000000001 36.16234) index_right 41 name Urban Residents Name: 1, dtype: object ... ... . . 3.4.2 Troubleshooting data issues . You will be building popups for the downtown art using the title and desc columns from the urban_art DataFrame. Here, you will inspect those columns to identify and clean up any problematic values. . # Print the urban_art titles print(urban_art.title) #Print the urban_art descriptions print(urban_art.desc) # Replace Nan and &#39; values in description urban_art.desc.fillna(&#39;&#39;, inplace = True) urban_art.desc = urban_art.desc.str.replace(&quot;&#39;&quot;, &quot;`&quot;) #Print the urban_art descriptions again print(urban_art.desc) . 1 [Fourth and Commerce Sculpture] 4 A Story of Nashville 21 Chet Atkins ... Name: title, dtype: object 1 NaN 4 Inside the Grand Reading Room, this is a serie... 21 A sculpture of a young Chet Atkins seated on a... ... Name: desc, dtype: object 1 4 Inside the Grand Reading Room, this is a serie... 21 A sculpture of a young Chet Atkins seated on a... ... Name: desc, dtype: object . 3.4.3 A map of downtown art . Now you will assign a popup to each marker to give information about the artwork at each location. In particular you will assign the art title and description to the popup for each marker. You will do so by creating the map object downtown_map , then add the popups, and finally use the display function to show your map. . One warning before you start: you’ll need to ensure that all instances of single quotes ( &#39; ) are removed from the pop-up message, otherwise your plot will not render! . # Construct downtown map downtown_map = folium.Map(location = nashville, zoom_start = 15) folium.GeoJson(urban_polygon).add_to(downtown_map) # Create popups inside the loop you built to create the markers for row in urban_art.iterrows(): row_values = row[1] location = [row_values[&#39;lat&#39;], row_values[&#39;lng&#39;]] popup = (str(row_values[&#39;title&#39;]) + &#39;: &#39; + str(row_values[&#39;desc&#39;])).replace(&quot;&#39;&quot;, &quot;`&quot;) marker = folium.Marker(location = location, popup = popup) marker.add_to(downtown_map) # Display the map. display(downtown_map) . . 4. Creating a choropleth building permit density in Nashville . . 4.1 What is a choropleth? . . 4.1.1 Finding counts from a spatial join . You will be using a dataset of the building permits issued in Nashville during 2017. This DataFrame called permits is in your workspace along with the council_districts GeoDataFrame. . permits.head(3) permit_id issued cost lat lng 0 2017032777 2017-05-24 226201.0 36.198241 -86.742235 1 2017061156 2017-10-23 15000.0 36.151554 -86.830222 2 2017074521 2017-11-20 13389.0 36.034239 -86.708892 council_districts.head(3) district geometry 0 1 (POLYGON ((-86.90738248774342 36.3905151283193... 1 2 (POLYGON ((-86.75902399986667 36.2309080000732... 2 8 (POLYGON ((-86.72850199989709 36.2832840002146... council_districts.crs {&#39;init&#39;: &#39;epsg:4326&#39;} . from shapely.geometry import Point # Create a shapely Point from lat and lng permits[&#39;geometry&#39;] = permits.apply(lambda x: Point((x.lng , x.lat)), axis = 1) # Build a GeoDataFrame: permits_geo permits_geo = gpd.GeoDataFrame(permits, crs = council_districts.crs, geometry = permits.geometry) # Spatial join of permits_geo and council_districts permits_by_district = gpd.sjoin(permits_geo, council_districts, op = &#39;within&#39;) print(permits_by_district.head(2)) # Create permit_counts permit_counts = permits_by_district.groupby([&#39;district&#39;]).size() print(permit_counts) . permit_id issued cost lat lng geometry index_right district 0 2017032777 2017-05-24 226201.0 36.198241 -86.742235 POINT (-86.74223499999999 36.198241) 5 5 68 2017053890 2017-09-05 0.0 36.185442 -86.768239 POINT (-86.76823900000001 36.185442) 5 5 district 1 146 10 119 11 239 ... dtype: int64 . Now you have a count of building permits issued for each council district. Next you’ll get the area of each council_district. . 4.1.2 Council district areas and permit counts . In order to create a normalized value for the building permits issued in each council district, you will need to find the area of each council district. Remember that you can leverage the area attribute of a GeoSeries to do this. You will need to convert permit_counts to a DataFrame so you can merge it with the council_districts data. Both permit_counts and council_districts are in your workspace. . # Create an area column in council_districts council_districts[&#39;area&#39;] = council_districts.geometry.area print(council_districts.head(2)) &#39;&#39;&#39; district geometry area 0 1 (POLYGON ((-86.90738248774342 36.3905151283193... 0.022786 1 2 (POLYGON ((-86.75902399986667 36.2309080000732... 0.002927 &#39;&#39;&#39; # Convert permit_counts to a DataFrame permits_df = permit_counts.to_frame() print(permits_df.head(2)) &#39;&#39;&#39; 0 district 1 146 10 119 &#39;&#39;&#39; # Reset index and column names permits_df.reset_index(inplace=True) permits_df.columns = [&#39;district&#39;, &#39;bldg_permits&#39;] print(permits_df.head(2)) &#39;&#39;&#39; district bldg_permits 0 1 146 1 10 119 &#39;&#39;&#39; # Merge council_districts and permits_df: districts_and_permits = pd.merge(council_districts, permits_df, on = &#39;district&#39;) print(districts_and_permits.head(2)) &#39;&#39;&#39; district geometry bldg_permits 0 1 (POLYGON ((-86.90738248774342 36.3905151283193... 146 1 2 (POLYGON ((-86.75902399986667 36.2309080000732... 399 &#39;&#39;&#39; . You have created a column with the area in the council_districts Geo DataFrame and built a DataFrame from the permit_counts . You have merged all the information into a single GeoDataFrame. Next you will calculate the permits by area for each council district. . 4.1.3 Calculating a normalized metric . Now you are ready to divide the number of building permits issued for projects in each council district by the area of that district to get a normalized value for the permits issued. First you will verify that the districts_and_permits is still a GeoDataFrame. . # Print the type of districts_and_permits print(type(districts_and_permits)) # Create permit_density column in districts_and_permits districts_and_permits[&#39;permit_density&#39;] = districts_and_permits.apply(lambda row: row.bldg_permits / row.area, axis = 1) # Print the head of districts_and_permits print(districts_and_permits.head()) . &lt;class &#39;geopandas.geodataframe.GeoDataFrame&#39;&gt; district geometry area bldg_permits permit_density center 0 1 (POLYGON ((-86.9073824877434 36.39051512831934... 350.194851 146 0.416911 POINT (-86.89459869514988 36.26266635824652) 1 2 (POLYGON ((-86.75902399986667 36.2309080000731... 44.956987 399 8.875150 POINT (-86.80270842421444 36.20859420830921) 2 8 (POLYGON ((-86.72850199989709 36.2832840002146... 38.667932 209 5.404995 POINT (-86.7377559683446 36.24515598511006) 3 9 (POLYGON ((-86.68680500011934 36.2867050001350... 44.295293 186 4.199092 POINT (-86.67436394441576 36.23852818463936) 4 4 (POLYGON ((-86.74488864807593 36.0531632050230... 31.441618 139 4.420892 POINT (-86.73914087216721 36.02939641896401) . . 4.2 Choropleths with geopandas . 4.2.1 Geopandas choropleths . First you will plot a choropleth of the building permit density for each council district using the default colormap. Then you will polish it by changing the colormap and adding labels and a title. . # Import packages import matplotlib.pyplot as plt import pandas as pd import geopandas as gpd # Simple plot of building permit_density districts_and_permits.plot(column = &#39;permit_density&#39;, legend = True); plt.show(); # Polished choropleth of building permit_density districts_and_permits.plot(column = &#39;permit_density&#39;, cmap = &#39;BuGn&#39;, edgecolor = &#39;black&#39;, legend = True) plt.xlabel(&#39;longitude&#39;) plt.ylabel(&#39;latitude&#39;) plt.xticks(rotation = &#39;vertical&#39;) plt.title(&#39;2017 Building Project Density by Council District&#39;) plt.show(); . . 4.2.2 Area in km squared, geometry in decimal degrees . In this exercise, you’ll start again with the council_districts GeoDataFrame and the permits DataFrame. You will change the council_districts to use the EPSG 3857 coordinate reference system before creating a column for area . Once the area column has been created, you will change the CRS back to EPSG 4326 so that the geometry is in decimal degrees. . # Change council_districts crs to epsg 3857 council_districts = council_districts.to_crs(epsg = 3857) print(council_districts.crs) print(council_districts.head()) # Create area in square km sqm_to_sqkm = 10**6 council_districts[&#39;area&#39;] = council_districts.geometry.area / sqm_to_sqkm # Change council_districts crs back to epsg 4326 council_districts = council_districts.to_crs(epsg = 4326) print(council_districts.crs) print(council_districts.head()) . {&#39;init&#39;: &#39;epsg:3857&#39;, &#39;no_defs&#39;: True} district geometry 0 1 (POLYGON ((-9674485.564711858 4354489.55569189... 1 2 (POLYGON ((-9657970.37338656 4332440.649821124... 2 8 (POLYGON ((-9654572.679891953 4339671.15221535... 3 9 (POLYGON ((-9649930.991109086 4340143.58970314... 4 4 (POLYGON ((-9656396.83322303 4307939.01495162,... {&#39;init&#39;: &#39;epsg:4326&#39;, &#39;no_defs&#39;: True} district geometry area 0 1 (POLYGON ((-86.9073824877434 36.39051512831934... 350.194851 1 2 (POLYGON ((-86.75902399986667 36.2309080000731... 44.956987 2 8 (POLYGON ((-86.72850199989709 36.2832840002146... 38.667932 3 9 (POLYGON ((-86.68680500011934 36.2867050001350... 44.295293 4 4 (POLYGON ((-86.74488864807593 36.0531632050230... 31.441618 . The council_districts have area in kilometers squared and geometry measures in decimal degrees. . 4.2.3 Spatially joining and getting counts . You will continue preparing your dataset for plotting a geopandas choropleth by creating a GeoDataFrame of the building permits spatially joined to the council districts. After that, you will be able to get counts of the building permits issued in each council district. . # Create permits_geo permits_geo = gpd.GeoDataFrame(permits, crs = council_districts.crs, geometry = permits.geometry) # Spatially join permits_geo and council_districts permits_by_district = gpd.sjoin(permits_geo, council_districts, op = &#39;within&#39;) print(permits_by_district.head(2)) # Count permits in each district permit_counts = permits_by_district.groupby(&#39;district&#39;).size() # Convert permit_counts to a df with 2 columns: district and bldg_permits counts_df = permit_counts.to_frame() counts_df.reset_index(inplace=True) counts_df.columns = [&#39;district&#39;, &#39;bldg_permits&#39;] print(counts_df.head(2)) . permit_id issued cost lat lng geometry index_right district area 0 2017032777 2017-05-24 226201.0 36.198241 -86.742235 POINT (-86.74223499999999 36.198241) 5 5 19.030612 68 2017053890 2017-09-05 0.0 36.185442 -86.768239 POINT (-86.76823900000001 36.185442) 5 5 19.030612 district bldg_permits 0 1 146 1 10 119 . 4.2.4 Building a polished Geopandas choropleth . After merging the counts_df with permits_by_district , you will create a column with normalized permit_density by dividing the count of permits in each council district by the area of that council district. Then you will plot your final geopandas choropleth of the building projects in each council district. . # Merge permits_by_district and counts_df districts_and_permits = pd.merge(permits_by_district, counts_df, on = &#39;district&#39;) # Create permit_density column districts_and_permits[&#39;permit_density&#39;] = districts_and_permits.apply(lambda row: row.bldg_permits / row.area, axis = 1) print(districts_and_permits.head(2)) # Create choropleth plot districts_and_permits.plot(column = &#39;permit_density&#39;, cmap = &#39;OrRd&#39;, edgecolor = &#39;black&#39;, legend = True) # Add axis labels and title plt.xlabel(&#39;longitude&#39;) plt.ylabel(&#39;latitude&#39;) plt.title(&#39;2017 Building Project Density by Council District&#39;) plt.show() . . . 4.3 Choropleths with folium . . 4.3.1 Folium choropleth . In this exercise, you will construct a folium choropleth to show the density of permitted construction projects in different Nashville council districts. You will be using a single data source, the districts_and_permits GeoDataFrame, which is in your workspace. . districts_and_permits.head(1) district geometry area bldg_permits permit_density 0 1 (POLYGON ((-86.9073824877434 36.39051512831934... 350.194851 146 0.416911 . # Center point for Nashville nashville = [36.1636,-86.7823] # Create map m = folium.Map(location=nashville, zoom_start=10) # Build choropleth m.choropleth( geo_data=districts_and_permits, name=&#39;geometry&#39;, data=districts_and_permits, columns=[&#39;district&#39;, &#39;permit_density&#39;], key_on=&#39;feature.properties.district&#39;, fill_color=&#39;Reds&#39;, fill_opacity=0.5, line_opacity=1.0, legend_name=&#39;2017 Permitted Building Projects per km squared&#39; ) # Create LayerControl and add it to the map folium.LayerControl().add_to(m) # Display the map display(m) . . 4.3.2 Folium choropleth with markers and popups . Now you will add a marker to the center of each council district that shows the district number along with the count of building permits issued in 2017 for that district. The map you created in the last exercise is in your workspace as m . . # Create center column for the centroid of each district districts_and_permits[&#39;center&#39;] = districts_and_permits.geometry.centroid # Build markers and popups for row in districts_and_permits.iterrows(): row_values = row[1] center_point = row_values[&#39;center&#39;] location = [center_point.y, center_point.x] popup = (&#39;Council District: &#39; + str(row_values[&#39;district&#39;]) + &#39;; &#39; + &#39;permits issued: &#39; + str(row_values[&#39;bldg_permits&#39;])) marker = folium.Marker(location = location, popup = popup) marker.add_to(m) # Display the map display(m) . . . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/visualizing-geospatial-data-in-python.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/visualizing-geospatial-data-in-python.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Statistical Thinking in Python (Part 2)",
            "content": "Statistical Thinking in Python (Part 2) . Parameter estimation by optimization . ### Optimal parameters . #### How often do we get no-hitters? . The number of games played between each no-hitter in the modern era (1901-2015) of Major League Baseball is stored in the array nohitter_times . . If you assume that no-hitters are described as a Poisson process, then the time between no-hitters is Exponentially distributed. As you have seen, the Exponential distribution has a single parameter, which we will call ττ, the typical interval time. The value of the parameter ττ that makes the exponential distribution best match the data is the mean interval time (where time is in units of number of games) between no-hitters. . Compute the value of this parameter from the data. Then, use np.random.exponential() to “repeat” the history of Major League Baseball by drawing inter-no-hitter times from an exponential distribution with the ττ you found and plot the histogram as an approximation to the PDF. . # Seed random number generator np.random.seed(42) # Compute mean no-hitter time: tau tau = np.mean(nohitter_times) # Draw out of an exponential distribution with parameter tau: inter_nohitter_time inter_nohitter_time = np.random.exponential(tau, 100000) # Plot the PDF and label axes _ = plt.hist(inter_nohitter_time, bins=50, normed=True, histtype=&#39;step&#39;) _ = plt.xlabel(&#39;Games between no-hitters&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() . . We see the typical shape of the Exponential distribution, going from a maximum at 0 and decaying to the right. . #### Do the data follow our story? . # Create an ECDF from real data: x, y x, y = ecdf(nohitter_times) # Create a CDF from theoretical samples: x_theor, y_theor x_theor, y_theor = ecdf(inter_nohitter_time) # Overlay the plots plt.plot(x_theor, y_theor) plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Margins and axis labels plt.margins(0.02) plt.xlabel(&#39;Games between no-hitters&#39;) plt.ylabel(&#39;CDF&#39;) # Show the plot plt.show() . . It looks like no-hitters in the modern era of Major League Baseball are Exponentially distributed. Based on the story of the Exponential distribution, this suggests that they are a random process; when a no-hitter will happen is independent of when the last no-hitter was. . #### How is this parameter optimal? . # Plot the theoretical CDFs plt.plot(x_theor, y_theor) plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;) plt.margins(0.02) plt.xlabel(&#39;Games between no-hitters&#39;) plt.ylabel(&#39;CDF&#39;) # Take samples with half tau: samples_half samples_half = np.random.exponential(tau/2, size=10000) # Take samples with double tau: samples_double samples_double = np.random.exponential(2*tau, size=10000) # Generate CDFs from these samples x_half, y_half = ecdf(samples_half) x_double, y_double = ecdf(samples_double) # Plot these CDFs as lines _ = plt.plot(x_half, y_half) _ = plt.plot(x_double, y_double) # Show the plot plt.show() . . red = half, purple = double . Notice how the value of tau given by the mean matches the data best. In this way, tau is an optimal parameter. . ### Linear regression by least squares . #### EDA of literacy/fertility data . # Plot the illiteracy rate versus fertility _ = plt.plot(illiteracy, fertility, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Set the margins and label axes plt.margins(0.02) _ = plt.xlabel(&#39;percent illiterate&#39;) _ = plt.ylabel(&#39;fertility&#39;) # Show the plot plt.show() # Show the Pearson correlation coefficient print(pearson_r(illiteracy, fertility)) 0.8041324026815344 . . You can see the correlation between illiteracy and fertility by eye, and by the substantial Pearson correlation coefficient of 0.8. It is difficult to resolve in the scatter plot, but there are many points around near-zero illiteracy and about 1.8 children/woman. . #### Linear regression . # Plot the illiteracy rate versus fertility _ = plt.plot(illiteracy, fertility, marker=&#39;.&#39;, linestyle=&#39;none&#39;) plt.margins(0.02) _ = plt.xlabel(&#39;percent illiterate&#39;) _ = plt.ylabel(&#39;fertility&#39;) # Perform a linear regression using np.polyfit(): a, b a, b = np.polyfit(illiteracy, fertility, deg=1) # Print the results to the screen print(&#39;slope =&#39;, a, &#39;children per woman / percent illiterate&#39;) print(&#39;intercept =&#39;, b, &#39;children per woman&#39;) # Make theoretical line to plot x = np.array([0, 100]) y = a * x + b # Add regression line to your plot _ = plt.plot(x, y) # Draw the plot plt.show() # slope = 0.04979854809063423 children per woman / percent illiterate # intercept = 1.888050610636557 children per woman . . #### How is it optimal? . # Specify slopes to consider: a_vals a_vals = np.linspace(0, 0.1, 200) # Initialize sum of square of residuals: rss rss = np.empty_like(a_vals) # Compute sum of square of residuals for each value of a_vals for i, a in enumerate(a_vals): rss[i] = np.sum((fertility - a*illiteracy - b)**2) # Plot the RSS plt.plot(a_vals, rss, &#39;-&#39;) plt.xlabel(&#39;slope (children per woman / percent illiterate)&#39;) plt.ylabel(&#39;sum of square of residuals&#39;) plt.show() . . Notice that the minimum on the plot, that is the value of the slope that gives the minimum sum of the square of the residuals, is the same value you got when performing the regression. . ### The importance of EDA: Anscombe’s quartet . . #### The importance of EDA . Why should exploratory data analysis be the first step in an analysis of data (after getting your data imported and cleaned, of course)? . You can be protected from misinterpretation of the type demonstrated by Anscombe’s quartet. | EDA provides a good starting point for planning the rest of your analysis. | EDA is not really any more difficult than any of the subsequent analysis, so there is no excuse for not exploring the data. | . #### Linear regression on appropriate Anscombe data . # Perform linear regression: a, b a, b = np.polyfit(x, y, deg=1) # Print the slope and intercept print(a, b) # Generate theoretical x and y data: x_theor, y_theor x_theor = np.array([3, 15]) y_theor = a * x_theor + b # Plot the Anscombe data and theoretical line _ = plt.plot(x, y, marker = &#39;.&#39;, linestyle = &#39;none&#39;) _ = plt.plot(x_theor, y_theor) # Label the axes plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;) # Show the plot plt.show() . . #### Linear regression on all Anscombe data . for i in range(4): plt.subplot(2,2,i+1) # plot the scatter plot plt.plot(anscombe_x[i], anscombe_y[i], marker = &#39;.&#39;, linestyle = &#39;none&#39;) # plot the regression line a, b = np.polyfit(anscombe_x[i], anscombe_y[i], deg=1) x_theor = np.array([np.min(anscombe_x[i]), np.max(anscombe_x[i])]) y_theor = a * x_theor + b plt.plot(x_theor, y_theor) # add label plt.xlabel(&#39;x&#39; + str(i+1)) plt.ylabel(&#39;y&#39; + str(i+1)) plt.show() # slope1: 0.5000909090909095 intercept: 3.000090909090909 # slope2: 0.5000000000000004 intercept: 3.0009090909090896 # slope3: 0.4997272727272731 intercept: 3.0024545454545453 # slope4: 0.4999090909090908 intercept: 3.0017272727272735 . . ### Generating bootstrap replicates . #### Getting the terminology down . If we have a data set with n repeated measurements, a bootstrap sample is an array of length n that was drawn from the original data with replacement. . Bootstrap replicate is a single value of a statistic computed from a bootstrap sample. . #### Visualizing bootstrap samples . np.random.choice() . for _ in range(50): # Generate bootstrap sample: bs_sample bs_sample = np.random.choice(rainfall, size=len(rainfall)) # Compute and plot ECDF from bootstrap sample x, y = ecdf(bs_sample) _ = plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;gray&#39;, alpha=0.1) # Compute and plot ECDF from original data x, y = ecdf(rainfall) _ = plt.plot(x, y, marker=&#39;.&#39;) # Make margins and label axes plt.margins(0.02) _ = plt.xlabel(&#39;yearly rainfall (mm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) # Show the plot plt.show() . . Bootstrap confidence intervals . #### Generating many bootstrap replicates . def bootstrap_replicate_1d(data, func): return func(np.random.choice(data, size=len(data))) def draw_bs_reps(data, func, size=1): &quot;&quot;&quot;Draw bootstrap replicates.&quot;&quot;&quot; # Initialize array of replicates: bs_replicates bs_replicates = np.empty(size) # Generate replicates for i in range(size): bs_replicates[i] = bootstrap_replicate_1d(data, func) return bs_replicates . #### Bootstrap replicates of the mean and the SEM ( standard error of the mean ) . In fact, it can be shown theoretically that under not-too-restrictive conditions, the value of the mean will always be Normally distributed. (This does not hold in general, just for the mean and a few other statistics.) . The standard deviation of this distribution, called the standard error of the mean , or SEM, is given by the standard deviation of the data divided by the square root of the number of data points. I.e., for a data set, sem = np.std(data) / np.sqrt(len(data)) . Using hacker statistics, you get this same result without the need to derive it, but you will verify this result from your bootstrap replicates. . # Take 10,000 bootstrap replicates of the mean: bs_replicates bs_replicates = draw_bs_reps(rainfall, np.mean, size=10000) # Compute and print SEM sem = np.std(rainfall) / np.sqrt(len(rainfall)) print(sem) # Compute and print standard deviation of bootstrap replicates bs_std = np.std(bs_replicates) print(bs_std) # Make a histogram of the results _ = plt.hist(bs_replicates, bins=50, normed=True) _ = plt.xlabel(&#39;mean annual rainfall (mm)&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() # 10.51054915050619 # 10.465927071184412 . . #### Confidence intervals of rainfall data . Use the bootstrap replicates you just generated to compute the 95% confidence interval. That is, give the 2.5th and 97.5th percentile of your bootstrap replicates stored as bs_replicates . What is the 95% confidence interval? . np.percentile(bs_replicates,2.5) 779.7699248120301 np.percentile(bs_replicates,97.5) 820.950432330827 . #### Bootstrap replicates of other statistics . def draw_bs_reps(data, func, size=1): return np.array([bootstrap_replicate_1d(data, func) for _ in range(size)]) # Generate 10,000 bootstrap replicates of the variance: bs_replicates bs_replicates = draw_bs_reps(rainfall, np.var, size=10000) # Put the variance in units of square centimeters bs_replicates /= 100 # Make a histogram of the results _ = plt.hist(bs_replicates, bins=50, normed=True) _ = plt.xlabel(&#39;variance of annual rainfall (sq. cm)&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() . . This is not normally distributed, as it has a longer tail to the right. Note that you can also compute a confidence interval on the variance, or any other statistic, using np.percentile() with your bootstrap replicates. . #### Confidence interval on the rate of no-hitters . # Draw bootstrap replicates of the mean no-hitter time (equal to tau): bs_replicates bs_replicates = draw_bs_reps(nohitter_times, np.mean, size=10000) # Compute the 95% confidence interval: conf_int conf_int = np.percentile(bs_replicates, [2.5, 97.5]) # Print the confidence interval print(&#39;95% confidence interval =&#39;, conf_int, &#39;games&#39;) # Plot the histogram of the replicates _ = plt.hist(bs_replicates, bins=50, normed=True) _ = plt.xlabel(r&#39;$ tau$ (games)&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() # 95% confidence interval = [660.67280876 871.63077689] games . . This gives you an estimate of what the typical time between no-hitters is. It could be anywhere between 660 and 870 games. . ### Pairs bootstrap . #### A function to do pairs bootstrap . def draw_bs_pairs_linreg(x, y, size=1): &quot;&quot;&quot;Perform pairs bootstrap for linear regression.&quot;&quot;&quot; # Set up array of indices to sample from: inds inds = np.arange(len(x)) # Initialize replicates: bs_slope_reps, bs_intercept_reps bs_slope_reps = np.empty(size) bs_intercept_reps = np.empty(size) # Generate replicates for i in range(size): bs_inds = np.random.choice(inds, size=len(inds)) bs_x, bs_y = x[bs_inds], y[bs_inds] bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, deg=1) return bs_slope_reps, bs_intercept_reps . #### Pairs bootstrap of literacy/fertility data . # Generate replicates of slope and intercept using pairs bootstrap bs_slope_reps, bs_intercept_reps = draw_bs_pairs_linreg(illiteracy, fertility, size=1000) # Compute and print 95% CI for slope print(np.percentile(bs_slope_reps, [2.5, 97.5])) # Plot the histogram _ = plt.hist(bs_slope_reps, bins=50, normed=True) _ = plt.xlabel(&#39;slope&#39;) _ = plt.ylabel(&#39;PDF&#39;) plt.show() # [0.04378061 0.0551616 ] . . #### Plotting bootstrap regressions . # Generate array of x-values for bootstrap lines: x x = np.array([0, 100]) # Plot the bootstrap lines for i in range(100): _ = plt.plot(x, bs_slope_reps[i]*x + bs_intercept_reps[i], linewidth=0.5, alpha=0.2, color=&#39;red&#39;) # Plot the data _ = plt.plot(illiteracy, fertility, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Label axes, set the margins, and show the plot _ = plt.xlabel(&#39;illiteracy&#39;) _ = plt.ylabel(&#39;fertility&#39;) plt.margins(0.02) plt.show() . . Introduction to hypothesis testing . ### Formulating and simulating a hypothesis . Null hypothesis another name for the hypothesis you are testing Permutation random reordering of entries in an array #### Generating a permutation sample . np.random.permutation() . Permutation sampling is a great way to simulate the hypothesis that two variables have identical probability distributions. This is often a hypothesis you want to test, so in this exercise, you will write a function to generate a permutation sample from two data sets. . def permutation_sample(data1, data2): &quot;&quot;&quot;Generate a permutation sample from two data sets.&quot;&quot;&quot; # Concatenate the data sets: data data = np.concatenate((data1, data2)) # Permute the concatenated array: permuted_data permuted_data = np.random.permutation(data) # Split the permuted array into two: perm_sample_1, perm_sample_2 perm_sample_1 = permuted_data[:len(data1)] perm_sample_2 = permuted_data[len(data1):] return perm_sample_1, perm_sample_2 . #### Visualizing permutation sampling . for _ in range(50): # Generate permutation samples perm_sample_1, perm_sample_2 = permutation_sample(rain_june, rain_november) # Compute ECDFs x_1, y_1 = ecdf(perm_sample_1) x_2, y_2 = ecdf(perm_sample_2) # Plot ECDFs of permutation sample _ = plt.plot(x_1, y_1, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;, alpha=0.02) _ = plt.plot(x_2, y_2, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;, alpha=0.02) # Create and plot ECDFs from original data x_1, y_1 = ecdf(rain_june) x_2, y_2 = ecdf(rain_november) _ = plt.plot(x_1, y_1, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;) _ = plt.plot(x_2, y_2, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;) # Label axes, set margin, and show plot plt.margins(0.02) _ = plt.xlabel(&#39;monthly rainfall (mm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) plt.show() . . Notice that the permutation samples ECDFs overlap and give a purple haze. None of the ECDFs from the permutation samples overlap with the observed data, suggesting that the hypothesis is not commensurate with the data. June and November rainfall are not identically distributed. . ### Test statistics and p-values . #### Test statistics . When performing hypothesis tests, your choice of test statistic should be pertinent to the question you are seeking to answer in your hypothesis test. . The most important thing to consider is: What are you asking? . #### What is a p-value? . The p-value is generally a measure of the probability of observing a test statistic equally or more extreme than the one you observed, given that the null hypothesis is true. . #### Generating permutation replicates . # In most circumstances, func will be a function you write yourself. def draw_perm_reps(data_1, data_2, func, size=1): &quot;&quot;&quot;Generate multiple permutation replicates.&quot;&quot;&quot; # Initialize array of replicates: perm_replicates perm_replicates = np.empty(size) for i in range(size): # Generate permutation sample perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2) # Compute the test statistic perm_replicates[i] = func(perm_sample_1, perm_sample_2) return perm_replicates . #### Look before you leap: EDA before hypothesis testing . Kleinteich and Gorb ( Sci. Rep. , 4 , 5225, 2014) performed an interesting experiment with South American horned frogs. They held a plate connected to a force transducer, along with a bait fly, in front of them. They then measured the impact force and adhesive force of the frog’s tongue when it struck the target. . Frog A is an adult and Frog B is a juvenile. The researchers measured the impact force of 20 strikes for each frog. In the next exercise, we will test the hypothesis that the two frogs have the same distribution of impact forces. But, remember, it is important to do EDA first! Let’s make a bee swarm plot for the data. They are stored in a Pandas data frame, df , where column ID is the identity of the frog and column impact_force is the impact force in Newtons (N). . df.head() ID impact_force 20 A 1.612 21 A 0.605 22 A 0.327 23 A 0.946 24 A 0.541 . # Make bee swarm plot _ = sns.swarmplot(x=&#39;ID&#39;, y=&#39;impact_force&#39;, data=df) # Label axes _ = plt.xlabel(&#39;frog&#39;) _ = plt.ylabel(&#39;impact force (N)&#39;) # Show the plot plt.show() . . Eyeballing it, it does not look like they come from the same distribution. Frog A, the adult, has three or four very hard strikes, and Frog B, the juvenile, has a couple weak ones. However, it is possible that with only 20 samples it might be too difficult to tell if they have difference distributions, so we should proceed with the hypothesis test. . #### Permutation test on frog data . The average strike force of Frog A was 0.71 Newtons (N), and that of Frog B was 0.42 N for a difference of 0.29 N. It is possible the frogs strike with the same force and this observed difference was by chance. You will compute the probability of getting at least a 0.29 N difference in mean strike force under the hypothesis that the distributions of strike forces for the two frogs are identical. We use a permutation test with a test statistic of the difference of means to test this hypothesis. . def diff_of_means(data_1, data_2): &quot;&quot;&quot;Difference in means of two arrays.&quot;&quot;&quot; # The difference of means of data_1, data_2: diff diff = np.mean(data_1) - np.mean(data_2) return diff # Compute difference of mean impact force from experiment: empirical_diff_means empirical_diff_means = diff_of_means(force_a, force_b) # Draw 10,000 permutation replicates: perm_replicates perm_replicates = draw_perm_reps(force_a, force_b, diff_of_means, size=10000) # Compute p-value: p p = np.sum(perm_replicates &gt;= empirical_diff_means) / len(perm_replicates) # Print the result print(&#39;p-value =&#39;, p) # p-value = 0.0063 # p-value = 0.63% . The p-value tells you that there is about a 0.6% chance that you would get the difference of means observed in the experiment if frogs were exactly the same. . A p-value below 0.01 is typically said to be “statistically significant,” but: warning! warning! warning! You have computed a p-value; it is a number. I encourage you not to distill it to a yes-or-no phrase. p = 0.006 and p = 0.000000006 are both said to be “statistically significant,” but they are definitely not the same! . ### Bootstrap hypothesis tests . . #### A one-sample bootstrap hypothesis test . Another juvenile frog was studied, Frog C, and you want to see if Frog B and Frog C have similar impact forces. Unfortunately, you do not have Frog C’s impact forces available, but you know they have a mean of 0.55 N. Because you don’t have the original data, you cannot do a permutation test, and you cannot assess the hypothesis that the forces from Frog B and Frog C come from the same distribution. You will therefore test another, less restrictive hypothesis: The mean strike force of Frog B is equal to that of Frog C. . To set up the bootstrap hypothesis test, you will take the mean as our test statistic. Remember, your goal is to calculate the probability of getting a mean impact force less than or equal to what was observed for Frog B if the hypothesis that the true mean of Frog B’s impact forces is equal to that of Frog C is true . You first translate all of the data of Frog B such that the mean is 0.55 N. This involves adding the mean force of Frog C and subtracting the mean force of Frog B from each measurement of Frog B. This leaves other properties of Frog B’s distribution, such as the variance, unchanged. . # Make an array of translated impact forces: translated_force_b translated_force_b = force_b - np.mean(force_b) + 0.55 # Take bootstrap replicates of Frog B&#39;s translated impact forces: bs_replicates bs_replicates = draw_bs_reps(translated_force_b, np.mean, 10000) # Compute fraction of replicates that are less than the observed Frog B force: p p = np.sum(bs_replicates &lt;= np.mean(force_b)) / 10000 # Print the p-value print(&#39;p = &#39;, p) # p = 0.0046 # p = 0.46% . The low p-value suggests that the null hypothesis that Frog B and Frog C have the same mean impact force is false. . #### A two-sample bootstrap hypothesis test for difference of means . We now want to test the hypothesis that Frog A and Frog B have the same mean impact force, but not necessarily the same distribution, which is also impossible with a permutation test. . To do the two-sample bootstrap test, we shift both arrays to have the same mean, since we are simulating the hypothesis that their means are, in fact, equal. We then draw bootstrap samples out of the shifted arrays and compute the difference in means. This constitutes a bootstrap replicate, and we generate many of them. The p-value is the fraction of replicates with a difference in means greater than or equal to what was observed. . # Compute mean of all forces: mean_force mean_force = np.mean(forces_concat) # Generate shifted arrays force_a_shifted = force_a - np.mean(force_a) + mean_force force_b_shifted = force_b - np.mean(force_b) + mean_force # Compute 10,000 bootstrap replicates from shifted arrays bs_replicates_a = draw_bs_reps(force_a_shifted, np.mean, size=10000) bs_replicates_b = draw_bs_reps(force_b_shifted, np.mean, size=10000) # Get replicates of difference of means: bs_replicates bs_replicates = bs_replicates_a - bs_replicates_b # Compute and print p-value: p p = np.sum(bs_replicates &gt;= empirical_diff_means) / 10000 print(&#39;p-value =&#39;, p) # p-value = 0.0043 # p-value = 0.43% . You got a similar result as when you did the permutation test. Nonetheless, remember that it is important to carefully think about what question you want to ask. Are you only interested in the mean impact force, or in the distribution of impact forces? . Hypothesis test examples . ### A/B testing . #### The vote for the Civil Rights Act in 1964 . The Civil Rights Act of 1964 was one of the most important pieces of legislation ever passed in the USA. Excluding “present” and “abstain” votes, 153 House Democrats and 136 Republicans voted yea. However, 91 Democrats and 35 Republicans voted nay. Did party affiliation make a difference in the vote? . To answer this question, you will evaluate the hypothesis that the party of a House member has no bearing on his or her vote. You will use the fraction of Democrats voting in favor as your test statistic and evaluate the probability of observing a fraction of Democrats voting in favor at least as small as the observed fraction of 153/244. (That’s right, at least as small as. In 1964, it was the Democrats who were less progressive on civil rights issues.) To do this, permute the party labels of the House voters and then arbitrarily divide them into “Democrats” and “Republicans” and compute the fraction of Democrats voting yea. . # Construct arrays of data: dems, reps dems = np.array([True] * 153 + [False] * 91) reps = np.array([True] * 136 + [False] * 35) def frac_yea_dems(dems, reps): &quot;&quot;&quot;Compute fraction of Democrat yea votes.&quot;&quot;&quot; frac = np.sum(dems) / len(dems) return frac # Acquire permutation samples: perm_replicates perm_replicates = draw_perm_reps(dems, reps, frac_yea_dems, size=10000) # Compute and print p-value: p p = np.sum(perm_replicates &lt;= 153/244) / len(perm_replicates) print(&#39;p-value =&#39;, p) # p-value = 0.0002 # p-value = 0.02% . This small p-value suggests that party identity had a lot to do with the voting. Importantly, the South had a higher fraction of Democrat representatives, and consequently also a more racist bias. . #### What is equivalent? . You have experience matching a stories to probability distributions. Similarly, you use the same procedure for two different A/B tests if their stories match. In the Civil Rights Act example you just did, you performed an A/B test on voting data, which has a Yes/No type of outcome for each subject (in that case, a voter). Which of the following situations involving testing by a web-based company has an equivalent set up for an A/B test as the one you just did with the Civil Rights Act of 1964? . You measure the number of people who click on an ad on your company’s website before and after changing its color. . The “Democrats” are those who view the ad before the color change, and the “Republicans” are those who view it after. . #### A time-on-website analog . It turns out that you already did a hypothesis test analogous to an A/B test where you are interested in how much time is spent on the website before and after an ad campaign. The frog tongue force (a continuous quantity like time on the website) is an analog. “Before” = Frog A and “after” = Frog B. Let’s practice this again with something that actually is a before/after scenario. . We return to the no-hitter data set. In 1920, Major League Baseball implemented important rule changes that ended the so-called dead ball era. Importantly, the pitcher was no longer allowed to spit on or scuff the ball, an activity that greatly favors pitchers. In this problem you will perform an A/B test to determine if these rule changes resulted in a slower rate of no-hitters (i.e., longer average time between no-hitters) using the difference in mean inter-no-hitter time as your test statistic. The inter-no-hitter times for the respective eras are stored in the arrays nht_dead and nht_live , where “nht” is meant to stand for “no-hitter time.” . def diff_of_means(data_1, data_2): &quot;&quot;&quot;Difference in means of two arrays.&quot;&quot;&quot; # The difference of means of data_1, data_2: diff diff = np.mean(data_1) - np.mean(data_2) return diff def permutation_sample(data1, data2): &quot;&quot;&quot;Generate a permutation sample from two data sets.&quot;&quot;&quot; # Concatenate the data sets: data data = np.concatenate((data1, data2)) # Permute the concatenated array: permuted_data permuted_data = np.random.permutation(data) # Split the permuted array into two: perm_sample_1, perm_sample_2 perm_sample_1 = permuted_data[:len(data1)] perm_sample_2 = permuted_data[len(data1):] return perm_sample_1, perm_sample_2 def draw_perm_reps(data_1, data_2, func, size=1): &quot;&quot;&quot;Generate multiple permutation replicates.&quot;&quot;&quot; # Initialize array of replicates: perm_replicates perm_replicates = np.empty(size) for i in range(size): # Generate permutation sample perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2) # Compute the test statistic perm_replicates[i] = func(perm_sample_1, perm_sample_2) return perm_replicates # Compute the observed difference in mean inter-no-hitter times: nht_diff_obs nht_diff_obs = diff_of_means(nht_dead, nht_live) # Acquire 10,000 permutation replicates of difference in mean no-hitter time: perm_replicates perm_replicates = draw_perm_reps(nht_dead, nht_live, diff_of_means, size=10000) # Compute and print the p-value: p p = np.sum(perm_replicates &lt;= nht_diff_obs) /len(perm_replicates) print(&#39;p-val =&#39;, p) p-val = 0.0001 . Your p-value is 0.0001, which means that only one out of your 10,000 replicates had a result as extreme as the actual difference between the dead ball and live ball eras. This suggests strong statistical significance. Watch out, though, you could very well have gotten zero replicates that were as extreme as the observed value. This just means that the p-value is quite small, almost certainly smaller than 0.001. . #### What should you have done first? . That was a nice hypothesis test you just did to check out whether the rule changes in 1920 changed the rate of no-hitters. But what should you have done with the data first? . Performed EDA, perhaps plotting the ECDFs of inter-no-hitter times in the dead ball and live ball eras. . Always a good idea to do first! I encourage you to go ahead and plot the ECDFs right now. You will see by eye that the null hypothesis that the distributions are the same is almost certainly not true. . # Create and plot ECDFs x_1, y_1 = ecdf(nht_dead) x_2, y_2 = ecdf(nht_live) _ = plt.plot(x_1, y_1, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;) _ = plt.plot(x_2, y_2, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;) # Label axes, set margin, and show plot plt.margins(0.02) _ = plt.xlabel(&#39;Games between no-hitters&#39;) _ = plt.ylabel(&#39;ECDF&#39;) plt.show() . . ### Test of correlation . . #### Simulating a null hypothesis concerning correlation . The observed correlation between female illiteracy and fertility in the data set of 162 countries may just be by chance; the fertility of a given country may actually be totally independent of its illiteracy. You will test this null hypothesis in the next exercise. . To do the test, you need to simulate the data assuming the null hypothesis is true. Of the following choices, which is the best way to to do it? . Answer: Do a permutation test: Permute the illiteracy values but leave the fertility values fixed to generate a new set of (illiteracy, fertility) data. . This exactly simulates the null hypothesis and does so more efficiently than the last option. It is exact because it uses all data and eliminates any correlation because which illiteracy value pairs to which fertility value is shuffled. . Last option: Do a permutation test: Permute both the illiteracy and fertility values to generate a new set of (illiteracy, fertility data). This exactly simulates the null hypothesis and does so more efficiently than the last option. It is exact because it uses all data and eliminates any correlation because which illiteracy value pairs to which fertility value is shuffled. . #### Hypothesis test on Pearson correlation . The observed correlation between female illiteracy and fertility may just be by chance; the fertility of a given country may actually be totally independent of its illiteracy. You will test this hypothesis. To do so, permute the illiteracy values but leave the fertility values fixed. This simulates the hypothesis that they are totally independent of each other. For each permutation, compute the Pearson correlation coefficient and assess how many of your permutation replicates have a Pearson correlation coefficient greater than the observed one. . def pearson_r(x, y): &quot;&quot;&quot;Compute Pearson correlation coefficient between two arrays.&quot;&quot;&quot; # Compute correlation matrix: corr_mat corr_mat = np.corrcoef(x, y) # Return entry [0,1] return corr_mat[0,1] # Compute observed correlation: r_obs r_obs = pearson_r(illiteracy, fertility) # r_obs = 0.8041324026815344 # Initialize permutation replicates: perm_replicates perm_replicates = np.empty(10000) # Draw replicates for i in range(10000): # Permute illiteracy measurments: illiteracy_permuted illiteracy_permuted = np.random.permutation(illiteracy) # Compute Pearson correlation perm_replicates[i] = pearson_r(illiteracy_permuted, fertility) # Compute p-value: p p = np.sum(perm_replicates &gt;= r_obs) /len(perm_replicates) print(&#39;p-val =&#39;, p) # p-val = 0.0 . You got a p-value of zero. In hacker statistics, this means that your p-value is very low, since you never got a single replicate in the 10,000 you took that had a Pearson correlation greater than the observed one. You could try increasing the number of replicates you take to continue to move the upper bound on your p-value lower and lower. . #### Do neonicotinoid insecticides have unintended consequences? . As a final exercise in hypothesis testing before we put everything together in our case study in the next chapter, you will investigate the effects of neonicotinoid insecticides on bee reproduction. These insecticides are very widely used in the United States to combat aphids and other pests that damage plants. . In a recent study, Straub, et al. ( Proc. Roy. Soc. B , 2016 ) investigated the effects of neonicotinoids on the sperm of pollinating bees. In this and the next exercise, you will study how the pesticide treatment affected the count of live sperm per half milliliter of semen. . First, we will do EDA, as usual. Plot ECDFs of the alive sperm count for untreated bees (stored in the Numpy array control ) and bees treated with pesticide (stored in the Numpy array treated ). . # Compute x,y values for ECDFs x_control, y_control = ecdf(control) x_treated, y_treated = ecdf(treated) # Plot the ECDFs plt.plot(x_control, y_control, marker=&#39;.&#39;, linestyle=&#39;none&#39;) plt.plot(x_treated, y_treated, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Set the margins plt.margins(0.02) # Add a legend plt.legend((&#39;control&#39;, &#39;treated&#39;), loc=&#39;lower right&#39;) # Label axes and show plot plt.xlabel(&#39;millions of alive sperm per mL&#39;) plt.ylabel(&#39;ECDF&#39;) plt.show() . . The ECDFs show a pretty clear difference between the treatment and control; treated bees have fewer alive sperm. Let’s now do a hypothesis test in the next exercise. . #### Bootstrap hypothesis test on bee sperm counts . Now, you will test the following hypothesis: . On average, male bees treated with neonicotinoid insecticide have the same number of active sperm per milliliter of semen than do untreated male bees. . You will use the difference of means as your test statistic. . def bootstrap_replicate_1d(data, func): return func(np.random.choice(data, size=len(data))) def draw_bs_reps(data, func, size=1): &quot;&quot;&quot;Draw bootstrap replicates.&quot;&quot;&quot; # Initialize array of replicates: bs_replicates bs_replicates = np.empty(size) # Generate replicates for i in range(size): bs_replicates[i] = bootstrap_replicate_1d(data, func) return bs_replicates # Compute the difference in mean sperm count: diff_means diff_means = np.mean(control) - np.mean(treated) # Compute mean of pooled data: mean_count mean_count = np.mean(np.concatenate((control, treated))) # Generate shifted data sets control_shifted = control - np.mean(control) + mean_count treated_shifted = treated - np.mean(treated) + mean_count # Generate bootstrap replicates bs_reps_control = draw_bs_reps(control_shifted, np.mean, size=10000) bs_reps_treated = draw_bs_reps(treated_shifted, np.mean, size=10000) # Get replicates of difference of means: bs_replicates bs_replicates = bs_reps_control - bs_reps_treated # Compute and print p-value: p p = np.sum(bs_replicates &gt;= np.mean(control) - np.mean(treated)) / len(bs_replicates) print(&#39;p-value =&#39;, p) # p-value = 0.0 . The p-value is small, most likely less than 0.0001, since you never saw a bootstrap replicated with a difference of means at least as extreme as what was observed. In fact, when I did the calculation with 10 million replicates, I got a p-value of 2e-05 . Putting it all together: a case study . ### Finch beaks and the need for statistics . . . #### EDA of beak depths of Darwin’s finches . For your first foray into the Darwin finch data, you will study how the beak depth (the distance, top to bottom, of a closed beak) of the finch species Geospiza scandens has changed over time. The Grants have noticed some changes of beak geometry depending on the types of seeds available on the island, and they also noticed that there was some interbreeding with another major species on Daphne Major, Geospiza fortis . These effects can lead to changes in the species over time. . In the next few problems, you will look at the beak depth of G. scandens on Daphne Major in 1975 and in 2012. To start with, let’s plot all of the beak depth measurements in 1975 and 2012 in a bee swarm plot. . The data are stored in a pandas DataFrame called df with columns &#39;year&#39; and &#39;beak_depth&#39; . The units of beak depth are millimeters (mm). . df.head() beak_depth year 0 8.4 1975 1 8.8 1975 2 8.4 1975 3 8.0 1975 4 7.9 1975 . # Create bee swarm plot _ = sns.swarmplot(&#39;year&#39;, &#39;beak_depth&#39;, data=df) # Label the axes _ = plt.xlabel(&#39;year&#39;) _ = plt.ylabel(&#39;beak depth (mm)&#39;) # Show the plot plt.show() . . It is kind of hard to see if there is a clear difference between the 1975 and 2012 data set. Eyeballing it, it appears as though the mean of the 2012 data set might be slightly higher, and it might have a bigger variance. . #### ECDFs of beak depths . While bee swarm plots are useful, we found that ECDFs are often even better when doing EDA. Plot the ECDFs for the 1975 and 2012 beak depth measurements on the same plot. . # Compute ECDFs x_1975, y_1975 = ecdf(bd_1975) x_2012, y_2012 = ecdf(bd_2012) # Plot the ECDFs _ = plt.plot(x_1975, y_1975, marker=&#39;.&#39;, linestyle=&#39;none&#39;) _ = plt.plot(x_2012, y_2012, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Set margins plt.margins(0.02) # Add axis labels and legend _ = plt.xlabel(&#39;beak depth (mm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) _ = plt.legend((&#39;1975&#39;, &#39;2012&#39;), loc=&#39;lower right&#39;) # Show the plot plt.show() . . The differences are much clearer in the ECDF. The mean is larger in the 2012 data, and the variance does appear larger as well. . #### Parameter estimates of beak depths . Estimate the difference of the mean beak depth of the G. scandens samples from 1975 and 2012 and report a 95% confidence interval. . def draw_bs_reps(data, func, size=1): &quot;&quot;&quot;Draw bootstrap replicates.&quot;&quot;&quot; # Initialize array of replicates: bs_replicates bs_replicates = np.empty(size) # Generate replicates for i in range(size): bs_replicates[i] = bootstrap_replicate_1d(data, func) return bs_replicates # Compute the difference of the sample means: mean_diff mean_diff = np.mean(bd_2012) - np.mean(bd_1975) # Get bootstrap replicates of means bs_replicates_1975 = draw_bs_reps(bd_1975, np.mean, size=10000) bs_replicates_2012 = draw_bs_reps(bd_2012, np.mean, size=10000) # Compute samples of difference of means: bs_diff_replicates bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975 # Compute 95% confidence interval: conf_int conf_int = np.percentile(bs_diff_replicates, [2.5, 97.5]) # Print the results print(&#39;difference of means =&#39;, mean_diff, &#39;mm&#39;) print(&#39;95% confidence interval =&#39;, conf_int, &#39;mm&#39;) # difference of means = 0.22622047244094645 mm # 95% confidence interval = [0.05633521 0.39190544] mm . #### Hypothesis test: Are beaks deeper in 2012? . Your plot of the ECDF and determination of the confidence interval make it pretty clear that the beaks of G. scandens on Daphne Major have gotten deeper. But is it possible that this effect is just due to random chance? In other words, what is the probability that we would get the observed difference in mean beak depth if the means were the same? . Be careful! The hypothesis we are testing is not that the beak depths come from the same distribution. For that we could use a permutation test. The hypothesis is that the means are equal. To perform this hypothesis test, we need to shift the two data sets so that they have the same mean and then use bootstrap sampling to compute the difference of means. . # Compute mean of combined data set: combined_mean combined_mean = np.mean(np.concatenate((bd_1975, bd_2012))) # Shift the samples # why shift the mean? # to make np.mean(bd_1975_shifted) - np.mean(bd_2012_shifted) = 0 #1 # why make #1 = 0? # because our hypothesis is &quot;beak depth are the same in 1975 and 2012&quot; bd_1975_shifted = bd_1975 - np.mean(bd_1975) + combined_mean bd_2012_shifted = bd_2012 - np.mean(bd_2012) + combined_mean # Get bootstrap replicates of shifted data sets bs_replicates_1975 = draw_bs_reps(bd_1975_shifted, np.mean, size=10000) bs_replicates_2012 = draw_bs_reps(bd_2012_shifted, np.mean, size=10000) # Compute replicates of difference of means: bs_diff_replicates bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975 # Compute the p-value p = np.sum(bs_diff_replicates &gt;= mean_diff) / len(bs_diff_replicates) # Print p-value print(&#39;p =&#39;, p) # p = 0.0034 # p = 0.34% . We get a p-value of 0.0034, which suggests that there is a statistically significant difference. But remember: it is very important to know how different they are! In the previous exercise, you got a difference of 0.2 mm between the means. You should combine this with the statistical significance. Changing by 0.2 mm in 37 years is substantial by evolutionary standards. If it kept changing at that rate, the beak depth would double in only 400 years. . ### Variation of beak shapes . #### EDA of beak length and depth . The beak length data are stored as bl_1975 and bl_2012 , again with units of millimeters (mm). You still have the beak depth data stored in bd_1975 and bd_2012 . Make scatter plots of beak depth (y-axis) versus beak length (x-axis) for the 1975 and 2012 specimens. . # Make scatter plot of 1975 data _ = plt.plot(bl_1975, bd_1975, marker=&#39;.&#39;, linestyle=&#39;None&#39;, color=&#39;blue&#39;, alpha=0.5) # Make scatter plot of 2012 data _ = plt.plot(bl_2012, bd_2012, marker=&#39;.&#39;, linestyle=&#39;None&#39;, color=&#39;red&#39;, alpha=0.5) # Label axes and make legend _ = plt.xlabel(&#39;beak length (mm)&#39;) _ = plt.ylabel(&#39;beak depth (mm)&#39;) _ = plt.legend((&#39;1975&#39;, &#39;2012&#39;), loc=&#39;upper left&#39;) # Show the plot plt.show() . . In looking at the plot, we see that beaks got deeper (the red points are higher up in the y-direction), but not really longer. If anything, they got a bit shorter, since the red dots are to the left of the blue dots. So, it does not look like the beaks kept the same shape; they became shorter and deeper. . #### Linear regressions . Perform a linear regression for both the 1975 and 2012 data. Then, perform pairs bootstrap estimates for the regression parameters. Report 95% confidence intervals on the slope and intercept of the regression line. . def draw_bs_pairs_linreg(x, y, size=1): &quot;&quot;&quot;Perform pairs bootstrap for linear regression.&quot;&quot;&quot; # Set up array of indices to sample from: inds inds = np.arange(len(x)) # Initialize replicates: bs_slope_reps, bs_intercept_reps bs_slope_reps = np.empty(size) bs_intercept_reps = np.empty(size) # Generate replicates for i in range(size): bs_inds = np.random.choice(inds, size=len(inds)) bs_x, bs_y = x[bs_inds], y[bs_inds] bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, deg=1) return bs_slope_reps, bs_intercept_reps # Compute the linear regressions slope_1975, intercept_1975 = np.polyfit(bl_1975, bd_1975, deg=1) slope_2012, intercept_2012 = np.polyfit(bl_2012, bd_2012, deg=1) # Perform pairs bootstrap for the linear regressions bs_slope_reps_1975, bs_intercept_reps_1975 = draw_bs_pairs_linreg(bl_1975, bd_1975, size=1000) bs_slope_reps_2012, bs_intercept_reps_2012 = draw_bs_pairs_linreg(bl_2012, bd_2012, size=1000) # Compute confidence intervals of slopes slope_conf_int_1975 = np.percentile(bs_slope_reps_1975, [2.5, 97.5]) slope_conf_int_2012 = np.percentile(bs_slope_reps_2012, [2.5, 97.5]) intercept_conf_int_1975 = np.percentile(bs_intercept_reps_1975, [2.5, 97.5]) intercept_conf_int_2012 = np.percentile(bs_intercept_reps_2012, [2.5, 97.5]) # Print the results print(&#39;1975: slope =&#39;, slope_1975, &#39;conf int =&#39;, slope_conf_int_1975) print(&#39;1975: intercept =&#39;, intercept_1975, &#39;conf int =&#39;, intercept_conf_int_1975) print(&#39;2012: slope =&#39;, slope_2012, &#39;conf int =&#39;, slope_conf_int_2012) print(&#39;2012: intercept =&#39;, intercept_2012, &#39;conf int =&#39;, intercept_conf_int_2012) # 1975: slope = 0.4652051691605937 conf int = [0.33851226 0.59306491] # 1975: intercept = 2.3908752365842263 conf int = [0.64892945 4.18037063] # 2012: slope = 0.462630358835313 conf int = [0.33137479 0.60695527] # 2012: intercept = 2.977247498236019 conf int = [1.06792753 4.70599387] . It looks like they have the same slope, but different intercepts. . #### Displaying the linear regression results . # Make scatter plot of 1975 data _ = plt.plot(bl_1975, bd_1975, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;, alpha=0.5) # Make scatter plot of 2012 data _ = plt.plot(bl_2012, bd_2012, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;, alpha=0.5) # Label axes and make legend _ = plt.xlabel(&#39;beak length (mm)&#39;) _ = plt.ylabel(&#39;beak depth (mm)&#39;) _ = plt.legend((&#39;1975&#39;, &#39;2012&#39;), loc=&#39;upper left&#39;) # Generate x-values for bootstrap lines: x x = np.array([10, 17]) # Plot the bootstrap lines for i in range(100): plt.plot(x, bs_slope_reps_1975[i] * x + bs_intercept_reps_1975[i], linewidth=0.5, alpha=0.2, color=&#39;blue&#39;) plt.plot(x, bs_slope_reps_2012[i] * x + bs_intercept_reps_2012[i], linewidth=0.5, alpha=0.2, color=&#39;red&#39;) # Draw the plot again plt.show() . . #### Beak length to depth ratio . The linear regressions showed interesting information about the beak geometry. The slope was the same in 1975 and 2012, suggesting that for every millimeter gained in beak length, the birds gained about half a millimeter in depth in both years. However, if we are interested in the shape of the beak, we want to compare the ratio of beak length to beak depth. Let’s make that comparison. . # Compute length-to-depth ratios ratio_1975 = bl_1975 / bd_1975 ratio_2012 = bl_2012 / bd_2012 # Compute means mean_ratio_1975 = np.mean(ratio_1975) mean_ratio_2012 = np.mean(ratio_2012) # Generate bootstrap replicates of the means bs_replicates_1975 = draw_bs_reps(ratio_1975, np.mean, size=10000) bs_replicates_2012 = draw_bs_reps(ratio_2012, np.mean, size=10000) # Compute the 99% confidence intervals conf_int_1975 = np.percentile(bs_replicates_1975, [0.5, 99.5]) conf_int_2012 = np.percentile(bs_replicates_2012, [0.5, 99.5]) # Print the results print(&#39;1975: mean ratio =&#39;, mean_ratio_1975, &#39;conf int =&#39;, conf_int_1975) print(&#39;2012: mean ratio =&#39;, mean_ratio_2012, &#39;conf int =&#39;, conf_int_2012) # 1975: mean ratio = 1.5788823771858533 conf int = [1.55668803 1.60073509] # 2012: mean ratio = 1.4658342276847767 conf int = [1.44363932 1.48729149] . #### How different is the ratio? . In the previous exercise, you computed the mean beak length to depth ratio with 99% confidence intervals for 1975 and for 2012. The results of that calculation are shown graphically in the plot accompanying this problem. In addition to these results, what would you say about the ratio of beak length to depth? . . The mean beak length-to-depth ratio decreased by about 0.1, or 7%, from 1975 to 2012. The 99% confidence intervals are not even close to overlapping, so this is a real change. The beak shape changed. . ### Calculation of heritability . #### EDA of heritability . The array bd_parent_scandens contains the average beak depth (in mm) of two parents of the species G. scandens . The array bd_offspring_scandens contains the average beak depth of the offspring of the respective parents. The arrays bd_parent_fortis and bd_offspring_fortis contain the same information about measurements from G. fortis birds. . Make a scatter plot of the average offspring beak depth (y-axis) versus average parental beak depth (x-axis) for both species. Use the alpha=0.5 keyword argument to help you see overlapping points. . # Make scatter plots _ = plt.plot(bd_parent_fortis, bd_offspring_fortis, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;, alpha=0.5) _ = plt.plot(bd_parent_scandens, bd_offspring_scandens, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;, alpha=0.5) # Label axes _ = plt.xlabel(&#39;parental beak depth (mm)&#39;) _ = plt.ylabel(&#39;offspring beak depth (mm)&#39;) # Add legend _ = plt.legend((&#39;G. fortis&#39;, &#39;G. scandens&#39;), loc=&#39;lower right&#39;) # Show plot plt.show() . . It appears as though there is a stronger correlation in G. fortis than than in G. scandens . This suggests that beak depth is more strongly inherited in G. fortis . We’ll quantify this correlation next. . #### Correlation of offspring and parental data . In an effort to quantify the correlation between offspring and parent beak depths, we would like to compute statistics, such as the Pearson correlation coefficient, between parents and offspring. To get confidence intervals on this, we need to do a pairs bootstrap. . You have already written a function to do pairs bootstrap to get estimates for parameters derived from linear regression. Your task in this exercise is to make a new function with call signature draw_bs_pairs(x, y, func, size=1) that performs pairs bootstrap and computes a single statistic on pairs samples defined. The statistic of interest is computed by calling func(bs_x, bs_y) . In the next exercise, you will use pearson_r for func . . def draw_bs_pairs(x, y, func, size=1): &quot;&quot;&quot;Perform pairs bootstrap for a single statistic.&quot;&quot;&quot; # Set up array of indices to sample from: inds inds = np.arange(len(x)) # Initialize replicates: bs_replicates bs_replicates = np.empty(size) # Generate replicates for i in range(size): bs_inds = np.random.choice(inds, size=len(inds)) bs_x, bs_y = x[bs_inds], y[bs_inds] bs_replicates[i] = func(bs_x, bs_y) return bs_replicates . #### Pearson correlation of offspring and parental data . The Pearson correlation coefficient seems like a useful measure of how strongly the beak depth of parents are inherited by their offspring. Compute the Pearson correlation coefficient between parental and offspring beak depths for G. scandens . Do the same for G. fortis . Then, use the function you wrote in the last exercise to compute a 95% confidence interval using pairs bootstrap. . def pearson_r(x, y): &quot;&quot;&quot;Compute Pearson correlation coefficient between two arrays.&quot;&quot;&quot; # Compute correlation matrix: corr_mat corr_mat = np.corrcoef(x, y) # Return entry [0,1] return corr_mat[0,1] # Compute the Pearson correlation coefficients r_scandens = pearson_r(bd_parent_scandens, bd_offspring_scandens) r_fortis = pearson_r(bd_parent_fortis, bd_offspring_fortis) # Acquire 1000 bootstrap replicates of Pearson r bs_replicates_scandens = draw_bs_pairs(bd_parent_scandens, bd_offspring_scandens, pearson_r, size=1000) bs_replicates_fortis = draw_bs_pairs(bd_parent_fortis, bd_offspring_fortis, pearson_r, size=1000) # Compute 95% confidence intervals conf_int_scandens = np.percentile(bs_replicates_scandens, [2.5, 97.5]) conf_int_fortis = np.percentile(bs_replicates_fortis, [2.5, 97.5]) # Print results print(&#39;G. scandens:&#39;, r_scandens, conf_int_scandens) print(&#39;G. fortis:&#39;, r_fortis, conf_int_fortis) # G. scandens: 0.4117063629401258 [0.26564228 0.54388972] # G. fortis: 0.7283412395518487 [0.6694112 0.77840616] . It is clear from the confidence intervals that beak depth of the offspring of G. fortis parents is more strongly correlated with their offspring than their G. scandens counterparts. . #### Measuring heritability . Remember that the Pearson correlation coefficient is the ratio of the covariance to the geometric mean of the variances of the two data sets. This is a measure of the correlation between parents and offspring, but might not be the best estimate of heritability. If we stop and think, it makes more sense to define heritability as the ratio of the covariance between parent and offspring to the variance of the parents alone . In this exercise, you will estimate the heritability and perform a pairs bootstrap calculation to get the 95% confidence interval. . This exercise highlights a very important point. Statistical inference (and data analysis in general) is not a plug-n-chug enterprise. You need to think carefully about the questions you are seeking to answer with your data and analyze them appropriately. If you are interested in how heritable traits are, the quantity we defined as the heritability is more apt than the off-the-shelf statistic, the Pearson correlation coefficient. . def heritability(parents, offspring): &quot;&quot;&quot;Compute the heritability from parent and offspring samples.&quot;&quot;&quot; covariance_matrix = np.cov(parents, offspring) return covariance_matrix[0,1] / covariance_matrix[0,0] # Compute the heritability heritability_scandens = heritability(bd_parent_scandens, bd_offspring_scandens) heritability_fortis = heritability(bd_parent_fortis, bd_offspring_fortis) # Acquire 1000 bootstrap replicates of heritability replicates_scandens = draw_bs_pairs( bd_parent_scandens, bd_offspring_scandens, heritability, size=1000) replicates_fortis = draw_bs_pairs( bd_parent_fortis, bd_offspring_fortis, heritability, size=1000) # Compute 95% confidence intervals conf_int_scandens = np.percentile(replicates_scandens, [2.5, 97.5]) conf_int_fortis = np.percentile(replicates_fortis, [2.5, 97.5]) # Print results print(&#39;G. scandens:&#39;, heritability_scandens, conf_int_scandens) print(&#39;G. fortis:&#39;, heritability_fortis, conf_int_fortis) # G. scandens: 0.5485340868685982 [0.34395487 0.75638267] # G. fortis: 0.7229051911438159 [0.64655013 0.79688342] . Here again, we see that G. fortis has stronger heritability than G. scandens . This suggests that the traits of G. fortis may be strongly incorporated into G. scandens by introgressive hybridization. . #### Is beak depth heritable at all in G. scandens? . The heritability of beak depth in G. scandens seems low. It could be that this observed heritability was just achieved by chance and beak depth is actually not really heritable in the species . You will test that hypothesis here. To do this, you will do a pairs permutation test. . # Initialize array of replicates: perm_replicates perm_replicates = np.empty(10000) # Draw replicates for i in range(10000): # Permute parent beak depths bd_parent_permuted = np.random.permutation(bd_parent_scandens) perm_replicates[i] = heritability(bd_parent_permuted, bd_offspring_scandens) # Compute p-value: p p = np.sum(perm_replicates &gt;= heritability_scandens) / len(perm_replicates) # Print the p-value print(&#39;p-val =&#39;, p) # p-val = 0.0 . You get a p-value of zero, which means that none of the 10,000 permutation pairs replicates you drew had a heritability high enough to match that which was observed. This strongly suggests that beak depth is heritable in G. scandens , just not as much as in G. fortis . If you like, you can plot a histogram of the heritability replicates to get a feel for how extreme of a value of heritability you might expect by chance. . plt.hist(perm_replicates) plt.axvline(x=heritability_scandens, color = &#39;red&#39;) plt.text(heritability_scandens, 1500, &#39;heritability_scandens&#39;, ha=&#39;center&#39;, va=&#39;center&#39;,rotation=&#39;vertical&#39;, backgroundcolor=&#39;white&#39;) plt.show() . Parameter estimation by optimization . ### Optimal parameters . #### How often do we get no-hitters? . The number of games played between each no-hitter in the modern era (1901-2015) of Major League Baseball is stored in the array nohitter_times . . If you assume that no-hitters are described as a Poisson process, then the time between no-hitters is Exponentially distributed. As you have seen, the Exponential distribution has a single parameter, which we will call ττ, the typical interval time. The value of the parameter ττ that makes the exponential distribution best match the data is the mean interval time (where time is in units of number of games) between no-hitters. . Compute the value of this parameter from the data. Then, use np.random.exponential() to “repeat” the history of Major League Baseball by drawing inter-no-hitter times from an exponential distribution with the ττ you found and plot the histogram as an approximation to the PDF. . # Seed random number generator np.random.seed(42) # Compute mean no-hitter time: tau tau = np.mean(nohitter_times) # Draw out of an exponential distribution with parameter tau: inter_nohitter_time inter_nohitter_time = np.random.exponential(tau, 100000) # Plot the PDF and label axes _ = plt.hist(inter_nohitter_time, bins=50, normed=True, histtype=&#39;step&#39;) _ = plt.xlabel(&#39;Games between no-hitters&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() . . We see the typical shape of the Exponential distribution, going from a maximum at 0 and decaying to the right. . #### Do the data follow our story? . # Create an ECDF from real data: x, y x, y = ecdf(nohitter_times) # Create a CDF from theoretical samples: x_theor, y_theor x_theor, y_theor = ecdf(inter_nohitter_time) # Overlay the plots plt.plot(x_theor, y_theor) plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Margins and axis labels plt.margins(0.02) plt.xlabel(&#39;Games between no-hitters&#39;) plt.ylabel(&#39;CDF&#39;) # Show the plot plt.show() . . It looks like no-hitters in the modern era of Major League Baseball are Exponentially distributed. Based on the story of the Exponential distribution, this suggests that they are a random process; when a no-hitter will happen is independent of when the last no-hitter was. . #### How is this parameter optimal? . # Plot the theoretical CDFs plt.plot(x_theor, y_theor) plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;) plt.margins(0.02) plt.xlabel(&#39;Games between no-hitters&#39;) plt.ylabel(&#39;CDF&#39;) # Take samples with half tau: samples_half samples_half = np.random.exponential(tau/2, size=10000) # Take samples with double tau: samples_double samples_double = np.random.exponential(2*tau, size=10000) # Generate CDFs from these samples x_half, y_half = ecdf(samples_half) x_double, y_double = ecdf(samples_double) # Plot these CDFs as lines _ = plt.plot(x_half, y_half) _ = plt.plot(x_double, y_double) # Show the plot plt.show() . . red = half, purple = double . Notice how the value of tau given by the mean matches the data best. In this way, tau is an optimal parameter. . ### Linear regression by least squares . #### EDA of literacy/fertility data . # Plot the illiteracy rate versus fertility _ = plt.plot(illiteracy, fertility, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Set the margins and label axes plt.margins(0.02) _ = plt.xlabel(&#39;percent illiterate&#39;) _ = plt.ylabel(&#39;fertility&#39;) # Show the plot plt.show() # Show the Pearson correlation coefficient print(pearson_r(illiteracy, fertility)) 0.8041324026815344 . . You can see the correlation between illiteracy and fertility by eye, and by the substantial Pearson correlation coefficient of 0.8. It is difficult to resolve in the scatter plot, but there are many points around near-zero illiteracy and about 1.8 children/woman. . #### Linear regression . # Plot the illiteracy rate versus fertility _ = plt.plot(illiteracy, fertility, marker=&#39;.&#39;, linestyle=&#39;none&#39;) plt.margins(0.02) _ = plt.xlabel(&#39;percent illiterate&#39;) _ = plt.ylabel(&#39;fertility&#39;) # Perform a linear regression using np.polyfit(): a, b a, b = np.polyfit(illiteracy, fertility, deg=1) # Print the results to the screen print(&#39;slope =&#39;, a, &#39;children per woman / percent illiterate&#39;) print(&#39;intercept =&#39;, b, &#39;children per woman&#39;) # Make theoretical line to plot x = np.array([0, 100]) y = a * x + b # Add regression line to your plot _ = plt.plot(x, y) # Draw the plot plt.show() # slope = 0.04979854809063423 children per woman / percent illiterate # intercept = 1.888050610636557 children per woman . . #### How is it optimal? . # Specify slopes to consider: a_vals a_vals = np.linspace(0, 0.1, 200) # Initialize sum of square of residuals: rss rss = np.empty_like(a_vals) # Compute sum of square of residuals for each value of a_vals for i, a in enumerate(a_vals): rss[i] = np.sum((fertility - a*illiteracy - b)**2) # Plot the RSS plt.plot(a_vals, rss, &#39;-&#39;) plt.xlabel(&#39;slope (children per woman / percent illiterate)&#39;) plt.ylabel(&#39;sum of square of residuals&#39;) plt.show() . . Notice that the minimum on the plot, that is the value of the slope that gives the minimum sum of the square of the residuals, is the same value you got when performing the regression. . ### The importance of EDA: Anscombe’s quartet . . #### The importance of EDA . Why should exploratory data analysis be the first step in an analysis of data (after getting your data imported and cleaned, of course)? . You can be protected from misinterpretation of the type demonstrated by Anscombe’s quartet. | EDA provides a good starting point for planning the rest of your analysis. | EDA is not really any more difficult than any of the subsequent analysis, so there is no excuse for not exploring the data. | . #### Linear regression on appropriate Anscombe data . # Perform linear regression: a, b a, b = np.polyfit(x, y, deg=1) # Print the slope and intercept print(a, b) # Generate theoretical x and y data: x_theor, y_theor x_theor = np.array([3, 15]) y_theor = a * x_theor + b # Plot the Anscombe data and theoretical line _ = plt.plot(x, y, marker = &#39;.&#39;, linestyle = &#39;none&#39;) _ = plt.plot(x_theor, y_theor) # Label the axes plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;) # Show the plot plt.show() . . #### Linear regression on all Anscombe data . for i in range(4): plt.subplot(2,2,i+1) # plot the scatter plot plt.plot(anscombe_x[i], anscombe_y[i], marker = &#39;.&#39;, linestyle = &#39;none&#39;) # plot the regression line a, b = np.polyfit(anscombe_x[i], anscombe_y[i], deg=1) x_theor = np.array([np.min(anscombe_x[i]), np.max(anscombe_x[i])]) y_theor = a * x_theor + b plt.plot(x_theor, y_theor) # add label plt.xlabel(&#39;x&#39; + str(i+1)) plt.ylabel(&#39;y&#39; + str(i+1)) plt.show() # slope1: 0.5000909090909095 intercept: 3.000090909090909 # slope2: 0.5000000000000004 intercept: 3.0009090909090896 # slope3: 0.4997272727272731 intercept: 3.0024545454545453 # slope4: 0.4999090909090908 intercept: 3.0017272727272735 . . ### Generating bootstrap replicates . #### Getting the terminology down . If we have a data set with n repeated measurements, a bootstrap sample is an array of length n that was drawn from the original data with replacement. . Bootstrap replicate is a single value of a statistic computed from a bootstrap sample. . #### Visualizing bootstrap samples . np.random.choice() . for _ in range(50): # Generate bootstrap sample: bs_sample bs_sample = np.random.choice(rainfall, size=len(rainfall)) # Compute and plot ECDF from bootstrap sample x, y = ecdf(bs_sample) _ = plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;gray&#39;, alpha=0.1) # Compute and plot ECDF from original data x, y = ecdf(rainfall) _ = plt.plot(x, y, marker=&#39;.&#39;) # Make margins and label axes plt.margins(0.02) _ = plt.xlabel(&#39;yearly rainfall (mm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) # Show the plot plt.show() . . Bootstrap confidence intervals . #### Generating many bootstrap replicates . def bootstrap_replicate_1d(data, func): return func(np.random.choice(data, size=len(data))) def draw_bs_reps(data, func, size=1): &quot;&quot;&quot;Draw bootstrap replicates.&quot;&quot;&quot; # Initialize array of replicates: bs_replicates bs_replicates = np.empty(size) # Generate replicates for i in range(size): bs_replicates[i] = bootstrap_replicate_1d(data, func) return bs_replicates . #### Bootstrap replicates of the mean and the SEM ( standard error of the mean ) . In fact, it can be shown theoretically that under not-too-restrictive conditions, the value of the mean will always be Normally distributed. (This does not hold in general, just for the mean and a few other statistics.) . The standard deviation of this distribution, called the standard error of the mean , or SEM, is given by the standard deviation of the data divided by the square root of the number of data points. I.e., for a data set, sem = np.std(data) / np.sqrt(len(data)) . Using hacker statistics, you get this same result without the need to derive it, but you will verify this result from your bootstrap replicates. . # Take 10,000 bootstrap replicates of the mean: bs_replicates bs_replicates = draw_bs_reps(rainfall, np.mean, size=10000) # Compute and print SEM sem = np.std(rainfall) / np.sqrt(len(rainfall)) print(sem) # Compute and print standard deviation of bootstrap replicates bs_std = np.std(bs_replicates) print(bs_std) # Make a histogram of the results _ = plt.hist(bs_replicates, bins=50, normed=True) _ = plt.xlabel(&#39;mean annual rainfall (mm)&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() # 10.51054915050619 # 10.465927071184412 . . #### Confidence intervals of rainfall data . Use the bootstrap replicates you just generated to compute the 95% confidence interval. That is, give the 2.5th and 97.5th percentile of your bootstrap replicates stored as bs_replicates . What is the 95% confidence interval? . np.percentile(bs_replicates,2.5) 779.7699248120301 np.percentile(bs_replicates,97.5) 820.950432330827 . #### Bootstrap replicates of other statistics . def draw_bs_reps(data, func, size=1): return np.array([bootstrap_replicate_1d(data, func) for _ in range(size)]) # Generate 10,000 bootstrap replicates of the variance: bs_replicates bs_replicates = draw_bs_reps(rainfall, np.var, size=10000) # Put the variance in units of square centimeters bs_replicates /= 100 # Make a histogram of the results _ = plt.hist(bs_replicates, bins=50, normed=True) _ = plt.xlabel(&#39;variance of annual rainfall (sq. cm)&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() . . This is not normally distributed, as it has a longer tail to the right. Note that you can also compute a confidence interval on the variance, or any other statistic, using np.percentile() with your bootstrap replicates. . #### Confidence interval on the rate of no-hitters . # Draw bootstrap replicates of the mean no-hitter time (equal to tau): bs_replicates bs_replicates = draw_bs_reps(nohitter_times, np.mean, size=10000) # Compute the 95% confidence interval: conf_int conf_int = np.percentile(bs_replicates, [2.5, 97.5]) # Print the confidence interval print(&#39;95% confidence interval =&#39;, conf_int, &#39;games&#39;) # Plot the histogram of the replicates _ = plt.hist(bs_replicates, bins=50, normed=True) _ = plt.xlabel(r&#39;$ tau$ (games)&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() # 95% confidence interval = [660.67280876 871.63077689] games . . This gives you an estimate of what the typical time between no-hitters is. It could be anywhere between 660 and 870 games. . ### Pairs bootstrap . #### A function to do pairs bootstrap . def draw_bs_pairs_linreg(x, y, size=1): &quot;&quot;&quot;Perform pairs bootstrap for linear regression.&quot;&quot;&quot; # Set up array of indices to sample from: inds inds = np.arange(len(x)) # Initialize replicates: bs_slope_reps, bs_intercept_reps bs_slope_reps = np.empty(size) bs_intercept_reps = np.empty(size) # Generate replicates for i in range(size): bs_inds = np.random.choice(inds, size=len(inds)) bs_x, bs_y = x[bs_inds], y[bs_inds] bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, deg=1) return bs_slope_reps, bs_intercept_reps . #### Pairs bootstrap of literacy/fertility data . # Generate replicates of slope and intercept using pairs bootstrap bs_slope_reps, bs_intercept_reps = draw_bs_pairs_linreg(illiteracy, fertility, size=1000) # Compute and print 95% CI for slope print(np.percentile(bs_slope_reps, [2.5, 97.5])) # Plot the histogram _ = plt.hist(bs_slope_reps, bins=50, normed=True) _ = plt.xlabel(&#39;slope&#39;) _ = plt.ylabel(&#39;PDF&#39;) plt.show() # [0.04378061 0.0551616 ] . . #### Plotting bootstrap regressions . # Generate array of x-values for bootstrap lines: x x = np.array([0, 100]) # Plot the bootstrap lines for i in range(100): _ = plt.plot(x, bs_slope_reps[i]*x + bs_intercept_reps[i], linewidth=0.5, alpha=0.2, color=&#39;red&#39;) # Plot the data _ = plt.plot(illiteracy, fertility, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Label axes, set the margins, and show the plot _ = plt.xlabel(&#39;illiteracy&#39;) _ = plt.ylabel(&#39;fertility&#39;) plt.margins(0.02) plt.show() . . Introduction to hypothesis testing . ### Formulating and simulating a hypothesis . Null hypothesis another name for the hypothesis you are testing Permutation random reordering of entries in an array #### Generating a permutation sample . np.random.permutation() . Permutation sampling is a great way to simulate the hypothesis that two variables have identical probability distributions. This is often a hypothesis you want to test, so in this exercise, you will write a function to generate a permutation sample from two data sets. . def permutation_sample(data1, data2): &quot;&quot;&quot;Generate a permutation sample from two data sets.&quot;&quot;&quot; # Concatenate the data sets: data data = np.concatenate((data1, data2)) # Permute the concatenated array: permuted_data permuted_data = np.random.permutation(data) # Split the permuted array into two: perm_sample_1, perm_sample_2 perm_sample_1 = permuted_data[:len(data1)] perm_sample_2 = permuted_data[len(data1):] return perm_sample_1, perm_sample_2 . #### Visualizing permutation sampling . for _ in range(50): # Generate permutation samples perm_sample_1, perm_sample_2 = permutation_sample(rain_june, rain_november) # Compute ECDFs x_1, y_1 = ecdf(perm_sample_1) x_2, y_2 = ecdf(perm_sample_2) # Plot ECDFs of permutation sample _ = plt.plot(x_1, y_1, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;, alpha=0.02) _ = plt.plot(x_2, y_2, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;, alpha=0.02) # Create and plot ECDFs from original data x_1, y_1 = ecdf(rain_june) x_2, y_2 = ecdf(rain_november) _ = plt.plot(x_1, y_1, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;) _ = plt.plot(x_2, y_2, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;) # Label axes, set margin, and show plot plt.margins(0.02) _ = plt.xlabel(&#39;monthly rainfall (mm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) plt.show() . . Notice that the permutation samples ECDFs overlap and give a purple haze. None of the ECDFs from the permutation samples overlap with the observed data, suggesting that the hypothesis is not commensurate with the data. June and November rainfall are not identically distributed. . ### Test statistics and p-values . #### Test statistics . When performing hypothesis tests, your choice of test statistic should be pertinent to the question you are seeking to answer in your hypothesis test. . The most important thing to consider is: What are you asking? . #### What is a p-value? . The p-value is generally a measure of the probability of observing a test statistic equally or more extreme than the one you observed, given that the null hypothesis is true. . #### Generating permutation replicates . # In most circumstances, func will be a function you write yourself. def draw_perm_reps(data_1, data_2, func, size=1): &quot;&quot;&quot;Generate multiple permutation replicates.&quot;&quot;&quot; # Initialize array of replicates: perm_replicates perm_replicates = np.empty(size) for i in range(size): # Generate permutation sample perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2) # Compute the test statistic perm_replicates[i] = func(perm_sample_1, perm_sample_2) return perm_replicates . #### Look before you leap: EDA before hypothesis testing . Kleinteich and Gorb ( Sci. Rep. , 4 , 5225, 2014) performed an interesting experiment with South American horned frogs. They held a plate connected to a force transducer, along with a bait fly, in front of them. They then measured the impact force and adhesive force of the frog’s tongue when it struck the target. . Frog A is an adult and Frog B is a juvenile. The researchers measured the impact force of 20 strikes for each frog. In the next exercise, we will test the hypothesis that the two frogs have the same distribution of impact forces. But, remember, it is important to do EDA first! Let’s make a bee swarm plot for the data. They are stored in a Pandas data frame, df , where column ID is the identity of the frog and column impact_force is the impact force in Newtons (N). . df.head() ID impact_force 20 A 1.612 21 A 0.605 22 A 0.327 23 A 0.946 24 A 0.541 . # Make bee swarm plot _ = sns.swarmplot(x=&#39;ID&#39;, y=&#39;impact_force&#39;, data=df) # Label axes _ = plt.xlabel(&#39;frog&#39;) _ = plt.ylabel(&#39;impact force (N)&#39;) # Show the plot plt.show() . . Eyeballing it, it does not look like they come from the same distribution. Frog A, the adult, has three or four very hard strikes, and Frog B, the juvenile, has a couple weak ones. However, it is possible that with only 20 samples it might be too difficult to tell if they have difference distributions, so we should proceed with the hypothesis test. . #### Permutation test on frog data . The average strike force of Frog A was 0.71 Newtons (N), and that of Frog B was 0.42 N for a difference of 0.29 N. It is possible the frogs strike with the same force and this observed difference was by chance. You will compute the probability of getting at least a 0.29 N difference in mean strike force under the hypothesis that the distributions of strike forces for the two frogs are identical. We use a permutation test with a test statistic of the difference of means to test this hypothesis. . def diff_of_means(data_1, data_2): &quot;&quot;&quot;Difference in means of two arrays.&quot;&quot;&quot; # The difference of means of data_1, data_2: diff diff = np.mean(data_1) - np.mean(data_2) return diff # Compute difference of mean impact force from experiment: empirical_diff_means empirical_diff_means = diff_of_means(force_a, force_b) # Draw 10,000 permutation replicates: perm_replicates perm_replicates = draw_perm_reps(force_a, force_b, diff_of_means, size=10000) # Compute p-value: p p = np.sum(perm_replicates &gt;= empirical_diff_means) / len(perm_replicates) # Print the result print(&#39;p-value =&#39;, p) # p-value = 0.0063 # p-value = 0.63% . The p-value tells you that there is about a 0.6% chance that you would get the difference of means observed in the experiment if frogs were exactly the same. . A p-value below 0.01 is typically said to be “statistically significant,” but: warning! warning! warning! You have computed a p-value; it is a number. I encourage you not to distill it to a yes-or-no phrase. p = 0.006 and p = 0.000000006 are both said to be “statistically significant,” but they are definitely not the same! . ### Bootstrap hypothesis tests . . #### A one-sample bootstrap hypothesis test . Another juvenile frog was studied, Frog C, and you want to see if Frog B and Frog C have similar impact forces. Unfortunately, you do not have Frog C’s impact forces available, but you know they have a mean of 0.55 N. Because you don’t have the original data, you cannot do a permutation test, and you cannot assess the hypothesis that the forces from Frog B and Frog C come from the same distribution. You will therefore test another, less restrictive hypothesis: The mean strike force of Frog B is equal to that of Frog C. . To set up the bootstrap hypothesis test, you will take the mean as our test statistic. Remember, your goal is to calculate the probability of getting a mean impact force less than or equal to what was observed for Frog B if the hypothesis that the true mean of Frog B’s impact forces is equal to that of Frog C is true . You first translate all of the data of Frog B such that the mean is 0.55 N. This involves adding the mean force of Frog C and subtracting the mean force of Frog B from each measurement of Frog B. This leaves other properties of Frog B’s distribution, such as the variance, unchanged. . # Make an array of translated impact forces: translated_force_b translated_force_b = force_b - np.mean(force_b) + 0.55 # Take bootstrap replicates of Frog B&#39;s translated impact forces: bs_replicates bs_replicates = draw_bs_reps(translated_force_b, np.mean, 10000) # Compute fraction of replicates that are less than the observed Frog B force: p p = np.sum(bs_replicates &lt;= np.mean(force_b)) / 10000 # Print the p-value print(&#39;p = &#39;, p) # p = 0.0046 # p = 0.46% . The low p-value suggests that the null hypothesis that Frog B and Frog C have the same mean impact force is false. . #### A two-sample bootstrap hypothesis test for difference of means . We now want to test the hypothesis that Frog A and Frog B have the same mean impact force, but not necessarily the same distribution, which is also impossible with a permutation test. . To do the two-sample bootstrap test, we shift both arrays to have the same mean, since we are simulating the hypothesis that their means are, in fact, equal. We then draw bootstrap samples out of the shifted arrays and compute the difference in means. This constitutes a bootstrap replicate, and we generate many of them. The p-value is the fraction of replicates with a difference in means greater than or equal to what was observed. . # Compute mean of all forces: mean_force mean_force = np.mean(forces_concat) # Generate shifted arrays force_a_shifted = force_a - np.mean(force_a) + mean_force force_b_shifted = force_b - np.mean(force_b) + mean_force # Compute 10,000 bootstrap replicates from shifted arrays bs_replicates_a = draw_bs_reps(force_a_shifted, np.mean, size=10000) bs_replicates_b = draw_bs_reps(force_b_shifted, np.mean, size=10000) # Get replicates of difference of means: bs_replicates bs_replicates = bs_replicates_a - bs_replicates_b # Compute and print p-value: p p = np.sum(bs_replicates &gt;= empirical_diff_means) / 10000 print(&#39;p-value =&#39;, p) # p-value = 0.0043 # p-value = 0.43% . You got a similar result as when you did the permutation test. Nonetheless, remember that it is important to carefully think about what question you want to ask. Are you only interested in the mean impact force, or in the distribution of impact forces? . Hypothesis test examples . ### A/B testing . #### The vote for the Civil Rights Act in 1964 . The Civil Rights Act of 1964 was one of the most important pieces of legislation ever passed in the USA. Excluding “present” and “abstain” votes, 153 House Democrats and 136 Republicans voted yea. However, 91 Democrats and 35 Republicans voted nay. Did party affiliation make a difference in the vote? . To answer this question, you will evaluate the hypothesis that the party of a House member has no bearing on his or her vote. You will use the fraction of Democrats voting in favor as your test statistic and evaluate the probability of observing a fraction of Democrats voting in favor at least as small as the observed fraction of 153/244. (That’s right, at least as small as. In 1964, it was the Democrats who were less progressive on civil rights issues.) To do this, permute the party labels of the House voters and then arbitrarily divide them into “Democrats” and “Republicans” and compute the fraction of Democrats voting yea. . # Construct arrays of data: dems, reps dems = np.array([True] * 153 + [False] * 91) reps = np.array([True] * 136 + [False] * 35) def frac_yea_dems(dems, reps): &quot;&quot;&quot;Compute fraction of Democrat yea votes.&quot;&quot;&quot; frac = np.sum(dems) / len(dems) return frac # Acquire permutation samples: perm_replicates perm_replicates = draw_perm_reps(dems, reps, frac_yea_dems, size=10000) # Compute and print p-value: p p = np.sum(perm_replicates &lt;= 153/244) / len(perm_replicates) print(&#39;p-value =&#39;, p) # p-value = 0.0002 # p-value = 0.02% . This small p-value suggests that party identity had a lot to do with the voting. Importantly, the South had a higher fraction of Democrat representatives, and consequently also a more racist bias. . #### What is equivalent? . You have experience matching a stories to probability distributions. Similarly, you use the same procedure for two different A/B tests if their stories match. In the Civil Rights Act example you just did, you performed an A/B test on voting data, which has a Yes/No type of outcome for each subject (in that case, a voter). Which of the following situations involving testing by a web-based company has an equivalent set up for an A/B test as the one you just did with the Civil Rights Act of 1964? . You measure the number of people who click on an ad on your company’s website before and after changing its color. . The “Democrats” are those who view the ad before the color change, and the “Republicans” are those who view it after. . #### A time-on-website analog . It turns out that you already did a hypothesis test analogous to an A/B test where you are interested in how much time is spent on the website before and after an ad campaign. The frog tongue force (a continuous quantity like time on the website) is an analog. “Before” = Frog A and “after” = Frog B. Let’s practice this again with something that actually is a before/after scenario. . We return to the no-hitter data set. In 1920, Major League Baseball implemented important rule changes that ended the so-called dead ball era. Importantly, the pitcher was no longer allowed to spit on or scuff the ball, an activity that greatly favors pitchers. In this problem you will perform an A/B test to determine if these rule changes resulted in a slower rate of no-hitters (i.e., longer average time between no-hitters) using the difference in mean inter-no-hitter time as your test statistic. The inter-no-hitter times for the respective eras are stored in the arrays nht_dead and nht_live , where “nht” is meant to stand for “no-hitter time.” . def diff_of_means(data_1, data_2): &quot;&quot;&quot;Difference in means of two arrays.&quot;&quot;&quot; # The difference of means of data_1, data_2: diff diff = np.mean(data_1) - np.mean(data_2) return diff def permutation_sample(data1, data2): &quot;&quot;&quot;Generate a permutation sample from two data sets.&quot;&quot;&quot; # Concatenate the data sets: data data = np.concatenate((data1, data2)) # Permute the concatenated array: permuted_data permuted_data = np.random.permutation(data) # Split the permuted array into two: perm_sample_1, perm_sample_2 perm_sample_1 = permuted_data[:len(data1)] perm_sample_2 = permuted_data[len(data1):] return perm_sample_1, perm_sample_2 def draw_perm_reps(data_1, data_2, func, size=1): &quot;&quot;&quot;Generate multiple permutation replicates.&quot;&quot;&quot; # Initialize array of replicates: perm_replicates perm_replicates = np.empty(size) for i in range(size): # Generate permutation sample perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2) # Compute the test statistic perm_replicates[i] = func(perm_sample_1, perm_sample_2) return perm_replicates # Compute the observed difference in mean inter-no-hitter times: nht_diff_obs nht_diff_obs = diff_of_means(nht_dead, nht_live) # Acquire 10,000 permutation replicates of difference in mean no-hitter time: perm_replicates perm_replicates = draw_perm_reps(nht_dead, nht_live, diff_of_means, size=10000) # Compute and print the p-value: p p = np.sum(perm_replicates &lt;= nht_diff_obs) /len(perm_replicates) print(&#39;p-val =&#39;, p) p-val = 0.0001 . Your p-value is 0.0001, which means that only one out of your 10,000 replicates had a result as extreme as the actual difference between the dead ball and live ball eras. This suggests strong statistical significance. Watch out, though, you could very well have gotten zero replicates that were as extreme as the observed value. This just means that the p-value is quite small, almost certainly smaller than 0.001. . #### What should you have done first? . That was a nice hypothesis test you just did to check out whether the rule changes in 1920 changed the rate of no-hitters. But what should you have done with the data first? . Performed EDA, perhaps plotting the ECDFs of inter-no-hitter times in the dead ball and live ball eras. . Always a good idea to do first! I encourage you to go ahead and plot the ECDFs right now. You will see by eye that the null hypothesis that the distributions are the same is almost certainly not true. . # Create and plot ECDFs x_1, y_1 = ecdf(nht_dead) x_2, y_2 = ecdf(nht_live) _ = plt.plot(x_1, y_1, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;) _ = plt.plot(x_2, y_2, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;) # Label axes, set margin, and show plot plt.margins(0.02) _ = plt.xlabel(&#39;Games between no-hitters&#39;) _ = plt.ylabel(&#39;ECDF&#39;) plt.show() . . ### Test of correlation . . #### Simulating a null hypothesis concerning correlation . The observed correlation between female illiteracy and fertility in the data set of 162 countries may just be by chance; the fertility of a given country may actually be totally independent of its illiteracy. You will test this null hypothesis in the next exercise. . To do the test, you need to simulate the data assuming the null hypothesis is true. Of the following choices, which is the best way to to do it? . Answer: Do a permutation test: Permute the illiteracy values but leave the fertility values fixed to generate a new set of (illiteracy, fertility) data. . This exactly simulates the null hypothesis and does so more efficiently than the last option. It is exact because it uses all data and eliminates any correlation because which illiteracy value pairs to which fertility value is shuffled. . Last option: Do a permutation test: Permute both the illiteracy and fertility values to generate a new set of (illiteracy, fertility data). This exactly simulates the null hypothesis and does so more efficiently than the last option. It is exact because it uses all data and eliminates any correlation because which illiteracy value pairs to which fertility value is shuffled. . #### Hypothesis test on Pearson correlation . The observed correlation between female illiteracy and fertility may just be by chance; the fertility of a given country may actually be totally independent of its illiteracy. You will test this hypothesis. To do so, permute the illiteracy values but leave the fertility values fixed. This simulates the hypothesis that they are totally independent of each other. For each permutation, compute the Pearson correlation coefficient and assess how many of your permutation replicates have a Pearson correlation coefficient greater than the observed one. . def pearson_r(x, y): &quot;&quot;&quot;Compute Pearson correlation coefficient between two arrays.&quot;&quot;&quot; # Compute correlation matrix: corr_mat corr_mat = np.corrcoef(x, y) # Return entry [0,1] return corr_mat[0,1] # Compute observed correlation: r_obs r_obs = pearson_r(illiteracy, fertility) # r_obs = 0.8041324026815344 # Initialize permutation replicates: perm_replicates perm_replicates = np.empty(10000) # Draw replicates for i in range(10000): # Permute illiteracy measurments: illiteracy_permuted illiteracy_permuted = np.random.permutation(illiteracy) # Compute Pearson correlation perm_replicates[i] = pearson_r(illiteracy_permuted, fertility) # Compute p-value: p p = np.sum(perm_replicates &gt;= r_obs) /len(perm_replicates) print(&#39;p-val =&#39;, p) # p-val = 0.0 . You got a p-value of zero. In hacker statistics, this means that your p-value is very low, since you never got a single replicate in the 10,000 you took that had a Pearson correlation greater than the observed one. You could try increasing the number of replicates you take to continue to move the upper bound on your p-value lower and lower. . #### Do neonicotinoid insecticides have unintended consequences? . As a final exercise in hypothesis testing before we put everything together in our case study in the next chapter, you will investigate the effects of neonicotinoid insecticides on bee reproduction. These insecticides are very widely used in the United States to combat aphids and other pests that damage plants. . In a recent study, Straub, et al. ( Proc. Roy. Soc. B , 2016 ) investigated the effects of neonicotinoids on the sperm of pollinating bees. In this and the next exercise, you will study how the pesticide treatment affected the count of live sperm per half milliliter of semen. . First, we will do EDA, as usual. Plot ECDFs of the alive sperm count for untreated bees (stored in the Numpy array control ) and bees treated with pesticide (stored in the Numpy array treated ). . # Compute x,y values for ECDFs x_control, y_control = ecdf(control) x_treated, y_treated = ecdf(treated) # Plot the ECDFs plt.plot(x_control, y_control, marker=&#39;.&#39;, linestyle=&#39;none&#39;) plt.plot(x_treated, y_treated, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Set the margins plt.margins(0.02) # Add a legend plt.legend((&#39;control&#39;, &#39;treated&#39;), loc=&#39;lower right&#39;) # Label axes and show plot plt.xlabel(&#39;millions of alive sperm per mL&#39;) plt.ylabel(&#39;ECDF&#39;) plt.show() . . The ECDFs show a pretty clear difference between the treatment and control; treated bees have fewer alive sperm. Let’s now do a hypothesis test in the next exercise. . #### Bootstrap hypothesis test on bee sperm counts . Now, you will test the following hypothesis: . On average, male bees treated with neonicotinoid insecticide have the same number of active sperm per milliliter of semen than do untreated male bees. . You will use the difference of means as your test statistic. . def bootstrap_replicate_1d(data, func): return func(np.random.choice(data, size=len(data))) def draw_bs_reps(data, func, size=1): &quot;&quot;&quot;Draw bootstrap replicates.&quot;&quot;&quot; # Initialize array of replicates: bs_replicates bs_replicates = np.empty(size) # Generate replicates for i in range(size): bs_replicates[i] = bootstrap_replicate_1d(data, func) return bs_replicates # Compute the difference in mean sperm count: diff_means diff_means = np.mean(control) - np.mean(treated) # Compute mean of pooled data: mean_count mean_count = np.mean(np.concatenate((control, treated))) # Generate shifted data sets control_shifted = control - np.mean(control) + mean_count treated_shifted = treated - np.mean(treated) + mean_count # Generate bootstrap replicates bs_reps_control = draw_bs_reps(control_shifted, np.mean, size=10000) bs_reps_treated = draw_bs_reps(treated_shifted, np.mean, size=10000) # Get replicates of difference of means: bs_replicates bs_replicates = bs_reps_control - bs_reps_treated # Compute and print p-value: p p = np.sum(bs_replicates &gt;= np.mean(control) - np.mean(treated)) / len(bs_replicates) print(&#39;p-value =&#39;, p) # p-value = 0.0 . The p-value is small, most likely less than 0.0001, since you never saw a bootstrap replicated with a difference of means at least as extreme as what was observed. In fact, when I did the calculation with 10 million replicates, I got a p-value of 2e-05 . Putting it all together: a case study . ### Finch beaks and the need for statistics . . . #### EDA of beak depths of Darwin’s finches . For your first foray into the Darwin finch data, you will study how the beak depth (the distance, top to bottom, of a closed beak) of the finch species Geospiza scandens has changed over time. The Grants have noticed some changes of beak geometry depending on the types of seeds available on the island, and they also noticed that there was some interbreeding with another major species on Daphne Major, Geospiza fortis . These effects can lead to changes in the species over time. . In the next few problems, you will look at the beak depth of G. scandens on Daphne Major in 1975 and in 2012. To start with, let’s plot all of the beak depth measurements in 1975 and 2012 in a bee swarm plot. . The data are stored in a pandas DataFrame called df with columns &#39;year&#39; and &#39;beak_depth&#39; . The units of beak depth are millimeters (mm). . df.head() beak_depth year 0 8.4 1975 1 8.8 1975 2 8.4 1975 3 8.0 1975 4 7.9 1975 . # Create bee swarm plot _ = sns.swarmplot(&#39;year&#39;, &#39;beak_depth&#39;, data=df) # Label the axes _ = plt.xlabel(&#39;year&#39;) _ = plt.ylabel(&#39;beak depth (mm)&#39;) # Show the plot plt.show() . . It is kind of hard to see if there is a clear difference between the 1975 and 2012 data set. Eyeballing it, it appears as though the mean of the 2012 data set might be slightly higher, and it might have a bigger variance. . #### ECDFs of beak depths . While bee swarm plots are useful, we found that ECDFs are often even better when doing EDA. Plot the ECDFs for the 1975 and 2012 beak depth measurements on the same plot. . # Compute ECDFs x_1975, y_1975 = ecdf(bd_1975) x_2012, y_2012 = ecdf(bd_2012) # Plot the ECDFs _ = plt.plot(x_1975, y_1975, marker=&#39;.&#39;, linestyle=&#39;none&#39;) _ = plt.plot(x_2012, y_2012, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Set margins plt.margins(0.02) # Add axis labels and legend _ = plt.xlabel(&#39;beak depth (mm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) _ = plt.legend((&#39;1975&#39;, &#39;2012&#39;), loc=&#39;lower right&#39;) # Show the plot plt.show() . . The differences are much clearer in the ECDF. The mean is larger in the 2012 data, and the variance does appear larger as well. . #### Parameter estimates of beak depths . Estimate the difference of the mean beak depth of the G. scandens samples from 1975 and 2012 and report a 95% confidence interval. . def draw_bs_reps(data, func, size=1): &quot;&quot;&quot;Draw bootstrap replicates.&quot;&quot;&quot; # Initialize array of replicates: bs_replicates bs_replicates = np.empty(size) # Generate replicates for i in range(size): bs_replicates[i] = bootstrap_replicate_1d(data, func) return bs_replicates # Compute the difference of the sample means: mean_diff mean_diff = np.mean(bd_2012) - np.mean(bd_1975) # Get bootstrap replicates of means bs_replicates_1975 = draw_bs_reps(bd_1975, np.mean, size=10000) bs_replicates_2012 = draw_bs_reps(bd_2012, np.mean, size=10000) # Compute samples of difference of means: bs_diff_replicates bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975 # Compute 95% confidence interval: conf_int conf_int = np.percentile(bs_diff_replicates, [2.5, 97.5]) # Print the results print(&#39;difference of means =&#39;, mean_diff, &#39;mm&#39;) print(&#39;95% confidence interval =&#39;, conf_int, &#39;mm&#39;) # difference of means = 0.22622047244094645 mm # 95% confidence interval = [0.05633521 0.39190544] mm . #### Hypothesis test: Are beaks deeper in 2012? . Your plot of the ECDF and determination of the confidence interval make it pretty clear that the beaks of G. scandens on Daphne Major have gotten deeper. But is it possible that this effect is just due to random chance? In other words, what is the probability that we would get the observed difference in mean beak depth if the means were the same? . Be careful! The hypothesis we are testing is not that the beak depths come from the same distribution. For that we could use a permutation test. The hypothesis is that the means are equal. To perform this hypothesis test, we need to shift the two data sets so that they have the same mean and then use bootstrap sampling to compute the difference of means. . # Compute mean of combined data set: combined_mean combined_mean = np.mean(np.concatenate((bd_1975, bd_2012))) # Shift the samples # why shift the mean? # to make np.mean(bd_1975_shifted) - np.mean(bd_2012_shifted) = 0 #1 # why make #1 = 0? # because our hypothesis is &quot;beak depth are the same in 1975 and 2012&quot; bd_1975_shifted = bd_1975 - np.mean(bd_1975) + combined_mean bd_2012_shifted = bd_2012 - np.mean(bd_2012) + combined_mean # Get bootstrap replicates of shifted data sets bs_replicates_1975 = draw_bs_reps(bd_1975_shifted, np.mean, size=10000) bs_replicates_2012 = draw_bs_reps(bd_2012_shifted, np.mean, size=10000) # Compute replicates of difference of means: bs_diff_replicates bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975 # Compute the p-value p = np.sum(bs_diff_replicates &gt;= mean_diff) / len(bs_diff_replicates) # Print p-value print(&#39;p =&#39;, p) # p = 0.0034 # p = 0.34% . We get a p-value of 0.0034, which suggests that there is a statistically significant difference. But remember: it is very important to know how different they are! In the previous exercise, you got a difference of 0.2 mm between the means. You should combine this with the statistical significance. Changing by 0.2 mm in 37 years is substantial by evolutionary standards. If it kept changing at that rate, the beak depth would double in only 400 years. . ### Variation of beak shapes . #### EDA of beak length and depth . The beak length data are stored as bl_1975 and bl_2012 , again with units of millimeters (mm). You still have the beak depth data stored in bd_1975 and bd_2012 . Make scatter plots of beak depth (y-axis) versus beak length (x-axis) for the 1975 and 2012 specimens. . # Make scatter plot of 1975 data _ = plt.plot(bl_1975, bd_1975, marker=&#39;.&#39;, linestyle=&#39;None&#39;, color=&#39;blue&#39;, alpha=0.5) # Make scatter plot of 2012 data _ = plt.plot(bl_2012, bd_2012, marker=&#39;.&#39;, linestyle=&#39;None&#39;, color=&#39;red&#39;, alpha=0.5) # Label axes and make legend _ = plt.xlabel(&#39;beak length (mm)&#39;) _ = plt.ylabel(&#39;beak depth (mm)&#39;) _ = plt.legend((&#39;1975&#39;, &#39;2012&#39;), loc=&#39;upper left&#39;) # Show the plot plt.show() . . In looking at the plot, we see that beaks got deeper (the red points are higher up in the y-direction), but not really longer. If anything, they got a bit shorter, since the red dots are to the left of the blue dots. So, it does not look like the beaks kept the same shape; they became shorter and deeper. . #### Linear regressions . Perform a linear regression for both the 1975 and 2012 data. Then, perform pairs bootstrap estimates for the regression parameters. Report 95% confidence intervals on the slope and intercept of the regression line. . def draw_bs_pairs_linreg(x, y, size=1): &quot;&quot;&quot;Perform pairs bootstrap for linear regression.&quot;&quot;&quot; # Set up array of indices to sample from: inds inds = np.arange(len(x)) # Initialize replicates: bs_slope_reps, bs_intercept_reps bs_slope_reps = np.empty(size) bs_intercept_reps = np.empty(size) # Generate replicates for i in range(size): bs_inds = np.random.choice(inds, size=len(inds)) bs_x, bs_y = x[bs_inds], y[bs_inds] bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, deg=1) return bs_slope_reps, bs_intercept_reps # Compute the linear regressions slope_1975, intercept_1975 = np.polyfit(bl_1975, bd_1975, deg=1) slope_2012, intercept_2012 = np.polyfit(bl_2012, bd_2012, deg=1) # Perform pairs bootstrap for the linear regressions bs_slope_reps_1975, bs_intercept_reps_1975 = draw_bs_pairs_linreg(bl_1975, bd_1975, size=1000) bs_slope_reps_2012, bs_intercept_reps_2012 = draw_bs_pairs_linreg(bl_2012, bd_2012, size=1000) # Compute confidence intervals of slopes slope_conf_int_1975 = np.percentile(bs_slope_reps_1975, [2.5, 97.5]) slope_conf_int_2012 = np.percentile(bs_slope_reps_2012, [2.5, 97.5]) intercept_conf_int_1975 = np.percentile(bs_intercept_reps_1975, [2.5, 97.5]) intercept_conf_int_2012 = np.percentile(bs_intercept_reps_2012, [2.5, 97.5]) # Print the results print(&#39;1975: slope =&#39;, slope_1975, &#39;conf int =&#39;, slope_conf_int_1975) print(&#39;1975: intercept =&#39;, intercept_1975, &#39;conf int =&#39;, intercept_conf_int_1975) print(&#39;2012: slope =&#39;, slope_2012, &#39;conf int =&#39;, slope_conf_int_2012) print(&#39;2012: intercept =&#39;, intercept_2012, &#39;conf int =&#39;, intercept_conf_int_2012) # 1975: slope = 0.4652051691605937 conf int = [0.33851226 0.59306491] # 1975: intercept = 2.3908752365842263 conf int = [0.64892945 4.18037063] # 2012: slope = 0.462630358835313 conf int = [0.33137479 0.60695527] # 2012: intercept = 2.977247498236019 conf int = [1.06792753 4.70599387] . It looks like they have the same slope, but different intercepts. . #### Displaying the linear regression results . # Make scatter plot of 1975 data _ = plt.plot(bl_1975, bd_1975, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;, alpha=0.5) # Make scatter plot of 2012 data _ = plt.plot(bl_2012, bd_2012, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;, alpha=0.5) # Label axes and make legend _ = plt.xlabel(&#39;beak length (mm)&#39;) _ = plt.ylabel(&#39;beak depth (mm)&#39;) _ = plt.legend((&#39;1975&#39;, &#39;2012&#39;), loc=&#39;upper left&#39;) # Generate x-values for bootstrap lines: x x = np.array([10, 17]) # Plot the bootstrap lines for i in range(100): plt.plot(x, bs_slope_reps_1975[i] * x + bs_intercept_reps_1975[i], linewidth=0.5, alpha=0.2, color=&#39;blue&#39;) plt.plot(x, bs_slope_reps_2012[i] * x + bs_intercept_reps_2012[i], linewidth=0.5, alpha=0.2, color=&#39;red&#39;) # Draw the plot again plt.show() . . #### Beak length to depth ratio . The linear regressions showed interesting information about the beak geometry. The slope was the same in 1975 and 2012, suggesting that for every millimeter gained in beak length, the birds gained about half a millimeter in depth in both years. However, if we are interested in the shape of the beak, we want to compare the ratio of beak length to beak depth. Let’s make that comparison. . # Compute length-to-depth ratios ratio_1975 = bl_1975 / bd_1975 ratio_2012 = bl_2012 / bd_2012 # Compute means mean_ratio_1975 = np.mean(ratio_1975) mean_ratio_2012 = np.mean(ratio_2012) # Generate bootstrap replicates of the means bs_replicates_1975 = draw_bs_reps(ratio_1975, np.mean, size=10000) bs_replicates_2012 = draw_bs_reps(ratio_2012, np.mean, size=10000) # Compute the 99% confidence intervals conf_int_1975 = np.percentile(bs_replicates_1975, [0.5, 99.5]) conf_int_2012 = np.percentile(bs_replicates_2012, [0.5, 99.5]) # Print the results print(&#39;1975: mean ratio =&#39;, mean_ratio_1975, &#39;conf int =&#39;, conf_int_1975) print(&#39;2012: mean ratio =&#39;, mean_ratio_2012, &#39;conf int =&#39;, conf_int_2012) # 1975: mean ratio = 1.5788823771858533 conf int = [1.55668803 1.60073509] # 2012: mean ratio = 1.4658342276847767 conf int = [1.44363932 1.48729149] . #### How different is the ratio? . In the previous exercise, you computed the mean beak length to depth ratio with 99% confidence intervals for 1975 and for 2012. The results of that calculation are shown graphically in the plot accompanying this problem. In addition to these results, what would you say about the ratio of beak length to depth? . . The mean beak length-to-depth ratio decreased by about 0.1, or 7%, from 1975 to 2012. The 99% confidence intervals are not even close to overlapping, so this is a real change. The beak shape changed. . ### Calculation of heritability . #### EDA of heritability . The array bd_parent_scandens contains the average beak depth (in mm) of two parents of the species G. scandens . The array bd_offspring_scandens contains the average beak depth of the offspring of the respective parents. The arrays bd_parent_fortis and bd_offspring_fortis contain the same information about measurements from G. fortis birds. . Make a scatter plot of the average offspring beak depth (y-axis) versus average parental beak depth (x-axis) for both species. Use the alpha=0.5 keyword argument to help you see overlapping points. . # Make scatter plots _ = plt.plot(bd_parent_fortis, bd_offspring_fortis, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;, alpha=0.5) _ = plt.plot(bd_parent_scandens, bd_offspring_scandens, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;, alpha=0.5) # Label axes _ = plt.xlabel(&#39;parental beak depth (mm)&#39;) _ = plt.ylabel(&#39;offspring beak depth (mm)&#39;) # Add legend _ = plt.legend((&#39;G. fortis&#39;, &#39;G. scandens&#39;), loc=&#39;lower right&#39;) # Show plot plt.show() . . It appears as though there is a stronger correlation in G. fortis than than in G. scandens . This suggests that beak depth is more strongly inherited in G. fortis . We’ll quantify this correlation next. . #### Correlation of offspring and parental data . In an effort to quantify the correlation between offspring and parent beak depths, we would like to compute statistics, such as the Pearson correlation coefficient, between parents and offspring. To get confidence intervals on this, we need to do a pairs bootstrap. . You have already written a function to do pairs bootstrap to get estimates for parameters derived from linear regression. Your task in this exercise is to make a new function with call signature draw_bs_pairs(x, y, func, size=1) that performs pairs bootstrap and computes a single statistic on pairs samples defined. The statistic of interest is computed by calling func(bs_x, bs_y) . In the next exercise, you will use pearson_r for func . . def draw_bs_pairs(x, y, func, size=1): &quot;&quot;&quot;Perform pairs bootstrap for a single statistic.&quot;&quot;&quot; # Set up array of indices to sample from: inds inds = np.arange(len(x)) # Initialize replicates: bs_replicates bs_replicates = np.empty(size) # Generate replicates for i in range(size): bs_inds = np.random.choice(inds, size=len(inds)) bs_x, bs_y = x[bs_inds], y[bs_inds] bs_replicates[i] = func(bs_x, bs_y) return bs_replicates . #### Pearson correlation of offspring and parental data . The Pearson correlation coefficient seems like a useful measure of how strongly the beak depth of parents are inherited by their offspring. Compute the Pearson correlation coefficient between parental and offspring beak depths for G. scandens . Do the same for G. fortis . Then, use the function you wrote in the last exercise to compute a 95% confidence interval using pairs bootstrap. . def pearson_r(x, y): &quot;&quot;&quot;Compute Pearson correlation coefficient between two arrays.&quot;&quot;&quot; # Compute correlation matrix: corr_mat corr_mat = np.corrcoef(x, y) # Return entry [0,1] return corr_mat[0,1] # Compute the Pearson correlation coefficients r_scandens = pearson_r(bd_parent_scandens, bd_offspring_scandens) r_fortis = pearson_r(bd_parent_fortis, bd_offspring_fortis) # Acquire 1000 bootstrap replicates of Pearson r bs_replicates_scandens = draw_bs_pairs(bd_parent_scandens, bd_offspring_scandens, pearson_r, size=1000) bs_replicates_fortis = draw_bs_pairs(bd_parent_fortis, bd_offspring_fortis, pearson_r, size=1000) # Compute 95% confidence intervals conf_int_scandens = np.percentile(bs_replicates_scandens, [2.5, 97.5]) conf_int_fortis = np.percentile(bs_replicates_fortis, [2.5, 97.5]) # Print results print(&#39;G. scandens:&#39;, r_scandens, conf_int_scandens) print(&#39;G. fortis:&#39;, r_fortis, conf_int_fortis) # G. scandens: 0.4117063629401258 [0.26564228 0.54388972] # G. fortis: 0.7283412395518487 [0.6694112 0.77840616] . It is clear from the confidence intervals that beak depth of the offspring of G. fortis parents is more strongly correlated with their offspring than their G. scandens counterparts. . #### Measuring heritability . Remember that the Pearson correlation coefficient is the ratio of the covariance to the geometric mean of the variances of the two data sets. This is a measure of the correlation between parents and offspring, but might not be the best estimate of heritability. If we stop and think, it makes more sense to define heritability as the ratio of the covariance between parent and offspring to the variance of the parents alone . In this exercise, you will estimate the heritability and perform a pairs bootstrap calculation to get the 95% confidence interval. . This exercise highlights a very important point. Statistical inference (and data analysis in general) is not a plug-n-chug enterprise. You need to think carefully about the questions you are seeking to answer with your data and analyze them appropriately. If you are interested in how heritable traits are, the quantity we defined as the heritability is more apt than the off-the-shelf statistic, the Pearson correlation coefficient. . def heritability(parents, offspring): &quot;&quot;&quot;Compute the heritability from parent and offspring samples.&quot;&quot;&quot; covariance_matrix = np.cov(parents, offspring) return covariance_matrix[0,1] / covariance_matrix[0,0] # Compute the heritability heritability_scandens = heritability(bd_parent_scandens, bd_offspring_scandens) heritability_fortis = heritability(bd_parent_fortis, bd_offspring_fortis) # Acquire 1000 bootstrap replicates of heritability replicates_scandens = draw_bs_pairs( bd_parent_scandens, bd_offspring_scandens, heritability, size=1000) replicates_fortis = draw_bs_pairs( bd_parent_fortis, bd_offspring_fortis, heritability, size=1000) # Compute 95% confidence intervals conf_int_scandens = np.percentile(replicates_scandens, [2.5, 97.5]) conf_int_fortis = np.percentile(replicates_fortis, [2.5, 97.5]) # Print results print(&#39;G. scandens:&#39;, heritability_scandens, conf_int_scandens) print(&#39;G. fortis:&#39;, heritability_fortis, conf_int_fortis) # G. scandens: 0.5485340868685982 [0.34395487 0.75638267] # G. fortis: 0.7229051911438159 [0.64655013 0.79688342] . Here again, we see that G. fortis has stronger heritability than G. scandens . This suggests that the traits of G. fortis may be strongly incorporated into G. scandens by introgressive hybridization. . #### Is beak depth heritable at all in G. scandens? . The heritability of beak depth in G. scandens seems low. It could be that this observed heritability was just achieved by chance and beak depth is actually not really heritable in the species . You will test that hypothesis here. To do this, you will do a pairs permutation test. . # Initialize array of replicates: perm_replicates perm_replicates = np.empty(10000) # Draw replicates for i in range(10000): # Permute parent beak depths bd_parent_permuted = np.random.permutation(bd_parent_scandens) perm_replicates[i] = heritability(bd_parent_permuted, bd_offspring_scandens) # Compute p-value: p p = np.sum(perm_replicates &gt;= heritability_scandens) / len(perm_replicates) # Print the p-value print(&#39;p-val =&#39;, p) # p-val = 0.0 . You get a p-value of zero, which means that none of the 10,000 permutation pairs replicates you drew had a heritability high enough to match that which was observed. This strongly suggests that beak depth is heritable in G. scandens , just not as much as in G. fortis . If you like, you can plot a histogram of the heritability replicates to get a feel for how extreme of a value of heritability you might expect by chance. . plt.hist(perm_replicates) plt.axvline(x=heritability_scandens, color = &#39;red&#39;) plt.text(heritability_scandens, 1500, &#39;heritability_scandens&#39;, ha=&#39;center&#39;, va=&#39;center&#39;,rotation=&#39;vertical&#39;, backgroundcolor=&#39;white&#39;) plt.show() . Parameter estimation by optimization . ### Optimal parameters . #### How often do we get no-hitters? . The number of games played between each no-hitter in the modern era (1901-2015) of Major League Baseball is stored in the array nohitter_times . . If you assume that no-hitters are described as a Poisson process, then the time between no-hitters is Exponentially distributed. As you have seen, the Exponential distribution has a single parameter, which we will call ττ, the typical interval time. The value of the parameter ττ that makes the exponential distribution best match the data is the mean interval time (where time is in units of number of games) between no-hitters. . Compute the value of this parameter from the data. Then, use np.random.exponential() to “repeat” the history of Major League Baseball by drawing inter-no-hitter times from an exponential distribution with the ττ you found and plot the histogram as an approximation to the PDF. . # Seed random number generator np.random.seed(42) # Compute mean no-hitter time: tau tau = np.mean(nohitter_times) # Draw out of an exponential distribution with parameter tau: inter_nohitter_time inter_nohitter_time = np.random.exponential(tau, 100000) # Plot the PDF and label axes _ = plt.hist(inter_nohitter_time, bins=50, normed=True, histtype=&#39;step&#39;) _ = plt.xlabel(&#39;Games between no-hitters&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() . . We see the typical shape of the Exponential distribution, going from a maximum at 0 and decaying to the right. . #### Do the data follow our story? . # Create an ECDF from real data: x, y x, y = ecdf(nohitter_times) # Create a CDF from theoretical samples: x_theor, y_theor x_theor, y_theor = ecdf(inter_nohitter_time) # Overlay the plots plt.plot(x_theor, y_theor) plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Margins and axis labels plt.margins(0.02) plt.xlabel(&#39;Games between no-hitters&#39;) plt.ylabel(&#39;CDF&#39;) # Show the plot plt.show() . . It looks like no-hitters in the modern era of Major League Baseball are Exponentially distributed. Based on the story of the Exponential distribution, this suggests that they are a random process; when a no-hitter will happen is independent of when the last no-hitter was. . #### How is this parameter optimal? . # Plot the theoretical CDFs plt.plot(x_theor, y_theor) plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;) plt.margins(0.02) plt.xlabel(&#39;Games between no-hitters&#39;) plt.ylabel(&#39;CDF&#39;) # Take samples with half tau: samples_half samples_half = np.random.exponential(tau/2, size=10000) # Take samples with double tau: samples_double samples_double = np.random.exponential(2*tau, size=10000) # Generate CDFs from these samples x_half, y_half = ecdf(samples_half) x_double, y_double = ecdf(samples_double) # Plot these CDFs as lines _ = plt.plot(x_half, y_half) _ = plt.plot(x_double, y_double) # Show the plot plt.show() . . red = half, purple = double . Notice how the value of tau given by the mean matches the data best. In this way, tau is an optimal parameter. . ### Linear regression by least squares . #### EDA of literacy/fertility data . # Plot the illiteracy rate versus fertility _ = plt.plot(illiteracy, fertility, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Set the margins and label axes plt.margins(0.02) _ = plt.xlabel(&#39;percent illiterate&#39;) _ = plt.ylabel(&#39;fertility&#39;) # Show the plot plt.show() # Show the Pearson correlation coefficient print(pearson_r(illiteracy, fertility)) 0.8041324026815344 . . You can see the correlation between illiteracy and fertility by eye, and by the substantial Pearson correlation coefficient of 0.8. It is difficult to resolve in the scatter plot, but there are many points around near-zero illiteracy and about 1.8 children/woman. . #### Linear regression . # Plot the illiteracy rate versus fertility _ = plt.plot(illiteracy, fertility, marker=&#39;.&#39;, linestyle=&#39;none&#39;) plt.margins(0.02) _ = plt.xlabel(&#39;percent illiterate&#39;) _ = plt.ylabel(&#39;fertility&#39;) # Perform a linear regression using np.polyfit(): a, b a, b = np.polyfit(illiteracy, fertility, deg=1) # Print the results to the screen print(&#39;slope =&#39;, a, &#39;children per woman / percent illiterate&#39;) print(&#39;intercept =&#39;, b, &#39;children per woman&#39;) # Make theoretical line to plot x = np.array([0, 100]) y = a * x + b # Add regression line to your plot _ = plt.plot(x, y) # Draw the plot plt.show() # slope = 0.04979854809063423 children per woman / percent illiterate # intercept = 1.888050610636557 children per woman . . #### How is it optimal? . # Specify slopes to consider: a_vals a_vals = np.linspace(0, 0.1, 200) # Initialize sum of square of residuals: rss rss = np.empty_like(a_vals) # Compute sum of square of residuals for each value of a_vals for i, a in enumerate(a_vals): rss[i] = np.sum((fertility - a*illiteracy - b)**2) # Plot the RSS plt.plot(a_vals, rss, &#39;-&#39;) plt.xlabel(&#39;slope (children per woman / percent illiterate)&#39;) plt.ylabel(&#39;sum of square of residuals&#39;) plt.show() . . Notice that the minimum on the plot, that is the value of the slope that gives the minimum sum of the square of the residuals, is the same value you got when performing the regression. . ### The importance of EDA: Anscombe’s quartet . . #### The importance of EDA . Why should exploratory data analysis be the first step in an analysis of data (after getting your data imported and cleaned, of course)? . You can be protected from misinterpretation of the type demonstrated by Anscombe’s quartet. | EDA provides a good starting point for planning the rest of your analysis. | EDA is not really any more difficult than any of the subsequent analysis, so there is no excuse for not exploring the data. | . #### Linear regression on appropriate Anscombe data . # Perform linear regression: a, b a, b = np.polyfit(x, y, deg=1) # Print the slope and intercept print(a, b) # Generate theoretical x and y data: x_theor, y_theor x_theor = np.array([3, 15]) y_theor = a * x_theor + b # Plot the Anscombe data and theoretical line _ = plt.plot(x, y, marker = &#39;.&#39;, linestyle = &#39;none&#39;) _ = plt.plot(x_theor, y_theor) # Label the axes plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;) # Show the plot plt.show() . . #### Linear regression on all Anscombe data . for i in range(4): plt.subplot(2,2,i+1) # plot the scatter plot plt.plot(anscombe_x[i], anscombe_y[i], marker = &#39;.&#39;, linestyle = &#39;none&#39;) # plot the regression line a, b = np.polyfit(anscombe_x[i], anscombe_y[i], deg=1) x_theor = np.array([np.min(anscombe_x[i]), np.max(anscombe_x[i])]) y_theor = a * x_theor + b plt.plot(x_theor, y_theor) # add label plt.xlabel(&#39;x&#39; + str(i+1)) plt.ylabel(&#39;y&#39; + str(i+1)) plt.show() # slope1: 0.5000909090909095 intercept: 3.000090909090909 # slope2: 0.5000000000000004 intercept: 3.0009090909090896 # slope3: 0.4997272727272731 intercept: 3.0024545454545453 # slope4: 0.4999090909090908 intercept: 3.0017272727272735 . . ### Generating bootstrap replicates . #### Getting the terminology down . If we have a data set with n repeated measurements, a bootstrap sample is an array of length n that was drawn from the original data with replacement. . Bootstrap replicate is a single value of a statistic computed from a bootstrap sample. . #### Visualizing bootstrap samples . np.random.choice() . for _ in range(50): # Generate bootstrap sample: bs_sample bs_sample = np.random.choice(rainfall, size=len(rainfall)) # Compute and plot ECDF from bootstrap sample x, y = ecdf(bs_sample) _ = plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;gray&#39;, alpha=0.1) # Compute and plot ECDF from original data x, y = ecdf(rainfall) _ = plt.plot(x, y, marker=&#39;.&#39;) # Make margins and label axes plt.margins(0.02) _ = plt.xlabel(&#39;yearly rainfall (mm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) # Show the plot plt.show() . . Bootstrap confidence intervals . #### Generating many bootstrap replicates . def bootstrap_replicate_1d(data, func): return func(np.random.choice(data, size=len(data))) def draw_bs_reps(data, func, size=1): &quot;&quot;&quot;Draw bootstrap replicates.&quot;&quot;&quot; # Initialize array of replicates: bs_replicates bs_replicates = np.empty(size) # Generate replicates for i in range(size): bs_replicates[i] = bootstrap_replicate_1d(data, func) return bs_replicates . #### Bootstrap replicates of the mean and the SEM ( standard error of the mean ) . In fact, it can be shown theoretically that under not-too-restrictive conditions, the value of the mean will always be Normally distributed. (This does not hold in general, just for the mean and a few other statistics.) . The standard deviation of this distribution, called the standard error of the mean , or SEM, is given by the standard deviation of the data divided by the square root of the number of data points. I.e., for a data set, sem = np.std(data) / np.sqrt(len(data)) . Using hacker statistics, you get this same result without the need to derive it, but you will verify this result from your bootstrap replicates. . # Take 10,000 bootstrap replicates of the mean: bs_replicates bs_replicates = draw_bs_reps(rainfall, np.mean, size=10000) # Compute and print SEM sem = np.std(rainfall) / np.sqrt(len(rainfall)) print(sem) # Compute and print standard deviation of bootstrap replicates bs_std = np.std(bs_replicates) print(bs_std) # Make a histogram of the results _ = plt.hist(bs_replicates, bins=50, normed=True) _ = plt.xlabel(&#39;mean annual rainfall (mm)&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() # 10.51054915050619 # 10.465927071184412 . . #### Confidence intervals of rainfall data . Use the bootstrap replicates you just generated to compute the 95% confidence interval. That is, give the 2.5th and 97.5th percentile of your bootstrap replicates stored as bs_replicates . What is the 95% confidence interval? . np.percentile(bs_replicates,2.5) 779.7699248120301 np.percentile(bs_replicates,97.5) 820.950432330827 . #### Bootstrap replicates of other statistics . def draw_bs_reps(data, func, size=1): return np.array([bootstrap_replicate_1d(data, func) for _ in range(size)]) # Generate 10,000 bootstrap replicates of the variance: bs_replicates bs_replicates = draw_bs_reps(rainfall, np.var, size=10000) # Put the variance in units of square centimeters bs_replicates /= 100 # Make a histogram of the results _ = plt.hist(bs_replicates, bins=50, normed=True) _ = plt.xlabel(&#39;variance of annual rainfall (sq. cm)&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() . . This is not normally distributed, as it has a longer tail to the right. Note that you can also compute a confidence interval on the variance, or any other statistic, using np.percentile() with your bootstrap replicates. . #### Confidence interval on the rate of no-hitters . # Draw bootstrap replicates of the mean no-hitter time (equal to tau): bs_replicates bs_replicates = draw_bs_reps(nohitter_times, np.mean, size=10000) # Compute the 95% confidence interval: conf_int conf_int = np.percentile(bs_replicates, [2.5, 97.5]) # Print the confidence interval print(&#39;95% confidence interval =&#39;, conf_int, &#39;games&#39;) # Plot the histogram of the replicates _ = plt.hist(bs_replicates, bins=50, normed=True) _ = plt.xlabel(r&#39;$ tau$ (games)&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() # 95% confidence interval = [660.67280876 871.63077689] games . . This gives you an estimate of what the typical time between no-hitters is. It could be anywhere between 660 and 870 games. . ### Pairs bootstrap . #### A function to do pairs bootstrap . def draw_bs_pairs_linreg(x, y, size=1): &quot;&quot;&quot;Perform pairs bootstrap for linear regression.&quot;&quot;&quot; # Set up array of indices to sample from: inds inds = np.arange(len(x)) # Initialize replicates: bs_slope_reps, bs_intercept_reps bs_slope_reps = np.empty(size) bs_intercept_reps = np.empty(size) # Generate replicates for i in range(size): bs_inds = np.random.choice(inds, size=len(inds)) bs_x, bs_y = x[bs_inds], y[bs_inds] bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, deg=1) return bs_slope_reps, bs_intercept_reps . #### Pairs bootstrap of literacy/fertility data . # Generate replicates of slope and intercept using pairs bootstrap bs_slope_reps, bs_intercept_reps = draw_bs_pairs_linreg(illiteracy, fertility, size=1000) # Compute and print 95% CI for slope print(np.percentile(bs_slope_reps, [2.5, 97.5])) # Plot the histogram _ = plt.hist(bs_slope_reps, bins=50, normed=True) _ = plt.xlabel(&#39;slope&#39;) _ = plt.ylabel(&#39;PDF&#39;) plt.show() # [0.04378061 0.0551616 ] . . #### Plotting bootstrap regressions . # Generate array of x-values for bootstrap lines: x x = np.array([0, 100]) # Plot the bootstrap lines for i in range(100): _ = plt.plot(x, bs_slope_reps[i]*x + bs_intercept_reps[i], linewidth=0.5, alpha=0.2, color=&#39;red&#39;) # Plot the data _ = plt.plot(illiteracy, fertility, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Label axes, set the margins, and show the plot _ = plt.xlabel(&#39;illiteracy&#39;) _ = plt.ylabel(&#39;fertility&#39;) plt.margins(0.02) plt.show() . . Introduction to hypothesis testing . ### Formulating and simulating a hypothesis . Null hypothesis another name for the hypothesis you are testing Permutation random reordering of entries in an array #### Generating a permutation sample . np.random.permutation() . Permutation sampling is a great way to simulate the hypothesis that two variables have identical probability distributions. This is often a hypothesis you want to test, so in this exercise, you will write a function to generate a permutation sample from two data sets. . def permutation_sample(data1, data2): &quot;&quot;&quot;Generate a permutation sample from two data sets.&quot;&quot;&quot; # Concatenate the data sets: data data = np.concatenate((data1, data2)) # Permute the concatenated array: permuted_data permuted_data = np.random.permutation(data) # Split the permuted array into two: perm_sample_1, perm_sample_2 perm_sample_1 = permuted_data[:len(data1)] perm_sample_2 = permuted_data[len(data1):] return perm_sample_1, perm_sample_2 . #### Visualizing permutation sampling . for _ in range(50): # Generate permutation samples perm_sample_1, perm_sample_2 = permutation_sample(rain_june, rain_november) # Compute ECDFs x_1, y_1 = ecdf(perm_sample_1) x_2, y_2 = ecdf(perm_sample_2) # Plot ECDFs of permutation sample _ = plt.plot(x_1, y_1, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;, alpha=0.02) _ = plt.plot(x_2, y_2, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;, alpha=0.02) # Create and plot ECDFs from original data x_1, y_1 = ecdf(rain_june) x_2, y_2 = ecdf(rain_november) _ = plt.plot(x_1, y_1, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;) _ = plt.plot(x_2, y_2, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;) # Label axes, set margin, and show plot plt.margins(0.02) _ = plt.xlabel(&#39;monthly rainfall (mm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) plt.show() . . Notice that the permutation samples ECDFs overlap and give a purple haze. None of the ECDFs from the permutation samples overlap with the observed data, suggesting that the hypothesis is not commensurate with the data. June and November rainfall are not identically distributed. . ### Test statistics and p-values . #### Test statistics . When performing hypothesis tests, your choice of test statistic should be pertinent to the question you are seeking to answer in your hypothesis test. . The most important thing to consider is: What are you asking? . #### What is a p-value? . The p-value is generally a measure of the probability of observing a test statistic equally or more extreme than the one you observed, given that the null hypothesis is true. . #### Generating permutation replicates . # In most circumstances, func will be a function you write yourself. def draw_perm_reps(data_1, data_2, func, size=1): &quot;&quot;&quot;Generate multiple permutation replicates.&quot;&quot;&quot; # Initialize array of replicates: perm_replicates perm_replicates = np.empty(size) for i in range(size): # Generate permutation sample perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2) # Compute the test statistic perm_replicates[i] = func(perm_sample_1, perm_sample_2) return perm_replicates . #### Look before you leap: EDA before hypothesis testing . Kleinteich and Gorb ( Sci. Rep. , 4 , 5225, 2014) performed an interesting experiment with South American horned frogs. They held a plate connected to a force transducer, along with a bait fly, in front of them. They then measured the impact force and adhesive force of the frog’s tongue when it struck the target. . Frog A is an adult and Frog B is a juvenile. The researchers measured the impact force of 20 strikes for each frog. In the next exercise, we will test the hypothesis that the two frogs have the same distribution of impact forces. But, remember, it is important to do EDA first! Let’s make a bee swarm plot for the data. They are stored in a Pandas data frame, df , where column ID is the identity of the frog and column impact_force is the impact force in Newtons (N). . df.head() ID impact_force 20 A 1.612 21 A 0.605 22 A 0.327 23 A 0.946 24 A 0.541 . # Make bee swarm plot _ = sns.swarmplot(x=&#39;ID&#39;, y=&#39;impact_force&#39;, data=df) # Label axes _ = plt.xlabel(&#39;frog&#39;) _ = plt.ylabel(&#39;impact force (N)&#39;) # Show the plot plt.show() . . Eyeballing it, it does not look like they come from the same distribution. Frog A, the adult, has three or four very hard strikes, and Frog B, the juvenile, has a couple weak ones. However, it is possible that with only 20 samples it might be too difficult to tell if they have difference distributions, so we should proceed with the hypothesis test. . #### Permutation test on frog data . The average strike force of Frog A was 0.71 Newtons (N), and that of Frog B was 0.42 N for a difference of 0.29 N. It is possible the frogs strike with the same force and this observed difference was by chance. You will compute the probability of getting at least a 0.29 N difference in mean strike force under the hypothesis that the distributions of strike forces for the two frogs are identical. We use a permutation test with a test statistic of the difference of means to test this hypothesis. . def diff_of_means(data_1, data_2): &quot;&quot;&quot;Difference in means of two arrays.&quot;&quot;&quot; # The difference of means of data_1, data_2: diff diff = np.mean(data_1) - np.mean(data_2) return diff # Compute difference of mean impact force from experiment: empirical_diff_means empirical_diff_means = diff_of_means(force_a, force_b) # Draw 10,000 permutation replicates: perm_replicates perm_replicates = draw_perm_reps(force_a, force_b, diff_of_means, size=10000) # Compute p-value: p p = np.sum(perm_replicates &gt;= empirical_diff_means) / len(perm_replicates) # Print the result print(&#39;p-value =&#39;, p) # p-value = 0.0063 # p-value = 0.63% . The p-value tells you that there is about a 0.6% chance that you would get the difference of means observed in the experiment if frogs were exactly the same. . A p-value below 0.01 is typically said to be “statistically significant,” but: warning! warning! warning! You have computed a p-value; it is a number. I encourage you not to distill it to a yes-or-no phrase. p = 0.006 and p = 0.000000006 are both said to be “statistically significant,” but they are definitely not the same! . ### Bootstrap hypothesis tests . . #### A one-sample bootstrap hypothesis test . Another juvenile frog was studied, Frog C, and you want to see if Frog B and Frog C have similar impact forces. Unfortunately, you do not have Frog C’s impact forces available, but you know they have a mean of 0.55 N. Because you don’t have the original data, you cannot do a permutation test, and you cannot assess the hypothesis that the forces from Frog B and Frog C come from the same distribution. You will therefore test another, less restrictive hypothesis: The mean strike force of Frog B is equal to that of Frog C. . To set up the bootstrap hypothesis test, you will take the mean as our test statistic. Remember, your goal is to calculate the probability of getting a mean impact force less than or equal to what was observed for Frog B if the hypothesis that the true mean of Frog B’s impact forces is equal to that of Frog C is true . You first translate all of the data of Frog B such that the mean is 0.55 N. This involves adding the mean force of Frog C and subtracting the mean force of Frog B from each measurement of Frog B. This leaves other properties of Frog B’s distribution, such as the variance, unchanged. . # Make an array of translated impact forces: translated_force_b translated_force_b = force_b - np.mean(force_b) + 0.55 # Take bootstrap replicates of Frog B&#39;s translated impact forces: bs_replicates bs_replicates = draw_bs_reps(translated_force_b, np.mean, 10000) # Compute fraction of replicates that are less than the observed Frog B force: p p = np.sum(bs_replicates &lt;= np.mean(force_b)) / 10000 # Print the p-value print(&#39;p = &#39;, p) # p = 0.0046 # p = 0.46% . The low p-value suggests that the null hypothesis that Frog B and Frog C have the same mean impact force is false. . #### A two-sample bootstrap hypothesis test for difference of means . We now want to test the hypothesis that Frog A and Frog B have the same mean impact force, but not necessarily the same distribution, which is also impossible with a permutation test. . To do the two-sample bootstrap test, we shift both arrays to have the same mean, since we are simulating the hypothesis that their means are, in fact, equal. We then draw bootstrap samples out of the shifted arrays and compute the difference in means. This constitutes a bootstrap replicate, and we generate many of them. The p-value is the fraction of replicates with a difference in means greater than or equal to what was observed. . # Compute mean of all forces: mean_force mean_force = np.mean(forces_concat) # Generate shifted arrays force_a_shifted = force_a - np.mean(force_a) + mean_force force_b_shifted = force_b - np.mean(force_b) + mean_force # Compute 10,000 bootstrap replicates from shifted arrays bs_replicates_a = draw_bs_reps(force_a_shifted, np.mean, size=10000) bs_replicates_b = draw_bs_reps(force_b_shifted, np.mean, size=10000) # Get replicates of difference of means: bs_replicates bs_replicates = bs_replicates_a - bs_replicates_b # Compute and print p-value: p p = np.sum(bs_replicates &gt;= empirical_diff_means) / 10000 print(&#39;p-value =&#39;, p) # p-value = 0.0043 # p-value = 0.43% . You got a similar result as when you did the permutation test. Nonetheless, remember that it is important to carefully think about what question you want to ask. Are you only interested in the mean impact force, or in the distribution of impact forces? . Hypothesis test examples . ### A/B testing . #### The vote for the Civil Rights Act in 1964 . The Civil Rights Act of 1964 was one of the most important pieces of legislation ever passed in the USA. Excluding “present” and “abstain” votes, 153 House Democrats and 136 Republicans voted yea. However, 91 Democrats and 35 Republicans voted nay. Did party affiliation make a difference in the vote? . To answer this question, you will evaluate the hypothesis that the party of a House member has no bearing on his or her vote. You will use the fraction of Democrats voting in favor as your test statistic and evaluate the probability of observing a fraction of Democrats voting in favor at least as small as the observed fraction of 153/244. (That’s right, at least as small as. In 1964, it was the Democrats who were less progressive on civil rights issues.) To do this, permute the party labels of the House voters and then arbitrarily divide them into “Democrats” and “Republicans” and compute the fraction of Democrats voting yea. . # Construct arrays of data: dems, reps dems = np.array([True] * 153 + [False] * 91) reps = np.array([True] * 136 + [False] * 35) def frac_yea_dems(dems, reps): &quot;&quot;&quot;Compute fraction of Democrat yea votes.&quot;&quot;&quot; frac = np.sum(dems) / len(dems) return frac # Acquire permutation samples: perm_replicates perm_replicates = draw_perm_reps(dems, reps, frac_yea_dems, size=10000) # Compute and print p-value: p p = np.sum(perm_replicates &lt;= 153/244) / len(perm_replicates) print(&#39;p-value =&#39;, p) # p-value = 0.0002 # p-value = 0.02% . This small p-value suggests that party identity had a lot to do with the voting. Importantly, the South had a higher fraction of Democrat representatives, and consequently also a more racist bias. . #### What is equivalent? . You have experience matching a stories to probability distributions. Similarly, you use the same procedure for two different A/B tests if their stories match. In the Civil Rights Act example you just did, you performed an A/B test on voting data, which has a Yes/No type of outcome for each subject (in that case, a voter). Which of the following situations involving testing by a web-based company has an equivalent set up for an A/B test as the one you just did with the Civil Rights Act of 1964? . You measure the number of people who click on an ad on your company’s website before and after changing its color. . The “Democrats” are those who view the ad before the color change, and the “Republicans” are those who view it after. . #### A time-on-website analog . It turns out that you already did a hypothesis test analogous to an A/B test where you are interested in how much time is spent on the website before and after an ad campaign. The frog tongue force (a continuous quantity like time on the website) is an analog. “Before” = Frog A and “after” = Frog B. Let’s practice this again with something that actually is a before/after scenario. . We return to the no-hitter data set. In 1920, Major League Baseball implemented important rule changes that ended the so-called dead ball era. Importantly, the pitcher was no longer allowed to spit on or scuff the ball, an activity that greatly favors pitchers. In this problem you will perform an A/B test to determine if these rule changes resulted in a slower rate of no-hitters (i.e., longer average time between no-hitters) using the difference in mean inter-no-hitter time as your test statistic. The inter-no-hitter times for the respective eras are stored in the arrays nht_dead and nht_live , where “nht” is meant to stand for “no-hitter time.” . def diff_of_means(data_1, data_2): &quot;&quot;&quot;Difference in means of two arrays.&quot;&quot;&quot; # The difference of means of data_1, data_2: diff diff = np.mean(data_1) - np.mean(data_2) return diff def permutation_sample(data1, data2): &quot;&quot;&quot;Generate a permutation sample from two data sets.&quot;&quot;&quot; # Concatenate the data sets: data data = np.concatenate((data1, data2)) # Permute the concatenated array: permuted_data permuted_data = np.random.permutation(data) # Split the permuted array into two: perm_sample_1, perm_sample_2 perm_sample_1 = permuted_data[:len(data1)] perm_sample_2 = permuted_data[len(data1):] return perm_sample_1, perm_sample_2 def draw_perm_reps(data_1, data_2, func, size=1): &quot;&quot;&quot;Generate multiple permutation replicates.&quot;&quot;&quot; # Initialize array of replicates: perm_replicates perm_replicates = np.empty(size) for i in range(size): # Generate permutation sample perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2) # Compute the test statistic perm_replicates[i] = func(perm_sample_1, perm_sample_2) return perm_replicates # Compute the observed difference in mean inter-no-hitter times: nht_diff_obs nht_diff_obs = diff_of_means(nht_dead, nht_live) # Acquire 10,000 permutation replicates of difference in mean no-hitter time: perm_replicates perm_replicates = draw_perm_reps(nht_dead, nht_live, diff_of_means, size=10000) # Compute and print the p-value: p p = np.sum(perm_replicates &lt;= nht_diff_obs) /len(perm_replicates) print(&#39;p-val =&#39;, p) p-val = 0.0001 . Your p-value is 0.0001, which means that only one out of your 10,000 replicates had a result as extreme as the actual difference between the dead ball and live ball eras. This suggests strong statistical significance. Watch out, though, you could very well have gotten zero replicates that were as extreme as the observed value. This just means that the p-value is quite small, almost certainly smaller than 0.001. . #### What should you have done first? . That was a nice hypothesis test you just did to check out whether the rule changes in 1920 changed the rate of no-hitters. But what should you have done with the data first? . Performed EDA, perhaps plotting the ECDFs of inter-no-hitter times in the dead ball and live ball eras. . Always a good idea to do first! I encourage you to go ahead and plot the ECDFs right now. You will see by eye that the null hypothesis that the distributions are the same is almost certainly not true. . # Create and plot ECDFs x_1, y_1 = ecdf(nht_dead) x_2, y_2 = ecdf(nht_live) _ = plt.plot(x_1, y_1, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;) _ = plt.plot(x_2, y_2, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;) # Label axes, set margin, and show plot plt.margins(0.02) _ = plt.xlabel(&#39;Games between no-hitters&#39;) _ = plt.ylabel(&#39;ECDF&#39;) plt.show() . . ### Test of correlation . . #### Simulating a null hypothesis concerning correlation . The observed correlation between female illiteracy and fertility in the data set of 162 countries may just be by chance; the fertility of a given country may actually be totally independent of its illiteracy. You will test this null hypothesis in the next exercise. . To do the test, you need to simulate the data assuming the null hypothesis is true. Of the following choices, which is the best way to to do it? . Answer: Do a permutation test: Permute the illiteracy values but leave the fertility values fixed to generate a new set of (illiteracy, fertility) data. . This exactly simulates the null hypothesis and does so more efficiently than the last option. It is exact because it uses all data and eliminates any correlation because which illiteracy value pairs to which fertility value is shuffled. . Last option: Do a permutation test: Permute both the illiteracy and fertility values to generate a new set of (illiteracy, fertility data). This exactly simulates the null hypothesis and does so more efficiently than the last option. It is exact because it uses all data and eliminates any correlation because which illiteracy value pairs to which fertility value is shuffled. . #### Hypothesis test on Pearson correlation . The observed correlation between female illiteracy and fertility may just be by chance; the fertility of a given country may actually be totally independent of its illiteracy. You will test this hypothesis. To do so, permute the illiteracy values but leave the fertility values fixed. This simulates the hypothesis that they are totally independent of each other. For each permutation, compute the Pearson correlation coefficient and assess how many of your permutation replicates have a Pearson correlation coefficient greater than the observed one. . def pearson_r(x, y): &quot;&quot;&quot;Compute Pearson correlation coefficient between two arrays.&quot;&quot;&quot; # Compute correlation matrix: corr_mat corr_mat = np.corrcoef(x, y) # Return entry [0,1] return corr_mat[0,1] # Compute observed correlation: r_obs r_obs = pearson_r(illiteracy, fertility) # r_obs = 0.8041324026815344 # Initialize permutation replicates: perm_replicates perm_replicates = np.empty(10000) # Draw replicates for i in range(10000): # Permute illiteracy measurments: illiteracy_permuted illiteracy_permuted = np.random.permutation(illiteracy) # Compute Pearson correlation perm_replicates[i] = pearson_r(illiteracy_permuted, fertility) # Compute p-value: p p = np.sum(perm_replicates &gt;= r_obs) /len(perm_replicates) print(&#39;p-val =&#39;, p) # p-val = 0.0 . You got a p-value of zero. In hacker statistics, this means that your p-value is very low, since you never got a single replicate in the 10,000 you took that had a Pearson correlation greater than the observed one. You could try increasing the number of replicates you take to continue to move the upper bound on your p-value lower and lower. . #### Do neonicotinoid insecticides have unintended consequences? . As a final exercise in hypothesis testing before we put everything together in our case study in the next chapter, you will investigate the effects of neonicotinoid insecticides on bee reproduction. These insecticides are very widely used in the United States to combat aphids and other pests that damage plants. . In a recent study, Straub, et al. ( Proc. Roy. Soc. B , 2016 ) investigated the effects of neonicotinoids on the sperm of pollinating bees. In this and the next exercise, you will study how the pesticide treatment affected the count of live sperm per half milliliter of semen. . First, we will do EDA, as usual. Plot ECDFs of the alive sperm count for untreated bees (stored in the Numpy array control ) and bees treated with pesticide (stored in the Numpy array treated ). . # Compute x,y values for ECDFs x_control, y_control = ecdf(control) x_treated, y_treated = ecdf(treated) # Plot the ECDFs plt.plot(x_control, y_control, marker=&#39;.&#39;, linestyle=&#39;none&#39;) plt.plot(x_treated, y_treated, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Set the margins plt.margins(0.02) # Add a legend plt.legend((&#39;control&#39;, &#39;treated&#39;), loc=&#39;lower right&#39;) # Label axes and show plot plt.xlabel(&#39;millions of alive sperm per mL&#39;) plt.ylabel(&#39;ECDF&#39;) plt.show() . . The ECDFs show a pretty clear difference between the treatment and control; treated bees have fewer alive sperm. Let’s now do a hypothesis test in the next exercise. . #### Bootstrap hypothesis test on bee sperm counts . Now, you will test the following hypothesis: . On average, male bees treated with neonicotinoid insecticide have the same number of active sperm per milliliter of semen than do untreated male bees. . You will use the difference of means as your test statistic. . def bootstrap_replicate_1d(data, func): return func(np.random.choice(data, size=len(data))) def draw_bs_reps(data, func, size=1): &quot;&quot;&quot;Draw bootstrap replicates.&quot;&quot;&quot; # Initialize array of replicates: bs_replicates bs_replicates = np.empty(size) # Generate replicates for i in range(size): bs_replicates[i] = bootstrap_replicate_1d(data, func) return bs_replicates # Compute the difference in mean sperm count: diff_means diff_means = np.mean(control) - np.mean(treated) # Compute mean of pooled data: mean_count mean_count = np.mean(np.concatenate((control, treated))) # Generate shifted data sets control_shifted = control - np.mean(control) + mean_count treated_shifted = treated - np.mean(treated) + mean_count # Generate bootstrap replicates bs_reps_control = draw_bs_reps(control_shifted, np.mean, size=10000) bs_reps_treated = draw_bs_reps(treated_shifted, np.mean, size=10000) # Get replicates of difference of means: bs_replicates bs_replicates = bs_reps_control - bs_reps_treated # Compute and print p-value: p p = np.sum(bs_replicates &gt;= np.mean(control) - np.mean(treated)) / len(bs_replicates) print(&#39;p-value =&#39;, p) # p-value = 0.0 . The p-value is small, most likely less than 0.0001, since you never saw a bootstrap replicated with a difference of means at least as extreme as what was observed. In fact, when I did the calculation with 10 million replicates, I got a p-value of 2e-05 . Putting it all together: a case study . ### Finch beaks and the need for statistics . . . #### EDA of beak depths of Darwin’s finches . For your first foray into the Darwin finch data, you will study how the beak depth (the distance, top to bottom, of a closed beak) of the finch species Geospiza scandens has changed over time. The Grants have noticed some changes of beak geometry depending on the types of seeds available on the island, and they also noticed that there was some interbreeding with another major species on Daphne Major, Geospiza fortis . These effects can lead to changes in the species over time. . In the next few problems, you will look at the beak depth of G. scandens on Daphne Major in 1975 and in 2012. To start with, let’s plot all of the beak depth measurements in 1975 and 2012 in a bee swarm plot. . The data are stored in a pandas DataFrame called df with columns &#39;year&#39; and &#39;beak_depth&#39; . The units of beak depth are millimeters (mm). . df.head() beak_depth year 0 8.4 1975 1 8.8 1975 2 8.4 1975 3 8.0 1975 4 7.9 1975 . # Create bee swarm plot _ = sns.swarmplot(&#39;year&#39;, &#39;beak_depth&#39;, data=df) # Label the axes _ = plt.xlabel(&#39;year&#39;) _ = plt.ylabel(&#39;beak depth (mm)&#39;) # Show the plot plt.show() . . It is kind of hard to see if there is a clear difference between the 1975 and 2012 data set. Eyeballing it, it appears as though the mean of the 2012 data set might be slightly higher, and it might have a bigger variance. . #### ECDFs of beak depths . While bee swarm plots are useful, we found that ECDFs are often even better when doing EDA. Plot the ECDFs for the 1975 and 2012 beak depth measurements on the same plot. . # Compute ECDFs x_1975, y_1975 = ecdf(bd_1975) x_2012, y_2012 = ecdf(bd_2012) # Plot the ECDFs _ = plt.plot(x_1975, y_1975, marker=&#39;.&#39;, linestyle=&#39;none&#39;) _ = plt.plot(x_2012, y_2012, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Set margins plt.margins(0.02) # Add axis labels and legend _ = plt.xlabel(&#39;beak depth (mm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) _ = plt.legend((&#39;1975&#39;, &#39;2012&#39;), loc=&#39;lower right&#39;) # Show the plot plt.show() . . The differences are much clearer in the ECDF. The mean is larger in the 2012 data, and the variance does appear larger as well. . #### Parameter estimates of beak depths . Estimate the difference of the mean beak depth of the G. scandens samples from 1975 and 2012 and report a 95% confidence interval. . def draw_bs_reps(data, func, size=1): &quot;&quot;&quot;Draw bootstrap replicates.&quot;&quot;&quot; # Initialize array of replicates: bs_replicates bs_replicates = np.empty(size) # Generate replicates for i in range(size): bs_replicates[i] = bootstrap_replicate_1d(data, func) return bs_replicates # Compute the difference of the sample means: mean_diff mean_diff = np.mean(bd_2012) - np.mean(bd_1975) # Get bootstrap replicates of means bs_replicates_1975 = draw_bs_reps(bd_1975, np.mean, size=10000) bs_replicates_2012 = draw_bs_reps(bd_2012, np.mean, size=10000) # Compute samples of difference of means: bs_diff_replicates bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975 # Compute 95% confidence interval: conf_int conf_int = np.percentile(bs_diff_replicates, [2.5, 97.5]) # Print the results print(&#39;difference of means =&#39;, mean_diff, &#39;mm&#39;) print(&#39;95% confidence interval =&#39;, conf_int, &#39;mm&#39;) # difference of means = 0.22622047244094645 mm # 95% confidence interval = [0.05633521 0.39190544] mm . #### Hypothesis test: Are beaks deeper in 2012? . Your plot of the ECDF and determination of the confidence interval make it pretty clear that the beaks of G. scandens on Daphne Major have gotten deeper. But is it possible that this effect is just due to random chance? In other words, what is the probability that we would get the observed difference in mean beak depth if the means were the same? . Be careful! The hypothesis we are testing is not that the beak depths come from the same distribution. For that we could use a permutation test. The hypothesis is that the means are equal. To perform this hypothesis test, we need to shift the two data sets so that they have the same mean and then use bootstrap sampling to compute the difference of means. . # Compute mean of combined data set: combined_mean combined_mean = np.mean(np.concatenate((bd_1975, bd_2012))) # Shift the samples # why shift the mean? # to make np.mean(bd_1975_shifted) - np.mean(bd_2012_shifted) = 0 #1 # why make #1 = 0? # because our hypothesis is &quot;beak depth are the same in 1975 and 2012&quot; bd_1975_shifted = bd_1975 - np.mean(bd_1975) + combined_mean bd_2012_shifted = bd_2012 - np.mean(bd_2012) + combined_mean # Get bootstrap replicates of shifted data sets bs_replicates_1975 = draw_bs_reps(bd_1975_shifted, np.mean, size=10000) bs_replicates_2012 = draw_bs_reps(bd_2012_shifted, np.mean, size=10000) # Compute replicates of difference of means: bs_diff_replicates bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975 # Compute the p-value p = np.sum(bs_diff_replicates &gt;= mean_diff) / len(bs_diff_replicates) # Print p-value print(&#39;p =&#39;, p) # p = 0.0034 # p = 0.34% . We get a p-value of 0.0034, which suggests that there is a statistically significant difference. But remember: it is very important to know how different they are! In the previous exercise, you got a difference of 0.2 mm between the means. You should combine this with the statistical significance. Changing by 0.2 mm in 37 years is substantial by evolutionary standards. If it kept changing at that rate, the beak depth would double in only 400 years. . ### Variation of beak shapes . #### EDA of beak length and depth . The beak length data are stored as bl_1975 and bl_2012 , again with units of millimeters (mm). You still have the beak depth data stored in bd_1975 and bd_2012 . Make scatter plots of beak depth (y-axis) versus beak length (x-axis) for the 1975 and 2012 specimens. . # Make scatter plot of 1975 data _ = plt.plot(bl_1975, bd_1975, marker=&#39;.&#39;, linestyle=&#39;None&#39;, color=&#39;blue&#39;, alpha=0.5) # Make scatter plot of 2012 data _ = plt.plot(bl_2012, bd_2012, marker=&#39;.&#39;, linestyle=&#39;None&#39;, color=&#39;red&#39;, alpha=0.5) # Label axes and make legend _ = plt.xlabel(&#39;beak length (mm)&#39;) _ = plt.ylabel(&#39;beak depth (mm)&#39;) _ = plt.legend((&#39;1975&#39;, &#39;2012&#39;), loc=&#39;upper left&#39;) # Show the plot plt.show() . . In looking at the plot, we see that beaks got deeper (the red points are higher up in the y-direction), but not really longer. If anything, they got a bit shorter, since the red dots are to the left of the blue dots. So, it does not look like the beaks kept the same shape; they became shorter and deeper. . #### Linear regressions . Perform a linear regression for both the 1975 and 2012 data. Then, perform pairs bootstrap estimates for the regression parameters. Report 95% confidence intervals on the slope and intercept of the regression line. . def draw_bs_pairs_linreg(x, y, size=1): &quot;&quot;&quot;Perform pairs bootstrap for linear regression.&quot;&quot;&quot; # Set up array of indices to sample from: inds inds = np.arange(len(x)) # Initialize replicates: bs_slope_reps, bs_intercept_reps bs_slope_reps = np.empty(size) bs_intercept_reps = np.empty(size) # Generate replicates for i in range(size): bs_inds = np.random.choice(inds, size=len(inds)) bs_x, bs_y = x[bs_inds], y[bs_inds] bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, deg=1) return bs_slope_reps, bs_intercept_reps # Compute the linear regressions slope_1975, intercept_1975 = np.polyfit(bl_1975, bd_1975, deg=1) slope_2012, intercept_2012 = np.polyfit(bl_2012, bd_2012, deg=1) # Perform pairs bootstrap for the linear regressions bs_slope_reps_1975, bs_intercept_reps_1975 = draw_bs_pairs_linreg(bl_1975, bd_1975, size=1000) bs_slope_reps_2012, bs_intercept_reps_2012 = draw_bs_pairs_linreg(bl_2012, bd_2012, size=1000) # Compute confidence intervals of slopes slope_conf_int_1975 = np.percentile(bs_slope_reps_1975, [2.5, 97.5]) slope_conf_int_2012 = np.percentile(bs_slope_reps_2012, [2.5, 97.5]) intercept_conf_int_1975 = np.percentile(bs_intercept_reps_1975, [2.5, 97.5]) intercept_conf_int_2012 = np.percentile(bs_intercept_reps_2012, [2.5, 97.5]) # Print the results print(&#39;1975: slope =&#39;, slope_1975, &#39;conf int =&#39;, slope_conf_int_1975) print(&#39;1975: intercept =&#39;, intercept_1975, &#39;conf int =&#39;, intercept_conf_int_1975) print(&#39;2012: slope =&#39;, slope_2012, &#39;conf int =&#39;, slope_conf_int_2012) print(&#39;2012: intercept =&#39;, intercept_2012, &#39;conf int =&#39;, intercept_conf_int_2012) # 1975: slope = 0.4652051691605937 conf int = [0.33851226 0.59306491] # 1975: intercept = 2.3908752365842263 conf int = [0.64892945 4.18037063] # 2012: slope = 0.462630358835313 conf int = [0.33137479 0.60695527] # 2012: intercept = 2.977247498236019 conf int = [1.06792753 4.70599387] . It looks like they have the same slope, but different intercepts. . #### Displaying the linear regression results . # Make scatter plot of 1975 data _ = plt.plot(bl_1975, bd_1975, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;, alpha=0.5) # Make scatter plot of 2012 data _ = plt.plot(bl_2012, bd_2012, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;, alpha=0.5) # Label axes and make legend _ = plt.xlabel(&#39;beak length (mm)&#39;) _ = plt.ylabel(&#39;beak depth (mm)&#39;) _ = plt.legend((&#39;1975&#39;, &#39;2012&#39;), loc=&#39;upper left&#39;) # Generate x-values for bootstrap lines: x x = np.array([10, 17]) # Plot the bootstrap lines for i in range(100): plt.plot(x, bs_slope_reps_1975[i] * x + bs_intercept_reps_1975[i], linewidth=0.5, alpha=0.2, color=&#39;blue&#39;) plt.plot(x, bs_slope_reps_2012[i] * x + bs_intercept_reps_2012[i], linewidth=0.5, alpha=0.2, color=&#39;red&#39;) # Draw the plot again plt.show() . . #### Beak length to depth ratio . The linear regressions showed interesting information about the beak geometry. The slope was the same in 1975 and 2012, suggesting that for every millimeter gained in beak length, the birds gained about half a millimeter in depth in both years. However, if we are interested in the shape of the beak, we want to compare the ratio of beak length to beak depth. Let’s make that comparison. . # Compute length-to-depth ratios ratio_1975 = bl_1975 / bd_1975 ratio_2012 = bl_2012 / bd_2012 # Compute means mean_ratio_1975 = np.mean(ratio_1975) mean_ratio_2012 = np.mean(ratio_2012) # Generate bootstrap replicates of the means bs_replicates_1975 = draw_bs_reps(ratio_1975, np.mean, size=10000) bs_replicates_2012 = draw_bs_reps(ratio_2012, np.mean, size=10000) # Compute the 99% confidence intervals conf_int_1975 = np.percentile(bs_replicates_1975, [0.5, 99.5]) conf_int_2012 = np.percentile(bs_replicates_2012, [0.5, 99.5]) # Print the results print(&#39;1975: mean ratio =&#39;, mean_ratio_1975, &#39;conf int =&#39;, conf_int_1975) print(&#39;2012: mean ratio =&#39;, mean_ratio_2012, &#39;conf int =&#39;, conf_int_2012) # 1975: mean ratio = 1.5788823771858533 conf int = [1.55668803 1.60073509] # 2012: mean ratio = 1.4658342276847767 conf int = [1.44363932 1.48729149] . #### How different is the ratio? . In the previous exercise, you computed the mean beak length to depth ratio with 99% confidence intervals for 1975 and for 2012. The results of that calculation are shown graphically in the plot accompanying this problem. In addition to these results, what would you say about the ratio of beak length to depth? . . The mean beak length-to-depth ratio decreased by about 0.1, or 7%, from 1975 to 2012. The 99% confidence intervals are not even close to overlapping, so this is a real change. The beak shape changed. . ### Calculation of heritability . #### EDA of heritability . The array bd_parent_scandens contains the average beak depth (in mm) of two parents of the species G. scandens . The array bd_offspring_scandens contains the average beak depth of the offspring of the respective parents. The arrays bd_parent_fortis and bd_offspring_fortis contain the same information about measurements from G. fortis birds. . Make a scatter plot of the average offspring beak depth (y-axis) versus average parental beak depth (x-axis) for both species. Use the alpha=0.5 keyword argument to help you see overlapping points. . # Make scatter plots _ = plt.plot(bd_parent_fortis, bd_offspring_fortis, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;, alpha=0.5) _ = plt.plot(bd_parent_scandens, bd_offspring_scandens, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;, alpha=0.5) # Label axes _ = plt.xlabel(&#39;parental beak depth (mm)&#39;) _ = plt.ylabel(&#39;offspring beak depth (mm)&#39;) # Add legend _ = plt.legend((&#39;G. fortis&#39;, &#39;G. scandens&#39;), loc=&#39;lower right&#39;) # Show plot plt.show() . . It appears as though there is a stronger correlation in G. fortis than than in G. scandens . This suggests that beak depth is more strongly inherited in G. fortis . We’ll quantify this correlation next. . #### Correlation of offspring and parental data . In an effort to quantify the correlation between offspring and parent beak depths, we would like to compute statistics, such as the Pearson correlation coefficient, between parents and offspring. To get confidence intervals on this, we need to do a pairs bootstrap. . You have already written a function to do pairs bootstrap to get estimates for parameters derived from linear regression. Your task in this exercise is to make a new function with call signature draw_bs_pairs(x, y, func, size=1) that performs pairs bootstrap and computes a single statistic on pairs samples defined. The statistic of interest is computed by calling func(bs_x, bs_y) . In the next exercise, you will use pearson_r for func . . def draw_bs_pairs(x, y, func, size=1): &quot;&quot;&quot;Perform pairs bootstrap for a single statistic.&quot;&quot;&quot; # Set up array of indices to sample from: inds inds = np.arange(len(x)) # Initialize replicates: bs_replicates bs_replicates = np.empty(size) # Generate replicates for i in range(size): bs_inds = np.random.choice(inds, size=len(inds)) bs_x, bs_y = x[bs_inds], y[bs_inds] bs_replicates[i] = func(bs_x, bs_y) return bs_replicates . #### Pearson correlation of offspring and parental data . The Pearson correlation coefficient seems like a useful measure of how strongly the beak depth of parents are inherited by their offspring. Compute the Pearson correlation coefficient between parental and offspring beak depths for G. scandens . Do the same for G. fortis . Then, use the function you wrote in the last exercise to compute a 95% confidence interval using pairs bootstrap. . def pearson_r(x, y): &quot;&quot;&quot;Compute Pearson correlation coefficient between two arrays.&quot;&quot;&quot; # Compute correlation matrix: corr_mat corr_mat = np.corrcoef(x, y) # Return entry [0,1] return corr_mat[0,1] # Compute the Pearson correlation coefficients r_scandens = pearson_r(bd_parent_scandens, bd_offspring_scandens) r_fortis = pearson_r(bd_parent_fortis, bd_offspring_fortis) # Acquire 1000 bootstrap replicates of Pearson r bs_replicates_scandens = draw_bs_pairs(bd_parent_scandens, bd_offspring_scandens, pearson_r, size=1000) bs_replicates_fortis = draw_bs_pairs(bd_parent_fortis, bd_offspring_fortis, pearson_r, size=1000) # Compute 95% confidence intervals conf_int_scandens = np.percentile(bs_replicates_scandens, [2.5, 97.5]) conf_int_fortis = np.percentile(bs_replicates_fortis, [2.5, 97.5]) # Print results print(&#39;G. scandens:&#39;, r_scandens, conf_int_scandens) print(&#39;G. fortis:&#39;, r_fortis, conf_int_fortis) # G. scandens: 0.4117063629401258 [0.26564228 0.54388972] # G. fortis: 0.7283412395518487 [0.6694112 0.77840616] . It is clear from the confidence intervals that beak depth of the offspring of G. fortis parents is more strongly correlated with their offspring than their G. scandens counterparts. . #### Measuring heritability . Remember that the Pearson correlation coefficient is the ratio of the covariance to the geometric mean of the variances of the two data sets. This is a measure of the correlation between parents and offspring, but might not be the best estimate of heritability. If we stop and think, it makes more sense to define heritability as the ratio of the covariance between parent and offspring to the variance of the parents alone . In this exercise, you will estimate the heritability and perform a pairs bootstrap calculation to get the 95% confidence interval. . This exercise highlights a very important point. Statistical inference (and data analysis in general) is not a plug-n-chug enterprise. You need to think carefully about the questions you are seeking to answer with your data and analyze them appropriately. If you are interested in how heritable traits are, the quantity we defined as the heritability is more apt than the off-the-shelf statistic, the Pearson correlation coefficient. . def heritability(parents, offspring): &quot;&quot;&quot;Compute the heritability from parent and offspring samples.&quot;&quot;&quot; covariance_matrix = np.cov(parents, offspring) return covariance_matrix[0,1] / covariance_matrix[0,0] # Compute the heritability heritability_scandens = heritability(bd_parent_scandens, bd_offspring_scandens) heritability_fortis = heritability(bd_parent_fortis, bd_offspring_fortis) # Acquire 1000 bootstrap replicates of heritability replicates_scandens = draw_bs_pairs( bd_parent_scandens, bd_offspring_scandens, heritability, size=1000) replicates_fortis = draw_bs_pairs( bd_parent_fortis, bd_offspring_fortis, heritability, size=1000) # Compute 95% confidence intervals conf_int_scandens = np.percentile(replicates_scandens, [2.5, 97.5]) conf_int_fortis = np.percentile(replicates_fortis, [2.5, 97.5]) # Print results print(&#39;G. scandens:&#39;, heritability_scandens, conf_int_scandens) print(&#39;G. fortis:&#39;, heritability_fortis, conf_int_fortis) # G. scandens: 0.5485340868685982 [0.34395487 0.75638267] # G. fortis: 0.7229051911438159 [0.64655013 0.79688342] . Here again, we see that G. fortis has stronger heritability than G. scandens . This suggests that the traits of G. fortis may be strongly incorporated into G. scandens by introgressive hybridization. . #### Is beak depth heritable at all in G. scandens? . The heritability of beak depth in G. scandens seems low. It could be that this observed heritability was just achieved by chance and beak depth is actually not really heritable in the species . You will test that hypothesis here. To do this, you will do a pairs permutation test. . # Initialize array of replicates: perm_replicates perm_replicates = np.empty(10000) # Draw replicates for i in range(10000): # Permute parent beak depths bd_parent_permuted = np.random.permutation(bd_parent_scandens) perm_replicates[i] = heritability(bd_parent_permuted, bd_offspring_scandens) # Compute p-value: p p = np.sum(perm_replicates &gt;= heritability_scandens) / len(perm_replicates) # Print the p-value print(&#39;p-val =&#39;, p) # p-val = 0.0 . You get a p-value of zero, which means that none of the 10,000 permutation pairs replicates you drew had a heritability high enough to match that which was observed. This strongly suggests that beak depth is heritable in G. scandens , just not as much as in G. fortis . If you like, you can plot a histogram of the heritability replicates to get a feel for how extreme of a value of heritability you might expect by chance. . plt.hist(perm_replicates) plt.axvline(x=heritability_scandens, color = &#39;red&#39;) plt.text(heritability_scandens, 1500, &#39;heritability_scandens&#39;, ha=&#39;center&#39;, va=&#39;center&#39;,rotation=&#39;vertical&#39;, backgroundcolor=&#39;white&#39;) plt.show() . Parameter estimation by optimization . ### Optimal parameters . #### How often do we get no-hitters? . The number of games played between each no-hitter in the modern era (1901-2015) of Major League Baseball is stored in the array nohitter_times . . If you assume that no-hitters are described as a Poisson process, then the time between no-hitters is Exponentially distributed. As you have seen, the Exponential distribution has a single parameter, which we will call ττ, the typical interval time. The value of the parameter ττ that makes the exponential distribution best match the data is the mean interval time (where time is in units of number of games) between no-hitters. . Compute the value of this parameter from the data. Then, use np.random.exponential() to “repeat” the history of Major League Baseball by drawing inter-no-hitter times from an exponential distribution with the ττ you found and plot the histogram as an approximation to the PDF. . # Seed random number generator np.random.seed(42) # Compute mean no-hitter time: tau tau = np.mean(nohitter_times) # Draw out of an exponential distribution with parameter tau: inter_nohitter_time inter_nohitter_time = np.random.exponential(tau, 100000) # Plot the PDF and label axes _ = plt.hist(inter_nohitter_time, bins=50, normed=True, histtype=&#39;step&#39;) _ = plt.xlabel(&#39;Games between no-hitters&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() . . We see the typical shape of the Exponential distribution, going from a maximum at 0 and decaying to the right. . #### Do the data follow our story? . # Create an ECDF from real data: x, y x, y = ecdf(nohitter_times) # Create a CDF from theoretical samples: x_theor, y_theor x_theor, y_theor = ecdf(inter_nohitter_time) # Overlay the plots plt.plot(x_theor, y_theor) plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Margins and axis labels plt.margins(0.02) plt.xlabel(&#39;Games between no-hitters&#39;) plt.ylabel(&#39;CDF&#39;) # Show the plot plt.show() . . It looks like no-hitters in the modern era of Major League Baseball are Exponentially distributed. Based on the story of the Exponential distribution, this suggests that they are a random process; when a no-hitter will happen is independent of when the last no-hitter was. . #### How is this parameter optimal? . # Plot the theoretical CDFs plt.plot(x_theor, y_theor) plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;) plt.margins(0.02) plt.xlabel(&#39;Games between no-hitters&#39;) plt.ylabel(&#39;CDF&#39;) # Take samples with half tau: samples_half samples_half = np.random.exponential(tau/2, size=10000) # Take samples with double tau: samples_double samples_double = np.random.exponential(2*tau, size=10000) # Generate CDFs from these samples x_half, y_half = ecdf(samples_half) x_double, y_double = ecdf(samples_double) # Plot these CDFs as lines _ = plt.plot(x_half, y_half) _ = plt.plot(x_double, y_double) # Show the plot plt.show() . . red = half, purple = double . Notice how the value of tau given by the mean matches the data best. In this way, tau is an optimal parameter. . ### Linear regression by least squares . #### EDA of literacy/fertility data . # Plot the illiteracy rate versus fertility _ = plt.plot(illiteracy, fertility, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Set the margins and label axes plt.margins(0.02) _ = plt.xlabel(&#39;percent illiterate&#39;) _ = plt.ylabel(&#39;fertility&#39;) # Show the plot plt.show() # Show the Pearson correlation coefficient print(pearson_r(illiteracy, fertility)) 0.8041324026815344 . . You can see the correlation between illiteracy and fertility by eye, and by the substantial Pearson correlation coefficient of 0.8. It is difficult to resolve in the scatter plot, but there are many points around near-zero illiteracy and about 1.8 children/woman. . #### Linear regression . # Plot the illiteracy rate versus fertility _ = plt.plot(illiteracy, fertility, marker=&#39;.&#39;, linestyle=&#39;none&#39;) plt.margins(0.02) _ = plt.xlabel(&#39;percent illiterate&#39;) _ = plt.ylabel(&#39;fertility&#39;) # Perform a linear regression using np.polyfit(): a, b a, b = np.polyfit(illiteracy, fertility, deg=1) # Print the results to the screen print(&#39;slope =&#39;, a, &#39;children per woman / percent illiterate&#39;) print(&#39;intercept =&#39;, b, &#39;children per woman&#39;) # Make theoretical line to plot x = np.array([0, 100]) y = a * x + b # Add regression line to your plot _ = plt.plot(x, y) # Draw the plot plt.show() # slope = 0.04979854809063423 children per woman / percent illiterate # intercept = 1.888050610636557 children per woman . . #### How is it optimal? . # Specify slopes to consider: a_vals a_vals = np.linspace(0, 0.1, 200) # Initialize sum of square of residuals: rss rss = np.empty_like(a_vals) # Compute sum of square of residuals for each value of a_vals for i, a in enumerate(a_vals): rss[i] = np.sum((fertility - a*illiteracy - b)**2) # Plot the RSS plt.plot(a_vals, rss, &#39;-&#39;) plt.xlabel(&#39;slope (children per woman / percent illiterate)&#39;) plt.ylabel(&#39;sum of square of residuals&#39;) plt.show() . . Notice that the minimum on the plot, that is the value of the slope that gives the minimum sum of the square of the residuals, is the same value you got when performing the regression. . ### The importance of EDA: Anscombe’s quartet . . #### The importance of EDA . Why should exploratory data analysis be the first step in an analysis of data (after getting your data imported and cleaned, of course)? . You can be protected from misinterpretation of the type demonstrated by Anscombe’s quartet. | EDA provides a good starting point for planning the rest of your analysis. | EDA is not really any more difficult than any of the subsequent analysis, so there is no excuse for not exploring the data. | . #### Linear regression on appropriate Anscombe data . # Perform linear regression: a, b a, b = np.polyfit(x, y, deg=1) # Print the slope and intercept print(a, b) # Generate theoretical x and y data: x_theor, y_theor x_theor = np.array([3, 15]) y_theor = a * x_theor + b # Plot the Anscombe data and theoretical line _ = plt.plot(x, y, marker = &#39;.&#39;, linestyle = &#39;none&#39;) _ = plt.plot(x_theor, y_theor) # Label the axes plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;) # Show the plot plt.show() . . #### Linear regression on all Anscombe data . for i in range(4): plt.subplot(2,2,i+1) # plot the scatter plot plt.plot(anscombe_x[i], anscombe_y[i], marker = &#39;.&#39;, linestyle = &#39;none&#39;) # plot the regression line a, b = np.polyfit(anscombe_x[i], anscombe_y[i], deg=1) x_theor = np.array([np.min(anscombe_x[i]), np.max(anscombe_x[i])]) y_theor = a * x_theor + b plt.plot(x_theor, y_theor) # add label plt.xlabel(&#39;x&#39; + str(i+1)) plt.ylabel(&#39;y&#39; + str(i+1)) plt.show() # slope1: 0.5000909090909095 intercept: 3.000090909090909 # slope2: 0.5000000000000004 intercept: 3.0009090909090896 # slope3: 0.4997272727272731 intercept: 3.0024545454545453 # slope4: 0.4999090909090908 intercept: 3.0017272727272735 . . ### Generating bootstrap replicates . #### Getting the terminology down . If we have a data set with n repeated measurements, a bootstrap sample is an array of length n that was drawn from the original data with replacement. . Bootstrap replicate is a single value of a statistic computed from a bootstrap sample. . #### Visualizing bootstrap samples . np.random.choice() . for _ in range(50): # Generate bootstrap sample: bs_sample bs_sample = np.random.choice(rainfall, size=len(rainfall)) # Compute and plot ECDF from bootstrap sample x, y = ecdf(bs_sample) _ = plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;gray&#39;, alpha=0.1) # Compute and plot ECDF from original data x, y = ecdf(rainfall) _ = plt.plot(x, y, marker=&#39;.&#39;) # Make margins and label axes plt.margins(0.02) _ = plt.xlabel(&#39;yearly rainfall (mm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) # Show the plot plt.show() . . Bootstrap confidence intervals . #### Generating many bootstrap replicates . def bootstrap_replicate_1d(data, func): return func(np.random.choice(data, size=len(data))) def draw_bs_reps(data, func, size=1): &quot;&quot;&quot;Draw bootstrap replicates.&quot;&quot;&quot; # Initialize array of replicates: bs_replicates bs_replicates = np.empty(size) # Generate replicates for i in range(size): bs_replicates[i] = bootstrap_replicate_1d(data, func) return bs_replicates . #### Bootstrap replicates of the mean and the SEM ( standard error of the mean ) . In fact, it can be shown theoretically that under not-too-restrictive conditions, the value of the mean will always be Normally distributed. (This does not hold in general, just for the mean and a few other statistics.) . The standard deviation of this distribution, called the standard error of the mean , or SEM, is given by the standard deviation of the data divided by the square root of the number of data points. I.e., for a data set, sem = np.std(data) / np.sqrt(len(data)) . Using hacker statistics, you get this same result without the need to derive it, but you will verify this result from your bootstrap replicates. . # Take 10,000 bootstrap replicates of the mean: bs_replicates bs_replicates = draw_bs_reps(rainfall, np.mean, size=10000) # Compute and print SEM sem = np.std(rainfall) / np.sqrt(len(rainfall)) print(sem) # Compute and print standard deviation of bootstrap replicates bs_std = np.std(bs_replicates) print(bs_std) # Make a histogram of the results _ = plt.hist(bs_replicates, bins=50, normed=True) _ = plt.xlabel(&#39;mean annual rainfall (mm)&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() # 10.51054915050619 # 10.465927071184412 . . #### Confidence intervals of rainfall data . Use the bootstrap replicates you just generated to compute the 95% confidence interval. That is, give the 2.5th and 97.5th percentile of your bootstrap replicates stored as bs_replicates . What is the 95% confidence interval? . np.percentile(bs_replicates,2.5) 779.7699248120301 np.percentile(bs_replicates,97.5) 820.950432330827 . #### Bootstrap replicates of other statistics . def draw_bs_reps(data, func, size=1): return np.array([bootstrap_replicate_1d(data, func) for _ in range(size)]) # Generate 10,000 bootstrap replicates of the variance: bs_replicates bs_replicates = draw_bs_reps(rainfall, np.var, size=10000) # Put the variance in units of square centimeters bs_replicates /= 100 # Make a histogram of the results _ = plt.hist(bs_replicates, bins=50, normed=True) _ = plt.xlabel(&#39;variance of annual rainfall (sq. cm)&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() . . This is not normally distributed, as it has a longer tail to the right. Note that you can also compute a confidence interval on the variance, or any other statistic, using np.percentile() with your bootstrap replicates. . #### Confidence interval on the rate of no-hitters . # Draw bootstrap replicates of the mean no-hitter time (equal to tau): bs_replicates bs_replicates = draw_bs_reps(nohitter_times, np.mean, size=10000) # Compute the 95% confidence interval: conf_int conf_int = np.percentile(bs_replicates, [2.5, 97.5]) # Print the confidence interval print(&#39;95% confidence interval =&#39;, conf_int, &#39;games&#39;) # Plot the histogram of the replicates _ = plt.hist(bs_replicates, bins=50, normed=True) _ = plt.xlabel(r&#39;$ tau$ (games)&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() # 95% confidence interval = [660.67280876 871.63077689] games . . This gives you an estimate of what the typical time between no-hitters is. It could be anywhere between 660 and 870 games. . ### Pairs bootstrap . #### A function to do pairs bootstrap . def draw_bs_pairs_linreg(x, y, size=1): &quot;&quot;&quot;Perform pairs bootstrap for linear regression.&quot;&quot;&quot; # Set up array of indices to sample from: inds inds = np.arange(len(x)) # Initialize replicates: bs_slope_reps, bs_intercept_reps bs_slope_reps = np.empty(size) bs_intercept_reps = np.empty(size) # Generate replicates for i in range(size): bs_inds = np.random.choice(inds, size=len(inds)) bs_x, bs_y = x[bs_inds], y[bs_inds] bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, deg=1) return bs_slope_reps, bs_intercept_reps . #### Pairs bootstrap of literacy/fertility data . # Generate replicates of slope and intercept using pairs bootstrap bs_slope_reps, bs_intercept_reps = draw_bs_pairs_linreg(illiteracy, fertility, size=1000) # Compute and print 95% CI for slope print(np.percentile(bs_slope_reps, [2.5, 97.5])) # Plot the histogram _ = plt.hist(bs_slope_reps, bins=50, normed=True) _ = plt.xlabel(&#39;slope&#39;) _ = plt.ylabel(&#39;PDF&#39;) plt.show() # [0.04378061 0.0551616 ] . . #### Plotting bootstrap regressions . # Generate array of x-values for bootstrap lines: x x = np.array([0, 100]) # Plot the bootstrap lines for i in range(100): _ = plt.plot(x, bs_slope_reps[i]*x + bs_intercept_reps[i], linewidth=0.5, alpha=0.2, color=&#39;red&#39;) # Plot the data _ = plt.plot(illiteracy, fertility, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Label axes, set the margins, and show the plot _ = plt.xlabel(&#39;illiteracy&#39;) _ = plt.ylabel(&#39;fertility&#39;) plt.margins(0.02) plt.show() . . Introduction to hypothesis testing . ### Formulating and simulating a hypothesis . Null hypothesis another name for the hypothesis you are testing Permutation random reordering of entries in an array #### Generating a permutation sample . np.random.permutation() . Permutation sampling is a great way to simulate the hypothesis that two variables have identical probability distributions. This is often a hypothesis you want to test, so in this exercise, you will write a function to generate a permutation sample from two data sets. . def permutation_sample(data1, data2): &quot;&quot;&quot;Generate a permutation sample from two data sets.&quot;&quot;&quot; # Concatenate the data sets: data data = np.concatenate((data1, data2)) # Permute the concatenated array: permuted_data permuted_data = np.random.permutation(data) # Split the permuted array into two: perm_sample_1, perm_sample_2 perm_sample_1 = permuted_data[:len(data1)] perm_sample_2 = permuted_data[len(data1):] return perm_sample_1, perm_sample_2 . #### Visualizing permutation sampling . for _ in range(50): # Generate permutation samples perm_sample_1, perm_sample_2 = permutation_sample(rain_june, rain_november) # Compute ECDFs x_1, y_1 = ecdf(perm_sample_1) x_2, y_2 = ecdf(perm_sample_2) # Plot ECDFs of permutation sample _ = plt.plot(x_1, y_1, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;, alpha=0.02) _ = plt.plot(x_2, y_2, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;, alpha=0.02) # Create and plot ECDFs from original data x_1, y_1 = ecdf(rain_june) x_2, y_2 = ecdf(rain_november) _ = plt.plot(x_1, y_1, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;) _ = plt.plot(x_2, y_2, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;) # Label axes, set margin, and show plot plt.margins(0.02) _ = plt.xlabel(&#39;monthly rainfall (mm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) plt.show() . . Notice that the permutation samples ECDFs overlap and give a purple haze. None of the ECDFs from the permutation samples overlap with the observed data, suggesting that the hypothesis is not commensurate with the data. June and November rainfall are not identically distributed. . ### Test statistics and p-values . #### Test statistics . When performing hypothesis tests, your choice of test statistic should be pertinent to the question you are seeking to answer in your hypothesis test. . The most important thing to consider is: What are you asking? . #### What is a p-value? . The p-value is generally a measure of the probability of observing a test statistic equally or more extreme than the one you observed, given that the null hypothesis is true. . #### Generating permutation replicates . # In most circumstances, func will be a function you write yourself. def draw_perm_reps(data_1, data_2, func, size=1): &quot;&quot;&quot;Generate multiple permutation replicates.&quot;&quot;&quot; # Initialize array of replicates: perm_replicates perm_replicates = np.empty(size) for i in range(size): # Generate permutation sample perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2) # Compute the test statistic perm_replicates[i] = func(perm_sample_1, perm_sample_2) return perm_replicates . #### Look before you leap: EDA before hypothesis testing . Kleinteich and Gorb ( Sci. Rep. , 4 , 5225, 2014) performed an interesting experiment with South American horned frogs. They held a plate connected to a force transducer, along with a bait fly, in front of them. They then measured the impact force and adhesive force of the frog’s tongue when it struck the target. . Frog A is an adult and Frog B is a juvenile. The researchers measured the impact force of 20 strikes for each frog. In the next exercise, we will test the hypothesis that the two frogs have the same distribution of impact forces. But, remember, it is important to do EDA first! Let’s make a bee swarm plot for the data. They are stored in a Pandas data frame, df , where column ID is the identity of the frog and column impact_force is the impact force in Newtons (N). . df.head() ID impact_force 20 A 1.612 21 A 0.605 22 A 0.327 23 A 0.946 24 A 0.541 . # Make bee swarm plot _ = sns.swarmplot(x=&#39;ID&#39;, y=&#39;impact_force&#39;, data=df) # Label axes _ = plt.xlabel(&#39;frog&#39;) _ = plt.ylabel(&#39;impact force (N)&#39;) # Show the plot plt.show() . . Eyeballing it, it does not look like they come from the same distribution. Frog A, the adult, has three or four very hard strikes, and Frog B, the juvenile, has a couple weak ones. However, it is possible that with only 20 samples it might be too difficult to tell if they have difference distributions, so we should proceed with the hypothesis test. . #### Permutation test on frog data . The average strike force of Frog A was 0.71 Newtons (N), and that of Frog B was 0.42 N for a difference of 0.29 N. It is possible the frogs strike with the same force and this observed difference was by chance. You will compute the probability of getting at least a 0.29 N difference in mean strike force under the hypothesis that the distributions of strike forces for the two frogs are identical. We use a permutation test with a test statistic of the difference of means to test this hypothesis. . def diff_of_means(data_1, data_2): &quot;&quot;&quot;Difference in means of two arrays.&quot;&quot;&quot; # The difference of means of data_1, data_2: diff diff = np.mean(data_1) - np.mean(data_2) return diff # Compute difference of mean impact force from experiment: empirical_diff_means empirical_diff_means = diff_of_means(force_a, force_b) # Draw 10,000 permutation replicates: perm_replicates perm_replicates = draw_perm_reps(force_a, force_b, diff_of_means, size=10000) # Compute p-value: p p = np.sum(perm_replicates &gt;= empirical_diff_means) / len(perm_replicates) # Print the result print(&#39;p-value =&#39;, p) # p-value = 0.0063 # p-value = 0.63% . The p-value tells you that there is about a 0.6% chance that you would get the difference of means observed in the experiment if frogs were exactly the same. . A p-value below 0.01 is typically said to be “statistically significant,” but: warning! warning! warning! You have computed a p-value; it is a number. I encourage you not to distill it to a yes-or-no phrase. p = 0.006 and p = 0.000000006 are both said to be “statistically significant,” but they are definitely not the same! . ### Bootstrap hypothesis tests . . #### A one-sample bootstrap hypothesis test . Another juvenile frog was studied, Frog C, and you want to see if Frog B and Frog C have similar impact forces. Unfortunately, you do not have Frog C’s impact forces available, but you know they have a mean of 0.55 N. Because you don’t have the original data, you cannot do a permutation test, and you cannot assess the hypothesis that the forces from Frog B and Frog C come from the same distribution. You will therefore test another, less restrictive hypothesis: The mean strike force of Frog B is equal to that of Frog C. . To set up the bootstrap hypothesis test, you will take the mean as our test statistic. Remember, your goal is to calculate the probability of getting a mean impact force less than or equal to what was observed for Frog B if the hypothesis that the true mean of Frog B’s impact forces is equal to that of Frog C is true . You first translate all of the data of Frog B such that the mean is 0.55 N. This involves adding the mean force of Frog C and subtracting the mean force of Frog B from each measurement of Frog B. This leaves other properties of Frog B’s distribution, such as the variance, unchanged. . # Make an array of translated impact forces: translated_force_b translated_force_b = force_b - np.mean(force_b) + 0.55 # Take bootstrap replicates of Frog B&#39;s translated impact forces: bs_replicates bs_replicates = draw_bs_reps(translated_force_b, np.mean, 10000) # Compute fraction of replicates that are less than the observed Frog B force: p p = np.sum(bs_replicates &lt;= np.mean(force_b)) / 10000 # Print the p-value print(&#39;p = &#39;, p) # p = 0.0046 # p = 0.46% . The low p-value suggests that the null hypothesis that Frog B and Frog C have the same mean impact force is false. . #### A two-sample bootstrap hypothesis test for difference of means . We now want to test the hypothesis that Frog A and Frog B have the same mean impact force, but not necessarily the same distribution, which is also impossible with a permutation test. . To do the two-sample bootstrap test, we shift both arrays to have the same mean, since we are simulating the hypothesis that their means are, in fact, equal. We then draw bootstrap samples out of the shifted arrays and compute the difference in means. This constitutes a bootstrap replicate, and we generate many of them. The p-value is the fraction of replicates with a difference in means greater than or equal to what was observed. . # Compute mean of all forces: mean_force mean_force = np.mean(forces_concat) # Generate shifted arrays force_a_shifted = force_a - np.mean(force_a) + mean_force force_b_shifted = force_b - np.mean(force_b) + mean_force # Compute 10,000 bootstrap replicates from shifted arrays bs_replicates_a = draw_bs_reps(force_a_shifted, np.mean, size=10000) bs_replicates_b = draw_bs_reps(force_b_shifted, np.mean, size=10000) # Get replicates of difference of means: bs_replicates bs_replicates = bs_replicates_a - bs_replicates_b # Compute and print p-value: p p = np.sum(bs_replicates &gt;= empirical_diff_means) / 10000 print(&#39;p-value =&#39;, p) # p-value = 0.0043 # p-value = 0.43% . You got a similar result as when you did the permutation test. Nonetheless, remember that it is important to carefully think about what question you want to ask. Are you only interested in the mean impact force, or in the distribution of impact forces? . Hypothesis test examples . ### A/B testing . #### The vote for the Civil Rights Act in 1964 . The Civil Rights Act of 1964 was one of the most important pieces of legislation ever passed in the USA. Excluding “present” and “abstain” votes, 153 House Democrats and 136 Republicans voted yea. However, 91 Democrats and 35 Republicans voted nay. Did party affiliation make a difference in the vote? . To answer this question, you will evaluate the hypothesis that the party of a House member has no bearing on his or her vote. You will use the fraction of Democrats voting in favor as your test statistic and evaluate the probability of observing a fraction of Democrats voting in favor at least as small as the observed fraction of 153/244. (That’s right, at least as small as. In 1964, it was the Democrats who were less progressive on civil rights issues.) To do this, permute the party labels of the House voters and then arbitrarily divide them into “Democrats” and “Republicans” and compute the fraction of Democrats voting yea. . # Construct arrays of data: dems, reps dems = np.array([True] * 153 + [False] * 91) reps = np.array([True] * 136 + [False] * 35) def frac_yea_dems(dems, reps): &quot;&quot;&quot;Compute fraction of Democrat yea votes.&quot;&quot;&quot; frac = np.sum(dems) / len(dems) return frac # Acquire permutation samples: perm_replicates perm_replicates = draw_perm_reps(dems, reps, frac_yea_dems, size=10000) # Compute and print p-value: p p = np.sum(perm_replicates &lt;= 153/244) / len(perm_replicates) print(&#39;p-value =&#39;, p) # p-value = 0.0002 # p-value = 0.02% . This small p-value suggests that party identity had a lot to do with the voting. Importantly, the South had a higher fraction of Democrat representatives, and consequently also a more racist bias. . #### What is equivalent? . You have experience matching a stories to probability distributions. Similarly, you use the same procedure for two different A/B tests if their stories match. In the Civil Rights Act example you just did, you performed an A/B test on voting data, which has a Yes/No type of outcome for each subject (in that case, a voter). Which of the following situations involving testing by a web-based company has an equivalent set up for an A/B test as the one you just did with the Civil Rights Act of 1964? . You measure the number of people who click on an ad on your company’s website before and after changing its color. . The “Democrats” are those who view the ad before the color change, and the “Republicans” are those who view it after. . #### A time-on-website analog . It turns out that you already did a hypothesis test analogous to an A/B test where you are interested in how much time is spent on the website before and after an ad campaign. The frog tongue force (a continuous quantity like time on the website) is an analog. “Before” = Frog A and “after” = Frog B. Let’s practice this again with something that actually is a before/after scenario. . We return to the no-hitter data set. In 1920, Major League Baseball implemented important rule changes that ended the so-called dead ball era. Importantly, the pitcher was no longer allowed to spit on or scuff the ball, an activity that greatly favors pitchers. In this problem you will perform an A/B test to determine if these rule changes resulted in a slower rate of no-hitters (i.e., longer average time between no-hitters) using the difference in mean inter-no-hitter time as your test statistic. The inter-no-hitter times for the respective eras are stored in the arrays nht_dead and nht_live , where “nht” is meant to stand for “no-hitter time.” . def diff_of_means(data_1, data_2): &quot;&quot;&quot;Difference in means of two arrays.&quot;&quot;&quot; # The difference of means of data_1, data_2: diff diff = np.mean(data_1) - np.mean(data_2) return diff def permutation_sample(data1, data2): &quot;&quot;&quot;Generate a permutation sample from two data sets.&quot;&quot;&quot; # Concatenate the data sets: data data = np.concatenate((data1, data2)) # Permute the concatenated array: permuted_data permuted_data = np.random.permutation(data) # Split the permuted array into two: perm_sample_1, perm_sample_2 perm_sample_1 = permuted_data[:len(data1)] perm_sample_2 = permuted_data[len(data1):] return perm_sample_1, perm_sample_2 def draw_perm_reps(data_1, data_2, func, size=1): &quot;&quot;&quot;Generate multiple permutation replicates.&quot;&quot;&quot; # Initialize array of replicates: perm_replicates perm_replicates = np.empty(size) for i in range(size): # Generate permutation sample perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2) # Compute the test statistic perm_replicates[i] = func(perm_sample_1, perm_sample_2) return perm_replicates # Compute the observed difference in mean inter-no-hitter times: nht_diff_obs nht_diff_obs = diff_of_means(nht_dead, nht_live) # Acquire 10,000 permutation replicates of difference in mean no-hitter time: perm_replicates perm_replicates = draw_perm_reps(nht_dead, nht_live, diff_of_means, size=10000) # Compute and print the p-value: p p = np.sum(perm_replicates &lt;= nht_diff_obs) /len(perm_replicates) print(&#39;p-val =&#39;, p) p-val = 0.0001 . Your p-value is 0.0001, which means that only one out of your 10,000 replicates had a result as extreme as the actual difference between the dead ball and live ball eras. This suggests strong statistical significance. Watch out, though, you could very well have gotten zero replicates that were as extreme as the observed value. This just means that the p-value is quite small, almost certainly smaller than 0.001. . #### What should you have done first? . That was a nice hypothesis test you just did to check out whether the rule changes in 1920 changed the rate of no-hitters. But what should you have done with the data first? . Performed EDA, perhaps plotting the ECDFs of inter-no-hitter times in the dead ball and live ball eras. . Always a good idea to do first! I encourage you to go ahead and plot the ECDFs right now. You will see by eye that the null hypothesis that the distributions are the same is almost certainly not true. . # Create and plot ECDFs x_1, y_1 = ecdf(nht_dead) x_2, y_2 = ecdf(nht_live) _ = plt.plot(x_1, y_1, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;) _ = plt.plot(x_2, y_2, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;) # Label axes, set margin, and show plot plt.margins(0.02) _ = plt.xlabel(&#39;Games between no-hitters&#39;) _ = plt.ylabel(&#39;ECDF&#39;) plt.show() . . ### Test of correlation . . #### Simulating a null hypothesis concerning correlation . The observed correlation between female illiteracy and fertility in the data set of 162 countries may just be by chance; the fertility of a given country may actually be totally independent of its illiteracy. You will test this null hypothesis in the next exercise. . To do the test, you need to simulate the data assuming the null hypothesis is true. Of the following choices, which is the best way to to do it? . Answer: Do a permutation test: Permute the illiteracy values but leave the fertility values fixed to generate a new set of (illiteracy, fertility) data. . This exactly simulates the null hypothesis and does so more efficiently than the last option. It is exact because it uses all data and eliminates any correlation because which illiteracy value pairs to which fertility value is shuffled. . Last option: Do a permutation test: Permute both the illiteracy and fertility values to generate a new set of (illiteracy, fertility data). This exactly simulates the null hypothesis and does so more efficiently than the last option. It is exact because it uses all data and eliminates any correlation because which illiteracy value pairs to which fertility value is shuffled. . #### Hypothesis test on Pearson correlation . The observed correlation between female illiteracy and fertility may just be by chance; the fertility of a given country may actually be totally independent of its illiteracy. You will test this hypothesis. To do so, permute the illiteracy values but leave the fertility values fixed. This simulates the hypothesis that they are totally independent of each other. For each permutation, compute the Pearson correlation coefficient and assess how many of your permutation replicates have a Pearson correlation coefficient greater than the observed one. . def pearson_r(x, y): &quot;&quot;&quot;Compute Pearson correlation coefficient between two arrays.&quot;&quot;&quot; # Compute correlation matrix: corr_mat corr_mat = np.corrcoef(x, y) # Return entry [0,1] return corr_mat[0,1] # Compute observed correlation: r_obs r_obs = pearson_r(illiteracy, fertility) # r_obs = 0.8041324026815344 # Initialize permutation replicates: perm_replicates perm_replicates = np.empty(10000) # Draw replicates for i in range(10000): # Permute illiteracy measurments: illiteracy_permuted illiteracy_permuted = np.random.permutation(illiteracy) # Compute Pearson correlation perm_replicates[i] = pearson_r(illiteracy_permuted, fertility) # Compute p-value: p p = np.sum(perm_replicates &gt;= r_obs) /len(perm_replicates) print(&#39;p-val =&#39;, p) # p-val = 0.0 . You got a p-value of zero. In hacker statistics, this means that your p-value is very low, since you never got a single replicate in the 10,000 you took that had a Pearson correlation greater than the observed one. You could try increasing the number of replicates you take to continue to move the upper bound on your p-value lower and lower. . #### Do neonicotinoid insecticides have unintended consequences? . As a final exercise in hypothesis testing before we put everything together in our case study in the next chapter, you will investigate the effects of neonicotinoid insecticides on bee reproduction. These insecticides are very widely used in the United States to combat aphids and other pests that damage plants. . In a recent study, Straub, et al. ( Proc. Roy. Soc. B , 2016 ) investigated the effects of neonicotinoids on the sperm of pollinating bees. In this and the next exercise, you will study how the pesticide treatment affected the count of live sperm per half milliliter of semen. . First, we will do EDA, as usual. Plot ECDFs of the alive sperm count for untreated bees (stored in the Numpy array control ) and bees treated with pesticide (stored in the Numpy array treated ). . # Compute x,y values for ECDFs x_control, y_control = ecdf(control) x_treated, y_treated = ecdf(treated) # Plot the ECDFs plt.plot(x_control, y_control, marker=&#39;.&#39;, linestyle=&#39;none&#39;) plt.plot(x_treated, y_treated, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Set the margins plt.margins(0.02) # Add a legend plt.legend((&#39;control&#39;, &#39;treated&#39;), loc=&#39;lower right&#39;) # Label axes and show plot plt.xlabel(&#39;millions of alive sperm per mL&#39;) plt.ylabel(&#39;ECDF&#39;) plt.show() . . The ECDFs show a pretty clear difference between the treatment and control; treated bees have fewer alive sperm. Let’s now do a hypothesis test in the next exercise. . #### Bootstrap hypothesis test on bee sperm counts . Now, you will test the following hypothesis: . On average, male bees treated with neonicotinoid insecticide have the same number of active sperm per milliliter of semen than do untreated male bees. . You will use the difference of means as your test statistic. . def bootstrap_replicate_1d(data, func): return func(np.random.choice(data, size=len(data))) def draw_bs_reps(data, func, size=1): &quot;&quot;&quot;Draw bootstrap replicates.&quot;&quot;&quot; # Initialize array of replicates: bs_replicates bs_replicates = np.empty(size) # Generate replicates for i in range(size): bs_replicates[i] = bootstrap_replicate_1d(data, func) return bs_replicates # Compute the difference in mean sperm count: diff_means diff_means = np.mean(control) - np.mean(treated) # Compute mean of pooled data: mean_count mean_count = np.mean(np.concatenate((control, treated))) # Generate shifted data sets control_shifted = control - np.mean(control) + mean_count treated_shifted = treated - np.mean(treated) + mean_count # Generate bootstrap replicates bs_reps_control = draw_bs_reps(control_shifted, np.mean, size=10000) bs_reps_treated = draw_bs_reps(treated_shifted, np.mean, size=10000) # Get replicates of difference of means: bs_replicates bs_replicates = bs_reps_control - bs_reps_treated # Compute and print p-value: p p = np.sum(bs_replicates &gt;= np.mean(control) - np.mean(treated)) / len(bs_replicates) print(&#39;p-value =&#39;, p) # p-value = 0.0 . The p-value is small, most likely less than 0.0001, since you never saw a bootstrap replicated with a difference of means at least as extreme as what was observed. In fact, when I did the calculation with 10 million replicates, I got a p-value of 2e-05 . Putting it all together: a case study . ### Finch beaks and the need for statistics . . . #### EDA of beak depths of Darwin’s finches . For your first foray into the Darwin finch data, you will study how the beak depth (the distance, top to bottom, of a closed beak) of the finch species Geospiza scandens has changed over time. The Grants have noticed some changes of beak geometry depending on the types of seeds available on the island, and they also noticed that there was some interbreeding with another major species on Daphne Major, Geospiza fortis . These effects can lead to changes in the species over time. . In the next few problems, you will look at the beak depth of G. scandens on Daphne Major in 1975 and in 2012. To start with, let’s plot all of the beak depth measurements in 1975 and 2012 in a bee swarm plot. . The data are stored in a pandas DataFrame called df with columns &#39;year&#39; and &#39;beak_depth&#39; . The units of beak depth are millimeters (mm). . df.head() beak_depth year 0 8.4 1975 1 8.8 1975 2 8.4 1975 3 8.0 1975 4 7.9 1975 . # Create bee swarm plot _ = sns.swarmplot(&#39;year&#39;, &#39;beak_depth&#39;, data=df) # Label the axes _ = plt.xlabel(&#39;year&#39;) _ = plt.ylabel(&#39;beak depth (mm)&#39;) # Show the plot plt.show() . . It is kind of hard to see if there is a clear difference between the 1975 and 2012 data set. Eyeballing it, it appears as though the mean of the 2012 data set might be slightly higher, and it might have a bigger variance. . #### ECDFs of beak depths . While bee swarm plots are useful, we found that ECDFs are often even better when doing EDA. Plot the ECDFs for the 1975 and 2012 beak depth measurements on the same plot. . # Compute ECDFs x_1975, y_1975 = ecdf(bd_1975) x_2012, y_2012 = ecdf(bd_2012) # Plot the ECDFs _ = plt.plot(x_1975, y_1975, marker=&#39;.&#39;, linestyle=&#39;none&#39;) _ = plt.plot(x_2012, y_2012, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Set margins plt.margins(0.02) # Add axis labels and legend _ = plt.xlabel(&#39;beak depth (mm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) _ = plt.legend((&#39;1975&#39;, &#39;2012&#39;), loc=&#39;lower right&#39;) # Show the plot plt.show() . . The differences are much clearer in the ECDF. The mean is larger in the 2012 data, and the variance does appear larger as well. . #### Parameter estimates of beak depths . Estimate the difference of the mean beak depth of the G. scandens samples from 1975 and 2012 and report a 95% confidence interval. . def draw_bs_reps(data, func, size=1): &quot;&quot;&quot;Draw bootstrap replicates.&quot;&quot;&quot; # Initialize array of replicates: bs_replicates bs_replicates = np.empty(size) # Generate replicates for i in range(size): bs_replicates[i] = bootstrap_replicate_1d(data, func) return bs_replicates # Compute the difference of the sample means: mean_diff mean_diff = np.mean(bd_2012) - np.mean(bd_1975) # Get bootstrap replicates of means bs_replicates_1975 = draw_bs_reps(bd_1975, np.mean, size=10000) bs_replicates_2012 = draw_bs_reps(bd_2012, np.mean, size=10000) # Compute samples of difference of means: bs_diff_replicates bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975 # Compute 95% confidence interval: conf_int conf_int = np.percentile(bs_diff_replicates, [2.5, 97.5]) # Print the results print(&#39;difference of means =&#39;, mean_diff, &#39;mm&#39;) print(&#39;95% confidence interval =&#39;, conf_int, &#39;mm&#39;) # difference of means = 0.22622047244094645 mm # 95% confidence interval = [0.05633521 0.39190544] mm . #### Hypothesis test: Are beaks deeper in 2012? . Your plot of the ECDF and determination of the confidence interval make it pretty clear that the beaks of G. scandens on Daphne Major have gotten deeper. But is it possible that this effect is just due to random chance? In other words, what is the probability that we would get the observed difference in mean beak depth if the means were the same? . Be careful! The hypothesis we are testing is not that the beak depths come from the same distribution. For that we could use a permutation test. The hypothesis is that the means are equal. To perform this hypothesis test, we need to shift the two data sets so that they have the same mean and then use bootstrap sampling to compute the difference of means. . # Compute mean of combined data set: combined_mean combined_mean = np.mean(np.concatenate((bd_1975, bd_2012))) # Shift the samples # why shift the mean? # to make np.mean(bd_1975_shifted) - np.mean(bd_2012_shifted) = 0 #1 # why make #1 = 0? # because our hypothesis is &quot;beak depth are the same in 1975 and 2012&quot; bd_1975_shifted = bd_1975 - np.mean(bd_1975) + combined_mean bd_2012_shifted = bd_2012 - np.mean(bd_2012) + combined_mean # Get bootstrap replicates of shifted data sets bs_replicates_1975 = draw_bs_reps(bd_1975_shifted, np.mean, size=10000) bs_replicates_2012 = draw_bs_reps(bd_2012_shifted, np.mean, size=10000) # Compute replicates of difference of means: bs_diff_replicates bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975 # Compute the p-value p = np.sum(bs_diff_replicates &gt;= mean_diff) / len(bs_diff_replicates) # Print p-value print(&#39;p =&#39;, p) # p = 0.0034 # p = 0.34% . We get a p-value of 0.0034, which suggests that there is a statistically significant difference. But remember: it is very important to know how different they are! In the previous exercise, you got a difference of 0.2 mm between the means. You should combine this with the statistical significance. Changing by 0.2 mm in 37 years is substantial by evolutionary standards. If it kept changing at that rate, the beak depth would double in only 400 years. . ### Variation of beak shapes . #### EDA of beak length and depth . The beak length data are stored as bl_1975 and bl_2012 , again with units of millimeters (mm). You still have the beak depth data stored in bd_1975 and bd_2012 . Make scatter plots of beak depth (y-axis) versus beak length (x-axis) for the 1975 and 2012 specimens. . # Make scatter plot of 1975 data _ = plt.plot(bl_1975, bd_1975, marker=&#39;.&#39;, linestyle=&#39;None&#39;, color=&#39;blue&#39;, alpha=0.5) # Make scatter plot of 2012 data _ = plt.plot(bl_2012, bd_2012, marker=&#39;.&#39;, linestyle=&#39;None&#39;, color=&#39;red&#39;, alpha=0.5) # Label axes and make legend _ = plt.xlabel(&#39;beak length (mm)&#39;) _ = plt.ylabel(&#39;beak depth (mm)&#39;) _ = plt.legend((&#39;1975&#39;, &#39;2012&#39;), loc=&#39;upper left&#39;) # Show the plot plt.show() . . In looking at the plot, we see that beaks got deeper (the red points are higher up in the y-direction), but not really longer. If anything, they got a bit shorter, since the red dots are to the left of the blue dots. So, it does not look like the beaks kept the same shape; they became shorter and deeper. . #### Linear regressions . Perform a linear regression for both the 1975 and 2012 data. Then, perform pairs bootstrap estimates for the regression parameters. Report 95% confidence intervals on the slope and intercept of the regression line. . def draw_bs_pairs_linreg(x, y, size=1): &quot;&quot;&quot;Perform pairs bootstrap for linear regression.&quot;&quot;&quot; # Set up array of indices to sample from: inds inds = np.arange(len(x)) # Initialize replicates: bs_slope_reps, bs_intercept_reps bs_slope_reps = np.empty(size) bs_intercept_reps = np.empty(size) # Generate replicates for i in range(size): bs_inds = np.random.choice(inds, size=len(inds)) bs_x, bs_y = x[bs_inds], y[bs_inds] bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, deg=1) return bs_slope_reps, bs_intercept_reps # Compute the linear regressions slope_1975, intercept_1975 = np.polyfit(bl_1975, bd_1975, deg=1) slope_2012, intercept_2012 = np.polyfit(bl_2012, bd_2012, deg=1) # Perform pairs bootstrap for the linear regressions bs_slope_reps_1975, bs_intercept_reps_1975 = draw_bs_pairs_linreg(bl_1975, bd_1975, size=1000) bs_slope_reps_2012, bs_intercept_reps_2012 = draw_bs_pairs_linreg(bl_2012, bd_2012, size=1000) # Compute confidence intervals of slopes slope_conf_int_1975 = np.percentile(bs_slope_reps_1975, [2.5, 97.5]) slope_conf_int_2012 = np.percentile(bs_slope_reps_2012, [2.5, 97.5]) intercept_conf_int_1975 = np.percentile(bs_intercept_reps_1975, [2.5, 97.5]) intercept_conf_int_2012 = np.percentile(bs_intercept_reps_2012, [2.5, 97.5]) # Print the results print(&#39;1975: slope =&#39;, slope_1975, &#39;conf int =&#39;, slope_conf_int_1975) print(&#39;1975: intercept =&#39;, intercept_1975, &#39;conf int =&#39;, intercept_conf_int_1975) print(&#39;2012: slope =&#39;, slope_2012, &#39;conf int =&#39;, slope_conf_int_2012) print(&#39;2012: intercept =&#39;, intercept_2012, &#39;conf int =&#39;, intercept_conf_int_2012) # 1975: slope = 0.4652051691605937 conf int = [0.33851226 0.59306491] # 1975: intercept = 2.3908752365842263 conf int = [0.64892945 4.18037063] # 2012: slope = 0.462630358835313 conf int = [0.33137479 0.60695527] # 2012: intercept = 2.977247498236019 conf int = [1.06792753 4.70599387] . It looks like they have the same slope, but different intercepts. . #### Displaying the linear regression results . # Make scatter plot of 1975 data _ = plt.plot(bl_1975, bd_1975, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;, alpha=0.5) # Make scatter plot of 2012 data _ = plt.plot(bl_2012, bd_2012, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;, alpha=0.5) # Label axes and make legend _ = plt.xlabel(&#39;beak length (mm)&#39;) _ = plt.ylabel(&#39;beak depth (mm)&#39;) _ = plt.legend((&#39;1975&#39;, &#39;2012&#39;), loc=&#39;upper left&#39;) # Generate x-values for bootstrap lines: x x = np.array([10, 17]) # Plot the bootstrap lines for i in range(100): plt.plot(x, bs_slope_reps_1975[i] * x + bs_intercept_reps_1975[i], linewidth=0.5, alpha=0.2, color=&#39;blue&#39;) plt.plot(x, bs_slope_reps_2012[i] * x + bs_intercept_reps_2012[i], linewidth=0.5, alpha=0.2, color=&#39;red&#39;) # Draw the plot again plt.show() . . #### Beak length to depth ratio . The linear regressions showed interesting information about the beak geometry. The slope was the same in 1975 and 2012, suggesting that for every millimeter gained in beak length, the birds gained about half a millimeter in depth in both years. However, if we are interested in the shape of the beak, we want to compare the ratio of beak length to beak depth. Let’s make that comparison. . # Compute length-to-depth ratios ratio_1975 = bl_1975 / bd_1975 ratio_2012 = bl_2012 / bd_2012 # Compute means mean_ratio_1975 = np.mean(ratio_1975) mean_ratio_2012 = np.mean(ratio_2012) # Generate bootstrap replicates of the means bs_replicates_1975 = draw_bs_reps(ratio_1975, np.mean, size=10000) bs_replicates_2012 = draw_bs_reps(ratio_2012, np.mean, size=10000) # Compute the 99% confidence intervals conf_int_1975 = np.percentile(bs_replicates_1975, [0.5, 99.5]) conf_int_2012 = np.percentile(bs_replicates_2012, [0.5, 99.5]) # Print the results print(&#39;1975: mean ratio =&#39;, mean_ratio_1975, &#39;conf int =&#39;, conf_int_1975) print(&#39;2012: mean ratio =&#39;, mean_ratio_2012, &#39;conf int =&#39;, conf_int_2012) # 1975: mean ratio = 1.5788823771858533 conf int = [1.55668803 1.60073509] # 2012: mean ratio = 1.4658342276847767 conf int = [1.44363932 1.48729149] . #### How different is the ratio? . In the previous exercise, you computed the mean beak length to depth ratio with 99% confidence intervals for 1975 and for 2012. The results of that calculation are shown graphically in the plot accompanying this problem. In addition to these results, what would you say about the ratio of beak length to depth? . . The mean beak length-to-depth ratio decreased by about 0.1, or 7%, from 1975 to 2012. The 99% confidence intervals are not even close to overlapping, so this is a real change. The beak shape changed. . ### Calculation of heritability . #### EDA of heritability . The array bd_parent_scandens contains the average beak depth (in mm) of two parents of the species G. scandens . The array bd_offspring_scandens contains the average beak depth of the offspring of the respective parents. The arrays bd_parent_fortis and bd_offspring_fortis contain the same information about measurements from G. fortis birds. . Make a scatter plot of the average offspring beak depth (y-axis) versus average parental beak depth (x-axis) for both species. Use the alpha=0.5 keyword argument to help you see overlapping points. . # Make scatter plots _ = plt.plot(bd_parent_fortis, bd_offspring_fortis, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;, alpha=0.5) _ = plt.plot(bd_parent_scandens, bd_offspring_scandens, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;, alpha=0.5) # Label axes _ = plt.xlabel(&#39;parental beak depth (mm)&#39;) _ = plt.ylabel(&#39;offspring beak depth (mm)&#39;) # Add legend _ = plt.legend((&#39;G. fortis&#39;, &#39;G. scandens&#39;), loc=&#39;lower right&#39;) # Show plot plt.show() . . It appears as though there is a stronger correlation in G. fortis than than in G. scandens . This suggests that beak depth is more strongly inherited in G. fortis . We’ll quantify this correlation next. . #### Correlation of offspring and parental data . In an effort to quantify the correlation between offspring and parent beak depths, we would like to compute statistics, such as the Pearson correlation coefficient, between parents and offspring. To get confidence intervals on this, we need to do a pairs bootstrap. . You have already written a function to do pairs bootstrap to get estimates for parameters derived from linear regression. Your task in this exercise is to make a new function with call signature draw_bs_pairs(x, y, func, size=1) that performs pairs bootstrap and computes a single statistic on pairs samples defined. The statistic of interest is computed by calling func(bs_x, bs_y) . In the next exercise, you will use pearson_r for func . . def draw_bs_pairs(x, y, func, size=1): &quot;&quot;&quot;Perform pairs bootstrap for a single statistic.&quot;&quot;&quot; # Set up array of indices to sample from: inds inds = np.arange(len(x)) # Initialize replicates: bs_replicates bs_replicates = np.empty(size) # Generate replicates for i in range(size): bs_inds = np.random.choice(inds, size=len(inds)) bs_x, bs_y = x[bs_inds], y[bs_inds] bs_replicates[i] = func(bs_x, bs_y) return bs_replicates . #### Pearson correlation of offspring and parental data . The Pearson correlation coefficient seems like a useful measure of how strongly the beak depth of parents are inherited by their offspring. Compute the Pearson correlation coefficient between parental and offspring beak depths for G. scandens . Do the same for G. fortis . Then, use the function you wrote in the last exercise to compute a 95% confidence interval using pairs bootstrap. . def pearson_r(x, y): &quot;&quot;&quot;Compute Pearson correlation coefficient between two arrays.&quot;&quot;&quot; # Compute correlation matrix: corr_mat corr_mat = np.corrcoef(x, y) # Return entry [0,1] return corr_mat[0,1] # Compute the Pearson correlation coefficients r_scandens = pearson_r(bd_parent_scandens, bd_offspring_scandens) r_fortis = pearson_r(bd_parent_fortis, bd_offspring_fortis) # Acquire 1000 bootstrap replicates of Pearson r bs_replicates_scandens = draw_bs_pairs(bd_parent_scandens, bd_offspring_scandens, pearson_r, size=1000) bs_replicates_fortis = draw_bs_pairs(bd_parent_fortis, bd_offspring_fortis, pearson_r, size=1000) # Compute 95% confidence intervals conf_int_scandens = np.percentile(bs_replicates_scandens, [2.5, 97.5]) conf_int_fortis = np.percentile(bs_replicates_fortis, [2.5, 97.5]) # Print results print(&#39;G. scandens:&#39;, r_scandens, conf_int_scandens) print(&#39;G. fortis:&#39;, r_fortis, conf_int_fortis) # G. scandens: 0.4117063629401258 [0.26564228 0.54388972] # G. fortis: 0.7283412395518487 [0.6694112 0.77840616] . It is clear from the confidence intervals that beak depth of the offspring of G. fortis parents is more strongly correlated with their offspring than their G. scandens counterparts. . #### Measuring heritability . Remember that the Pearson correlation coefficient is the ratio of the covariance to the geometric mean of the variances of the two data sets. This is a measure of the correlation between parents and offspring, but might not be the best estimate of heritability. If we stop and think, it makes more sense to define heritability as the ratio of the covariance between parent and offspring to the variance of the parents alone . In this exercise, you will estimate the heritability and perform a pairs bootstrap calculation to get the 95% confidence interval. . This exercise highlights a very important point. Statistical inference (and data analysis in general) is not a plug-n-chug enterprise. You need to think carefully about the questions you are seeking to answer with your data and analyze them appropriately. If you are interested in how heritable traits are, the quantity we defined as the heritability is more apt than the off-the-shelf statistic, the Pearson correlation coefficient. . def heritability(parents, offspring): &quot;&quot;&quot;Compute the heritability from parent and offspring samples.&quot;&quot;&quot; covariance_matrix = np.cov(parents, offspring) return covariance_matrix[0,1] / covariance_matrix[0,0] # Compute the heritability heritability_scandens = heritability(bd_parent_scandens, bd_offspring_scandens) heritability_fortis = heritability(bd_parent_fortis, bd_offspring_fortis) # Acquire 1000 bootstrap replicates of heritability replicates_scandens = draw_bs_pairs( bd_parent_scandens, bd_offspring_scandens, heritability, size=1000) replicates_fortis = draw_bs_pairs( bd_parent_fortis, bd_offspring_fortis, heritability, size=1000) # Compute 95% confidence intervals conf_int_scandens = np.percentile(replicates_scandens, [2.5, 97.5]) conf_int_fortis = np.percentile(replicates_fortis, [2.5, 97.5]) # Print results print(&#39;G. scandens:&#39;, heritability_scandens, conf_int_scandens) print(&#39;G. fortis:&#39;, heritability_fortis, conf_int_fortis) # G. scandens: 0.5485340868685982 [0.34395487 0.75638267] # G. fortis: 0.7229051911438159 [0.64655013 0.79688342] . Here again, we see that G. fortis has stronger heritability than G. scandens . This suggests that the traits of G. fortis may be strongly incorporated into G. scandens by introgressive hybridization. . #### Is beak depth heritable at all in G. scandens? . The heritability of beak depth in G. scandens seems low. It could be that this observed heritability was just achieved by chance and beak depth is actually not really heritable in the species . You will test that hypothesis here. To do this, you will do a pairs permutation test. . # Initialize array of replicates: perm_replicates perm_replicates = np.empty(10000) # Draw replicates for i in range(10000): # Permute parent beak depths bd_parent_permuted = np.random.permutation(bd_parent_scandens) perm_replicates[i] = heritability(bd_parent_permuted, bd_offspring_scandens) # Compute p-value: p p = np.sum(perm_replicates &gt;= heritability_scandens) / len(perm_replicates) # Print the p-value print(&#39;p-val =&#39;, p) # p-val = 0.0 . You get a p-value of zero, which means that none of the 10,000 permutation pairs replicates you drew had a heritability high enough to match that which was observed. This strongly suggests that beak depth is heritable in G. scandens , just not as much as in G. fortis . If you like, you can plot a histogram of the heritability replicates to get a feel for how extreme of a value of heritability you might expect by chance. . plt.hist(perm_replicates) plt.axvline(x=heritability_scandens, color = &#39;red&#39;) plt.text(heritability_scandens, 1500, &#39;heritability_scandens&#39;, ha=&#39;center&#39;, va=&#39;center&#39;,rotation=&#39;vertical&#39;, backgroundcolor=&#39;white&#39;) plt.show() . Parameter estimation by optimization . ### Optimal parameters . #### How often do we get no-hitters? . The number of games played between each no-hitter in the modern era (1901-2015) of Major League Baseball is stored in the array nohitter_times . . If you assume that no-hitters are described as a Poisson process, then the time between no-hitters is Exponentially distributed. As you have seen, the Exponential distribution has a single parameter, which we will call ττ, the typical interval time. The value of the parameter ττ that makes the exponential distribution best match the data is the mean interval time (where time is in units of number of games) between no-hitters. . Compute the value of this parameter from the data. Then, use np.random.exponential() to “repeat” the history of Major League Baseball by drawing inter-no-hitter times from an exponential distribution with the ττ you found and plot the histogram as an approximation to the PDF. . # Seed random number generator np.random.seed(42) # Compute mean no-hitter time: tau tau = np.mean(nohitter_times) # Draw out of an exponential distribution with parameter tau: inter_nohitter_time inter_nohitter_time = np.random.exponential(tau, 100000) # Plot the PDF and label axes _ = plt.hist(inter_nohitter_time, bins=50, normed=True, histtype=&#39;step&#39;) _ = plt.xlabel(&#39;Games between no-hitters&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() . . We see the typical shape of the Exponential distribution, going from a maximum at 0 and decaying to the right. . #### Do the data follow our story? . # Create an ECDF from real data: x, y x, y = ecdf(nohitter_times) # Create a CDF from theoretical samples: x_theor, y_theor x_theor, y_theor = ecdf(inter_nohitter_time) # Overlay the plots plt.plot(x_theor, y_theor) plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Margins and axis labels plt.margins(0.02) plt.xlabel(&#39;Games between no-hitters&#39;) plt.ylabel(&#39;CDF&#39;) # Show the plot plt.show() . . It looks like no-hitters in the modern era of Major League Baseball are Exponentially distributed. Based on the story of the Exponential distribution, this suggests that they are a random process; when a no-hitter will happen is independent of when the last no-hitter was. . #### How is this parameter optimal? . # Plot the theoretical CDFs plt.plot(x_theor, y_theor) plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;) plt.margins(0.02) plt.xlabel(&#39;Games between no-hitters&#39;) plt.ylabel(&#39;CDF&#39;) # Take samples with half tau: samples_half samples_half = np.random.exponential(tau/2, size=10000) # Take samples with double tau: samples_double samples_double = np.random.exponential(2*tau, size=10000) # Generate CDFs from these samples x_half, y_half = ecdf(samples_half) x_double, y_double = ecdf(samples_double) # Plot these CDFs as lines _ = plt.plot(x_half, y_half) _ = plt.plot(x_double, y_double) # Show the plot plt.show() . . red = half, purple = double . Notice how the value of tau given by the mean matches the data best. In this way, tau is an optimal parameter. . ### Linear regression by least squares . #### EDA of literacy/fertility data . # Plot the illiteracy rate versus fertility _ = plt.plot(illiteracy, fertility, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Set the margins and label axes plt.margins(0.02) _ = plt.xlabel(&#39;percent illiterate&#39;) _ = plt.ylabel(&#39;fertility&#39;) # Show the plot plt.show() # Show the Pearson correlation coefficient print(pearson_r(illiteracy, fertility)) 0.8041324026815344 . . You can see the correlation between illiteracy and fertility by eye, and by the substantial Pearson correlation coefficient of 0.8. It is difficult to resolve in the scatter plot, but there are many points around near-zero illiteracy and about 1.8 children/woman. . #### Linear regression . # Plot the illiteracy rate versus fertility _ = plt.plot(illiteracy, fertility, marker=&#39;.&#39;, linestyle=&#39;none&#39;) plt.margins(0.02) _ = plt.xlabel(&#39;percent illiterate&#39;) _ = plt.ylabel(&#39;fertility&#39;) # Perform a linear regression using np.polyfit(): a, b a, b = np.polyfit(illiteracy, fertility, deg=1) # Print the results to the screen print(&#39;slope =&#39;, a, &#39;children per woman / percent illiterate&#39;) print(&#39;intercept =&#39;, b, &#39;children per woman&#39;) # Make theoretical line to plot x = np.array([0, 100]) y = a * x + b # Add regression line to your plot _ = plt.plot(x, y) # Draw the plot plt.show() # slope = 0.04979854809063423 children per woman / percent illiterate # intercept = 1.888050610636557 children per woman . . #### How is it optimal? . # Specify slopes to consider: a_vals a_vals = np.linspace(0, 0.1, 200) # Initialize sum of square of residuals: rss rss = np.empty_like(a_vals) # Compute sum of square of residuals for each value of a_vals for i, a in enumerate(a_vals): rss[i] = np.sum((fertility - a*illiteracy - b)**2) # Plot the RSS plt.plot(a_vals, rss, &#39;-&#39;) plt.xlabel(&#39;slope (children per woman / percent illiterate)&#39;) plt.ylabel(&#39;sum of square of residuals&#39;) plt.show() . . Notice that the minimum on the plot, that is the value of the slope that gives the minimum sum of the square of the residuals, is the same value you got when performing the regression. . ### The importance of EDA: Anscombe’s quartet . . #### The importance of EDA . Why should exploratory data analysis be the first step in an analysis of data (after getting your data imported and cleaned, of course)? . You can be protected from misinterpretation of the type demonstrated by Anscombe’s quartet. | EDA provides a good starting point for planning the rest of your analysis. | EDA is not really any more difficult than any of the subsequent analysis, so there is no excuse for not exploring the data. | . #### Linear regression on appropriate Anscombe data . # Perform linear regression: a, b a, b = np.polyfit(x, y, deg=1) # Print the slope and intercept print(a, b) # Generate theoretical x and y data: x_theor, y_theor x_theor = np.array([3, 15]) y_theor = a * x_theor + b # Plot the Anscombe data and theoretical line _ = plt.plot(x, y, marker = &#39;.&#39;, linestyle = &#39;none&#39;) _ = plt.plot(x_theor, y_theor) # Label the axes plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;) # Show the plot plt.show() . . #### Linear regression on all Anscombe data . for i in range(4): plt.subplot(2,2,i+1) # plot the scatter plot plt.plot(anscombe_x[i], anscombe_y[i], marker = &#39;.&#39;, linestyle = &#39;none&#39;) # plot the regression line a, b = np.polyfit(anscombe_x[i], anscombe_y[i], deg=1) x_theor = np.array([np.min(anscombe_x[i]), np.max(anscombe_x[i])]) y_theor = a * x_theor + b plt.plot(x_theor, y_theor) # add label plt.xlabel(&#39;x&#39; + str(i+1)) plt.ylabel(&#39;y&#39; + str(i+1)) plt.show() # slope1: 0.5000909090909095 intercept: 3.000090909090909 # slope2: 0.5000000000000004 intercept: 3.0009090909090896 # slope3: 0.4997272727272731 intercept: 3.0024545454545453 # slope4: 0.4999090909090908 intercept: 3.0017272727272735 . . ### Generating bootstrap replicates . #### Getting the terminology down . If we have a data set with n repeated measurements, a bootstrap sample is an array of length n that was drawn from the original data with replacement. . Bootstrap replicate is a single value of a statistic computed from a bootstrap sample. . #### Visualizing bootstrap samples . np.random.choice() . for _ in range(50): # Generate bootstrap sample: bs_sample bs_sample = np.random.choice(rainfall, size=len(rainfall)) # Compute and plot ECDF from bootstrap sample x, y = ecdf(bs_sample) _ = plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;gray&#39;, alpha=0.1) # Compute and plot ECDF from original data x, y = ecdf(rainfall) _ = plt.plot(x, y, marker=&#39;.&#39;) # Make margins and label axes plt.margins(0.02) _ = plt.xlabel(&#39;yearly rainfall (mm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) # Show the plot plt.show() . . Bootstrap confidence intervals . #### Generating many bootstrap replicates . def bootstrap_replicate_1d(data, func): return func(np.random.choice(data, size=len(data))) def draw_bs_reps(data, func, size=1): &quot;&quot;&quot;Draw bootstrap replicates.&quot;&quot;&quot; # Initialize array of replicates: bs_replicates bs_replicates = np.empty(size) # Generate replicates for i in range(size): bs_replicates[i] = bootstrap_replicate_1d(data, func) return bs_replicates . #### Bootstrap replicates of the mean and the SEM ( standard error of the mean ) . In fact, it can be shown theoretically that under not-too-restrictive conditions, the value of the mean will always be Normally distributed. (This does not hold in general, just for the mean and a few other statistics.) . The standard deviation of this distribution, called the standard error of the mean , or SEM, is given by the standard deviation of the data divided by the square root of the number of data points. I.e., for a data set, sem = np.std(data) / np.sqrt(len(data)) . Using hacker statistics, you get this same result without the need to derive it, but you will verify this result from your bootstrap replicates. . # Take 10,000 bootstrap replicates of the mean: bs_replicates bs_replicates = draw_bs_reps(rainfall, np.mean, size=10000) # Compute and print SEM sem = np.std(rainfall) / np.sqrt(len(rainfall)) print(sem) # Compute and print standard deviation of bootstrap replicates bs_std = np.std(bs_replicates) print(bs_std) # Make a histogram of the results _ = plt.hist(bs_replicates, bins=50, normed=True) _ = plt.xlabel(&#39;mean annual rainfall (mm)&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() # 10.51054915050619 # 10.465927071184412 . . #### Confidence intervals of rainfall data . Use the bootstrap replicates you just generated to compute the 95% confidence interval. That is, give the 2.5th and 97.5th percentile of your bootstrap replicates stored as bs_replicates . What is the 95% confidence interval? . np.percentile(bs_replicates,2.5) 779.7699248120301 np.percentile(bs_replicates,97.5) 820.950432330827 . #### Bootstrap replicates of other statistics . def draw_bs_reps(data, func, size=1): return np.array([bootstrap_replicate_1d(data, func) for _ in range(size)]) # Generate 10,000 bootstrap replicates of the variance: bs_replicates bs_replicates = draw_bs_reps(rainfall, np.var, size=10000) # Put the variance in units of square centimeters bs_replicates /= 100 # Make a histogram of the results _ = plt.hist(bs_replicates, bins=50, normed=True) _ = plt.xlabel(&#39;variance of annual rainfall (sq. cm)&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() . . This is not normally distributed, as it has a longer tail to the right. Note that you can also compute a confidence interval on the variance, or any other statistic, using np.percentile() with your bootstrap replicates. . #### Confidence interval on the rate of no-hitters . # Draw bootstrap replicates of the mean no-hitter time (equal to tau): bs_replicates bs_replicates = draw_bs_reps(nohitter_times, np.mean, size=10000) # Compute the 95% confidence interval: conf_int conf_int = np.percentile(bs_replicates, [2.5, 97.5]) # Print the confidence interval print(&#39;95% confidence interval =&#39;, conf_int, &#39;games&#39;) # Plot the histogram of the replicates _ = plt.hist(bs_replicates, bins=50, normed=True) _ = plt.xlabel(r&#39;$ tau$ (games)&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() # 95% confidence interval = [660.67280876 871.63077689] games . . This gives you an estimate of what the typical time between no-hitters is. It could be anywhere between 660 and 870 games. . ### Pairs bootstrap . #### A function to do pairs bootstrap . def draw_bs_pairs_linreg(x, y, size=1): &quot;&quot;&quot;Perform pairs bootstrap for linear regression.&quot;&quot;&quot; # Set up array of indices to sample from: inds inds = np.arange(len(x)) # Initialize replicates: bs_slope_reps, bs_intercept_reps bs_slope_reps = np.empty(size) bs_intercept_reps = np.empty(size) # Generate replicates for i in range(size): bs_inds = np.random.choice(inds, size=len(inds)) bs_x, bs_y = x[bs_inds], y[bs_inds] bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, deg=1) return bs_slope_reps, bs_intercept_reps . #### Pairs bootstrap of literacy/fertility data . # Generate replicates of slope and intercept using pairs bootstrap bs_slope_reps, bs_intercept_reps = draw_bs_pairs_linreg(illiteracy, fertility, size=1000) # Compute and print 95% CI for slope print(np.percentile(bs_slope_reps, [2.5, 97.5])) # Plot the histogram _ = plt.hist(bs_slope_reps, bins=50, normed=True) _ = plt.xlabel(&#39;slope&#39;) _ = plt.ylabel(&#39;PDF&#39;) plt.show() # [0.04378061 0.0551616 ] . . #### Plotting bootstrap regressions . # Generate array of x-values for bootstrap lines: x x = np.array([0, 100]) # Plot the bootstrap lines for i in range(100): _ = plt.plot(x, bs_slope_reps[i]*x + bs_intercept_reps[i], linewidth=0.5, alpha=0.2, color=&#39;red&#39;) # Plot the data _ = plt.plot(illiteracy, fertility, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Label axes, set the margins, and show the plot _ = plt.xlabel(&#39;illiteracy&#39;) _ = plt.ylabel(&#39;fertility&#39;) plt.margins(0.02) plt.show() . . Introduction to hypothesis testing . ### Formulating and simulating a hypothesis . Null hypothesis another name for the hypothesis you are testing Permutation random reordering of entries in an array #### Generating a permutation sample . np.random.permutation() . Permutation sampling is a great way to simulate the hypothesis that two variables have identical probability distributions. This is often a hypothesis you want to test, so in this exercise, you will write a function to generate a permutation sample from two data sets. . def permutation_sample(data1, data2): &quot;&quot;&quot;Generate a permutation sample from two data sets.&quot;&quot;&quot; # Concatenate the data sets: data data = np.concatenate((data1, data2)) # Permute the concatenated array: permuted_data permuted_data = np.random.permutation(data) # Split the permuted array into two: perm_sample_1, perm_sample_2 perm_sample_1 = permuted_data[:len(data1)] perm_sample_2 = permuted_data[len(data1):] return perm_sample_1, perm_sample_2 . #### Visualizing permutation sampling . for _ in range(50): # Generate permutation samples perm_sample_1, perm_sample_2 = permutation_sample(rain_june, rain_november) # Compute ECDFs x_1, y_1 = ecdf(perm_sample_1) x_2, y_2 = ecdf(perm_sample_2) # Plot ECDFs of permutation sample _ = plt.plot(x_1, y_1, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;, alpha=0.02) _ = plt.plot(x_2, y_2, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;, alpha=0.02) # Create and plot ECDFs from original data x_1, y_1 = ecdf(rain_june) x_2, y_2 = ecdf(rain_november) _ = plt.plot(x_1, y_1, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;) _ = plt.plot(x_2, y_2, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;) # Label axes, set margin, and show plot plt.margins(0.02) _ = plt.xlabel(&#39;monthly rainfall (mm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) plt.show() . . Notice that the permutation samples ECDFs overlap and give a purple haze. None of the ECDFs from the permutation samples overlap with the observed data, suggesting that the hypothesis is not commensurate with the data. June and November rainfall are not identically distributed. . ### Test statistics and p-values . #### Test statistics . When performing hypothesis tests, your choice of test statistic should be pertinent to the question you are seeking to answer in your hypothesis test. . The most important thing to consider is: What are you asking? . #### What is a p-value? . The p-value is generally a measure of the probability of observing a test statistic equally or more extreme than the one you observed, given that the null hypothesis is true. . #### Generating permutation replicates . # In most circumstances, func will be a function you write yourself. def draw_perm_reps(data_1, data_2, func, size=1): &quot;&quot;&quot;Generate multiple permutation replicates.&quot;&quot;&quot; # Initialize array of replicates: perm_replicates perm_replicates = np.empty(size) for i in range(size): # Generate permutation sample perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2) # Compute the test statistic perm_replicates[i] = func(perm_sample_1, perm_sample_2) return perm_replicates . #### Look before you leap: EDA before hypothesis testing . Kleinteich and Gorb ( Sci. Rep. , 4 , 5225, 2014) performed an interesting experiment with South American horned frogs. They held a plate connected to a force transducer, along with a bait fly, in front of them. They then measured the impact force and adhesive force of the frog’s tongue when it struck the target. . Frog A is an adult and Frog B is a juvenile. The researchers measured the impact force of 20 strikes for each frog. In the next exercise, we will test the hypothesis that the two frogs have the same distribution of impact forces. But, remember, it is important to do EDA first! Let’s make a bee swarm plot for the data. They are stored in a Pandas data frame, df , where column ID is the identity of the frog and column impact_force is the impact force in Newtons (N). . df.head() ID impact_force 20 A 1.612 21 A 0.605 22 A 0.327 23 A 0.946 24 A 0.541 . # Make bee swarm plot _ = sns.swarmplot(x=&#39;ID&#39;, y=&#39;impact_force&#39;, data=df) # Label axes _ = plt.xlabel(&#39;frog&#39;) _ = plt.ylabel(&#39;impact force (N)&#39;) # Show the plot plt.show() . . Eyeballing it, it does not look like they come from the same distribution. Frog A, the adult, has three or four very hard strikes, and Frog B, the juvenile, has a couple weak ones. However, it is possible that with only 20 samples it might be too difficult to tell if they have difference distributions, so we should proceed with the hypothesis test. . #### Permutation test on frog data . The average strike force of Frog A was 0.71 Newtons (N), and that of Frog B was 0.42 N for a difference of 0.29 N. It is possible the frogs strike with the same force and this observed difference was by chance. You will compute the probability of getting at least a 0.29 N difference in mean strike force under the hypothesis that the distributions of strike forces for the two frogs are identical. We use a permutation test with a test statistic of the difference of means to test this hypothesis. . def diff_of_means(data_1, data_2): &quot;&quot;&quot;Difference in means of two arrays.&quot;&quot;&quot; # The difference of means of data_1, data_2: diff diff = np.mean(data_1) - np.mean(data_2) return diff # Compute difference of mean impact force from experiment: empirical_diff_means empirical_diff_means = diff_of_means(force_a, force_b) # Draw 10,000 permutation replicates: perm_replicates perm_replicates = draw_perm_reps(force_a, force_b, diff_of_means, size=10000) # Compute p-value: p p = np.sum(perm_replicates &gt;= empirical_diff_means) / len(perm_replicates) # Print the result print(&#39;p-value =&#39;, p) # p-value = 0.0063 # p-value = 0.63% . The p-value tells you that there is about a 0.6% chance that you would get the difference of means observed in the experiment if frogs were exactly the same. . A p-value below 0.01 is typically said to be “statistically significant,” but: warning! warning! warning! You have computed a p-value; it is a number. I encourage you not to distill it to a yes-or-no phrase. p = 0.006 and p = 0.000000006 are both said to be “statistically significant,” but they are definitely not the same! . ### Bootstrap hypothesis tests . . #### A one-sample bootstrap hypothesis test . Another juvenile frog was studied, Frog C, and you want to see if Frog B and Frog C have similar impact forces. Unfortunately, you do not have Frog C’s impact forces available, but you know they have a mean of 0.55 N. Because you don’t have the original data, you cannot do a permutation test, and you cannot assess the hypothesis that the forces from Frog B and Frog C come from the same distribution. You will therefore test another, less restrictive hypothesis: The mean strike force of Frog B is equal to that of Frog C. . To set up the bootstrap hypothesis test, you will take the mean as our test statistic. Remember, your goal is to calculate the probability of getting a mean impact force less than or equal to what was observed for Frog B if the hypothesis that the true mean of Frog B’s impact forces is equal to that of Frog C is true . You first translate all of the data of Frog B such that the mean is 0.55 N. This involves adding the mean force of Frog C and subtracting the mean force of Frog B from each measurement of Frog B. This leaves other properties of Frog B’s distribution, such as the variance, unchanged. . # Make an array of translated impact forces: translated_force_b translated_force_b = force_b - np.mean(force_b) + 0.55 # Take bootstrap replicates of Frog B&#39;s translated impact forces: bs_replicates bs_replicates = draw_bs_reps(translated_force_b, np.mean, 10000) # Compute fraction of replicates that are less than the observed Frog B force: p p = np.sum(bs_replicates &lt;= np.mean(force_b)) / 10000 # Print the p-value print(&#39;p = &#39;, p) # p = 0.0046 # p = 0.46% . The low p-value suggests that the null hypothesis that Frog B and Frog C have the same mean impact force is false. . #### A two-sample bootstrap hypothesis test for difference of means . We now want to test the hypothesis that Frog A and Frog B have the same mean impact force, but not necessarily the same distribution, which is also impossible with a permutation test. . To do the two-sample bootstrap test, we shift both arrays to have the same mean, since we are simulating the hypothesis that their means are, in fact, equal. We then draw bootstrap samples out of the shifted arrays and compute the difference in means. This constitutes a bootstrap replicate, and we generate many of them. The p-value is the fraction of replicates with a difference in means greater than or equal to what was observed. . # Compute mean of all forces: mean_force mean_force = np.mean(forces_concat) # Generate shifted arrays force_a_shifted = force_a - np.mean(force_a) + mean_force force_b_shifted = force_b - np.mean(force_b) + mean_force # Compute 10,000 bootstrap replicates from shifted arrays bs_replicates_a = draw_bs_reps(force_a_shifted, np.mean, size=10000) bs_replicates_b = draw_bs_reps(force_b_shifted, np.mean, size=10000) # Get replicates of difference of means: bs_replicates bs_replicates = bs_replicates_a - bs_replicates_b # Compute and print p-value: p p = np.sum(bs_replicates &gt;= empirical_diff_means) / 10000 print(&#39;p-value =&#39;, p) # p-value = 0.0043 # p-value = 0.43% . You got a similar result as when you did the permutation test. Nonetheless, remember that it is important to carefully think about what question you want to ask. Are you only interested in the mean impact force, or in the distribution of impact forces? . Hypothesis test examples . ### A/B testing . #### The vote for the Civil Rights Act in 1964 . The Civil Rights Act of 1964 was one of the most important pieces of legislation ever passed in the USA. Excluding “present” and “abstain” votes, 153 House Democrats and 136 Republicans voted yea. However, 91 Democrats and 35 Republicans voted nay. Did party affiliation make a difference in the vote? . To answer this question, you will evaluate the hypothesis that the party of a House member has no bearing on his or her vote. You will use the fraction of Democrats voting in favor as your test statistic and evaluate the probability of observing a fraction of Democrats voting in favor at least as small as the observed fraction of 153/244. (That’s right, at least as small as. In 1964, it was the Democrats who were less progressive on civil rights issues.) To do this, permute the party labels of the House voters and then arbitrarily divide them into “Democrats” and “Republicans” and compute the fraction of Democrats voting yea. . # Construct arrays of data: dems, reps dems = np.array([True] * 153 + [False] * 91) reps = np.array([True] * 136 + [False] * 35) def frac_yea_dems(dems, reps): &quot;&quot;&quot;Compute fraction of Democrat yea votes.&quot;&quot;&quot; frac = np.sum(dems) / len(dems) return frac # Acquire permutation samples: perm_replicates perm_replicates = draw_perm_reps(dems, reps, frac_yea_dems, size=10000) # Compute and print p-value: p p = np.sum(perm_replicates &lt;= 153/244) / len(perm_replicates) print(&#39;p-value =&#39;, p) # p-value = 0.0002 # p-value = 0.02% . This small p-value suggests that party identity had a lot to do with the voting. Importantly, the South had a higher fraction of Democrat representatives, and consequently also a more racist bias. . #### What is equivalent? . You have experience matching a stories to probability distributions. Similarly, you use the same procedure for two different A/B tests if their stories match. In the Civil Rights Act example you just did, you performed an A/B test on voting data, which has a Yes/No type of outcome for each subject (in that case, a voter). Which of the following situations involving testing by a web-based company has an equivalent set up for an A/B test as the one you just did with the Civil Rights Act of 1964? . You measure the number of people who click on an ad on your company’s website before and after changing its color. . The “Democrats” are those who view the ad before the color change, and the “Republicans” are those who view it after. . #### A time-on-website analog . It turns out that you already did a hypothesis test analogous to an A/B test where you are interested in how much time is spent on the website before and after an ad campaign. The frog tongue force (a continuous quantity like time on the website) is an analog. “Before” = Frog A and “after” = Frog B. Let’s practice this again with something that actually is a before/after scenario. . We return to the no-hitter data set. In 1920, Major League Baseball implemented important rule changes that ended the so-called dead ball era. Importantly, the pitcher was no longer allowed to spit on or scuff the ball, an activity that greatly favors pitchers. In this problem you will perform an A/B test to determine if these rule changes resulted in a slower rate of no-hitters (i.e., longer average time between no-hitters) using the difference in mean inter-no-hitter time as your test statistic. The inter-no-hitter times for the respective eras are stored in the arrays nht_dead and nht_live , where “nht” is meant to stand for “no-hitter time.” . def diff_of_means(data_1, data_2): &quot;&quot;&quot;Difference in means of two arrays.&quot;&quot;&quot; # The difference of means of data_1, data_2: diff diff = np.mean(data_1) - np.mean(data_2) return diff def permutation_sample(data1, data2): &quot;&quot;&quot;Generate a permutation sample from two data sets.&quot;&quot;&quot; # Concatenate the data sets: data data = np.concatenate((data1, data2)) # Permute the concatenated array: permuted_data permuted_data = np.random.permutation(data) # Split the permuted array into two: perm_sample_1, perm_sample_2 perm_sample_1 = permuted_data[:len(data1)] perm_sample_2 = permuted_data[len(data1):] return perm_sample_1, perm_sample_2 def draw_perm_reps(data_1, data_2, func, size=1): &quot;&quot;&quot;Generate multiple permutation replicates.&quot;&quot;&quot; # Initialize array of replicates: perm_replicates perm_replicates = np.empty(size) for i in range(size): # Generate permutation sample perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2) # Compute the test statistic perm_replicates[i] = func(perm_sample_1, perm_sample_2) return perm_replicates # Compute the observed difference in mean inter-no-hitter times: nht_diff_obs nht_diff_obs = diff_of_means(nht_dead, nht_live) # Acquire 10,000 permutation replicates of difference in mean no-hitter time: perm_replicates perm_replicates = draw_perm_reps(nht_dead, nht_live, diff_of_means, size=10000) # Compute and print the p-value: p p = np.sum(perm_replicates &lt;= nht_diff_obs) /len(perm_replicates) print(&#39;p-val =&#39;, p) p-val = 0.0001 . Your p-value is 0.0001, which means that only one out of your 10,000 replicates had a result as extreme as the actual difference between the dead ball and live ball eras. This suggests strong statistical significance. Watch out, though, you could very well have gotten zero replicates that were as extreme as the observed value. This just means that the p-value is quite small, almost certainly smaller than 0.001. . #### What should you have done first? . That was a nice hypothesis test you just did to check out whether the rule changes in 1920 changed the rate of no-hitters. But what should you have done with the data first? . Performed EDA, perhaps plotting the ECDFs of inter-no-hitter times in the dead ball and live ball eras. . Always a good idea to do first! I encourage you to go ahead and plot the ECDFs right now. You will see by eye that the null hypothesis that the distributions are the same is almost certainly not true. . # Create and plot ECDFs x_1, y_1 = ecdf(nht_dead) x_2, y_2 = ecdf(nht_live) _ = plt.plot(x_1, y_1, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;) _ = plt.plot(x_2, y_2, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;) # Label axes, set margin, and show plot plt.margins(0.02) _ = plt.xlabel(&#39;Games between no-hitters&#39;) _ = plt.ylabel(&#39;ECDF&#39;) plt.show() . . ### Test of correlation . . #### Simulating a null hypothesis concerning correlation . The observed correlation between female illiteracy and fertility in the data set of 162 countries may just be by chance; the fertility of a given country may actually be totally independent of its illiteracy. You will test this null hypothesis in the next exercise. . To do the test, you need to simulate the data assuming the null hypothesis is true. Of the following choices, which is the best way to to do it? . Answer: Do a permutation test: Permute the illiteracy values but leave the fertility values fixed to generate a new set of (illiteracy, fertility) data. . This exactly simulates the null hypothesis and does so more efficiently than the last option. It is exact because it uses all data and eliminates any correlation because which illiteracy value pairs to which fertility value is shuffled. . Last option: Do a permutation test: Permute both the illiteracy and fertility values to generate a new set of (illiteracy, fertility data). This exactly simulates the null hypothesis and does so more efficiently than the last option. It is exact because it uses all data and eliminates any correlation because which illiteracy value pairs to which fertility value is shuffled. . #### Hypothesis test on Pearson correlation . The observed correlation between female illiteracy and fertility may just be by chance; the fertility of a given country may actually be totally independent of its illiteracy. You will test this hypothesis. To do so, permute the illiteracy values but leave the fertility values fixed. This simulates the hypothesis that they are totally independent of each other. For each permutation, compute the Pearson correlation coefficient and assess how many of your permutation replicates have a Pearson correlation coefficient greater than the observed one. . def pearson_r(x, y): &quot;&quot;&quot;Compute Pearson correlation coefficient between two arrays.&quot;&quot;&quot; # Compute correlation matrix: corr_mat corr_mat = np.corrcoef(x, y) # Return entry [0,1] return corr_mat[0,1] # Compute observed correlation: r_obs r_obs = pearson_r(illiteracy, fertility) # r_obs = 0.8041324026815344 # Initialize permutation replicates: perm_replicates perm_replicates = np.empty(10000) # Draw replicates for i in range(10000): # Permute illiteracy measurments: illiteracy_permuted illiteracy_permuted = np.random.permutation(illiteracy) # Compute Pearson correlation perm_replicates[i] = pearson_r(illiteracy_permuted, fertility) # Compute p-value: p p = np.sum(perm_replicates &gt;= r_obs) /len(perm_replicates) print(&#39;p-val =&#39;, p) # p-val = 0.0 . You got a p-value of zero. In hacker statistics, this means that your p-value is very low, since you never got a single replicate in the 10,000 you took that had a Pearson correlation greater than the observed one. You could try increasing the number of replicates you take to continue to move the upper bound on your p-value lower and lower. . #### Do neonicotinoid insecticides have unintended consequences? . As a final exercise in hypothesis testing before we put everything together in our case study in the next chapter, you will investigate the effects of neonicotinoid insecticides on bee reproduction. These insecticides are very widely used in the United States to combat aphids and other pests that damage plants. . In a recent study, Straub, et al. ( Proc. Roy. Soc. B , 2016 ) investigated the effects of neonicotinoids on the sperm of pollinating bees. In this and the next exercise, you will study how the pesticide treatment affected the count of live sperm per half milliliter of semen. . First, we will do EDA, as usual. Plot ECDFs of the alive sperm count for untreated bees (stored in the Numpy array control ) and bees treated with pesticide (stored in the Numpy array treated ). . # Compute x,y values for ECDFs x_control, y_control = ecdf(control) x_treated, y_treated = ecdf(treated) # Plot the ECDFs plt.plot(x_control, y_control, marker=&#39;.&#39;, linestyle=&#39;none&#39;) plt.plot(x_treated, y_treated, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Set the margins plt.margins(0.02) # Add a legend plt.legend((&#39;control&#39;, &#39;treated&#39;), loc=&#39;lower right&#39;) # Label axes and show plot plt.xlabel(&#39;millions of alive sperm per mL&#39;) plt.ylabel(&#39;ECDF&#39;) plt.show() . . The ECDFs show a pretty clear difference between the treatment and control; treated bees have fewer alive sperm. Let’s now do a hypothesis test in the next exercise. . #### Bootstrap hypothesis test on bee sperm counts . Now, you will test the following hypothesis: . On average, male bees treated with neonicotinoid insecticide have the same number of active sperm per milliliter of semen than do untreated male bees. . You will use the difference of means as your test statistic. . def bootstrap_replicate_1d(data, func): return func(np.random.choice(data, size=len(data))) def draw_bs_reps(data, func, size=1): &quot;&quot;&quot;Draw bootstrap replicates.&quot;&quot;&quot; # Initialize array of replicates: bs_replicates bs_replicates = np.empty(size) # Generate replicates for i in range(size): bs_replicates[i] = bootstrap_replicate_1d(data, func) return bs_replicates # Compute the difference in mean sperm count: diff_means diff_means = np.mean(control) - np.mean(treated) # Compute mean of pooled data: mean_count mean_count = np.mean(np.concatenate((control, treated))) # Generate shifted data sets control_shifted = control - np.mean(control) + mean_count treated_shifted = treated - np.mean(treated) + mean_count # Generate bootstrap replicates bs_reps_control = draw_bs_reps(control_shifted, np.mean, size=10000) bs_reps_treated = draw_bs_reps(treated_shifted, np.mean, size=10000) # Get replicates of difference of means: bs_replicates bs_replicates = bs_reps_control - bs_reps_treated # Compute and print p-value: p p = np.sum(bs_replicates &gt;= np.mean(control) - np.mean(treated)) / len(bs_replicates) print(&#39;p-value =&#39;, p) # p-value = 0.0 . The p-value is small, most likely less than 0.0001, since you never saw a bootstrap replicated with a difference of means at least as extreme as what was observed. In fact, when I did the calculation with 10 million replicates, I got a p-value of 2e-05 . Putting it all together: a case study . ### Finch beaks and the need for statistics . . . #### EDA of beak depths of Darwin’s finches . For your first foray into the Darwin finch data, you will study how the beak depth (the distance, top to bottom, of a closed beak) of the finch species Geospiza scandens has changed over time. The Grants have noticed some changes of beak geometry depending on the types of seeds available on the island, and they also noticed that there was some interbreeding with another major species on Daphne Major, Geospiza fortis . These effects can lead to changes in the species over time. . In the next few problems, you will look at the beak depth of G. scandens on Daphne Major in 1975 and in 2012. To start with, let’s plot all of the beak depth measurements in 1975 and 2012 in a bee swarm plot. . The data are stored in a pandas DataFrame called df with columns &#39;year&#39; and &#39;beak_depth&#39; . The units of beak depth are millimeters (mm). . df.head() beak_depth year 0 8.4 1975 1 8.8 1975 2 8.4 1975 3 8.0 1975 4 7.9 1975 . # Create bee swarm plot _ = sns.swarmplot(&#39;year&#39;, &#39;beak_depth&#39;, data=df) # Label the axes _ = plt.xlabel(&#39;year&#39;) _ = plt.ylabel(&#39;beak depth (mm)&#39;) # Show the plot plt.show() . . It is kind of hard to see if there is a clear difference between the 1975 and 2012 data set. Eyeballing it, it appears as though the mean of the 2012 data set might be slightly higher, and it might have a bigger variance. . #### ECDFs of beak depths . While bee swarm plots are useful, we found that ECDFs are often even better when doing EDA. Plot the ECDFs for the 1975 and 2012 beak depth measurements on the same plot. . # Compute ECDFs x_1975, y_1975 = ecdf(bd_1975) x_2012, y_2012 = ecdf(bd_2012) # Plot the ECDFs _ = plt.plot(x_1975, y_1975, marker=&#39;.&#39;, linestyle=&#39;none&#39;) _ = plt.plot(x_2012, y_2012, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Set margins plt.margins(0.02) # Add axis labels and legend _ = plt.xlabel(&#39;beak depth (mm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) _ = plt.legend((&#39;1975&#39;, &#39;2012&#39;), loc=&#39;lower right&#39;) # Show the plot plt.show() . . The differences are much clearer in the ECDF. The mean is larger in the 2012 data, and the variance does appear larger as well. . #### Parameter estimates of beak depths . Estimate the difference of the mean beak depth of the G. scandens samples from 1975 and 2012 and report a 95% confidence interval. . def draw_bs_reps(data, func, size=1): &quot;&quot;&quot;Draw bootstrap replicates.&quot;&quot;&quot; # Initialize array of replicates: bs_replicates bs_replicates = np.empty(size) # Generate replicates for i in range(size): bs_replicates[i] = bootstrap_replicate_1d(data, func) return bs_replicates # Compute the difference of the sample means: mean_diff mean_diff = np.mean(bd_2012) - np.mean(bd_1975) # Get bootstrap replicates of means bs_replicates_1975 = draw_bs_reps(bd_1975, np.mean, size=10000) bs_replicates_2012 = draw_bs_reps(bd_2012, np.mean, size=10000) # Compute samples of difference of means: bs_diff_replicates bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975 # Compute 95% confidence interval: conf_int conf_int = np.percentile(bs_diff_replicates, [2.5, 97.5]) # Print the results print(&#39;difference of means =&#39;, mean_diff, &#39;mm&#39;) print(&#39;95% confidence interval =&#39;, conf_int, &#39;mm&#39;) # difference of means = 0.22622047244094645 mm # 95% confidence interval = [0.05633521 0.39190544] mm . #### Hypothesis test: Are beaks deeper in 2012? . Your plot of the ECDF and determination of the confidence interval make it pretty clear that the beaks of G. scandens on Daphne Major have gotten deeper. But is it possible that this effect is just due to random chance? In other words, what is the probability that we would get the observed difference in mean beak depth if the means were the same? . Be careful! The hypothesis we are testing is not that the beak depths come from the same distribution. For that we could use a permutation test. The hypothesis is that the means are equal. To perform this hypothesis test, we need to shift the two data sets so that they have the same mean and then use bootstrap sampling to compute the difference of means. . # Compute mean of combined data set: combined_mean combined_mean = np.mean(np.concatenate((bd_1975, bd_2012))) # Shift the samples # why shift the mean? # to make np.mean(bd_1975_shifted) - np.mean(bd_2012_shifted) = 0 #1 # why make #1 = 0? # because our hypothesis is &quot;beak depth are the same in 1975 and 2012&quot; bd_1975_shifted = bd_1975 - np.mean(bd_1975) + combined_mean bd_2012_shifted = bd_2012 - np.mean(bd_2012) + combined_mean # Get bootstrap replicates of shifted data sets bs_replicates_1975 = draw_bs_reps(bd_1975_shifted, np.mean, size=10000) bs_replicates_2012 = draw_bs_reps(bd_2012_shifted, np.mean, size=10000) # Compute replicates of difference of means: bs_diff_replicates bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975 # Compute the p-value p = np.sum(bs_diff_replicates &gt;= mean_diff) / len(bs_diff_replicates) # Print p-value print(&#39;p =&#39;, p) # p = 0.0034 # p = 0.34% . We get a p-value of 0.0034, which suggests that there is a statistically significant difference. But remember: it is very important to know how different they are! In the previous exercise, you got a difference of 0.2 mm between the means. You should combine this with the statistical significance. Changing by 0.2 mm in 37 years is substantial by evolutionary standards. If it kept changing at that rate, the beak depth would double in only 400 years. . ### Variation of beak shapes . #### EDA of beak length and depth . The beak length data are stored as bl_1975 and bl_2012 , again with units of millimeters (mm). You still have the beak depth data stored in bd_1975 and bd_2012 . Make scatter plots of beak depth (y-axis) versus beak length (x-axis) for the 1975 and 2012 specimens. . # Make scatter plot of 1975 data _ = plt.plot(bl_1975, bd_1975, marker=&#39;.&#39;, linestyle=&#39;None&#39;, color=&#39;blue&#39;, alpha=0.5) # Make scatter plot of 2012 data _ = plt.plot(bl_2012, bd_2012, marker=&#39;.&#39;, linestyle=&#39;None&#39;, color=&#39;red&#39;, alpha=0.5) # Label axes and make legend _ = plt.xlabel(&#39;beak length (mm)&#39;) _ = plt.ylabel(&#39;beak depth (mm)&#39;) _ = plt.legend((&#39;1975&#39;, &#39;2012&#39;), loc=&#39;upper left&#39;) # Show the plot plt.show() . . In looking at the plot, we see that beaks got deeper (the red points are higher up in the y-direction), but not really longer. If anything, they got a bit shorter, since the red dots are to the left of the blue dots. So, it does not look like the beaks kept the same shape; they became shorter and deeper. . #### Linear regressions . Perform a linear regression for both the 1975 and 2012 data. Then, perform pairs bootstrap estimates for the regression parameters. Report 95% confidence intervals on the slope and intercept of the regression line. . def draw_bs_pairs_linreg(x, y, size=1): &quot;&quot;&quot;Perform pairs bootstrap for linear regression.&quot;&quot;&quot; # Set up array of indices to sample from: inds inds = np.arange(len(x)) # Initialize replicates: bs_slope_reps, bs_intercept_reps bs_slope_reps = np.empty(size) bs_intercept_reps = np.empty(size) # Generate replicates for i in range(size): bs_inds = np.random.choice(inds, size=len(inds)) bs_x, bs_y = x[bs_inds], y[bs_inds] bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, deg=1) return bs_slope_reps, bs_intercept_reps # Compute the linear regressions slope_1975, intercept_1975 = np.polyfit(bl_1975, bd_1975, deg=1) slope_2012, intercept_2012 = np.polyfit(bl_2012, bd_2012, deg=1) # Perform pairs bootstrap for the linear regressions bs_slope_reps_1975, bs_intercept_reps_1975 = draw_bs_pairs_linreg(bl_1975, bd_1975, size=1000) bs_slope_reps_2012, bs_intercept_reps_2012 = draw_bs_pairs_linreg(bl_2012, bd_2012, size=1000) # Compute confidence intervals of slopes slope_conf_int_1975 = np.percentile(bs_slope_reps_1975, [2.5, 97.5]) slope_conf_int_2012 = np.percentile(bs_slope_reps_2012, [2.5, 97.5]) intercept_conf_int_1975 = np.percentile(bs_intercept_reps_1975, [2.5, 97.5]) intercept_conf_int_2012 = np.percentile(bs_intercept_reps_2012, [2.5, 97.5]) # Print the results print(&#39;1975: slope =&#39;, slope_1975, &#39;conf int =&#39;, slope_conf_int_1975) print(&#39;1975: intercept =&#39;, intercept_1975, &#39;conf int =&#39;, intercept_conf_int_1975) print(&#39;2012: slope =&#39;, slope_2012, &#39;conf int =&#39;, slope_conf_int_2012) print(&#39;2012: intercept =&#39;, intercept_2012, &#39;conf int =&#39;, intercept_conf_int_2012) # 1975: slope = 0.4652051691605937 conf int = [0.33851226 0.59306491] # 1975: intercept = 2.3908752365842263 conf int = [0.64892945 4.18037063] # 2012: slope = 0.462630358835313 conf int = [0.33137479 0.60695527] # 2012: intercept = 2.977247498236019 conf int = [1.06792753 4.70599387] . It looks like they have the same slope, but different intercepts. . #### Displaying the linear regression results . # Make scatter plot of 1975 data _ = plt.plot(bl_1975, bd_1975, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;, alpha=0.5) # Make scatter plot of 2012 data _ = plt.plot(bl_2012, bd_2012, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;, alpha=0.5) # Label axes and make legend _ = plt.xlabel(&#39;beak length (mm)&#39;) _ = plt.ylabel(&#39;beak depth (mm)&#39;) _ = plt.legend((&#39;1975&#39;, &#39;2012&#39;), loc=&#39;upper left&#39;) # Generate x-values for bootstrap lines: x x = np.array([10, 17]) # Plot the bootstrap lines for i in range(100): plt.plot(x, bs_slope_reps_1975[i] * x + bs_intercept_reps_1975[i], linewidth=0.5, alpha=0.2, color=&#39;blue&#39;) plt.plot(x, bs_slope_reps_2012[i] * x + bs_intercept_reps_2012[i], linewidth=0.5, alpha=0.2, color=&#39;red&#39;) # Draw the plot again plt.show() . . #### Beak length to depth ratio . The linear regressions showed interesting information about the beak geometry. The slope was the same in 1975 and 2012, suggesting that for every millimeter gained in beak length, the birds gained about half a millimeter in depth in both years. However, if we are interested in the shape of the beak, we want to compare the ratio of beak length to beak depth. Let’s make that comparison. . # Compute length-to-depth ratios ratio_1975 = bl_1975 / bd_1975 ratio_2012 = bl_2012 / bd_2012 # Compute means mean_ratio_1975 = np.mean(ratio_1975) mean_ratio_2012 = np.mean(ratio_2012) # Generate bootstrap replicates of the means bs_replicates_1975 = draw_bs_reps(ratio_1975, np.mean, size=10000) bs_replicates_2012 = draw_bs_reps(ratio_2012, np.mean, size=10000) # Compute the 99% confidence intervals conf_int_1975 = np.percentile(bs_replicates_1975, [0.5, 99.5]) conf_int_2012 = np.percentile(bs_replicates_2012, [0.5, 99.5]) # Print the results print(&#39;1975: mean ratio =&#39;, mean_ratio_1975, &#39;conf int =&#39;, conf_int_1975) print(&#39;2012: mean ratio =&#39;, mean_ratio_2012, &#39;conf int =&#39;, conf_int_2012) # 1975: mean ratio = 1.5788823771858533 conf int = [1.55668803 1.60073509] # 2012: mean ratio = 1.4658342276847767 conf int = [1.44363932 1.48729149] . #### How different is the ratio? . In the previous exercise, you computed the mean beak length to depth ratio with 99% confidence intervals for 1975 and for 2012. The results of that calculation are shown graphically in the plot accompanying this problem. In addition to these results, what would you say about the ratio of beak length to depth? . . The mean beak length-to-depth ratio decreased by about 0.1, or 7%, from 1975 to 2012. The 99% confidence intervals are not even close to overlapping, so this is a real change. The beak shape changed. . ### Calculation of heritability . #### EDA of heritability . The array bd_parent_scandens contains the average beak depth (in mm) of two parents of the species G. scandens . The array bd_offspring_scandens contains the average beak depth of the offspring of the respective parents. The arrays bd_parent_fortis and bd_offspring_fortis contain the same information about measurements from G. fortis birds. . Make a scatter plot of the average offspring beak depth (y-axis) versus average parental beak depth (x-axis) for both species. Use the alpha=0.5 keyword argument to help you see overlapping points. . # Make scatter plots _ = plt.plot(bd_parent_fortis, bd_offspring_fortis, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;, alpha=0.5) _ = plt.plot(bd_parent_scandens, bd_offspring_scandens, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;, alpha=0.5) # Label axes _ = plt.xlabel(&#39;parental beak depth (mm)&#39;) _ = plt.ylabel(&#39;offspring beak depth (mm)&#39;) # Add legend _ = plt.legend((&#39;G. fortis&#39;, &#39;G. scandens&#39;), loc=&#39;lower right&#39;) # Show plot plt.show() . . It appears as though there is a stronger correlation in G. fortis than than in G. scandens . This suggests that beak depth is more strongly inherited in G. fortis . We’ll quantify this correlation next. . #### Correlation of offspring and parental data . In an effort to quantify the correlation between offspring and parent beak depths, we would like to compute statistics, such as the Pearson correlation coefficient, between parents and offspring. To get confidence intervals on this, we need to do a pairs bootstrap. . You have already written a function to do pairs bootstrap to get estimates for parameters derived from linear regression. Your task in this exercise is to make a new function with call signature draw_bs_pairs(x, y, func, size=1) that performs pairs bootstrap and computes a single statistic on pairs samples defined. The statistic of interest is computed by calling func(bs_x, bs_y) . In the next exercise, you will use pearson_r for func . . def draw_bs_pairs(x, y, func, size=1): &quot;&quot;&quot;Perform pairs bootstrap for a single statistic.&quot;&quot;&quot; # Set up array of indices to sample from: inds inds = np.arange(len(x)) # Initialize replicates: bs_replicates bs_replicates = np.empty(size) # Generate replicates for i in range(size): bs_inds = np.random.choice(inds, size=len(inds)) bs_x, bs_y = x[bs_inds], y[bs_inds] bs_replicates[i] = func(bs_x, bs_y) return bs_replicates . #### Pearson correlation of offspring and parental data . The Pearson correlation coefficient seems like a useful measure of how strongly the beak depth of parents are inherited by their offspring. Compute the Pearson correlation coefficient between parental and offspring beak depths for G. scandens . Do the same for G. fortis . Then, use the function you wrote in the last exercise to compute a 95% confidence interval using pairs bootstrap. . def pearson_r(x, y): &quot;&quot;&quot;Compute Pearson correlation coefficient between two arrays.&quot;&quot;&quot; # Compute correlation matrix: corr_mat corr_mat = np.corrcoef(x, y) # Return entry [0,1] return corr_mat[0,1] # Compute the Pearson correlation coefficients r_scandens = pearson_r(bd_parent_scandens, bd_offspring_scandens) r_fortis = pearson_r(bd_parent_fortis, bd_offspring_fortis) # Acquire 1000 bootstrap replicates of Pearson r bs_replicates_scandens = draw_bs_pairs(bd_parent_scandens, bd_offspring_scandens, pearson_r, size=1000) bs_replicates_fortis = draw_bs_pairs(bd_parent_fortis, bd_offspring_fortis, pearson_r, size=1000) # Compute 95% confidence intervals conf_int_scandens = np.percentile(bs_replicates_scandens, [2.5, 97.5]) conf_int_fortis = np.percentile(bs_replicates_fortis, [2.5, 97.5]) # Print results print(&#39;G. scandens:&#39;, r_scandens, conf_int_scandens) print(&#39;G. fortis:&#39;, r_fortis, conf_int_fortis) # G. scandens: 0.4117063629401258 [0.26564228 0.54388972] # G. fortis: 0.7283412395518487 [0.6694112 0.77840616] . It is clear from the confidence intervals that beak depth of the offspring of G. fortis parents is more strongly correlated with their offspring than their G. scandens counterparts. . #### Measuring heritability . Remember that the Pearson correlation coefficient is the ratio of the covariance to the geometric mean of the variances of the two data sets. This is a measure of the correlation between parents and offspring, but might not be the best estimate of heritability. If we stop and think, it makes more sense to define heritability as the ratio of the covariance between parent and offspring to the variance of the parents alone . In this exercise, you will estimate the heritability and perform a pairs bootstrap calculation to get the 95% confidence interval. . This exercise highlights a very important point. Statistical inference (and data analysis in general) is not a plug-n-chug enterprise. You need to think carefully about the questions you are seeking to answer with your data and analyze them appropriately. If you are interested in how heritable traits are, the quantity we defined as the heritability is more apt than the off-the-shelf statistic, the Pearson correlation coefficient. . def heritability(parents, offspring): &quot;&quot;&quot;Compute the heritability from parent and offspring samples.&quot;&quot;&quot; covariance_matrix = np.cov(parents, offspring) return covariance_matrix[0,1] / covariance_matrix[0,0] # Compute the heritability heritability_scandens = heritability(bd_parent_scandens, bd_offspring_scandens) heritability_fortis = heritability(bd_parent_fortis, bd_offspring_fortis) # Acquire 1000 bootstrap replicates of heritability replicates_scandens = draw_bs_pairs( bd_parent_scandens, bd_offspring_scandens, heritability, size=1000) replicates_fortis = draw_bs_pairs( bd_parent_fortis, bd_offspring_fortis, heritability, size=1000) # Compute 95% confidence intervals conf_int_scandens = np.percentile(replicates_scandens, [2.5, 97.5]) conf_int_fortis = np.percentile(replicates_fortis, [2.5, 97.5]) # Print results print(&#39;G. scandens:&#39;, heritability_scandens, conf_int_scandens) print(&#39;G. fortis:&#39;, heritability_fortis, conf_int_fortis) # G. scandens: 0.5485340868685982 [0.34395487 0.75638267] # G. fortis: 0.7229051911438159 [0.64655013 0.79688342] . Here again, we see that G. fortis has stronger heritability than G. scandens . This suggests that the traits of G. fortis may be strongly incorporated into G. scandens by introgressive hybridization. . #### Is beak depth heritable at all in G. scandens? . The heritability of beak depth in G. scandens seems low. It could be that this observed heritability was just achieved by chance and beak depth is actually not really heritable in the species . You will test that hypothesis here. To do this, you will do a pairs permutation test. . # Initialize array of replicates: perm_replicates perm_replicates = np.empty(10000) # Draw replicates for i in range(10000): # Permute parent beak depths bd_parent_permuted = np.random.permutation(bd_parent_scandens) perm_replicates[i] = heritability(bd_parent_permuted, bd_offspring_scandens) # Compute p-value: p p = np.sum(perm_replicates &gt;= heritability_scandens) / len(perm_replicates) # Print the p-value print(&#39;p-val =&#39;, p) # p-val = 0.0 . You get a p-value of zero, which means that none of the 10,000 permutation pairs replicates you drew had a heritability high enough to match that which was observed. This strongly suggests that beak depth is heritable in G. scandens , just not as much as in G. fortis . If you like, you can plot a histogram of the heritability replicates to get a feel for how extreme of a value of heritability you might expect by chance. . plt.hist(perm_replicates) plt.axvline(x=heritability_scandens, color = &#39;red&#39;) plt.text(heritability_scandens, 1500, &#39;heritability_scandens&#39;, ha=&#39;center&#39;, va=&#39;center&#39;,rotation=&#39;vertical&#39;, backgroundcolor=&#39;white&#39;) plt.show() . .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/statistical-thinking-in-python-(part-2).html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/statistical-thinking-in-python-(part-2).html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Statistical Thinking in Python (Part 1)",
            "content": "Statistical Thinking in Python (Part 1) . This is the memo of the 1st course (5 courses in all) of ‘Statistics Fundamentals with Python’ skill track. . You can find the original course HERE . . ### . Graphical exploratory data analysis | Quantitative exploratory data analysis | Thinking probabilistically– Discrete variables | Thinking probabilistically– Continuous variables | 1. Graphical exploratory data analysis . . ### Introduction to exploratory data analysis . #### Tukey’s comments on EDA . Exploratory data analysis is detective work. | There is no excuse for failing to plot and look. | The greatest value of a picture is that it forces us to notice what we never expected to see. | It is important to understand what you can do before you learn how to measure how well you seem to have done it. | . #### Advantages of graphical EDA . It often involves converting tabular data into graphical form. | If done well, graphical representations can allow for more rapid interpretation of data. | There is no excuse for neglecting to do graphical EDA. | . ### Plotting a histogram . #### Plotting a histogram of iris data . versicolor_petal_length array([4.7, 4.5, 4.9, 4. , 4.6, 4.5, 4.7, 3.3, 4.6, 3.9, 3.5, 4.2, 4. , 4.7, 3.6, 4.4, 4.5, 4.1, 4.5, 3.9, 4.8, 4. , 4.9, 4.7, 4.3, 4.4, 4.8, 5. , 4.5, 3.5, 3.8, 3.7, 3.9, 5.1, 4.5, 4.5, 4.7, 4.4, 4.1, 4. , 4.4, 4.6, 4. , 3.3, 4.2, 4.2, 4.2, 4.3, 3. , 4.1]) . # Import plotting modules import matplotlib.pyplot as plt import seaborn as sns # Set default Seaborn style sns.set() # Plot histogram of versicolor petal lengths plt.hist(versicolor_petal_length) # Show histogram plt.show() . . #### Axis labels! . # Plot histogram of versicolor petal lengths _ = plt.hist(versicolor_petal_length) # Label axes plt.xlabel(&#39;petal length (cm)&#39;) plt.ylabel(&#39;count&#39;) # Show histogram plt.show() . . #### Adjusting the number of bins in a histogram . The “square root rule” is a commonly-used rule of thumb for choosing number of bins: choose the number of bins to be the square root of the number of samples. . # Import numpy import numpy as np # Compute number of data points: n_data n_data = len(versicolor_petal_length) # Number of bins is the square root of number of data points: n_bins n_bins = np.sqrt(n_data) # Convert number of bins to integer: n_bins n_bins = int(n_bins) # Plot the histogram _ = plt.hist(versicolor_petal_length, bins=n_bins) # Label axes _ = plt.xlabel(&#39;petal length (cm)&#39;) _ = plt.ylabel(&#39;count&#39;) # Show histogram plt.show() . . ### Plotting all of your data: Bee swarm plots . #### Bee swarm plot . # Create bee swarm plot with Seaborn&#39;s default settings sns.swarmplot(x=&#39;species&#39;, y = &#39;petal length (cm)&#39;, data=df) # Label the axes plt.xlabel(&#39;species&#39;) plt.ylabel(&#39;petal length (cm)&#39;) # Show the plot plt.show() . . #### Interpreting a bee swarm plot . I. virginica petals tend to be the longest, and I. setosa petals tend to be the shortest of the three species. . ### Plotting all of your data: Empirical cumulative distribution functions (ECDF) . #### Computing the ECDF . def ecdf(data): &quot;&quot;&quot;Compute ECDF for a one-dimensional array of measurements.&quot;&quot;&quot; # Number of data points: n n = len(data) # x-data for the ECDF: x x = np.sort(data) # y-data for the ECDF: y y = np.arange(1, n + 1) / n return x, y . #### Plotting the ECDF . # Compute ECDF for versicolor data: x_vers, y_vers x_vers, y_vers = ecdf(versicolor_petal_length) # Generate plot plt.plot(x_vers, y_vers, marker = &#39;.&#39;, linestyle = &#39;none&#39;) # Label the axes plt.xlabel(&#39;petal length (cm)&#39;) plt.ylabel(&#39;ECDF&#39;) # Display the plot plt.show() . . #### Comparison of ECDFs . # Compute ECDFs x_set, y_set = ecdf(setosa_petal_length) x_vers, y_vers = ecdf(versicolor_petal_length) x_virg, y_virg = ecdf(virginica_petal_length) # Plot all ECDFs on the same plot plt.plot(x_set, y_set, marker = &#39;.&#39;, linestyle = &#39;none&#39;) plt.plot(x_vers, y_vers, marker = &#39;.&#39;, linestyle = &#39;none&#39;) plt.plot(x_virg, y_virg, marker = &#39;.&#39;, linestyle = &#39;none&#39;) # Annotate the plot plt.legend((&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;), loc=&#39;lower right&#39;) _ = plt.xlabel(&#39;petal length (cm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) # Display the plot plt.show() . . 2. Quantitative exploratory data analysis . . ### Introduction to summary statistics: The sample mean and median . #### Means and medians . An outlier can significantly affect the value of the mean, but not the median. . #### Computing means . # Compute the mean: mean_length_vers mean_length_vers = np.mean(versicolor_petal_length) # Print the result with some nice formatting print(&#39;I. versicolor:&#39;, mean_length_vers, &#39;cm&#39;) I. versicolor: 4.26 cm . ### Percentiles, outliers, and box plots . #### Computing percentiles . # Specify array of percentiles: percentiles percentiles = np.array([2.5, 25, 50, 75, 97.5]) # Compute percentiles: ptiles_vers ptiles_vers = np.percentile(versicolor_petal_length, percentiles) # Print the result print(ptiles_vers) [3.3 4. 4.35 4.6 4.9775] . #### Comparing percentiles to ECDF . # Plot the ECDF _ = plt.plot(x_vers, y_vers, &#39;.&#39;) _ = plt.xlabel(&#39;petal length (cm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) # Overlay percentiles as red diamonds. _ = plt.plot(ptiles_vers, percentiles/100, marker=&#39;D&#39;, color=&#39;red&#39;, linestyle=&#39;none&#39;) # Show the plot plt.show() . . #### Box-and-whisker plot . # Create box plot with Seaborn&#39;s default settings sns.boxplot(x=&#39;species&#39;, y=&#39;petal length (cm)&#39;, data=df) # Label the axes plt.xlabel(&#39;species&#39;) plt.ylabel(&#39;petal length (cm)&#39;) # Show the plot plt.show() . . ### Variance and standard deviation . #### Computing the variance . # Array of differences to mean: differences differences = versicolor_petal_length - np.mean(versicolor_petal_length) # Square the differences: diff_sq diff_sq = differences ** 2 # Compute the mean square difference: variance_explicit variance_explicit = np.mean(diff_sq) # Compute the variance using NumPy: variance_np variance_np = np.var(versicolor_petal_length) # Print the results print(variance_explicit, variance_np) 0.21640000000000004 0.21640000000000004 . #### The standard deviation and the variance . # Compute the variance: variance variance = np.var(versicolor_petal_length) # Print the square root of the variance print(variance ** 0.5) # Print the standard deviation print(np.std(versicolor_petal_length)) 0.4651881339845203 0.4651881339845203 . ### Covariance and Pearson correlation coefficient . . . #### Scatter plots . # Make a scatter plot plt.plot(versicolor_petal_length, versicolor_petal_width, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Label the axes plt.xlabel(&#39;petal length (cm)&#39;) plt.ylabel(&#39;petal width (cm)&#39;) # Show the result plt.show() . . #### Variance and covariance by looking . Consider four scatter plots of x-y data, appearing to the right. Which has, respectively, . the highest variance in the variable x, d | the highest covariance, c | negative covariance, b | . . #### Computing the covariance . # Compute the covariance matrix: covariance_matrix covariance_matrix = np.cov(versicolor_petal_length, versicolor_petal_width) # Print covariance matrix print(covariance_matrix) # Extract covariance of length and width of petals: petal_cov petal_cov = covariance_matrix[0,1] # Print the length/width covariance print(petal_cov) [[0.22081633 0.07310204] [0.07310204 0.03910612]] 0.07310204081632653 . #### Computing the Pearson correlation coefficient . def pearson_r(x, y): &quot;&quot;&quot;Compute Pearson correlation coefficient between two arrays.&quot;&quot;&quot; # Compute correlation matrix: corr_mat corr_mat = np.corrcoef(x, y) # Return entry [0,1] return corr_mat[0,1] # Compute Pearson correlation coefficient for I. versicolor: r r = pearson_r(versicolor_petal_length , versicolor_petal_width) # Print the result print(r) 0.7866680885228169 . 3. Thinking probabilistically– Discrete variables . . ### Probabilistic logic and statistical inference . #### What is the goal of statistical inference? . Why do we do statistical inference? . To draw probabilistic conclusions about what we might expect if we collected the same data again. | To draw actionable conclusions from data. | To draw more general conclusions from relatively few data or observations. | . #### Why do we use the language of probability? . Why we use probabilistic language in statistical inference? . Probability provides a measure of uncertainty. | Data are almost never exactly the same when acquired again, and probability allows us to say how much we expect them to vary. | . ### Random number generators and hacker statistics . ### Generating random numbers using the np.random module . # Seed the random number generator np.random.seed(42) # Initialize random numbers: random_numbers random_numbers = np.empty(100000) # Generate random numbers by looping over range(100000) for i in range(100000): random_numbers[i] = np.random.random() # Plot a histogram _ = plt.hist(random_numbers) # Show the plot plt.show() . . #### The np.random module and Bernoulli trials . # Seed random number generator np.random.seed(42) # Initialize the number of defaults: n_defaults n_defaults = np.empty(1000) # Compute the number of defaults for i in range(1000): n_defaults[i] = perform_bernoulli_trials(100, 0.05) # Plot the histogram with default number of bins; label your axes _ = plt.hist(n_defaults, normed=True) _ = plt.xlabel(&#39;number of defaults out of 100 loans&#39;) _ = plt.ylabel(&#39;probability&#39;) # Show the plot plt.show() . . #### Will the bank fail? . # Compute ECDF: x, y x, y = ecdf(n_defaults) # Plot the ECDF with labeled axes plt.plot(x, y, marker = &#39;.&#39;, linestyle = &#39;none&#39;) plt.xlabel(&#39;number of defaults out of 100&#39;) plt.ylabel(&#39;CDF&#39;) # Show the plot plt.show() # Compute the number of 100-loan simulations with 10 or more defaults: n_lose_money n_lose_money = np.sum(n_defaults &gt;= 10) # Compute and print probability of losing money print(&#39;Probability of losing money =&#39;, n_lose_money / len(n_defaults)) Probability of losing money = 0.022 . . ### Probability distributions and stories: The Binomial distribution . #### Sampling out of the Binomial distribution . # Take 10,000 samples out of the binomial distribution: n_defaults n_defaults = np.random.binomial(n=100, p=0.05, size=10000) # Compute CDF: x, y x,y = ecdf(n_defaults) # Plot the CDF with axis labels plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;) plt.xlabel(&#39;number of defaults out of 100 loans&#39;) plt.ylabel(&#39;CDF&#39;) # Show the plot plt.show() . . #### Plotting the Binomial PMF(probability mass function) . # Compute bin edges: bins bins = np.arange(0, max(n_defaults) + 1.5) - 0.5 # Generate histogram plt.hist(n_defaults, normed=True, bins=bins) # Label axes plt.xlabel(&#39;number of defaults out of 100 loans&#39;) plt.ylabel(&#39;PMF&#39;) # Show the plot plt.show() . . ### Poisson processes and the Poisson distribution . #### Relationship between Binomial and Poisson distributions . # Draw 10,000 samples out of Poisson distribution: samples_poisson samples_poisson = np.random.poisson(10, size=10000) # Print the mean and standard deviation print(&#39;Poisson: &#39;, np.mean(samples_poisson), np.std(samples_poisson)) # Specify values of n and p to consider for Binomial: n, p n = [20, 100, 1000] p = [0.5, 0.1, 0.01] # Draw 10,000 samples for each n,p pair: samples_binomial for i in range(3): samples_binomial = np.random.binomial(n[i], p[i], size=10000) # Print results print(&#39;n =&#39;, n[i], &#39;Binom:&#39;, np.mean(samples_binomial), np.std(samples_binomial)) Poisson: 10.0186 3.144813832327758 n = 20 Binom: 9.9637 2.2163443572694206 n = 100 Binom: 9.9947 3.0135812433050484 n = 1000 Binom: 9.9985 3.139378561116833 . #### How many no-hitters in a season? . In baseball, a no-hitter is a game in which a pitcher does not allow the other team to get a hit. This is a rare event, and since the beginning of the so-called modern era of baseball (starting in 1901), there have only been 251 of them through the 2015 season in over 200,000 games. The ECDF of the number of no-hitters in a season is shown to the below. Which probability distribution would be appropriate to describe the number of no-hitters we would expect in a given season? . Both Binomial and Poisson, though Poisson is easier to model and compute. . When we have rare events (low p, high n), the Binomial distribution is Poisson. This has a single parameter, the mean number of successes per time interval, in our case the mean number of no-hitters per season. . . ### Was 2015 anomalous? . # Draw 10,000 samples out of Poisson distribution: n_nohitters n_nohitters = np.random.poisson(251/115, size=10000) # Compute number of samples that are seven or greater: n_large n_large = np.sum(n_nohitters &gt;= 7) # Compute probability of getting seven or more: p_large p_large = n_large / 10000 # Print the result print(&#39;Probability of seven or more no-hitters:&#39;, p_large) Probability of seven or more no-hitters: 0.0067 . 4. Thinking probabilistically– Continuous variables . . ### Probability density functions . #### Interpreting PDFs . . x is more likely than not greater than 10. . The probability is given by the area under the PDF , and there is more area to the left of 10 than to the right. . #### Interpreting CDFs . . Above is the CDF corresponding to the PDF. Using the CDF, what is the probability that x is greater than 10? . 0.25 . The value of the CDF at x = 10 is 0.75, so the probability that x &lt; 10 is 0.75. Thus, the probability that x &gt; 10 is 0.25. . ### Introduction to the Normal distribution . #### The Normal PDF(Probability density functions) . # Draw 100000 samples from Normal distribution with stds of interest: samples_std1, samples_std3, samples_std10 samples_std1 = np.random.normal(20, 1, size=100000) samples_std3 = np.random.normal(20, 3, size=100000) samples_std10 = np.random.normal(20, 10, size=100000) # Make histograms plt.hist(samples_std1, normed=True, histtype=&#39;step&#39;, bins=100) plt.hist(samples_std3, normed=True, histtype=&#39;step&#39;, bins=100) plt.hist(samples_std10, normed=True, histtype=&#39;step&#39;, bins=100) # Make a legend, set limits and show plot _ = plt.legend((&#39;std = 1&#39;, &#39;std = 3&#39;, &#39;std = 10&#39;)) plt.ylim(-0.01, 0.42) plt.show() . . #### The Normal CDF . # Generate CDFs x_std1, y_std1 = ecdf(samples_std1) x_std3, y_std3 = ecdf(samples_std3) x_std10, y_std10 = ecdf(samples_std10) # Plot CDFs plt.plot(x_std1, y_std1, marker = &#39;.&#39;, linestyle = &#39;none&#39;) plt.plot(x_std3, y_std3, marker = &#39;.&#39;, linestyle = &#39;none&#39;) plt.plot(x_std10, y_std10, marker = &#39;.&#39;, linestyle = &#39;none&#39;) # Make a legend and show the plot _ = plt.legend((&#39;std = 1&#39;, &#39;std = 3&#39;, &#39;std = 10&#39;), loc=&#39;lower right&#39;) plt.show() . . The CDFs all pass through the mean at the 50th percentile; the mean and median of a Normal distribution are equal. The width of the CDF varies with the standard deviation. . ### The Normal distribution: Properties and warnings . #### Gauss and the 10 Deutschmark banknote . . source . What are the mean and standard deviation, respectively, of the Normal distribution that was on the 10 Deutschmark banknote ? . mean = 3, std = 1 . #### Are the Belmont Stakes results Normally distributed? . data souce: Belmont_Stakes . belmont_no_outliers array([148.51, 146.65, 148.52, 150.7 , 150.42, 150.88, 151.57, 147.54, 149.65, 148.74, 147.86, 148.75, 147.5 , 148.26, 149.71, 146.56, 151.19, 147.88, 149.16, 148.82, 148.96, 152.02, 146.82, 149.97, 146.13, 148.1 , 147.2 , 146. , 146.4 , 148.2 , 149.8 , 147. , 147.2 , 147.8 , 148.2 , 149. , 149.8 , 148.6 , 146.8 , 149.6 , 149. , 148.2 , 149.2 , 148. , 150.4 , 148.8 , 147.2 , 148.8 , 149.6 , 148.4 , 148.4 , 150.2 , 148.8 , 149.2 , 149.2 , 148.4 , 150.2 , 146.6 , 149.8 , 149. , 150.8 , 148.6 , 150.2 , 149. , 148.6 , 150.2 , 148.2 , 149.4 , 150.8 , 150.2 , 152.2 , 148.2 , 149.2 , 151. , 149.6 , 149.6 , 149.4 , 148.6 , 150. , 150.6 , 149.2 , 152.6 , 152.8 , 149.6 , 151.6 , 152.8 , 153.2 , 152.4 , 152.2 ]) . # Compute mean and standard deviation: mu, sigma mu = np.mean(belmont_no_outliers) sigma = np.std(belmont_no_outliers) # Sample out of a normal distribution with this mu and sigma: samples samples = np.random.normal(mu, sigma, size=10000) # Get the CDF of the samples and of the data x_theor, y_theor = ecdf(samples) x, y = ecdf(belmont_no_outliers) # Plot the CDFs and show the plot _ = plt.plot(x_theor, y_theor) _ = plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;) _ = plt.xlabel(&#39;Belmont winning time (sec.)&#39;) _ = plt.ylabel(&#39;CDF&#39;) plt.show() . . #### What are the chances of a horse matching or beating Secretariat’s record? . # Take a million samples out of the Normal distribution: samples samples = np.random.normal(mu, sigma, size=1000000) # Compute the fraction that are faster than 144 seconds: prob prob = np.sum(samples &lt; 144) / 1000000 # Print the result print(&#39;Probability of beating Secretariat:&#39;, prob) Probability of beating Secretariat: 0.000635 . ### The Exponential distribution . #### Matching a story and a distribution . How might we expect the time between Major League no-hitters to be distributed? Be careful here: a few exercises ago, we considered the probability distribution for the number of no-hitters in a season. Now, we are looking at the probability distribution of the time between no hitters. . Exponential . #### Waiting for the next Secretariat . Unfortunately, Justin was not alive when Secretariat ran the Belmont in 1973. Do you think he will get to see a performance like that? To answer this, you are interested in how many years you would expect to wait until you see another performance like Secretariat’s. How is the waiting time until the next performance as good or better than Secretariat’s distributed? . Exponential: A horse as fast as Secretariat is a rare event, which can be modeled as a Poisson process, and the waiting time between arrivals of a Poisson process is Exponentially distributed. . The Exponential distribution describes the waiting times between rare events, and Secretariat is rare ! . #### If you have a story, you can simulate it! . def successive_poisson(tau1, tau2, size=1): &quot;&quot;&quot;Compute time for arrival of 2 successive Poisson processes.&quot;&quot;&quot; # Draw samples out of first exponential distribution: t1 t1 = np.random.exponential(tau1, size) # Draw samples out of second exponential distribution: t2 t2 = np.random.exponential(tau2, size) return t1 + t2 . #### Distribution of no-hitters and cycles . In baseball, hitting for the cycle is the accomplishment of one batter hitting a single, a double, a triple, and a home run in the same game. . Now, you’ll use your sampling function to compute the waiting time to observe a no-hitter and hitting of the cycle. The mean waiting time for a no-hitter is 764 games, and the mean waiting time for hitting the cycle is 715 games. . # Draw samples of waiting times: waiting_times waiting_times = successive_poisson(764, 715, size=100000) # Make the histogram plt.hist(waiting_times, bins=100, normed=True, histtype=&#39;step&#39;) # Label axes plt.xlabel(&#39;total waiting time (games)&#39;) plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() . . x, y = ecdf(waiting_times) plt.plot(x, y) plt.xlabel(&#39;total waiting time (games)&#39;) plt.ylabel(&#39;CDF&#39;) plt.show() . .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/statistical-thinking-in-python-(part-1).html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/statistical-thinking-in-python-(part-1).html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Statistical Simulation in Python",
            "content": "Statistical Simulation in Python . This is the memo of the 4th course (5 courses in all) of ‘Statistics Fundamentals with Python’ skill track. . You can find the original course HERE . . ### . Basics of randomness &amp; simulation | Probability &amp; data generation process | Resampling methods | Advanced Applications of Simulation | 1. Basics of randomness &amp; simulation . . 1.1 Introduction to random variables . . A probability distribution is a mapping from the set of possible outcomes of a random variable to the probability of observing that outcome. . It tells you how likely you are to observe a given outcome or a set of outcomes. . #### np.random.choice() . In this exercise, you will be introduced to the np.random.choice() function. This is a remarkably useful function for simulations and you will be making extensive use of it later in the course. As a first step, let’s try to understand the basics of this function. . np.random.choice? Docstring: choice(a, size=None, replace=True, p=None) Generates a random sample from a given 1-D array .. versionadded:: 1.7.0 Parameters -- a : 1-D array-like or int If an ndarray, a random sample is generated from its elements. If an int, the random sample is generated as if a were np.arange(a) size : int or tuple of ints, optional Output shape. If the given shape is, e.g., ``(m, n, k)``, then ``m * n * k`` samples are drawn. Default is None, in which case a single value is returned. replace : boolean, optional Whether the sample is with or without replacement p : 1-D array-like, optional The probabilities associated with each entry in a. If not given the sample assumes a uniform distribution over all entries in a. Returns -- samples : single item or ndarray The generated random samples Raises - ValueError If a is an int and less than zero, if a or p are not 1-dimensional, if a is an array-like of size 0, if p is not a vector of probabilities, if a and p have different lengths, or if replace=False and the sample size is greater than the population size See Also randint, shuffle, permutation . The np.random.choice() function will work even if you only provide the input array a . Make sure to make use of the help() function throughout the course, it will help get you through some tough exercises! . #### Poisson random variable . The numpy.random module also has a number of useful probability distributions for both discrete and continuous random variables. In this exercise, you will learn how to draw samples from a probability distribution. . In particular, you will draw samples from a very important discrete probability distribution, the Poisson distribution, which is typically used for modeling the average rate at which events occur. . Following the exercise, you should be able to apply these steps to any of the probability distributions found in numpy.random . In addition, you will also see how the sample mean changes as we draw more samples from a distribution. . # Initialize seed and parameters np.random.seed(123) lam, size_1, size_2 = 5, 3, 1000 # Draw samples &amp; calculate absolute difference between lambda and sample mean samples_1 = np.random.poisson(lam, size_1) samples_2 = np.random.poisson(lam, size_2) answer_1 = abs(np.mean(samples_1) - lam) answer_2 = abs(np.mean(samples_2) - lam) print(&quot;|Lambda - sample mean| with {} samples is {} and with {} samples is {}. &quot;.format(size_1, answer_1, size_2, answer_2)) # |Lambda - sample mean| with 3 samples is 0.33333333333333304 and with 1000 samples is 0.07699999999999996. . import matplotlib.pyplot as plt plt.hist(samples_2) plt.xlabel(&#39;samples_2 value&#39;) plt.ylabel(&#39;count&#39;) plt.title(&#39;np.random.poisson result nlamda:5&#39;) plt.show() . . Why do you think the larger size gives us a better result? . #### Shuffling a deck of cards . Often times we are interested in randomizing the order of a set of items. Consider a game of cards where you first shuffle the deck of cards or a game of scrabble where the letters are first mixed in a bag. As the final exercise of this section, you will learn another useful function – np.random.shuffle() . This function allows you to randomly shuffle a sequence in place. At the end of this exercise, you will know how to shuffle a deck of cards or any sequence of items. . Examine deck_of_cards in the shell. . print(deck_of_cards) # Shuffle the deck np.random.shuffle(deck_of_cards) # Print out the top three cards card_choices_after_shuffle = deck_of_cards[:3] print(card_choices_after_shuffle) . [(&#39;Heart&#39;, 0), (&#39;Heart&#39;, 1), (&#39;Heart&#39;, 2), ..., (&#39;Diamond&#39;, 10), (&#39;Diamond&#39;, 11), (&#39;Diamond&#39;, 12)] [(&#39;Diamond&#39;, 9), (&#39;Spade&#39;, 9), (&#39;Spade&#39;, 4)] . . 1.2 Simulation basics . . #### Throwing a fair die . Once you grasp the basics of designing a simulation, you can apply it to any system or process. Next, we will learn how each step is implemented using some basic examples. . As we have learned, simulation involves repeated random sampling. The first step then is to get one random sample. Once we have that, all we do is repeat the process multiple times. This exercise will focus on understanding how we get one random sample. We will study this in the context of throwing a fair six-sided die. . By the end of this exercise, you will be familiar with how to implement the first two steps of running a simulation – defining a random variable and assigning probabilities. . For the rest of the course, look to the IPython shell to find out what seed has been set. . # The seed for this exercise is set to 123 # Define die outcomes and probabilities die, probabilities, throws = [1,2,3,4,5,6], [1/6]*6, 1 # Use np.random.choice to throw the die once and record the outcome outcome = np.random.choice(die, size=1, p=probabilities) print(&quot;Outcome of the throw: {}&quot;.format(outcome[0])) # Outcome of the throw: 5 . #### Throwing two fair dice . We now know how to implement the first two steps of a simulation. Now let’s implement the next step – defining the relationship between random variables. . Often times, our simulation will involve not just one, but multiple random variables. Consider a game where throw you two dice and win if each die shows the same number. Here we have two random variables – the two dice – and a relationship between each of them – we win if they show the same number, lose if they don’t. In reality, the relationship between random variables can be much more complex, especially when simulating things like weather patterns. . By the end of this exercise, you will be familiar with how to implement the third step of running a simulation – defining relationships between random variables. . # The seed for this exercise is set to 223 # Initialize number of dice, simulate &amp; record outcome die, probabilities, num_dice = [1,2,3,4,5,6], [1/6, 1/6, 1/6, 1/6, 1/6, 1/6], 2 outcomes = np.random.choice(die, size=num_dice, p=probabilities) # Win if the two dice show the same number if outcomes[0] == outcomes[1]: answer = &#39;win&#39; else: answer = &#39;lose&#39; print(&quot;The dice show {} and {}. You {}!&quot;.format(outcomes[0], outcomes[1], answer)) # The dice show 5 and 5. You win! . #### Simulating the dice game . We now know how to implement the first three steps of a simulation. Now let’s consider the next step – repeated random sampling. . Simulating an outcome once doesn’t tell us much about how often we can expect to see that outcome. In the case of the dice game from the previous exercise, it’s great that we won once. But suppose we want to see how many times we can expect to win if we played this game multiple times, we need to repeat the random sampling process many times. Repeating the process of random sampling is helpful to understand and visualize inherent uncertainty and deciding next steps. . Following this exercise, you will be familiar with implementing the fourth step of running a simulation – sampling repeatedly and generating outcomes. . # The seed for this exercise is set to 223 # Initialize model parameters &amp; simulate dice throw die, probabilities, num_dice = [1,2,3,4,5,6], [1/6, 1/6, 1/6, 1/6, 1/6, 1/6], 2 sims, wins = 100, 0 for i in range(sims): outcomes = np.random.choice(die, size=num_dice, p=probabilities) # Increment `wins` by 1 if the dice show same number if outcomes[0] == outcomes[1]: wins = wins + 1 print(&quot;In {} games, you win {} times&quot;.format(sims, wins)) # In 100 games, you win 25 times . . 1.3 Using simulation for decision-making . . #### Simulating one lottery drawing . In the last three exercises of this chapter, we will be bringing together everything you’ve learned so far. We will run a complete simulation, take a decision based on our observed outcomes, and learn to modify inputs to the simulation model. . We will use simulations to figure out whether or not we want to buy a lottery ticket. Suppose you have the opportunity to buy a lottery ticket which gives you a shot at a grand prize of $1 Million. Since there are 1000 tickets in total, your probability of winning is 1 in 1000. Each ticket costs $10. Let’s use our understanding of basic simulations to first simulate one drawing of the lottery. . # The seed for this exercise is set to 123 # Pre-defined constant variables lottery_ticket_cost, num_tickets, grand_prize = 10, 1000, 1000000 # Probability of winning chance_of_winning = 1/num_tickets # Simulate a single drawing of the lottery gains = [-lottery_ticket_cost, grand_prize-lottery_ticket_cost] probability = [1-chance_of_winning, chance_of_winning] outcome = np.random.choice(a=gains, size=1, p=probability, replace=True) print(&quot;Outcome of one drawing of the lottery is {}&quot;.format(outcome)) # Outcome of one drawing of the lottery is [-10] . #### Should we buy? . In the last exercise, we simulated the random drawing of the lottery ticket once. In this exercise, we complete the simulation process by repeating the process multiple times. . Repeating the process gives us multiple outcomes. We can think of this as multiple universes where the same lottery drawing occurred. We can then determine the average winnings across all these universes. If the average winnings are greater than what we pay for the ticket then it makes sense to buy it, otherwise, we might not want to buy the ticket. . This is typically how simulations are used for evaluating business investments. After completing this exercise, you will have the basic tools required to use simulations for decision-making. . # Initialize size and simulate outcome lottery_ticket_cost, num_tickets, grand_prize = 10, 1000, 1000000 chance_of_winning = 1/num_tickets size = 2000 payoffs = [-lottery_ticket_cost, grand_prize-lottery_ticket_cost] probs = [1-chance_of_winning,chance_of_winning] outcomes = np.random.choice(a=payoffs, size=size, p=probs, replace=True) # Mean of outcomes. answer = np.mean(outcomes) print(&quot;Average payoff from {} simulations = {}&quot;.format(size, answer)) # Average payoff from 2000 simulations = 1990.0 . Is it worth spending $10 on the ticket for this average payoff? . #### Calculating a break-even lottery price . Simulations allow us to ask more nuanced questions that might not necessarily have an easy analytical solution. Rather than solving a complex mathematical formula, we directly get multiple sample outcomes. We can run experiments by modifying inputs and studying how those changes impact the system. For example, once we have a moderately reasonable model of global weather patterns, we could evaluate the impact of increased greenhouse gas emissions. . In the lottery example, we might want to know how expensive the ticket needs to be for it to not make sense to buy it. To understand this, we need to modify the ticket cost to see when the expected payoff is negative. . grand_prize , num_tickets , and chance_of_winning are loaded in the environment. . # The seed for this exercise is set to 333 # Initialize simulations and cost of ticket sims, lottery_ticket_cost = 3000, 0 # Use a while loop to increment `lottery_ticket_cost` till average value of outcomes falls below zero while 1: outcomes = np.random.choice([-lottery_ticket_cost, grand_prize-lottery_ticket_cost], size=sims, p=[1-chance_of_winning, chance_of_winning], replace=True) if outcomes.mean() &lt; 0: break else: lottery_ticket_cost += 1 answer = lottery_ticket_cost - 1 print(&quot;The highest price at which it makes sense to buy the ticket is {}&quot;.format(answer)) # The highest price at which it makes sense to buy the ticket is 9 python # **2. Probability &amp; data generation process** ## **2.1 Probability basics** ![Desktop View](/blog/assets/datacamp/statistical-simulation-in-python/10-1.png?w=1024) #### **Queen or spade** In this example, you’ll use the generalized probability formula P(A∪B)=P(A)+P(B)−P(A∩B) to calculate the probability of two events. Consider a deck of cards (13 cards x 4 suites = 52 cards in total). One card is drawn at random. What is the probability of getting a queen or a spade? Here event A is the card being a queen and event B is the card being a spade. Think carefully about whether the two events have anything in common. python # P(Queens) + P(Spades) - P(Queen of Spades) 4/52 + 13/52 - 1/52 = 16/52 . #### Two of a kind . Now let’s use simulation to estimate probabilities. Suppose you’ve been invited to a game of poker at your friend’s home. In this variation of the game, you are dealt five cards and the player with the better hand wins. You will use a simulation to estimate the probabilities of getting certain hands. Let’s work on estimating the probability of getting at least two of a kind. Two of a kind is when you get two cards of different suites but having the same numeric value (e.g., 2 of hearts, 2 of spades, and 3 other cards). . By the end of this exercise, you will know how to use simulation to calculate probabilities for card games. . # The seed for this exercise is set to 123 # Shuffle deck &amp; count card occurrences in the hand n_sims, two_kind = 10000, 0 for i in range(n_sims): np.random.shuffle(deck_of_cards) hand, cards_in_hand = deck_of_cards[0:5], {} for card in hand: # Use .get() method on cards_in_hand cards_in_hand[card[1]] = cards_in_hand.get(card[1], 0) + 1 # Condition for getting at least 2 of a kind highest_card = max(cards_in_hand.values()) if highest_card&gt;=2: two_kind += 1 print(&quot;Probability of seeing at least two of a kind = {} &quot;.format(two_kind/n_sims)) # Probability of seeing at least two of a kind = 0.4952 . np.random.shuffle(deck_of_cards) hand, cards_in_hand = deck_of_cards[0:5], {} for card in hand: # Use .get() method on cards_in_hand cards_in_hand[card[1]] = cards_in_hand.get(card[1], 0) + 1 hand [(&#39;Club&#39;, 1), (&#39;Diamond&#39;, 4), (&#39;Heart&#39;, 8), (&#39;Spade&#39;, 4), (&#39;Spade&#39;, 8)] cards_in_hand {1: 1, 4: 2, 8: 2} cards_in_hand.values() dict_values([1, 2, 2]) . #### Game of thirteen . A famous French mathematician Pierre Raymond De Montmart, who was known for his work in combinatorics, proposed a simple game called as Game of Thirteen. You have a deck of 13 cards, each numbered from 1 through 13. Shuffle this deck and draw cards one by one. A coincidence is when the number on the card matches the order in which the card is drawn. For instance, if the 5th card you draw happens to be a 5, it’s a coincidence. You win the game if you get through all the cards without any coincidences. Let’s calculate the probability of winning at this game using simulation. . By completing this exercise, you will further strengthen your ability to cast abstract problems into the simulation framework for estimating probabilities. . # Pre-set constant variables deck, sims, coincidences = np.arange(1, 14), 10000, 0 for _ in range(sims): # Draw all the cards without replacement to simulate one game draw = np.random.choice(deck, size=len(deck), replace=False) # Check if there are any coincidences coincidence = (draw == list(np.arange(1, 14))).any() if coincidence == True: coincidences += 1 # Calculate probability of winning prob_of_winning = 1 - coincidences / sims print(&quot;Probability of winning = {}&quot;.format(prob_of_winning)) # Probability of winning = 0.36950000000000005 . . 2.2 More probability concepts . . #### The conditional urn . As we’ve learned, conditional probability is defined as the probability of an event given another event. To illustrate this concept, let’s turn to an urn problem. . We have an urn that contains 7 white and 6 black balls. Four balls are drawn at random. We’d like to know the probability that the first and third balls are white, while the second and the fourth balls are black. . Upon completion, you will learn to manipulate simulations to calculate simple conditional probabilities. . # Initialize success, sims and urn success, sims = 0, 5000 urn = [&#39;w&#39;] * 7 + [&#39;b&#39;] * 6 for _ in range(sims): # Draw 4 balls without replacement draw = np.random.choice(urn, replace=False, size=4) # Count the number of successes if (draw == [&#39;w&#39;,&#39;b&#39;,&#39;w&#39;,&#39;b&#39;]).all(): success +=1 print(&quot;Probability of success = {}&quot;.format(success/sims)) # Probability of success = 0.0722 . #### Birthday problem . Now we’ll use simulation to solve a famous probability puzzle – the birthday problem. It sounds quite straightforward – How many people do you need in a room to ensure at least a 50% chance that two of them share the same birthday? . With 366 people in a 365-day year, we are 100% sure that at least two have the same birthday, but we only need to be 50% sure. Simulation gives us an elegant way of solving this problem. . Upon completion of this exercise, you will begin to understand how to cast problems in a simulation framework. . # Draw a sample of birthdays &amp; check if each birthday is unique days = np.arange(1,366) people = 2 def birthday_sim(people): sims, unique_birthdays = 2000, 0 for _ in range(sims): draw = np.random.choice(days, size=people, replace=True) if len(draw) == len(set(draw)): unique_birthdays += 1 out = 1 - unique_birthdays / sims return out # Break out of the loop if probability greater than 0.5 while (people &gt; 0): prop_bds = birthday_sim(people) if prop_bds &gt; 0.5: break people += 1 print(&quot;With {} people, there&#39;s a 50% chance that two share a birthday.&quot;.format(people)) # With 23 people, there&#39;s a 50% chance that two share a birthday. . 23 seems surprisingly low, but it’s enough to have a 50% chance! . #### Full house . Let’s return to our poker game. Last time, we calculated the probability of getting at least two of a kind. This time we are interested in a full house. A full house is when you get two cards of different suits that share the same numeric value and three other cards that have the same numeric value (e.g., 2 of hearts &amp; spades, jacks of clubs, diamonds, &amp; spades). . Thus, a full house is the probability of getting exactly three of a kind conditional on getting exactly two of a kind of another value. Using the same code as before, modify the success condition to get the desired output. This exercise will teach you to estimate conditional probabilities in card games and build your foundation in framing abstract problems for simulation. . #Shuffle deck &amp; count card occurrences in the hand n_sims, full_house, deck_of_cards = 50000, 0, deck.copy() for i in range(n_sims): np.random.shuffle(deck_of_cards) hand, cards_in_hand = deck_of_cards[0:5], {} for card in hand: # Use .get() method to count occurrences of each card cards_in_hand[card[1]] = cards_in_hand.get(card[1], 0) + 1 # Condition for getting full house condition = (max(cards_in_hand.values()) ==3) &amp; (min(cards_in_hand.values())==2) if condition == True: full_house += 1 print(&quot;Probability of seeing a full house = {}&quot;.format(full_house/n_sims)) # Probability of seeing a full house = 0.0014 . Look at how small this probability is compared to that of at least two of a kind. . . 2.3 Data generating process . . #### Driving test . Through the next exercises, we will learn how to build a data generating process (DGP) through progressively complex examples. . In this exercise, you will simulate a very simple DGP. Suppose that you are about to take a driving test tomorrow. Based on your own practice and based on data you have gathered, you know that the probability of you passing the test is 90% when it’s sunny and only 30% when it’s raining. Your local weather station forecasts that there’s a 40% chance of rain tomorrow. Based on this information, you want to know what is the probability of you passing the driving test tomorrow. . This is a simple problem and can be solved analytically. Here, you will learn how to model a simple DGP and see how it can be used for simulation. . sims, outcomes, p_rain, p_pass = 1000, [], 0.40, {&#39;sun&#39;:0.9, &#39;rain&#39;:0.3} def test_outcome(p_rain): # Simulate whether it will rain or not weather = np.random.choice([&#39;sun&#39;, &#39;rain&#39;], p=[1-p_rain, p_rain]) # Simulate and return whether you will pass or fail return np.random.choice([&#39;pass&#39;, &#39;fail&#39;], p=[p_pass[weather], 1-p_pass[weather]]) for _ in range(sims): outcomes.append(test_outcome(p_rain)) # Calculate fraction of outcomes where you pass pass_outcomes_frac = outcomes.count(&#39;pass&#39;) / len(outcomes) print(&quot;Probability of Passing the driving test = {}&quot;.format(pass_outcomes_frac)) # Probability of Passing the driving test = 0.654 . #### National elections . This exercise will give you a taste of how you can model a DGP at different levels of complexity. . Consider national elections in a country with two political parties – Red and Blue. This country has 50 states and the party that wins the most states wins the elections. You have the probability pp of Red winning in each individual state and want to know the probability of Red winning nationally. . Let’s model the DGP to understand the distribution. Suppose the election outcome in each state follows a binomial distribution with probability pp such that 00 indicates a loss for Red and 11 indicates a win. We then simulate a number of election outcomes. Finally, we can ask rich questions like what is the probability of Red winning less than 45% of the states? . # probabilities of red win in a state p array([0.52076814, 0.67846401, 0.82731745, 0.64722761, 0.03665174, ... 0.96263926, 0.0548948 , 0.14092758, 0.54739446, 0.54555576]) . outcomes, sims, probs = [], 1000, p for _ in range(sims): # Simulate elections in the 50 states election = np.random.binomial(p=probs, n=1) # Get average of Red wins and add to `outcomes` outcomes.append(np.sum(election)/len(election)) # Calculate probability of Red winning in less than 45% of the states prob_red_wins = sum(x&lt;0.45 for x in outcomes)/ len(outcomes) print(&quot;Probability of Red winning in less than 45% of the states = {}&quot;.format(prob_red_wins)) # Probability of Red winning in less than 45% of the states = 0.196 . Now think about what you would do if you were given the probabilities of winning in each county within a state. . #### Fitness goals . Let’s model how activity levels impact weight loss using modern fitness trackers. On days when you go to the gym, you average around 15k steps, and around 5k steps otherwise. You go to the gym 40% of the time. Let’s model the step counts in a day as a Poisson random variable with a mean λ dependent on whether or not you go to the gym. . For simplicity, let’s say you have an 80% chance of losing 1lb and a 20% chance of gaining 1lb when you get more than 10k steps. The probabilities are reversed when you get less than 8k steps. Otherwise, there’s an even chance of gaining or losing 1lb. Given all this information, find the probability of losing weight in a month. . # Simulate steps &amp; choose prob for _ in range(sims): w = [] for i in range(days): lam = np.random.choice([5000, 15000], p=[0.6, 0.4], size=1) steps = np.random.poisson(lam) if steps &gt; 10000: prob = [0.2, 0.8] elif steps &lt; 8000: prob = [0.8, 0.2] else: prob = [0.5, 0.5] w.append(np.random.choice([1, -1], p=prob)) outcomes.append(sum(w)) # Calculate fraction of outcomes where there was a weight loss weight_loss_outcomes_frac = sum(x&lt;0 for x in outcomes)/ len(outcomes) print(&quot;Probability of Weight Loss = {}&quot;.format(weight_loss_outcomes_frac)) # Probability of Weight Loss = 0.215 . You now know how easy it is to break down complex DGPs into simple components. . . 2.4 eCommerce Ad Simulation . . #### Sign up Flow . We will now model the DGP of an eCommerce ad flow starting with sign-ups. . On any day, we get many ad impressions, which can be modeled as Poisson random variables (RV). You are told that λλ is normally distributed with a mean of 100k visitors and standard deviation 2000. . During the signup journey, the customer sees an ad, decides whether or not to click, and then whether or not to signup. Thus both clicks and signups are binary, modeled using binomial RVs. What about probability pp of success? Our current low-cost option gives us a click-through rate of 1% and a sign-up rate of 20%. A higher cost option could increase the clickthrough and signup rate by up to 20%, but we are unsure of the level of improvement, so we model it as a uniform RV. . # Initialize click-through rate and signup rate dictionaries ct_rate = {&#39;low&#39;:0.01, &#39;high&#39;:np.random.uniform(low=0.01, high=1.2*0.01)} su_rate = {&#39;low&#39;:0.2, &#39;high&#39;:np.random.uniform(low=0.2, high=1.2*0.2)} def get_signups(cost, ct_rate, su_rate, sims): lam = np.random.normal(loc=100000, scale=2000, size=sims) # Simulate impressions(poisson), clicks(binomial) and signups(binomial) impressions = np.random.poisson(lam=lam) clicks = np.random.binomial(n=impressions, p=ct_rate[cost]) signups = np.random.binomial(n=clicks, p=su_rate[cost]) return signups print(&quot;Simulated Signups = {}&quot;.format(get_signups(&#39;high&#39;, ct_rate, su_rate, 1))) # Simulated Signups = [268] . Now that we have signups, let’s see how to model the purchases from these signups. . #### Purchase Flow . After signups, let’s model the revenue generation process. Once the customer has signed up, they decide whether or not to purchase – a natural candidate for a binomial RV. Let’s assume that 10% of signups result in a purchase. . Although customers can make many purchases, let’s assume one purchase. The purchase value could be modeled by any continuous RV, but one nice candidate is the exponential RV. Suppose we know that purchase value per customer has averaged around $1000. We use this information to create the purchase_values RV. The revenue, then, is simply the sum of all purchase values. . The variables ct_rate , su_rate and the function get_signups() from the last exercise are pre-loaded for you. . def get_revenue(signups): rev = [] np.random.seed(123) for s in signups: # Model purchases as binomial, purchase_values as exponential purchases = np.random.binomial(s, p=0.1) purchase_values = np.random.exponential(size=purchases, scale=1000) # Append to revenue the sum of all purchase values. rev.append(np.sum(purchase_values)) return rev print(&quot;Simulated Revenue = ${}&quot;.format(get_revenue(get_signups(&#39;low&#39;, ct_rate, su_rate, 1))[0])) # Simulated Revenue = $22404.217742298042 . What are some other distributions you could have used in place of exponential? . #### Probability of losing money . In this exercise, we will use the DGP model to estimate probability. . As seen earlier, this company has the option of spending extra money, let’s say $3000, to redesign the ad. This could potentially get them higher clickthrough and signup rates, but this is not guaranteed. We would like to know whether or not to spend this extra $3000 by calculating the probability of losing money. In other words, the probability that the revenue from the high-cost option minus the revenue from the low-cost option is lesser than the cost. . Once we have simulated revenue outcomes, we can ask a rich set of questions that might not have been accessible using traditional analytical methods. . This simple yet powerful framework forms the basis of Bayesian methods for getting probabilities. . # Initialize cost_diff sims, cost_diff = 10000, 3000 # Get revenue when the cost is &#39;low&#39; and when the cost is &#39;high&#39; rev_low = get_revenue(get_signups(&#39;low&#39;, ct_rate, su_rate, sims)) rev_high = get_revenue(get_signups(&#39;high&#39;, ct_rate, su_rate, sims)) # calculate fraction of times rev_high - rev_low is less than cost_diff frac = np.sum(np.array(rev_high) - np.array(rev_low) &lt; cost_diff) / len(rev_low) print(&quot;Probability of losing money = {}&quot;.format(frac)) # Probability of losing money = 0.4659 python # **3. Resampling methods** -- ## **3.1 Introduction to resampling methods** ![Desktop View](/blog/assets/datacamp/statistical-simulation-in-python/4-3.png?w=1024) ![Desktop View](/blog/assets/datacamp/statistical-simulation-in-python/5-3.png?w=1024) ![Desktop View](/blog/assets/datacamp/statistical-simulation-in-python/6-3.png?w=984) #### **Probability example** In this exercise, we will review the difference between sampling with and without replacement. We will calculate the probability of an event using simulation, but vary our sampling method to see how it impacts probability. Consider a bowl filled with colored candies – three blue, two green, and five yellow. Draw three candies at random, with replacement and without replacement. You want to know the probability of **drawing a yellow candy on the third draw given that the first candy was blue and the second candy was green.** python # Set up the bowl success_rep, success_no_rep, sims = 0, 0, 10000 bowl = [&#39;b&#39;] * 3 + [&#39;g&#39;] * 2 + [&#39;y&#39;] * 5 for i in range(sims): # Sample with and without replacement &amp; increment success counters sample_rep = np.random.choice(bowl, replace=True, size=3) sample_no_rep = np.random.choice(bowl, replace=False, size=3) if (sample_rep[0] == &#39;b&#39;) &amp; (sample_rep[1] == &#39;g&#39;) &amp; (sample_rep[2] == &#39;y&#39;): success_rep += 1 if (sample_no_rep[0] == &#39;b&#39;) &amp; (sample_no_rep[1] == &#39;g&#39;) &amp; (sample_no_rep[2] == &#39;y&#39;): success_no_rep += 1 # Calculate probabilities prob_with_replacement = success_rep / sims prob_without_replacement = success_no_rep / sims print(&quot;Probability with replacement = {}, without replacement = {}&quot;.format(prob_with_replacement, prob_without_replacement)) # Probability with replacement = 0.0266, without replacement = 0.0415 . Does the difference between sampling with and without replacement make sense now? . . 3.2 Bootstrapping . . #### Running a simple bootstrap . Welcome to the first exercise in the bootstrapping section. We will work through an example where we learn to run a simple bootstrap. As we saw in the video, the main idea behind bootstrapping is sampling with replacement. . Suppose you own a factory that produces wrenches. You want to be able to characterize the average length of the wrenches and ensure that they meet some specifications. Your factory produces thousands of wrenches every day, but it’s infeasible to measure the length of each wrench. However, you have access to a representative sample of 100 wrenches. Let’s use bootstrapping to get the 95% confidence interval (CI) for the average lengths. . Examine the list wrench_lengths , which has 100 observed lengths of wrenches, in the shell. . # Draw some random sample with replacement and append mean to mean_lengths. mean_lengths, sims = [], 1000 for i in range(sims): temp_sample = np.random.choice(wrench_lengths, replace=True, size=100) sample_mean = np.mean(temp_sample) mean_lengths.append(sample_mean) # Calculate bootstrapped mean and 95% confidence interval. boot_mean = np.mean(mean_lengths) boot_95_ci = np.percentile(mean_lengths, [2.5, 97.5]) print(&quot;Bootstrapped Mean Length = {}, 95% CI = {}&quot;.format(boot_mean, boot_95_ci)) # Bootstrapped Mean Length = 10.027059690070363, 95% CI = [ 9.78662216 10.24854356] . #### Non-standard estimators . In the last exercise, you ran a simple bootstrap that we will now modify for more complicated estimators. . Suppose you are studying the health of students. You are given the height and weight of 1000 students and are interested in the median height as well as the correlation between height and weight and the associated 95% CI for these quantities. Let’s use bootstrapping. . Examine the pandas DataFrame df with the heights and weights of 1000 students. Using this, calculate the 95% CI for both the median height as well as the correlation between height and weight. . # Sample with replacement and calculate quantities of interest sims, data_size, height_medians, hw_corr = 1000, df.shape[0], [], [] for i in range(sims): tmp_df = df.sample(n=data_size, replace=True) height_medians.append(np.median(tmp_df.heights)) hw_corr.append(tmp_df.weights.corr(tmp_df.heights)) # Calculate confidence intervals height_median_ci = np.percentile(height_medians, [2.5, 97.5]) height_weight_corr_ci = np.percentile(hw_corr, [2.5, 97.5]) print(&quot;Height Median CI = {} nHeight Weight Correlation CI = {}&quot;.format( height_median_ci, height_weight_corr_ci)) # Height Median CI = [5.25262253 5.55928686] # Height Weight Correlation CI = [0.93892136 0.95103152] . #### Bootstrapping regression . Now let’s see how bootstrapping works with regression. Bootstrapping helps estimate the uncertainty of non-standard estimators. Consider the R2 statistic associated with a regression. When you run a simple least squares regression, you get a value for R2. But let’s see how can we get a 95% CI for R2. . Examine the DataFrame df with a dependent variable yy and two independent variables X1 and X2 using df.head() . We’ve already fit this regression with statsmodels ( sm ) using: . reg_fit = sm.OLS(df[&#39;y&#39;], df.iloc[:,1:]).fit() . Examine the result using reg_fit.summary() to find that R2=0.3504. Use bootstrapping to calculate the 95% CI. . df.head() y Intercept X1 X2 0 1.217851 1.0 0.696469 0.286139 1 1.555250 1.0 0.226851 0.551315 2 0.888520 1.0 0.719469 0.423106 3 1.736052 1.0 0.980764 0.684830 4 1.632073 1.0 0.480932 0.392118 . rsquared_boot, coefs_boot, sims = [], [], 1000 reg_fit = sm.OLS(df[&#39;y&#39;], df.iloc[:,1:]).fit() # Run 1K iterations for i in range(sims): # First create a bootstrap sample with replacement with n=df.shape[0] bootstrap = df.sample(n=df.shape[0], replace=True) # Fit the regression and append the r square to rsquared_boot rsquared_boot.append(sm.OLS(bootstrap[&#39;y&#39;],bootstrap.iloc[:,1:]).fit().rsquared) # Calculate 95% CI on rsquared_boot r_sq_95_ci = np.percentile(rsquared_boot, [2.5, 97.5]) print(&quot;R Squared 95% CI = {}&quot;.format(r_sq_95_ci)) # R Squared 95% CI = [0.31089312 0.40543591] . . 3.3 Jackknife resampling . . #### Basic jackknife estimation – mean . Jackknife resampling is an older procedure, which isn’t used as often compared as bootstrapping. However, it’s still useful to know how to run a basic jackknife estimation procedure. In this first exercise, we will calculate the jackknife estimate for the mean. Let’s return to the wrench factory. . You own a wrench factory and want to measure the average length of the wrenches to ensure that they meet some specifications. Your factory produces thousands of wrenches every day, but it’s infeasible to measure the length of each wrench. However, you have access to a representative sample of 100 wrenches. Let’s use jackknife estimation to get the average lengths. . Examine the variable wrench_lengths in the shell. . # Leave one observation out from wrench_lengths to get the jackknife sample and store the mean length mean_lengths, n = [], len(wrench_lengths) index = np.arange(n) for i in range(n): jk_sample = wrench_lengths[index != i] mean_lengths.append(np.mean(jk_sample)) # The jackknife estimate is the mean of the mean lengths from each sample mean_lengths_jk = np.mean(np.array(mean_lengths)) print(&quot;Jackknife estimate of the mean = {}&quot;.format(mean_lengths_jk)) . #### Jackknife confidence interval for the median . In this exercise, we will calculate the jackknife 95% CI for a non-standard estimator. Here, we will look at the median. Keep in mind that the variance of a jackknife estimator is n-1 times the variance of the individual jackknife sample estimates where n is the number of observations in the original sample. . Returning to the wrench factory, you are now interested in estimating the median length of the wrenches along with a 95% CI to ensure that the wrenches are within tolerance. . Let’s revisit the code from the previous exercise, but this time in the context of median lengths. By the end of this exercise, you will have a much better idea of how to use jackknife resampling to calculate confidence intervals for non-standard estimators. . n = 100 # Leave one observation out to get the jackknife sample and store the median length median_lengths = [] for i in range(n): jk_sample = wrench_lengths[index != i] median_lengths.append(np.median(jk_sample)) median_lengths = np.array(median_lengths) # Calculate jackknife estimate and it&#39;s variance jk_median_length = np.mean(median_lengths) jk_var = (n-1)*np.var(median_lengths) # Assuming normality, calculate lower and upper 95% confidence intervals jk_lower_ci = jk_median_length - 1.96*np.sqrt(jk_var) jk_upper_ci = jk_median_length + 1.96*np.sqrt(jk_var) print(&quot;Jackknife 95% CI lower = {}, upper = {}&quot;.format(jk_lower_ci, jk_upper_ci)) # Jackknife 95% CI lower = 9.138592467547202, upper = 10.754868069037098 . . 3.4 Permutation testing . . #### Generating a single permutation . In the next few exercises, we will run a significance test using permutation testing. As discussed in the video, we want to see if there’s any difference in the donations generated by the two designs – A and B. Suppose that you have been running both the versions for a few days and have generated 500 donations on A and 700 donations on B, stored in the variables donations_A and donations_B . . We first need to generate a null distribution for the difference in means. We will achieve this by generating multiple permutations of the dataset and calculating the difference in means for each case. . First, let’s generate one permutation and calculate the difference in means for the permuted dataset. . # Concatenate the two arrays donations_A and donations_B into data len_A, len_B = len(donations_A), len(donations_B) data = np.concatenate([donations_A, donations_B]) # Get a single permutation of the concatenated length perm = np.random.permutation(len(donations_A) + len(donations_B)) # Calculate the permutated datasets and difference in means permuted_A = data[perm[:len(donations_A)]] permuted_B = data[perm[len(donations_A):]] diff_in_means = np.mean(permuted_A) - np.mean(permuted_B) print(&quot;Difference in the permuted mean values = {}.&quot;.format(diff_in_means)) # Difference in the permuted mean values = -0.13886241452516757. . #### Hypothesis testing – Difference of means . We want to test the hypothesis that there is a difference in the average donations received from A and B. Previously, you learned how to generate one permutation of the data. Now, we will generate a null distribution of the difference in means and then calculate the p-value. . For the null distribution, we first generate multiple permuted datasets and store the difference in means for each case. We then calculate the test statistic as the difference in means with the original dataset. Finally, we calculate the p-value as twice the fraction of cases where the difference is greater than or equal to the absolute value of the test statistic (2-sided hypothesis). A p-value of less than say 0.05 could then determine statistical significance. . perm array([[ 850, 473, 1067, ..., 592, 644, 944], [ 594, 535, 125, ..., 1198, 1052, 233], [ 494, 246, 809, ..., 179, 448, 953], ..., [ 923, 888, 393, ..., 314, 40, 258], [ 915, 1140, 953, ..., 20, 526, 272], [ 925, 480, 1102, ..., 371, 85, 379]]) permuted_A_datasets array([[2.49959605e+00, 1.94959571e+00, 4.10970395e+00, ..., 1.28042313e+00, 3.09147765e+00, 3.00804073e-01], ..., [6.33883845e-01, 5.64755610e-01, 1.75817616e+00, ..., 5.46401636e-01, 1.97500056e+00, 5.43297027e+00]]) reps 1000 . # Generate permutations equal to the number of repetitions perm = np.array([np.random.permutation(len(donations_A) + len(donations_B)) for i in range(reps)]) permuted_A_datasets = data[perm[:, :len(donations_A)]] permuted_B_datasets = data[perm[:, len(donations_A):]] # Calculate the difference in means for each of the datasets samples = np.mean(permuted_A_datasets, axis=1) - np.mean(permuted_B_datasets, axis=1) # Calculate the test statistic and p-value test_stat = np.mean(donations_A) - np.mean(donations_B) p_val = 2*np.sum(samples &gt;= np.abs(test_stat))/reps print(&quot;p-value = {}&quot;.format(p_val)) # p-value = 0.002 . #### Hypothesis testing – Non-standard statistics . In the previous two exercises, we ran a permutation test for the difference in mean values. Now let’s look at non-standard statistics. . Suppose that you’re interested in understanding the distribution of the donations received from websites A and B. For this, you want to see if there’s a statistically significant difference in the median and the 80th percentile of the donations. Permutation testing gives you a wonderfully flexible framework for attacking such problems. . Let’s go through running a test to see if there’s a difference in the median and the 80th percentile of the distribution of donations. As before, you’re given the donations from the websites A and B in the variables donations_A and donations_B respectively. . # Calculate the difference in 80th percentile and median for each of the permuted datasets (A and B) samples_percentile = np.percentile(permuted_A_datasets, 80, axis=1) - np.percentile(permuted_B_datasets, 80, axis=1) samples_median = np.median(permuted_A_datasets, axis=1) - np.median(permuted_B_datasets, axis=1) # Calculate the test statistic from the original dataset and corresponding p-values test_stat_percentile = np.percentile(donations_A, 80) - np.percentile(donations_B, 80) test_stat_median = np.median(donations_A) - np.median(donations_B) p_val_percentile = 2*np.sum(samples_percentile &gt;= np.abs(test_stat_percentile))/reps p_val_median = 2*np.sum(samples_median &gt;= np.abs(test_stat_median))/reps print(&quot;80th Percentile: test statistic = {}, p-value = {}&quot;.format(test_stat_percentile, p_val_percentile)) print(&quot;Median: test statistic = {}, p-value = {}&quot;.format(test_stat_median, p_val_median)) . # 80th Percentile: test statistic = 1.6951624543447839, p-value = 0.026 # Median: test statistic = 0.6434965714975927, p-value = 0.014 python # **4. Advanced Applications of Simulation** - ## **4.1 Simulation for Business Planning** ![Desktop View](/blog/assets/datacamp/statistical-simulation-in-python/1-5.png?w=582) ![Desktop View](/blog/assets/datacamp/statistical-simulation-in-python/2-6.png?w=989) ![Desktop View](/blog/assets/datacamp/statistical-simulation-in-python/3-4.png?w=1024) #### **Modeling Corn Production** Suppose that you manage a small corn farm and are interested in optimizing your costs. In this exercise, we will model the production of corn. For simplicity, let’s assume that corn production depends on only two factors: rain, which you don’t control, and cost, which you control. Rain is normally distributed with mean 50 and standard deviation 15. For now, let’s fix cost at 5,000. Corn produced in any season is a Poisson random variable while the average corn production is governed by the equation: 100×(cost)^0.1×(rain)^0.2 Let’s model this production function and simulate one outcome. python # Initialize variables cost = 5000 rain = np.random.normal(loc=50, scale=15) # Corn Production Model def corn_produced(rain, cost): mean_corn = 100 * cost ** 0.1 * rain ** 0.2 corn = np.random.poisson(mean_corn) return corn # Simulate and print corn production corn_result = corn_produced(rain, cost) print(&quot;Simulated Corn Production = {}&quot;.format(corn_result)) # Simulated Corn Production = 560 . #### Modeling Profits . In the previous exercise, you built a model of corn production. For a small farm, you typically have no control over the price or demand for corn. Suppose that price is normally distributed with mean 40 and standard deviation 10. You are given a function corn_demanded() , which takes the price and determines the demand for corn. This is reasonable because demand is usually determined by the market and is not in your control. . In this exercise, you will work on a function to calculate the profit by pulling together all the other simulated variables. The only input to this function will be the cost. Upon completion, you will have a function that will give you one simulated profit outcome for a given cost. This function can then be used for planning your costs. . # Function to calculate profits def profits(cost): rain = np.random.normal(50, 15) price = np.random.normal(40, 10) supply = corn_produced(rain, cost) demand = corn_demanded(price) equil_short = supply &lt;= demand if equil_short == True: tmp = supply*price - cost return tmp else: tmp2 = demand*price - cost return tmp2 result = profits(cost) print(&quot;Simulated profit = {}&quot;.format(result)) # Simulated profit = 20675.3291075312 . #### Optimizing Costs . Now we will use the functions you’ve built to optimize our cost of production. We are interested in maximizing average profits. However, our profits depend on a number of factors, but we only control cost. Thus, we can simulate the uncertainty in the other factors and vary cost to see how our profits are impacted. . Since you manage the small corn farm, you have the ability to choose your cost – from $100 to $5,000. You want to choose the cost that gives you the maximum average profit. In this exercise, we will simulate multiple outcomes for each cost level and calculate an average. We will then choose the cost that gives us the maximum mean profit. Upon completion, you will have a framework for selecting optimal inputs for business decisions. . # Initialize results and cost_levels variables sims, results = 1000, {} cost_levels = np.arange(100, 5100, 100) # For each cost level, simulate profits and store mean profit for cost in cost_levels: tmp_profits = [] for i in range(sims): tmp_profits.append(profits(cost)) results[cost] = np.mean(tmp_profits) # Get the cost that maximizes average profit cost_max = [x for x in results.keys() if results[x] == max(results.values())][0] print(&quot;Average profit is maximized when cost = {}&quot;.format(cost_max)) # Average profit is maximized when cost = 1400 . Businesses use a similar framework with more details to help in a number of decisions. . 4.2 Monte Carlo Integration . . #### Integrating a Simple Function . This is a simple exercise introducing the concept of Monte Carlo Integration. . Here we will evaluate a simple integral ∫10xexdx∫01xexdx. We know that the exact answer is 11, but simulation will give us an approximate solution, so we can expect an answer close to 11. As we saw in the video, it’s a simple process. For a function of a single variable f(x): . Get the limits of the x-axis (xmin,xmax) and y-axis (max(f(x)),min(min(f(x)),0)). | Generate a number of uniformly distributed point in this box. | Multiply the area of the box ((max(f(x)−min(f(x))×(xmax−xmin) by the fraction of points that lie below f(x). | Upon completion, you will have a framework for handling definite integrals using Monte Carlo Integration. . # Define the sim_integrate function def sim_integrate(func, xmin, xmax, sims): x = np.random.uniform(xmin, xmax, sims) y = np.random.uniform(min(min(func(x)), 0), max(func(x)), sims) area = (max(y) - min(y))*(xmax-xmin) result = area * sum(abs(y) &lt; abs(func(x)))/sims return result # Call the sim_integrate function and print results result = sim_integrate(func = lambda x: x * np.exp(x), xmin = 0, xmax = 1, sims = 50) print(&quot;Simulated answer = {}, Actual Answer = 1&quot;.format(result)) # Simulated answer = 0.7240166789450252, Actual Answer = 1 . Try seeing what happens to the answer when you increase or decrease sims . . #### Calculating the value of pi . Now we work through a classic example – estimating the value of π. . Imagine a square of side 2 with the origin (0,0) as its center and the four corners having coordinates (1,1),(1,−1),(−1,1),(−1,−1). The area of this square is 2×2=4. Now imagine a circle of radius 1 with its center at the origin fitting perfectly inside this square. The area of the circle will be π×radius^2=π. . To estimate π, we randomly sample multiple points in this square &amp; get the fraction of points inside the circle (x^2+y^2&lt;=1). The area of the circle then is 4 times this fraction, which gives us our estimate of π. . After this exercise, you’ll have a grasp of how to use simulation for computation. . # Initialize sims and circle_points sims, circle_points = 10000, 0 for i in range(sims): # Generate the two coordinates of a point point = np.random.uniform(-1,1,size=2) # if the point lies within the unit circle, increment counter within_circle = point[0]**2 + point[1]**2 &lt;= 1 if within_circle == True: circle_points +=1 # Estimate pi as 4 times the avg number of points in the circle. pi_sim = 4*circle_points/sims print(&quot;Simulated value of pi = {}&quot;.format(pi_sim)) # Simulated value of pi = 3.1468 . . 4.3 Simulation for Power Analysis . . #### Factors influencing Statistical Power . In this exercise, you will refresh some basic concepts to test your understanding of statistical power. It is very important to understand statistical power, especially if you are designing an A/B test . . Consider the following four options and select the factor that influence the statistical power of an experiment: . Magnitude of the effect / Effect size | Sample Size | Statistical Significance Level (α) | . Note: The number of simulations doesn’t really impact the statistical power of an experiment. . #### Power Analysis – Part I . Now we turn to power analysis. You typically want to ensure that any experiment or A/B test you run has at least 80% power. One way to ensure this is to calculate the sample size required to achieve 80% power. . Suppose that you are in charge of a news media website and you are interested in increasing the amount of time users spend on your website. Currently, the time users spend on your website is normally distributed with a mean of 1 minute and a variance of 0.5 minutes. Suppose that you are introducing a feature that loads pages faster and want to know the sample size required to measure a 10% increase in time spent on the website. . In this exercise, we will set up the framework to run one simulation, run a t-test, &amp; calculate the p-value. . import scipy.stats as st # Initialize effect_size, control_mean, control_sd effect_size, sample_size, control_mean, control_sd = 0.05, 50, 1, 0.5 # Simulate control_time_spent and treatment_time_spent, assuming equal variance control_time_spent = np.random.normal(loc=control_mean, scale=control_sd, size=sample_size) treatment_time_spent = np.random.normal(loc=control_mean*(1+effect_size), scale=control_sd, size=sample_size) # Run the t-test and get the p_value t_stat, p_value = st.ttest_ind(control_time_spent, treatment_time_spent) stat_sig = p_value &lt; 0.05 print(&quot;P-value: {}, Statistically Significant? {}&quot;.format(p_value, stat_sig)) # P-value: 0.5766409395002308, Statistically Significant? False . #### Power Analysis – Part II . Previously, we simulated one instance of the experiment &amp; generated a p-value. We will now use this framework to calculate statistical power. Power of an experiment is the experiment’s ability to detect a difference between treatment &amp; control if the difference really exists. It’s good statistical hygiene to strive for 80% power. . For our website, we want to know how many people need to visit each variant, such that we can detect a 10% increase in time spent with 80% power. For this, we start with a small sample (50), simulate multiple instances of this experiment &amp; check power. If 80% power is reached, we stop. If not, we increase the sample size &amp; try again. . sample_size = 50 # Keep incrementing sample size by 10 till we reach required power while 1: control_time_spent = np.random.normal(loc=control_mean, scale=control_sd, size=(sample_size, sims)) treatment_time_spent = np.random.normal(loc=control_mean*(1+effect_size), scale=control_sd, size=(sample_size, sims)) t, p = st.ttest_ind(treatment_time_spent, control_time_spent) # Power is the fraction of times in the simulation when the p-value was less than 0.05 power = (p &lt; 0.05).sum()/sims if power &gt;= 0.8: break else: sample_size += 10 print(&quot;For 80% power, sample size required = {}&quot;.format(sample_size)) # For 80% power, sample size required = 360 . . 4.4 Applications in Finance . . #### Portfolio Simulation – Part I . In the next few exercises, you will calculate the expected returns of a stock portfolio &amp; characterize its uncertainty. . Suppose you have invested $10,000 in your portfolio comprising of multiple stocks. You want to evaluate the portfolio’s performance over 10 years. You can tweak your overall expected rate of return and volatility (standard deviation of the rate of return). Assume the rate of return follows a normal distribution. . First, let’s write a function that takes the principal (initial investment), number of years, expected rate of return and volatility as inputs and returns the portfolio’s total value after 10 years. . Upon completion of this exercise, you will have a function you can call to determine portfolio performance. . # rates is a Normal random variable and has size equal to number of years def portfolio_return(yrs, avg_return, sd_of_return, principal): np.random.seed(123) rates = np.random.normal(loc=avg_return, scale=sd_of_return, size=yrs) # Calculate the return at the end of the period end_return = principal for x in rates: end_return = end_return * (1 + x) return end_return result = portfolio_return(yrs = 5, avg_return = 0.07, sd_of_return = 0.15, principal = 1000) print(&quot;Portfolio return after 5 years = {}&quot;.format(result)) # Portfolio return after 5 years = 1021.4013412039292 . #### Portfolio Simulation – Part II . Now we will use the simulation function you built to evaluate 10-year returns. . Your stock-heavy portfolio has an initial investment of $10,000, an expected return of 7% and a volatility of 30%. You want to get a 95% confidence interval of what your investment will be worth in 10 years. We will simulate multiple samples of 10-year returns and calculate the confidence intervals on the distribution of returns. . By the end of this exercise, you will have run a complete portfolio simulation. . The function portfolio_return() from the previous exercise is already initialized in the environment. . # Run 1,000 iterations and store the results sims, rets = 1000, [] for i in range(sims): rets.append(portfolio_return(yrs = 10, avg_return = 0.07, volatility = 0.3, principal = 10000)) # Calculate the 95% CI lower_ci = np.percentile(rets, 2.5) upper_ci = np.percentile(rets, 97.5) print(&quot;95% CI of Returns: Lower = {}, Upper = {}&quot;.format(lower_ci, upper_ci)) # 95% CI of Returns: Lower = 1236.4468015417674, Upper = 79510.31743325583 . #### Portfolio Simulation – Part III . Previously, we ran a complete simulation to get a distribution for 10-year returns. Now we will use simulation for decision making. . Let’s go back to your stock-heavy portfolio with an expected return of 7% and a volatility of 30%. You have the choice of rebalancing your portfolio with some bonds such that the expected return is 4% &amp; volatility is 10%. You have a principal of $10,000. You want to select a strategy based on how much your portfolio will be worth in 10 years. Let’s simulate returns for both the portfolios and choose based on the least amount you can expect with 75% probability (25th percentile). . Upon completion, you will know how to use a portfolio simulation for investment decisions. . The portfolio_return() function is again pre-loaded in the environment. . for i in range(sims): rets_stock.append(portfolio_return(yrs = 10, avg_return = 0.07, volatility = 0.3, principal = 10000)) rets_bond.append(portfolio_return(yrs = 10, avg_return = 0.04, volatility = 0.1, principal = 10000)) # Calculate the 25th percentile of the distributions and the amount you&#39;d lose or gain rets_stock_perc = np.percentile(rets_stock, 25) rets_bond_perc = np.percentile(rets_bond, 25) additional_returns = rets_stock_perc - rets_bond_perc print(&quot;Sticking to stocks gets you an additional return of {}&quot;.format(additional_returns)) # Sticking to stocks gets you an additional return of -5518.530403193416 . #### Summary . . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/statistical-simulation-in-python.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/statistical-simulation-in-python.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Preprocessing for Machine Learning in Python",
            "content": "Preprocessing for Machine Learning in Python . This is the memo of the 8th course (23 courses in all) of ‘Machine Learning Scientist with Python’ skill track. . You can find the original course HERE . . ### Course Description . This course covers the basics of how and when to perform data preprocessing. This essential step in any machine learning project is when you get your data ready for modeling. Between importing and cleaning your data and fitting your machine learning model is when preprocessing comes into play. You’ll learn how to standardize your data so that it’s in the right form for your model, create new features to best leverage the information in your dataset, and select the best features to improve your model fit. Finally, you’ll have some practice preprocessing by getting a dataset on UFO sightings ready for modeling. . ### . Introduction to Data Preprocessing | Standardizing Data | Feature Engineering | Selecting features for modeling | Putting it all together | 1. Introduction to Data Preprocessing . . 1.1 What is data preprocessing? . . 1.1.1 Missing data – columns . We have a dataset comprised of volunteer information from New York City. The dataset has a number of features, but we want to get rid of features that have at least 3 missing values. . How many features are in the original dataset, and how many features are in the set after columns with at least 3 missing values are removed? . volunteer.shape # (665, 35) volunteer.dropna(axis=1,thresh=3).shape # (665, 24) . 1.1.2 Missing data – rows . Taking a look at the volunteer dataset again, we want to drop rows where the category_desc column values are missing. We’re going to do this using boolean indexing, by checking to see if we have any null values, and then filtering the dataset so that we only have rows with those values. . # Check how many values are missing in the category_desc column print(volunteer[&#39;category_desc&#39;].isnull().sum()) # 48 # Subset the volunteer dataset volunteer_subset = volunteer[volunteer[&#39;category_desc&#39;].notnull()] # Print out the shape of the subset print(volunteer_subset.shape) # (617, 35) . . 1.2 Working with data types . . 1.2.1 Exploring data types . Taking another look at the dataset comprised of volunteer information from New York City, we want to know what types we’ll be working with as we start to do more preprocessing. . Which data types are present in the volunteer dataset? . set(volunteer.dtypes.values) {dtype(&#39;int64&#39;), dtype(&#39;float64&#39;), dtype(&#39;O&#39;)} . 1.2.2 Converting a column type . If you take a look at the volunteer dataset types, you’ll see that the column hits is type object . But, if you actually look at the column, you’ll see that it consists of integers. Let’s convert that column to type int . . volunteer[&quot;hits&quot;].dtype # dtype(&#39;O&#39;) # Convert the hits column to type int volunteer[&quot;hits&quot;] = volunteer[&quot;hits&quot;].astype(&#39;int&#39;) volunteer[&quot;hits&quot;].dtype # dtype(&#39;int64&#39;) . . 1.3 Class distribution . . 1.3.1 Class imbalance . In the volunteer dataset, we’re thinking about trying to predict the category_desc variable using the other features in the dataset. First, though, we need to know what the class distribution (and imbalance) is for that label. . Which descriptions occur less than 50 times in the volunteer dataset? . volunteer.category_desc.value_counts() Strengthening Communities 307 Helping Neighbors in Need 119 Education 92 Health 52 Environment 32 Emergency Preparedness 15 Name: category_desc, dtype: int64 . 1.3.2 Stratified sampling . We know that the distribution of variables in the category_desc column in the volunteer dataset is uneven. If we wanted to train a model to try to predict category_desc , we would want to train the model on a sample of data that is representative of the entire dataset. Stratified sampling is a way to achieve this. . # Create a data with all columns except category_desc volunteer_X = volunteer.drop(&#39;category_desc&#39;, axis=1) # Create a category_desc labels dataset volunteer_y = volunteer[[&#39;category_desc&#39;]] # Use stratified sampling to split up the dataset according to the volunteer_y dataset X_train, X_test, y_train, y_test = train_test_split(volunteer_X, volunteer_y, stratify=volunteer_y) # Print out the category_desc counts on the training y labels print(y_train[&#39;category_desc&#39;].value_counts()) . Strengthening Communities 230 Helping Neighbors in Need 89 Education 69 Health 39 Environment 24 Emergency Preparedness 11 Name: category_desc, dtype: int64 . 2. Standardizing Data . . 2.1 Standardizing Data . . 2.1.1 When to standardize . A column you want to use for modeling has extremely high variance. | You have a dataset with several continuous columns on different scales and you’d like to use a linear model to train the data. | The models you’re working with use some sort of distance metric in a linear space, like the Euclidean metric. | . 2.1.2 Modeling without normalizing . Let’s take a look at what might happen to your model’s accuracy if you try to model data without doing some sort of standardization first. Here we have a subset of the wine dataset. One of the columns, Proline , has an extremely high variance compared to the other columns. This is an example of where a technique like log normalization would come in handy, which you’ll learn about in the next section. . # Split the dataset and labels into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y) # Fit the k-nearest neighbors model to the training data knn.fit(X_train,y_train) # Score the model on the test data print(knn.score(X_test,y_test)) # 0.5333333333333333 . . 2.2 Log normalization . . 2.2.1 Checking the variance . Check the variance of the columns in the wine dataset. Which column is a candidate for normalization? . wine.var() Type 0.600679 Alcohol 0.659062 Malic acid 1.248015 Ash 0.075265 Alcalinity of ash 11.152686 Magnesium 203.989335 Total phenols 0.391690 Flavanoids 0.997719 Nonflavanoid phenols 0.015489 Proanthocyanins 0.327595 Color intensity 5.374449 Hue 0.052245 OD280/OD315 of diluted wines 0.504086 Proline 99166.717355 dtype: float64 . Proline 99166.717355 . 2.2.2 Log normalization in Python . Now that we know that the Proline column in our wine dataset has a large amount of variance, let’s log normalize it. . # Print out the variance of the Proline column print(wine.Proline.var()) # 99166.71735542436 # Apply the log normalization function to the Proline column wine[&#39;Proline_log&#39;] = np.log(wine.Proline) # Check the variance of the Proline column again print(wine.Proline_log.var()) # 0.17231366191842012 . . 2.3 Scaling data for feature comparison . . 2.3.1 Scaling data – investigating columns . We want to use the Ash , Alcalinity of ash , and Magnesium columns in the wine dataset to train a linear model, but it’s possible that these columns are all measured in different ways, which would bias a linear model. . wine[[&#39;Ash&#39;,&#39;Alcalinity of ash&#39;,&#39;Magnesium&#39;]].describe() Ash Alcalinity of ash Magnesium count 178.000000 178.000000 178.000000 mean 2.366517 19.494944 99.741573 std 0.274344 3.339564 14.282484 min 1.360000 10.600000 70.000000 25% 2.210000 17.200000 88.000000 50% 2.360000 19.500000 98.000000 75% 2.557500 21.500000 107.000000 max 3.230000 30.000000 162.000000 . 2.3.2 Scaling data – standardizing columns . Since we know that the Ash , Alcalinity of ash , and Magnesium columns in the wine dataset are all on different scales, let’s standardize them in a way that allows for use in a linear model. . # Import StandardScaler from scikit-learn from sklearn.preprocessing import StandardScaler # Create the scaler ss = StandardScaler() # Take a subset of the DataFrame you want to scale wine_subset = wine[[&#39;Ash&#39;,&#39;Alcalinity of ash&#39;,&#39;Magnesium&#39;]] # Apply the scaler to the DataFrame subset wine_subset_scaled = ss.fit_transform(wine_subset) . wine_subset_scaled[:5] array([[ 0.23205254, -1.16959318, 1.91390522], [-0.82799632, -2.49084714, 0.01814502], [ 1.10933436, -0.2687382 , 0.08835836], [ 0.4879264 , -0.80925118, 0.93091845], [ 1.84040254, 0.45194578, 1.28198515]]) . . 2.4 Standardized data and modeling . 2.4.1 KNN on non-scaled data . Let’s first take a look at the accuracy of a K-nearest neighbors model on the wine dataset without standardizing the data. . # Split the dataset and labels into training and test sets X_train, X_test, y_train, y_test = train_test_split(X,y) # Fit the k-nearest neighbors model to the training data knn.fit(X_train, y_train) # Score the model on the test data print(knn.score(X_test, y_test)) # 0.6444444444444445 . 2.4.2 KNN on scaled data . The accuracy score on the unscaled wine dataset was decent, but we can likely do better if we scale the dataset. The process is mostly the same as the previous exercise, with the added step of scaling the data. . # Create the scaling method. ss = StandardScaler() # Apply the scaling method to the dataset used for modeling. X_scaled = ss.fit_transform(X) X_train, X_test, y_train, y_test = train_test_split(X_scaled, y) # Fit the k-nearest neighbors model to the training data. knn.fit(X_train,y_train) # Score the model on the test data. print(knn.score(X_test,y_test)) # 0.9555555555555556 . 3. Feature Engineering . . 3.1 Feature engineering . . 3.1.1 Examples for creating new features . timestamps | newspaper headlines | . Timestamps can be broken into days or months, and headlines can be used for natural language processing. . 3.1.2 Identifying areas for feature engineering . volunteer[[&#39;title&#39;,&#39;created_date&#39;,&#39;category_desc&#39;]].head(1) title created_date category_desc 0 Volunteers Needed For Rise Up &amp; Stay Put! Home... January 13 2011 Strengthening Communities . All of these columns will require some feature engineering before modeling. . . 3.2 Encoding categorical variables . . 3.2.1 Encoding categorical variables – binary . Take a look at the hiking dataset. There are several columns here that need encoding, one of which is the Accessible column, which needs to be encoded in order to be modeled. Accessible is a binary feature, so it has two values – either Y or N – so it needs to be encoded into 1s and 0s. Use scikit-learn’s LabelEncoder method to do that transformation. . # Set up the LabelEncoder object enc = LabelEncoder() # Apply the encoding to the &quot;Accessible&quot; column hiking[&#39;Accessible_enc&#39;] = enc.fit_transform(hiking.Accessible) # Compare the two columns print(hiking[[&#39;Accessible&#39;, &#39;Accessible_enc&#39;]].head()) . Accessible Accessible_enc 0 Y 1 1 N 0 2 N 0 3 N 0 4 N 0 . 3.2.2 Encoding categorical variables – one-hot . One of the columns in the volunteer dataset, category_desc , gives category descriptions for the volunteer opportunities listed. Because it is a categorical variable with more than two categories, we need to use one-hot encoding to transform this column numerically. . # Transform the category_desc column category_enc = pd.get_dummies(volunteer[&quot;category_desc&quot;]) # Take a look at the encoded columns print(category_enc.head()) . Education Emergency Preparedness ... Helping Neighbors in Need Strengthening Communities 0 0 0 ... 0 0 1 0 0 ... 0 1 2 0 0 ... 0 1 3 0 0 ... 0 1 4 0 0 ... 0 0 [5 rows x 6 columns] . . 3.3 Engineering numerical features . . 3.3.1 Engineering numerical features – taking an average . A good use case for taking an aggregate statistic to create a new feature is to take the mean of columns. Here, you have a DataFrame of running times named running_times_5k . For each name in the dataset, take the mean of their 5 run times. . # Create a list of the columns to average run_columns = [&#39;run1&#39;, &#39;run2&#39;, &#39;run3&#39;, &#39;run4&#39;, &#39;run5&#39;] # Use apply to create a mean column # axis=1 = row wise running_times_5k[&quot;mean&quot;] = running_times_5k.apply(lambda row: row[run_columns].mean(), axis=1) # Take a look at the results print(running_times_5k) . name run1 run2 run3 run4 run5 mean 0 Sue 20.1 18.5 19.6 20.3 18.3 19.36 1 Mark 16.5 17.1 16.9 17.6 17.3 17.08 2 Sean 23.5 25.1 25.2 24.6 23.9 24.46 3 Erin 21.7 21.1 20.9 22.1 22.2 21.60 4 Jenny 25.8 27.1 26.1 26.7 26.9 26.52 5 Russell 30.9 29.6 31.4 30.4 29.9 30.44 . 3.3.2 Engineering numerical features – datetime . There are several columns in the volunteer dataset comprised of datetimes. Let’s take a look at the start_date_date column and extract just the month to use as a feature for modeling. . # First, convert string column to date column volunteer[&quot;start_date_converted&quot;] = pd.to_datetime(volunteer[&quot;start_date_date&quot;]) # Extract just the month from the converted column volunteer[&quot;start_date_month&quot;] = volunteer[&quot;start_date_converted&quot;].apply(lambda row: row.month) # Take a look at the converted and new month columns print(volunteer[[&#39;start_date_converted&#39;, &#39;start_date_month&#39;]].head()) . start_date_converted start_date_month 0 2011-07-30 7 1 2011-02-01 2 2 2011-01-29 1 3 2011-02-14 2 4 2011-02-05 2 . . 3.4 Text classification . . 3.4.1 Engineering features from strings – extraction . The Length column in the hiking dataset is a column of strings, but contained in the column is the mileage for the hike. We’re going to extract this mileage using regular expressions, and then use a lambda in Pandas to apply the extraction to the DataFrame. . # Write a pattern to extract numbers and decimals def return_mileage(length): pattern = re.compile(r&quot; d+ . d+&quot;) # Search the text for matches mile = re.match(pattern, length) # If a value is returned, use group(0) to return the found value if mile is not None: return float(mile.group(0)) # Apply the function to the Length column and take a look at both columns hiking[&quot;Length_num&quot;] = hiking[&#39;Length&#39;].apply(lambda row: return_mileage(row)) print(hiking[[&quot;Length&quot;, &quot;Length_num&quot;]].head()) . Length Length_num 0 0.8 miles 0.80 1 1.0 mile 1.00 2 0.75 miles 0.75 3 0.5 miles 0.50 4 0.5 miles 0.50 . 3.4.2 Engineering features from strings – tf/idf . Let’s transform the volunteer dataset’s title column into a text vector, to use in a prediction task in the next exercise. . # Take the title text title_text = volunteer[&#39;title&#39;] # Create the vectorizer method tfidf_vec = TfidfVectorizer() # Transform the text into tf-idf vectors text_tfidf = tfidf_vec.fit_transform(title_text) . text_tfidf &lt;665x1136 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39; with 3397 stored elements in Compressed Sparse Row format&gt; . 3.4.3 Text classification using tf/idf vectors . Now that we’ve encoded the volunteer dataset’s title column into tf/idf vectors, let’s use those vectors to try to predict the category_desc column. . text_tfidf.toarray() array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]]) text_tfidf.toarray().shape # (617, 1089) volunteer[&quot;category_desc&quot;].head() 1 Strengthening Communities 2 Strengthening Communities 3 Strengthening Communities 4 Environment 5 Environment Name: category_desc, dtype: object . # Split the dataset according to the class distribution of category_desc y = volunteer[&quot;category_desc&quot;] X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y) # Fit the model to the training data nb.fit(X_train, y_train) # Print out the model&#39;s accuracy print(nb.score(X_test, y_test)) # 0.567741935483871 . Notice that the model doesn’t score very well. We’ll work on selecting the best features for modeling in the next chapter. . 4. Selecting features for modeling . . 4.1 Feature selection . . 4.1.1 Identifying areas for feature selection . Take an exploratory look at the post-feature engineering hiking dataset. Which of the following columns is a good candidate for feature selection? . hiking.columns Index([&#39;Accessible&#39;, &#39;Difficulty&#39;, &#39;Length&#39;, &#39;Limited_Access&#39;, &#39;Location&#39;, &#39;Name&#39;, &#39;Other_Details&#39;, &#39;Park_Name&#39;, &#39;Prop_ID&#39;, &#39;lat&#39;, &#39;lon&#39;, &#39;Length_extract&#39;, &#39;accessible_enc&#39;, &#39;&#39;, &#39;Easy&#39;, &#39;Easy &#39;, &#39;Easy/Moderate&#39;, &#39;Moderate&#39;, &#39;Moderate/Difficult&#39;, &#39;Various&#39;], dtype=&#39;object&#39;) . Length | Difficulty | Accessible | . All three of these columns are good candidates for feature selection. . . 4.2 Removing redundant features . . 4.2.1 Selecting relevant features . Now let’s identify the redundant columns in the volunteer dataset and perform feature selection on the dataset to return a DataFrame of the relevant features. . For example, if you explore the volunteer dataset in the console, you’ll see three features which are related to location: locality , region , and postalcode . They contain repeated information, so it would make sense to keep only one of the features. . There are also features that have gone through the feature engineering process: columns like Education and Emergency Preparedness are a product of encoding the categorical variable category_desc , so category_desc itself is redundant now. . Take a moment to examine the features of volunteer in the console, and try to identify the redundant features. . # Create a list of redundant column names to drop to_drop = [&quot;locality&quot;, &quot;region&quot;, &quot;category_desc&quot;, &quot;created_date&quot;, &quot;vol_requests&quot;] # Drop those columns from the dataset volunteer_subset = volunteer.drop(to_drop, axis=1) # Print out the head of the new dataset print(volunteer_subset.head()) . volunteer_subset.columns Index([&#39;title&#39;, &#39;hits&#39;, &#39;postalcode&#39;, &#39;vol_requests_lognorm&#39;, &#39;created_month&#39;, &#39;Education&#39;, &#39;Emergency Preparedness&#39;, &#39;Environment&#39;, &#39;Health&#39;, &#39;Helping Neighbors in Need&#39;, &#39;Strengthening Communities&#39;], dtype=&#39;object&#39;) . 4.2.2 Checking for correlated features . Let’s take a look at the wine dataset again, which is made up of continuous, numerical features. Run Pearson’s correlation coefficient on the dataset to determine which columns are good candidates for eliminating. Then, remove those columns from the DataFrame. . # Print out the column correlations of the wine dataset print(wine.corr()) # Take a minute to find the column where the correlation value is greater than 0.75 at least twice to_drop = &quot;Flavanoids&quot; # Drop that column from the DataFrame wine = wine.drop(to_drop, axis=1) . print(wine.corr()) Flavanoids Total phenols Malic acid OD280/OD315 of diluted wines Hue Flavanoids 1.000000 0.864564 -0.411007 0.787194 0.543479 Total phenols 0.864564 1.000000 -0.335167 0.699949 0.433681 Malic acid -0.411007 -0.335167 1.000000 -0.368710 -0.561296 OD280/OD315 of diluted wines 0.787194 0.699949 -0.368710 1.000000 0.565468 Hue 0.543479 0.433681 -0.561296 0.565468 1.000000 . . 4.3 Selecting features using text vectors . . 4.3.1 Exploring text vectors, part 1 . Let’s expand on the text vector exploration method we just learned about, using the volunteer dataset’s title tf/idf vectors. In this first part of text vector exploration, we’re going to add to that function we learned about in the slides. We’ll return a list of numbers with the function. In the next exercise, we’ll write another function to collect the top words across all documents, extract them, and then use that list to filter down our text_tfidf vector. . vocab {1048: &#39;web&#39;, 278: &#39;designer&#39;, 1017: &#39;urban&#39;, ...} tfidf_vec TfidfVectorizer(analyzer=&#39;word&#39;, binary=False, decode_error=&#39;strict&#39;, dtype=&lt;class &#39;numpy.int64&#39;&gt;, encoding=&#39;utf-8&#39;, input=&#39;content&#39;, lowercase=True, max_df=1.0, max_features=None, min_df=1, ngram_range=(1, 1), norm=&#39;l2&#39;, preprocessor=None, smooth_idf=True, stop_words=None, strip_accents=None, sublinear_tf=False, token_pattern=&#39;(?u) b w w+ b&#39;, tokenizer=None, use_idf=True, vocabulary=None) tfidf_vec.vocabulary_ {&#39;web&#39;: 1048, &#39;designer&#39;: 278, &#39;urban&#39;: 1017, ...} text_tfidf &lt;617x1089 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39; with 3172 stored elements in Compressed Sparse Row format&gt; . # Add in the rest of the parameters def return_weights(vocab, original_vocab, vector, vector_index, top_n): zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data)) # Let&#39;s transform that zipped dict into a series zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices}) # Let&#39;s sort the series to pull out the top n weighted words zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index return [original_vocab[i] for i in zipped_index] # Print out the weighted words print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, vector_index=8, top_n=3)) # [189, 942, 466] . 4.3.2 Exploring text vectors, part 2 . Using the function we wrote in the previous exercise, we’re going to extract the top words from each document in the text vector, return a list of the word indices, and use that list to filter the text vector down to those top words. . def words_to_filter(vocab, original_vocab, vector, top_n): filter_list = [] for i in range(0, vector.shape[0]): # Here we&#39;ll call the function from the previous exercise, and extend the list we&#39;re creating filtered = return_weights(vocab, original_vocab, vector, i, top_n) filter_list.extend(filtered) # Return the list in a set, so we don&#39;t get duplicate word indices return set(filter_list) # Call the function to get the list of word indices filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3) # By converting filtered_words back to a list, we can use it to filter the columns in the text vector filtered_text = text_tfidf[:, list(filtered_words)] filtered_text &lt;617x1008 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39; with 2948 stored elements in Compressed Sparse Row format&gt; . 4.3.3 Training Naive Bayes with feature selection . Let’s re-run the Naive Bayes text classification model we ran at the end of chapter 3, with our selection choices from the previous exercise, on the volunteer dataset’s title and category_desc columns. . # Split the dataset according to the class distribution of category_desc, using the filtered_text vector train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y) # Fit the model to the training data nb.fit(train_X, train_y) # Print out the model&#39;s accuracy print(nb.score(test_X,test_y)) # 0.567741935483871 . You can see that our accuracy score wasn’t that different from the score at the end of chapter 3. That’s okay; the title field is a very small text field, appropriate for demonstrating how filtering vectors works. . . 4.4 Dimensionality reduction . . 4.4.1 Using PCA . Let’s apply PCA to the wine dataset, to see if we can get an increase in our model’s accuracy. . from sklearn.decomposition import PCA # Set up PCA and the X vector for diminsionality reduction pca = PCA() wine_X = wine.drop(&quot;Type&quot;, axis=1) # Apply PCA to the wine dataset X vector transformed_X = pca.fit_transform(wine_X) # Look at the percentage of variance explained by the different components print(pca.explained_variance_ratio_) . [9.98091230e-01 1.73591562e-03 9.49589576e-05 5.02173562e-05 1.23636847e-05 8.46213034e-06 2.80681456e-06 1.52308053e-06 1.12783044e-06 7.21415811e-07 3.78060267e-07 2.12013755e-07 8.25392788e-08] . 4.4.2 Training a model with PCA . Now that we have run PCA on the wine dataset, let’s try training a model with it. . # Split the transformed X and the y labels into training and test sets X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(transformed_X,y) # Fit knn to the training data knn.fit(X_wine_train,y_wine_train) # Score knn on the test data and print it out knn.score(X_wine_test,y_wine_test) # 0.6444444444444445 . 5. Putting it all together . . 5.1 UFOs and preprocessing . ufo.head() date city state country type seconds 2 2002-11-21 05:45:00 clemmons nc us triangle 300.0 4 2012-06-16 23:00:00 san diego ca us light 600.0 7 2013-06-09 00:00:00 oakville (canada) on ca light 120.0 8 2013-04-26 23:27:00 lacey wa us light 120.0 9 2013-09-13 20:30:00 ben avon pa us sphere 300.0 length_of_time desc 2 about 5 minutes It was a large, triangular shaped flying ob... 4 10 minutes Dancing lights that would fly around and then ... 7 2 minutes Brilliant orange light or chinese lantern at o... 8 2 minutes Bright red light moving north to north west fr... 9 5 minutes North-east moving south-west. First 7 or so li... recorded lat long 2 12/23/2002 36.021389 -80.382222 4 7/4/2012 32.715278 -117.156389 7 7/3/2013 43.433333 -79.666667 8 5/15/2013 47.034444 -122.821944 9 9/30/2013 40.508056 -80.083333 . . 5.1.1 Checking column types . Take a look at the UFO dataset’s column types using the dtypes attribute. Two columns jump out for transformation: the seconds column, which is a numeric column but is being read in as object , and the date column, which can be transformed into the datetime type. That will make our feature engineering efforts easier later on. . # Check the column types print(ufo.dtypes) # Change the type of seconds to float ufo[&quot;seconds&quot;] = ufo.seconds.astype(&#39;float&#39;) # Change the date column to type datetime ufo[&quot;date&quot;] = pd.to_datetime(ufo[&#39;date&#39;]) # Check the column types print(ufo[[&#39;seconds&#39;,&#39;date&#39;]].dtypes) . date object city object state object country object type object seconds object length_of_time object desc object recorded object lat object long float64 dtype: object seconds float64 date datetime64[ns] dtype: object . 5.1.2 Dropping missing data . Let’s remove some of the rows where certain columns have missing values. We’re going to look at the length_of_time column, the state column, and the type column. If any of the values in these columns are missing, we’re going to drop the rows. . # Check how many values are missing in the length_of_time, state, and type columns print(ufo[[&#39;length_of_time&#39;, &#39;state&#39;, &#39;type&#39;]].isnull().sum()) # Keep only rows where length_of_time, state, and type are not null ufo_no_missing = ufo[ufo[&#39;length_of_time&#39;].notnull() &amp; ufo[&#39;state&#39;].notnull() &amp; ufo[&#39;type&#39;].notnull()] # Print out the shape of the new dataset print(ufo_no_missing.shape) . length_of_time 143 state 419 type 159 dtype: int64 (4283, 4) . . 5.2 Categorical variables and standardization . . 5.2.1 Extracting numbers from strings . The length_of_time field in the UFO dataset is a text field that has the number of minutes within the string. Here, you’ll extract that number from that text field using regular expressions. . def return_minutes(time_string): # Use d+ to grab digits pattern = re.compile(r&quot; d+&quot;) # Use match on the pattern and column num = re.match(pattern, time_string) if num is not None: return int(num.group(0)) # Apply the extraction to the length_of_time column ufo[&quot;minutes&quot;] = ufo[&quot;length_of_time&quot;].apply(return_minutes) # Take a look at the head of both of the columns print(ufo[[&#39;length_of_time&#39;,&#39;minutes&#39;]].head()) . length_of_time minutes 2 about 5 minutes NaN 4 10 minutes 10.0 7 2 minutes 2.0 8 2 minutes 2.0 9 5 minutes 5.0 . 5.2.2 Identifying features for standardization . In this section, you’ll investigate the variance of columns in the UFO dataset to determine which features should be standardized. After taking a look at the variances of the seconds and minutes column, you’ll see that the variance of the seconds column is extremely high. Because seconds and minutes are related to each other (an issue we’ll deal with when we select features for modeling), let’s log normlize the seconds column. . # Check the variance of the seconds and minutes columns print(ufo[[&#39;seconds&#39;,&#39;minutes&#39;]].var()) # Log normalize the seconds column ufo[&quot;seconds_log&quot;] = np.log(ufo[[&#39;seconds&#39;]]) # Print out the variance of just the seconds_log column print(ufo[&quot;seconds_log&quot;].var()) . seconds 424087.417474 minutes 117.546372 dtype: float64 1.1223923881183004 . . 5.3 Engineering new features . . 5.3.1 Encoding categorical variables . There are couple of columns in the UFO dataset that need to be encoded before they can be modeled through scikit-learn. You’ll do that transformation here, using both binary and one-hot encoding methods. . # Use Pandas to encode us values as 1 and others as 0 ufo[&quot;country_enc&quot;] = ufo[&quot;country&quot;].apply(lambda x: 1 if x==&#39;us&#39; else 0) # Print the number of unique type values print(len(ufo[&#39;type&#39;].unique())) # 21 # Create a one-hot encoded set of the type values type_set = pd.get_dummies(ufo[&#39;type&#39;]) # Concatenate this set back to the ufo DataFrame ufo = pd.concat([ufo, type_set], axis=1) . 5.3.2 Features from dates . Another feature engineering task to perform is month and year extraction. Perform this task on the date column of the ufo dataset. . # Look at the first 5 rows of the date column print(ufo[&#39;date&#39;].head()) # Extract the month from the date column ufo[&quot;month&quot;] = ufo[&quot;date&quot;].apply(lambda x:x.month) # Extract the year from the date column ufo[&quot;year&quot;] = ufo[&quot;date&quot;].apply(lambda x:x.year) # Take a look at the head of all three columns print(ufo[[&#39;date&#39;,&#39;month&#39;,&#39;year&#39;]].head()) . 0 2002-11-21 05:45:00 1 2012-06-16 23:00:00 2 2013-06-09 00:00:00 3 2013-04-26 23:27:00 4 2013-09-13 20:30:00 Name: date, dtype: datetime64[ns] date month year 0 2002-11-21 05:45:00 11 2002 1 2012-06-16 23:00:00 6 2012 2 2013-06-09 00:00:00 6 2013 3 2013-04-26 23:27:00 4 2013 4 2013-09-13 20:30:00 9 2013 . 5.3.3 Text vectorization . Let’s transform the desc column in the UFO dataset into tf/idf vectors, since there’s likely something we can learn from this field. . # Take a look at the head of the desc field print(ufo[&quot;desc&quot;].head()) # Create the tfidf vectorizer object vec = TfidfVectorizer() # Use vec&#39;s fit_transform method on the desc field desc_tfidf = vec.fit_transform(ufo[&quot;desc&quot;]) # Look at the number of columns this creates print(desc_tfidf.shape) . 0 It was a large, triangular shaped flying ob... 1 Dancing lights that would fly around and then ... 2 Brilliant orange light or chinese lantern at o... 3 Bright red light moving north to north west fr... 4 North-east moving south-west. First 7 or so li... Name: desc, dtype: object (1866, 3422) . . 5.4 Feature selection and modeling . . 5.4.1 Selecting the ideal dataset . Let’s get rid of some of the unnecessary features. Because we have an encoded country column, country_enc , keep it and drop other columns related to location: city , country , lat , long , state . . We have columns related to month and year , so we don’t need the date or recorded columns. . We vectorized desc , so we don’t need it anymore. For now we’ll keep type . . We’ll keep seconds_log and drop seconds and minutes . . Let’s also get rid of the length_of_time column, which is unnecessary after extracting minutes . . # Check the correlation between the seconds, seconds_log, and minutes columns print(ufo[[&#39;seconds&#39;,&#39;seconds_log&#39;,&#39;minutes&#39;]].corr()) # Make a list of features to drop to_drop = [&#39;city&#39;, &#39;country&#39;, &#39;date&#39;, &#39;desc&#39;, &#39;lat&#39;, &#39;length_of_time&#39;, &#39;long&#39;, &#39;minutes&#39;, &#39;recorded&#39;, &#39;seconds&#39;, &#39;state&#39;] # Drop those features ufo_dropped = ufo.drop(to_drop,axis=1) # Let&#39;s also filter some words out of the text vector we created filtered_words = words_to_filter(vocab, vec.vocabulary_, desc_tfidf, 4) . seconds seconds_log minutes seconds 1.000000 0.853371 0.980341 seconds_log 0.853371 1.000000 0.824493 minutes 0.980341 0.824493 1.000000 . 5.4.2 Modeling the UFO dataset, part 1 . In this exercise, we’re going to build a k-nearest neighbor model to predict which country the UFO sighting took place in. Our X dataset has the log-normalized seconds column, the one-hot encoded type columns, as well as the month and year when the sighting took place. The y labels are the encoded country column, where 1 is us and 0 is ca . . # Take a look at the features in the X set of data print(X.columns) # Split the X and y sets using train_test_split, setting stratify=y train_X, test_X, train_y, test_y = train_test_split(X,y,stratify=y) # Fit knn to the training sets knn.fit(train_X,train_y) # Print the score of knn on the test sets print(knn.score(test_X,test_y)) # 0.8693790149892934 . Index([&#39;seconds_log&#39;, &#39;changing&#39;, &#39;chevron&#39;, &#39;cigar&#39;, &#39;circle&#39;, &#39;cone&#39;, &#39;cross&#39;, &#39;cylinder&#39;, &#39;diamond&#39;, &#39;disk&#39;, &#39;egg&#39;, &#39;fireball&#39;, &#39;flash&#39;, &#39;formation&#39;, &#39;light&#39;, &#39;other&#39;, &#39;oval&#39;, &#39;rectangle&#39;, &#39;sphere&#39;, &#39;teardrop&#39;, &#39;triangle&#39;, &#39;unknown&#39;, &#39;month&#39;, &#39;year&#39;], dtype=&#39;object&#39;) . 5.4.3 Modeling the UFO dataset, part 2 . Finally, let’s build a model using the text vector we created, desc_tfidf , using the filtered_words list to create a filtered text vector. Let’s see if we can predict the type of the sighting based on the text. We’ll use a Naive Bayes model for this. . # Use the list of filtered words we created to filter the text vector filtered_text = desc_tfidf[:, list(filtered_words)] # Split the X and y sets using train_test_split, setting stratify=y train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y) # Fit nb to the training sets nb.fit(train_X,train_y) # Print the score of nb on the test sets print(nb.score(test_X,test_y)) # 0.16274089935760172 . As you can see, this model performs very poorly on this text data. This is a clear case where iteration would be necessary to figure out what subset of text improves the model, and if perhaps any of the other features are useful in predicting type . . . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/preprocessing-for-machine-learning-in-python.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/preprocessing-for-machine-learning-in-python.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Model Validation in Python",
            "content": "Model Validation in Python . This is the memo of the 11th course (23 courses in all) of ‘Machine Learning Scientist with Python’ skill track. . You can find the original course HERE . . ### Course Description . Machine learning models are easier to implement now more than ever before. Without proper validation, the results of running new data through a model might not be as accurate as expected. Model validation allows analysts to confidently answer the question, how good is your model? We will answer this question for classification models using the complete set of tic-tac-toe endgame scenarios, and for regression models using fivethirtyeight’s ultimate Halloween candy power ranking dataset. In this course, we will cover the basics of model validation, discuss various validation techniques, and begin to develop tools for creating validated and high performing models. . ### . Basic Modeling in scikit-learn | Validation Basics | Cross Validation | Selecting the best model with Hyperparameter tuning | . 1. Basic Modeling in scikit-learn . . 1.1 Introduction to model validation . . 1.1.1 Seen vs. unseen data . Model’s tend to have higher accuracy on observations they have seen before. In the candy dataset, predicting the popularity of Skittles will likely have higher accuracy than predicting the popularity of Andes Mints; Skittles is in the dataset, and Andes Mints is not. . You’ve built a model based on 50 candies using the dataset X_train and need to report how accurate the model is at predicting the popularity of the 50 candies the model was built on, and the 35 candies ( X_test ) it has never seen. You will use the mean absolute error, mae() , as the accuracy metric. . # The model is fit using X_train and y_train model.fit(X_train, y_train) # Create vectors of predictions train_predictions = model.predict(X_train) test_predictions = model.predict(X_test) # Train/Test Errors train_error = mae(y_true=y_train, y_pred=train_predictions) test_error = mae(y_true=y_test, y_pred=test_predictions) # Print the accuracy for seen and unseen data print(&quot;Model error on seen data: {0:.2f}.&quot;.format(train_error)) print(&quot;Model error on unseen data: {0:.2f}.&quot;.format(test_error)) # Model error on seen data: 3.28. # Model error on unseen data: 11.07. . When models perform differently on training and testing data, you should look to model validation to ensure you have the best performing model. In the next lesson, you will start building models to validate. . . 1.2 Regression models . . 1.2.1 Set parameters and fit a model . Predictive tasks fall into one of two categories: regression or classification. In the candy dataset, the outcome is a continuous variable describing how often the candy was chosen over another candy in a series of 1-on-1 match-ups. To predict this value (the win-percentage), you will use a regression model. . In this exercise, you will specify a few parameters using a random forest regression model rfr . . # Set the number of trees rfr.n_estimators = 100 # Add a maximum depth rfr.max_depth = 6 # Set the random state rfr.random_state = 1111 # Fit the model rfr.fit(X_train, y_train) . You have updated parameters after the model was initialized. This approach is helpful when you need to update parameters. Before making predictions, let’s see which candy characteristics were most important to the model. . 1.2.2 Feature importances . Although some candy attributes, such as chocolate, may be extremely popular, it doesn’t mean they will be important to model prediction. After a random forest model has been fit, you can review the model’s attribute, .feature_importances_ , to see which variables had the biggest impact. You can check how important each variable was in the model by looping over the feature importance array using enumerate() . . If you are unfamiliar with Python’s enumerate() function, it can loop over a list while also creating an automatic counter. . # Fit the model using X and y rfr.fit(X_train, y_train) # Print how important each column is to the model for i, item in enumerate(rfr.feature_importances_): # Use i and item to print out the feature importance of each column print(&quot;{0:s}: {1:.2f}&quot;.format(X_train.columns[i], item)) . chocolate: 0.44 fruity: 0.03 caramel: 0.02 peanutyalmondy: 0.05 nougat: 0.01 crispedricewafer: 0.03 hard: 0.01 bar: 0.02 pluribus: 0.02 sugarpercent: 0.17 pricepercent: 0.19 . No surprise here – chocolate is the most important variable. .feature_importances_ is a great way to see which variables were important to your random forest model. . . 1.3 Classification models . . 1.3.1 Classification predictions . In model validation, it is often important to know more about the predictions than just the final classification. When predicting who will win a game, most people are also interested in how likely it is a team will win. . | Probability | Prediction | Meaning | | — | — | — | | 0 &lt; .50 | 0 | Team Loses | | .50 + | 1 | Team Wins | . In this exercise, you look at the methods, .predict() and .predict_proba() using the tic_tac_toe dataset. The first method will give a prediction of whether Player One will win the game, and the second method will provide the probability of Player One winning. Use rfc as the random forest classification model. . # Fit the rfc model. rfc.fit(X_train, y_train) # Create arrays of predictions classification_predictions = rfc.predict(X_test) probability_predictions = rfc.predict_proba(X_test) # Print out count of binary predictions print(pd.Series(classification_predictions).value_counts()) # Print the first value from probability_predictions print(&#39;The first predicted probabilities are: {}&#39;.format(probability_predictions[0])) . 1 563 0 204 dtype: int64 The first predicted probabilities are: [0.26524423 0.73475577] . You can see there were 563 observations where Player One was predicted to win the Tic-Tac-Toe game. Also, note that the predicted_probabilities array contains lists with only two values because you only have two possible responses (win or lose). Remember these two methods, as you will use them a lot throughout this course. . 1.3.2 Reusing model parameters . Replicating model performance is vital in model validation. Replication is also important when sharing models with co-workers, reusing models on new data or asking questions on a website such as Stack Overflow . You might use such a site to ask other coders about model errors, output, or performance. The best way to do this is to replicate your work by reusing model parameters. . In this exercise, you use various methods to recall which parameters were used in a model. . rfc = RandomForestClassifier(n_estimators=50, max_depth=6, random_state=1111) # Print the classification model print(rfc) # Print the classification model&#39;s random state parameter print(&#39;The random state is: {}&#39;.format(rfc.random_state)) # Print all parameters print(&#39;Printing the parameters dictionary: {}&#39;.format(rfc.get_params())) . RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=6, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None, oob_score=False, random_state=1111, verbose=0, warm_start=False) The random state is: 1111 Printing the parameters dictionary: {&#39;bootstrap&#39;: True, &#39;class_weight&#39;: None, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 6, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: None, &#39;min_impurity_decrease&#39;: 0.0, &#39;min_impurity_split&#39;: None, &#39;min_samples_leaf&#39;: 1, &#39;min_samples_split&#39;: 2, &#39;min_weight_fraction_leaf&#39;: 0.0, &#39;n_estimators&#39;: 50, &#39;n_jobs&#39;: None, &#39;oob_score&#39;: False, &#39;random_state&#39;: 1111, &#39;verbose&#39;: 0, &#39;warm_start&#39;: False} . Recalling which parameters were used will be helpful going forward. Model validation and performance rely heavily on which parameters were used, and there is no way to replicate a model without keeping track of the parameters used! . 1.3.3 Random forest classifier . This exercise reviews the four modeling steps discussed throughout this chapter using a random forest classification model. You will: . Create a random forest classification model. | Fit the model using the tic_tac_toe dataset. | Make predictions on whether Player One will win (1) or lose (0) the current game. | Finally, you will evaluate the overall accuracy of the model. | Let’s get started! . from sklearn.ensemble import RandomForestClassifier # Create a random forest classifier rfc = RandomForestClassifier(n_estimators=50, max_depth=6, random_state=1111) # Fit rfc using X_train and y_train rfc.fit(X_train, y_train) # Create predictions on X_test predictions = rfc.predict(X_test) print(predictions[0:5]) # [1 1 1 1 1] # Print model accuracy using score() and the testing data print(rfc.score(X_test, y_test)) # 0.817470664928292 . That’s all the steps! Notice the first five predictions were all 1, indicating that Player One is predicted to win all five of those games. You also see the model accuracy was only 82%. . Let’s move on to Chapter 2 and increase our model validation toolbox by learning about splitting datasets, standard accuracy metrics, and the bias-variance tradeoff. . 2. Validation Basics . . 2.1 Creating train, test, and validation datasets . . 2.1.1 Create one holdout set . Your boss has asked you to create a simple random forest model on the tic_tac_toe dataset. She doesn’t want you to spend much time selecting parameters; rather she wants to know how well the model will perform on future data. For future Tic-Tac-Toe games, it would be nice to know if your model can predict which player will win. . The dataset tic_tac_toe has been loaded for your use. . Note that in Python, = indicates the code was too long for one line and has been split across two lines. . # Create dummy variables using pandas X = pd.get_dummies(tic_tac_toe.iloc[:,0:9]) y = tic_tac_toe.iloc[:, 9] # Create training and testing datasets. Use 10% for the test set X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1111) . Good! Remember, without the holdout set, you cannot truly validate a model. Let’s move on to creating two holdout sets. . 2.1.2 Create two holdout sets . You recently created a simple random forest model to predict Tic-Tac-Toe game wins for your boss, and at her request, you did not do any parameter tuning. Unfortunately, the overall model accuracy was too low for her standards. This time around, she has asked you to focus on model performance. . Before you start testing different models and parameter sets, you will need to split the data into training, validation, and testing datasets. Remember that after splitting the data into training and testing datasets, the validation dataset is created by splitting the training dataset. . The datasets X and y have been loaded for your use. . # Create temporary training and final testing datasets X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=1111) # Create the final training and validation datasets X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=1111) . Great! You now have training, validation, and testing datasets, but do you know when you need both validation and testing datasets? Keep going! The next exercise will help make sure you understand when to use validation datasets. . 2.1.3 Why use holdout sets . It is important to understand when you would use three datasets (training, validation, and testing) instead of two (training and testing). There is no point in creating an additional dataset split if you are not going to use it. . When should you consider using training, validation, and testing datasets? . When testing parameters, tuning hyper-parameters, or anytime you are frequently evaluating model performance. . Correct! Anytime we are evaluating model performance repeatedly we need to create training, validation, and testing datasets. . . 2.2 Accuracy metrics: regression models . . 2.2.1 Mean absolute error . Communicating modeling results can be difficult. However, most clients understand that on average, a predictive model was off by some number. This makes explaining the mean absolute error easy. For example, when predicting the number of wins for a basketball team, if you predict 42, and they end up with 40, you can easily explain that the error was two wins. . In this exercise, you are interviewing for a new position and are provided with two arrays. y_test , the true number of wins for all 30 NBA teams in 2017 and predictions , which contains a prediction for each team. To test your understanding, you are asked to both manually calculate the MAE and use sklearn . . from sklearn.metrics import mean_absolute_error # Manually calculate the MAE n = len(predictions) mae_one = sum(abs(y_test - predictions)) / n print(&#39;With a manual calculation, the error is {}&#39;.format(mae_one)) # With a manual calculation, the error is 5.9 # Use scikit-learn to calculate the MAE mae_two = mean_absolute_error(y_test, predictions) print(&#39;Using scikit-lean, the error is {}&#39;.format(mae_two)) # Using scikit-lean, the error is 5.9 . Well done! These predictions were about six wins off on average. This isn’t too bad considering NBA teams play 82 games a year. Let’s see how these errors would look if you used the mean squared error instead. . 2.2.2 Mean squared error . Let’s focus on the 2017 NBA predictions again. Every year, there are at least a couple of NBA teams that win way more games than expected. If you use the MAE, this accuracy metric does not reflect the bad predictions as much as if you use the MSE. Squaring the large errors from bad predictions will make the accuracy look worse. . In this example, NBA executives want to better predict team wins. You will use the mean squared error to calculate the prediction error. The actual wins are loaded as y_test and the predictions as predictions . . from sklearn.metrics import mean_squared_error n = len(predictions) # Finish the manual calculation of the MSE mse_one = sum((y_test - predictions)**2) / n print(&#39;With a manual calculation, the error is {}&#39;.format(mse_one)) # With a manual calculation, the error is 49.1 # Use the scikit-learn function to calculate MSE mse_two = mean_squared_error(y_test, predictions) print(&#39;Using scikit-lean, the error is {}&#39;.format(mse_two)) # Using scikit-lean, the error is 49.1 . Good job! If you run any additional models, you will try to beat an MSE of 49.1, which is the average squared error of using your model. Although the MSE is not as interpretable as the MAE, it will help us select a model that has fewer ‘large’ errors. . 2.2.3 Performance on data subsets . In professional basketball, there are two conferences, the East and the West. Coaches and fans often only care about how teams in their own conference will do this year. . You have been working on an NBA prediction model and would like to determine if the predictions were better for the East or West conference. You added a third array to your data called labels , which contains an “E” for the East teams, and a “W” for the West. y_test and predictions have again been loaded for your use. . # Find the East conference teams east_teams = labels == &quot;E&quot; # Create arrays for the true and predicted values true_east = y_test[east_teams] preds_east = predictions[east_teams] # Print the accuracy metrics print(&#39;The MAE for East teams is {}&#39;.format( mae(true_east, preds_east))) # The MAE for East teams is 6.733333333333333 # Print the West accuracy print(&#39;The MAE for West conference is {}&#39;.format(west_error)) # The MAE for West conference is 5.01 . Great! It looks like the Western conference predictions were about two games better on average. Over the past few seasons, the Western teams have generally won the same number of games as the experts have predicted. Teams in the East are just not as predictable as those in the West. . . 2.3 Classification metrics . . 2.3.1 Confusion matrices . Confusion matrices are a great way to start exploring your model’s accuracy. They provide the values needed to calculate a wide range of metrics, including sensitivity, specificity, and the F1-score. . You have built a classification model to predict if a person has a broken arm based on an X-ray image. On the testing set, you have the following confusion matrix: . | | Prediction: 0 | Prediction: 1 | | — | — | — | | Actual: 0 | 324 (TN) | 15 (FP) | | Actual: 1 | 123 (FN) | 491 (TP) | . # Calculate and print the accuracy accuracy = (324 + 491) / (953) print(&quot;The overall accuracy is {0: 0.2f}&quot;.format(accuracy)) # Calculate and print the precision precision = (491) / (15 + 491) print(&quot;The precision is {0: 0.2f}&quot;.format(precision)) # Calculate and print the recall recall = (491) / (123 + 491) print(&quot;The recall is {0: 0.2f}&quot;.format(recall)) . The overall accuracy is 0.86 The precision is 0.97 The recall is 0.80 . Well done! In this case, a true positive is a picture of an actual broken arm that was also predicted to be broken. Doctors are okay with a few additional false positives (predicted broken, not actually broken), as long as you don’t miss anyone who needs immediate medical attention. . 2.3.2 Confusion matrices, again . Creating a confusion matrix in Python is simple. The biggest challenge will be making sure you understand the orientation of the matrix. This exercise makes sure you understand the sklearn implementation of confusion matrices. Here, you have created a random forest model using the tic_tac_toe dataset rfc to predict outcomes of 0 (loss) or 1 (a win) for Player One. . Note: If you read about confusion matrices on another website or for another programming language, the values might be reversed. . from sklearn.metrics import confusion_matrix # Create predictions test_predictions = rfc.predict(X_test) # Create and print the confusion matrix cm = confusion_matrix(y_test, test_predictions) print(cm) # Print the true positives (actual 1s that were predicted 1s) print(&quot;The number of true positives is: {}&quot;.format(cm[1, 1])) . [[177 123] [ 92 471]] The number of true positives is: 471 . Good job! Row 1, column 1 represents the number of actual 1s that were predicted 1s (the true positives). Always make sure you understand the orientation of the confusion matrix before you start using it! . 2.3.3 Precision vs. recall . The accuracy metrics you use to evaluate your model should always be based on the specific application. For this example, let’s assume you are a really sore loser when it comes to playing Tic-Tac-Toe, but only when you are certain that you are going to win. . Choose the most appropriate accuracy metric, either precision or recall, to complete this example. But remember, if you think you are going to win, you better win! . Use rfc , which is a random forest classification model built on the tic_tac_toe dataset. . from sklearn.metrics import precision_score test_predictions = rfc.predict(X_test) # Create precision or recall score based on the metric you imported score = precision_score(y_test, test_predictions) # Print the final result print(&quot;The precision value is {0:.2f}&quot;.format(score)) # The precision value is 0.79 . Great job! Precision is the correct metric here. Sore-losers can’t stand losing when they are certain they will win! For that reason, our model needs to be as precise as possible. With a precision of only 79%, you may need to try some other modeling techniques to improve this score. . . 2.4 The bias-variance tradeoff . . 2.4.1 Error due to under/over-fitting . The candy dataset is prime for overfitting. With only 85 observations, if you use 20% for the testing dataset, you are losing a lot of vital data that could be used for modeling. Imagine the scenario where most of the chocolate candies ended up in the training data and very few in the holdout sample. Our model might only see that chocolate is a vital factor, but fail to find that other attributes are also important. In this exercise, you’ll explore how using too many features (columns) in a random forest model can lead to overfitting. . A feature represents which columns of the data are used in a decision tree. The parameter max_features limits the number of features available. . # Update the rfr model rfr = RandomForestRegressor(n_estimators=25, random_state=1111, max_features=2) rfr.fit(X_train, y_train) # Print the training and testing accuracies print(&#39;The training error is {0:.2f}&#39;.format( mae(y_train, rfr.predict(X_train)))) print(&#39;The testing error is {0:.2f}&#39;.format( mae(y_test, rfr.predict(X_test)))) . The training error is 3.88 . The testing error is 9.15 . # Update the rfr model rfr = RandomForestRegressor(n_estimators=25, random_state=1111, max_features=11) rfr.fit(X_train, y_train) # Print the training and testing accuracies print(&#39;The training error is {0:.2f}&#39;.format( mae(y_train, rfr.predict(X_train)))) print(&#39;The testing error is {0:.2f}&#39;.format( mae(y_test, rfr.predict(X_test)))) . The training error is 3.57 . The testing error is 10.05 . # Update the rfr model rfr = RandomForestRegressor(n_estimators=25, random_state=1111, max_features=4) rfr.fit(X_train, y_train) # Print the training and testing accuracies print(&#39;The training error is {0:.2f}&#39;.format( mae(y_train, rfr.predict(X_train)))) print(&#39;The testing error is {0:.2f}&#39;.format( mae(y_test, rfr.predict(X_test)))) . The training error is 3.60 . The testing error is 8.79 . Great job! The chart below shows the performance at various max feature values. Sometimes, setting parameter values can make a huge difference in model performance. . . 2.4.2 Am I underfitting? . You are creating a random forest model to predict if you will win a future game of Tic-Tac-Toe. Using the tic_tac_toe dataset, you have created training and testing datasets, X_train , X_test , y_train , and y_test . . You have decided to create a bunch of random forest models with varying amounts of trees (1, 2, 3, 4, 5, 10, 20, and 50). The more trees you use, the longer your random forest model will take to run. However, if you don’t use enough trees, you risk underfitting. You have created a for loop to test your model at the different number of trees. . from sklearn.metrics import accuracy_score test_scores, train_scores = [], [] for i in [1, 2, 3, 4, 5, 10, 20, 50]: rfc = RandomForestClassifier(n_estimators=i, random_state=1111) rfc.fit(X_train, y_train) # Create predictions for the X_train and X_test datasets. train_predictions = rfc.predict(X_train) test_predictions = rfc.predict(X_test) # Append the accuracy score for the test and train predictions. train_scores.append(round(accuracy_score(y_train, train_predictions), 2)) test_scores.append(round(accuracy_score(y_test, test_predictions), 2)) # Print the train and test scores. print(&quot;The training scores were: {}&quot;.format(train_scores)) print(&quot;The testing scores were: {}&quot;.format(test_scores)) . The training scores were: [0.94, 0.93, 0.98, 0.97, 0.99, 1.0, 1.0, 1.0] The testing scores were: [0.83, 0.79, 0.89, 0.91, 0.91, 0.93, 0.97, 0.98] . Excellent! Notice that with only one tree, both the train and test scores are low. As you add more trees, both errors improve. Even at 50 trees, this still might not be enough. Every time you use more trees, you achieve higher accuracy. At some point though, more trees increase training time, but do not decrease testing error. . 3. Cross Validation . . 3.1 The problems with holdout sets . . 3.1.2 Two samples . After building several classification models based on the tic_tac_toe dataset, you realize that some models do not generalize as well as others. You have created training and testing splits just as you have been taught, so you are curious why your validation process is not working. . After trying a different training, test split, you noticed differing accuracies for your machine learning model. Before getting too frustrated with the varying results, you have decided to see what else could be going on. . # Create two different samples of 200 observations sample1 = tic_tac_toe.sample(200, random_state=1111) sample2 = tic_tac_toe.sample(200, random_state=1171) # Print the number of common observations print(len([index for index in sample1.index if index in sample2.index])) # 40 # Print the number of observations in the Class column for both samples print(sample1[&#39;Class&#39;].value_counts()) print(sample2[&#39;Class&#39;].value_counts()) . positive 134 negative 66 Name: Class, dtype: int64 positive 123 negative 77 Name: Class, dtype: int64 . Well done! Notice that there are a varying number of positive observations for both sample test sets. Sometimes creating a single test holdout sample is not enough to achieve the high levels of model validation you want. You need to use something more robust. . 3.1.3 Potential problems . Using different data splitting methods may lead to varying data in the final holdout samples. | If you have limited data, your holdout accuracy may be misleading. | . If our models are not generalizing well or if we have limited data, we should be careful using a single training/validation split. You should use the next lesson’s topic: cross-validation. . . 3.2 Cross-validation . . 3.2.1 scikit-learn’s KFold() . You just finished running a colleagues code that creates a random forest model and calculates an out-of-sample accuracy. You noticed that your colleague’s code did not have a random state, and the errors you found were completely different than the errors your colleague reported. . To get a better estimate for how accurate this random forest model will be on new data, you have decided to generate some indices to use for KFold cross-validation. . from sklearn.model_selection import KFold # Use KFold kf = KFold(n_splits=5, shuffle=True, random_state=1111) # Create splits splits = kf.split(X) # Print the number of indices for train_index, val_index in splits: print(&quot;Number of training indices: %s&quot; % len(train_index)) print(&quot;Number of validation indices: %s&quot; % len(val_index)) . Number of training indices: 68 Number of validation indices: 17 Number of training indices: 68 Number of validation indices: 17 Number of training indices: 68 Number of validation indices: 17 Number of training indices: 68 Number of validation indices: 17 Number of training indices: 68 Number of validation indices: 17 . Good job! This dataset has 85 rows. You have created five splits – each containing 68 training and 17 validation indices. You can use these indices to complete 5-fold cross-validation. . 3.2.2 Using KFold indices . You have already created splits , which contains indices for the candy-data dataset to complete 5-fold cross-validation. To get a better estimate for how well a colleague’s random forest model will perform on a new data, you want to run this model on the five different training and validation indices you just created. . In this exercise, you will use these indices to check the accuracy of this model using the five different splits. A for loop has been provided to assist with this process. . from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error rfc = RandomForestRegressor(n_estimators=25, random_state=1111) # Access the training and validation indices of splits for train_index, val_index in splits: # Setup the training and validation data X_train, y_train = X[train_index], y[train_index] X_val, y_val = X[val_index], y[val_index] # Fit the random forest model rfc.fit(X_train, y_train) # Make predictions, and print the accuracy predictions = rfc.predict(X_val) print(&quot;Split accuracy: &quot; + str(mean_squared_error(y_val, predictions))) . Split accuracy: 178.75586448813047 Split accuracy: 98.29560208158634 Split accuracy: 86.2673010849621 Split accuracy: 217.4185114496197 Split accuracy: 140.5437661156536 . Nice work! KFold() is a great method for accessing individual indices when completing cross-validation. One drawback is needing a for loop to work through the indices though. In the next lesson, you will look at an automated method for cross-validation using sklearn . . . 3.3 sklearn’s cross_val_score() . . 3.3.1 scikit-learn’s methods . You have decided to build a regression model to predict the number of new employees your company will successfully hire next month. You open up a new Python script to get started, but you quickly realize that sklearn has a lot of different modules. Let’s make sure you understand the names of the modules, the methods, and which module contains which method. . Follow the instructions below to load in all of the necessary methods for completing cross-validation using sklearn . You will use modules: . metrics | model_selection | ensemble | . # Instruction 1: Load the cross-validation method from sklearn.model_selection import cross_val_score # Instruction 2: Load the random forest regression model from sklearn.ensemble import RandomForestRegressor # Instruction 3: Load the mean squared error method # Instruction 4: Load the function for creating a scorer from sklearn.metrics import mean_squared_error, make_scorer . Well done! It is easy to see how all of the methods can get mixed up, but it is important to know the names of the methods you need. You can always review the scikit-learn documentation should you need any help. . 3.3.2 Implement cross_val_score() . Your company has created several new candies to sell, but they are not sure if they should release all five of them. To predict the popularity of these new candies, you have been asked to build a regression model using the candy dataset. Remember that the response value is a head-to-head win-percentage against other candies. . Before you begin trying different regression models, you have decided to run cross-validation on a simple random forest model to get a baseline error to compare with any future results. . rfc = RandomForestRegressor(n_estimators=25, random_state=1111) mse = make_scorer(mean_squared_error) # Set up cross_val_score cv = cross_val_score(estimator=rfc, X=X_train, y=y_train, cv=10, scoring=mse) # Print the mean error print(cv.mean()) # 155.55845080026586 . Nice! You now have a baseline score to build on. If you decide to build additional models or try new techniques, you should try to get an error lower than 155.56. Lower errors indicate that your popularity predictions are improving. . . 3.4 Leave-one-out-cross-validation (LOOCV) . . 3.4.1 When to use LOOCV . Which of the following are reasons you might NOT run LOOCV on the provided X dataset? The X data has been loaded for you to explore as you see fit. . A The X dataset has 122,624 data points, which might be computationally expensive and slow. | B You cannot run LOOCV on classification problems. | C You want to test different values for 15 different parameters | . A&amp;C . Well done! This many observations will definitely slow things down and could be computationally expensive. If you don’t have time to wait while your computer runs through 1,000 models, you might want to use 5 or 10-fold cross-validation. . 3.4.2 Leave-one-out-cross-validation . Let’s assume your favorite candy is not in the candy dataset, and that you are interested in the popularity of this candy. Using 5-fold cross-validation will train on only 80% of the data at a time. The candy dataset only has 85 rows though, and leaving out 20% of the data could hinder our model. However, using leave-one-out-cross-validation allows us to make the most out of our limited dataset and will give you the best estimate for your favorite candy’s popularity! . In this exercise, you will use cross_val_score() to perform LOOCV. . from sklearn.metrics import mean_absolute_error, make_scorer # Create scorer mae_scorer = make_scorer(mean_absolute_error) rfr = RandomForestRegressor(n_estimators=15, random_state=1111) # Implement LOOCV scores = cross_val_score(rfr, X=X, y=y, cv=X.shape[0], scoring=mae_scorer) # Print the mean and standard deviation print(&quot;The mean of the errors is: %s.&quot; % np.mean(scores)) print(&quot;The standard deviation of the errors is: %s.&quot; % np.std(scores)) # The mean of the errors is: 9.464989603398694. # The standard deviation of the errors is: 7.265762094853885. . Very good! You have come along way with model validation techniques. The final chapter will wrap up model validation by discussing how to select the best model and give an introduction to parameter tuning. . 4. Selecting the best model with Hyperparameter tuning . . 4.1 Introduction to hyperparameter tuning . . 4.1.1 Creating Hyperparameters . For a school assignment, your professor has asked your class to create a random forest model to predict the average test score for the final exam. . After developing an initial random forest model, you are unsatisfied with the overall accuracy. You realize that there are too many hyperparameters to choose from, and each one has a lot of possible values. You have decided to make a list of possible ranges for the hyperparameters you might use in your next model. . Your professor has provided de-identified data for the last ten quizzes to act as the training data. There are 30 students in your class. . # Review the parameters of rfr print(rfr.get_params()) # Maximum Depth max_depth = [4, 8, 12] # Minimum samples for a split min_samples_split = [2, 5, 10] # Max features max_features = [4, 6, 8, 10] . {&#39;bootstrap&#39;: True, &#39;criterion&#39;: &#39;mse&#39;, &#39;max_depth&#39;: None, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: None, &#39;min_impurity_decrease&#39;: 0.0, &#39;min_impurity_split&#39;: None, &#39;min_samples_leaf&#39;: 1, &#39;min_samples_split&#39;: 2, &#39;min_weight_fraction_leaf&#39;: 0.0, &#39;n_estimators&#39;: &#39;warn&#39;, &#39;n_jobs&#39;: None, &#39;oob_score&#39;: False, &#39;random_state&#39;: 1111, &#39;verbose&#39;: 0, &#39;warm_start&#39;: False} . Good job! Hyperparameter tuning requires selecting parameters to tune, as well the possible values these parameters can be set to. . 4.1.2 Running a model using ranges . You have just finished creating a list of hyperparameters and ranges to use when tuning a predictive model for an assignment. You have used max_depth , min_samples_split , and max_features as your range variable names. . from sklearn.ensemble import RandomForestRegressor # Fill in rfr using your variables rfr = RandomForestRegressor( n_estimators=100, max_depth=random.choice(max_depth), min_samples_split=random.choice(min_samples_split), max_features=random.choice(max_features)) # Print out the parameters print(rfr.get_params()) . {&#39;bootstrap&#39;: True, &#39;criterion&#39;: &#39;mse&#39;, &#39;max_depth&#39;: 12, &#39;max_features&#39;: 10, &#39;max_leaf_nodes&#39;: None, &#39;min_impurity_decrease&#39;: 0.0, &#39;min_impurity_split&#39;: None, &#39;min_samples_leaf&#39;: 1, &#39;min_samples_split&#39;: 2, &#39;min_weight_fraction_leaf&#39;: 0.0, &#39;n_estimators&#39;: 100, &#39;n_jobs&#39;: None, &#39;oob_score&#39;: False, &#39;random_state&#39;: None, &#39;verbose&#39;: 0, &#39;warm_start&#39;: False} . Good job! Notice that min_samples_split was randomly set to 2. Since you specified a random state, min_samples_split will always be set to 2 if you only run this model one time. . . 4.2 RandomizedSearchCV . . 4.2.1 Preparing for RandomizedSearch . Last semester your professor challenged your class to build a predictive model to predict final exam test scores. You tried running a few different models by randomly selecting hyperparameters. However, running each model required you to code it individually. . After learning about RandomizedSearchCV() , you’re revisiting your professors challenge to build the best model. In this exercise, you will prepare the three necessary inputs for completing a random search. . from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import make_scorer, mean_squared_error # Finish the dictionary by adding the max_depth parameter param_dist = {&quot;max_depth&quot;: [2, 4, 6, 8], &quot;max_features&quot;: [2, 4, 6, 8, 10], &quot;min_samples_split&quot;: [2, 4, 8, 16]} # Create a random forest regression model rfr = RandomForestRegressor(n_estimators=10, random_state=1111) # Create a scorer to use (use the mean squared error) scorer = make_scorer(mean_squared_error) . Well done! To use RandomizedSearchCV() , you need a distribution dictionary, an estimator, and a scorer—once you’ve got these, you can run a random search to find the best parameters for your model. . 4.2.2 Implementing RandomizedSearchCV . You are hoping that using a random search algorithm will help you improve predictions for a class assignment. You professor has challenged your class to predict the overall final exam average score. . In preparation for completing a random search, you have created: . param_dist the hyperparameter distributions | rfr a random forest regression model | scorer a scoring method to use | . # Import the method for random search from sklearn.model_selection import RandomizedSearchCV # Build a random search using param_dist, rfr, and scorer random_search = RandomizedSearchCV( estimator=rfr, param_distributions=param_dist, n_iter=10, cv=5, scoring=scorer) . Nice! Although it takes a lot of steps, hyperparameter tuning with random search is well worth it and can improve the accuracy of your models. Plus, you are already using cross-validation to validate your best model. . . 4.3 Selecting your final model . . 4.3.1 Best classification accuracy . You are in a competition at work to build the best model for predicting the winner of a Tic-Tac-Toe game. You already ran a random search and saved the results of the most accurate model to rs . . Which parameter set produces the best classification accuracy? . rs.best_estimator_ RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=12, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=4, min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=None, oob_score=False, random_state=1111, verbose=0, warm_start=False) . Perfect! These parameters do produce the best testing accuracy. Good job! Remember, to reuse this model you can use rs.best_estimator_ . . 4.3.2 Selecting the best precision model . Your boss has offered to pay for you to see three sports games this year. Of the 41 home games your favorite team plays, you want to ensure you go to three home games that they will definitely win. You build a model to decide which games your team will win. . To do this, you will build a random search algorithm and focus on model precision (to ensure your team wins). You also want to keep track of your best model and best parameters, so that you can use them again next year (if the model does well, of course). You have already decided on using the random forest classification model rfc and generated a parameter distribution param_dist . . from sklearn.metrics import precision_score, make_scorer # Create a precision scorer precision = make_scorer(precision_score) # Finalize the random search rs = RandomizedSearchCV( estimator=rfc, param_distributions=param_dist, scoring = precision, cv=5, n_iter=10, random_state=1111) rs.fit(X, y) # print the mean test scores: print(&#39;The accuracy for each run was: {}.&#39;.format(rs.cv_results_[&#39;mean_test_score&#39;])) # print the best model score: print(&#39;The best accuracy for a single model was: {}&#39;.format(rs.best_score_)) . The accuracy for each run was: [0.86446668 0.75302055 0.67570816 0.88459939 0.88381178 0.86917588 0.68014695 0.81721906 0.87895856 0.92917474]. The best accuracy for a single model was: 0.9291747446879924 . Wow – Your model’s precision was 93%! The best model accurately predicts a winning game 93% of the time. If you look at the mean test scores, you can tell some of the other parameter sets did really poorly. Also, since you used cross-validation, you can be confident in your predictions. Well done! . . 4.4 Course completed! . . . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/model-validation-in-python.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/model-validation-in-python.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Manipulating Time Series Data in Python",
            "content": "Manipulating Time Series Data in Python . This is a memo. This course does not have a track yet. . You can find the original course HERE . . ##### PREREQUISITES . Introduction to Python | Intermediate Python for Data Science | . 1. Working with Time Series in Pandas . . 1.1 How to use dates &amp; times with pandas . . #### Your first time series . You have learned in the video how to create a sequence of dates using pd.date_range() . You have also seen that each date in the resulting pd.DatetimeIndex is a pd.Timestamp with various attributes that you can access to obtain information about the date. . Now, you’ll create a week of data, iterate over the result, and obtain the dayofweek and weekday_name for each date. . # import libraries import pandas as pd import matplotlib.pyplot as plt . # Create the range of dates here seven_days = pd.date_range(start=&#39;2017-1-1&#39;, periods=7) # Iterate over the dates and print the number and name of the weekday for day in seven_days: print(day.dayofweek, day.weekday_name) 6 Sunday 0 Monday 1 Tuesday 2 Wednesday 3 Thursday 4 Friday 5 Saturday seven_days DatetimeIndex([&#39;2017-01-01&#39;, &#39;2017-01-02&#39;, &#39;2017-01-03&#39;, &#39;2017-01-04&#39;, &#39;2017-01-05&#39;, &#39;2017-01-06&#39;, &#39;2017-01-07&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) . . 1.2 Indexing &amp; resampling time series . . #### Create a time series of air quality data . You have seen in the video how to deal with dates that are not in the correct format, but instead are provided as string types, represented as dtype object in pandas . . We have prepared a data set with air quality data (ozone, pm25, and carbon monoxide for NYC, 2000-2017) for you to practice the use of pd.to_datetime() . . # original data data.head() date ozone pm25 co 0 1999-07-01 0.012024 20.000000 1.300686 1 1999-07-02 0.027699 23.900000 0.958194 2 1999-07-03 0.043969 36.700000 1.194444 3 1999-07-04 0.035162 39.000000 1.081548 4 1999-07-05 0.038359 28.171429 0.939583 print(data.info()) &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 6317 entries, 0 to 6316 Data columns (total 4 columns): date 6317 non-null object ozone 6317 non-null float64 pm25 6317 non-null float64 co 6317 non-null float64 dtypes: float64(3), object(1) memory usage: 197.5+ KB None . # Convert the date column to datetime64 data.date = pd.to_datetime(data.date) # Set date column as index data.set_index(&#39;date&#39;, inplace=True) # Plot data data.plot(subplots=True) plt.show() . . # After transform date to datetime type and set it to index data.head() ozone pm25 co date 1999-07-01 0.012024 20.000000 1.300686 1999-07-02 0.027699 23.900000 0.958194 1999-07-03 0.043969 36.700000 1.194444 1999-07-04 0.035162 39.000000 1.081548 1999-07-05 0.038359 28.171429 0.939583 print(data.info()) &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; DatetimeIndex: 6317 entries, 1999-07-01 to 2017-03-31 Data columns (total 3 columns): ozone 6317 non-null float64 pm25 6317 non-null float64 co 6317 non-null float64 dtypes: float64(3) memory usage: 197.4 KB None . #### Compare annual stock price trends . You’ll compare the performance for three years of Yahoo stock prices. . yahoo.head() price date 2013-01-02 20.08 2013-01-03 19.78 2013-01-04 19.86 2013-01-07 19.40 2013-01-08 19.66 yahoo.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; DatetimeIndex: 782 entries, 2013-01-02 to 2015-12-31 Data columns (total 1 columns): price 756 non-null float64 dtypes: float64(1) memory usage: 32.2 KB . # Create dataframe prices here prices = pd.DataFrame() # Select data for each year and concatenate with prices here for year in [&#39;2013&#39;, &#39;2014&#39;, &#39;2015&#39;]: price_per_year = yahoo.loc[year, [&#39;price&#39;]].reset_index(drop=True) price_per_year.rename(columns={&#39;price&#39;: year}, inplace=True) prices = pd.concat([prices, price_per_year], axis=1) # Plot prices prices.plot() plt.show() . . prices.head() 2013 2014 2015 0 20.08 NaN NaN 1 19.78 39.59 50.17 2 19.86 40.12 49.13 3 19.40 39.93 49.21 4 19.66 40.92 48.59 In [4]: prices.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 261 entries, 0 to 260 Data columns (total 3 columns): 2013 252 non-null float64 2014 252 non-null float64 2015 252 non-null float64 dtypes: float64(3) memory usage: 6.2 KB . #### Set and change time series frequency . You have seen how to assign a frequency to a DateTimeIndex , and then change this frequency. . Now, you’ll use data on the daily carbon monoxide concentration in NYC, LA and Chicago from 2005-17. . You’ll set the frequency to calendar daily and then resample to monthly frequency, and visualize both series to see how the different frequencies affect the data. . co.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; DatetimeIndex: 1898 entries, 2005-01-01 to 2010-12-31 Data columns (total 3 columns): Chicago 1898 non-null float64 Los Angeles 1898 non-null float64 New York 1898 non-null float64 dtypes: float64(3) memory usage: 139.3 KB co.head() Chicago Los Angeles New York date 2005-01-01 0.317763 0.777657 0.639830 2005-01-03 0.520833 0.349547 0.969572 2005-01-04 0.477083 0.626630 0.905208 2005-01-05 0.348822 0.613814 0.769176 2005-01-06 0.572917 0.792596 0.815761 . # Set the frequency to calendar daily co = co.asfreq(&#39;D&#39;) # Plot the data co.plot(subplots=True) plt.show() # Set frequency to monthly co = co.asfreq(&#39;M&#39;) # Plot the data co.plot(subplots=True) plt.show() . . . 1.3 Lags, changes, and returns for stock price series . . #### Shifting stock prices across time . The first method to manipulate time series is .shift() , which allows you shift all values in a Series or DataFrame by a number of periods to a different time along the DateTimeIndex . . Let’s use this to visually compare a stock price series for Google shifted 90 business days into both past and future. . # original data google.head() Close Date 2014-01-02 556.00 2014-01-03 551.95 2014-01-06 558.10 2014-01-07 568.86 2014-01-08 570.04 . # Import data here google = pd.read_csv(&#39;google.csv&#39;, parse_dates=[&#39;Date&#39;], index_col=&#39;Date&#39;) # Set data frequency to business daily google = google.asfreq(&#39;B&#39;) # Create &#39;lagged&#39; and &#39;shifted&#39; google[&#39;lagged&#39;] = google.Close.shift(-90) google[&#39;shifted&#39;] = google.Close.shift(90) # Plot the google price series google.plot() plt.show() . . # shifted data google.head() Close lagged shifted Date 2014-01-02 556.00 511.00 NaN 2014-01-03 551.95 518.73 NaN 2014-01-06 558.10 529.92 NaN 2014-01-07 568.86 533.09 NaN 2014-01-08 570.04 526.65 NaN . #### Calculating stock price changes . Now you’ll practice a similar calculation to calculate absolute changes from current and shifted prices, and compare the result to the function .diff() . . # Created shifted_30 here yahoo[&#39;shifted_30&#39;] = yahoo.price.shift(periods=30) # Subtract shifted_30 from price yahoo[&#39;change_30&#39;] = yahoo[&#39;price&#39;] - yahoo[&#39;shifted_30&#39;] # Get the 30-day price difference yahoo[&#39;diff_30&#39;] = yahoo.price.diff(periods=30) # Inspect the last five rows of price print(yahoo.tail()) # Show the value_counts of the difference between change_30 and diff_30 print(yahoo.change_30.sub(yahoo.diff_30).value_counts()) . price shifted_30 change_30 diff_30 date 2015-12-25 NaN 32.19 NaN NaN 2015-12-28 33.60 32.94 0.66 0.66 2015-12-29 34.04 32.86 1.18 1.18 2015-12-30 33.37 32.98 0.39 0.39 2015-12-31 33.26 32.62 0.64 0.64 0.0 703 dtype: int64 . There’s usually more than one way to get to the same result when working with data. . #### Plotting multi-period returns . The last time series method you have learned about was .pct_change() . Let’s use this function to calculate returns for various calendar day periods, and plot the result to compare the different patterns. . We’ll be using Google stock prices from 2014-2016. . # Create daily_return google[&#39;daily_return&#39;] = google.Close.pct_change(1).mul(100) # Create monthly_return google[&#39;monthly_return&#39;] = google.Close.pct_change(30).mul(100) # Create annual_return google[&#39;annual_return&#39;] = google.Close.pct_change(360).mul(100) # Plot the result google.plot(subplots=True) plt.show() . . 2. Basic Time Series Metrics &amp; Resampling . . 2.1 Compare time series growth rates . . #### Compare the performance of several asset classes . You have seen how you can easily compare several time series by normalizing their starting points to 100, and plot the result. . To broaden your perspective on financial markets, let’s compare four key assets: stocks, bonds, gold, and oil. . print(prices.info()) &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; DatetimeIndex: 2469 entries, 2007-06-29 to 2017-06-26 Data columns (total 4 columns): SP500 2469 non-null float64 Bonds 2469 non-null float64 Gold 2469 non-null float64 Oil 2469 non-null float64 dtypes: float64(4) memory usage: 96.4 KB None prices.head() SP500 Bonds Gold Oil DATE 2007-06-29 1503.35 402.15 648.50 70.47 2007-07-02 1519.43 402.96 650.50 71.11 2007-07-03 1524.87 402.02 657.25 71.41 2007-07-05 1525.40 400.15 655.90 71.81 2007-07-06 1530.44 399.31 647.75 72.80 . # Import data here prices = pd.read_csv(&#39;asset_classes.csv&#39;, parse_dates=[&#39;DATE&#39;], index_col=&#39;DATE&#39;) # Inspect prices here print(prices.info()) # Select first prices first_prices = prices.iloc[0] # Create normalized normalized = prices.div(first_prices).mul(100) # Plot normalized normalized.plot() plt.show() . first_prices SP500 1503.35 Bonds 402.15 Gold 648.50 Oil 70.47 Name: 2007-06-29 00:00:00, dtype: float64 normalized.head() SP500 Bonds Gold Oil DATE 2007-06-29 100.000000 100.000000 100.000000 100.000000 2007-07-02 101.069611 100.201417 100.308404 100.908188 2007-07-03 101.431470 99.967674 101.349268 101.333901 2007-07-05 101.466724 99.502673 101.141095 101.901518 2007-07-06 101.801976 99.293796 99.884348 103.306372 . . Normalizing series is a common step in time series analysis. . #### Comparing stock prices with a benchmark . You also learned in the video how to compare the performance of various stocks against a benchmark. Now you’ll learn more about the stock market by comparing the three largest stocks on the NYSE to the Dow Jones Industrial Average, which contains the 30 largest US companies. . The three largest companies on the NYSE are: . | Company | Stock Ticker | | — | — | | Johnson &amp; Johnson | JNJ | | Exxon Mobil | XOM | | JP Morgan Chase | JPM | . # Import stock prices and index here stocks = pd.read_csv(&#39;nyse.csv&#39;, parse_dates=[&#39;date&#39;], index_col=&#39;date&#39;) dow_jones = pd.read_csv(&#39;dow_jones.csv&#39;, parse_dates=[&#39;date&#39;], index_col=&#39;date&#39;) # Concatenate data and inspect result here data = pd.concat([stocks, dow_jones], axis=1) print(data.info()) # Normalize and plot your data here data.div(data.iloc[0]).mul(100).plot() plt.show() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; DatetimeIndex: 1762 entries, 2010-01-04 to 2016-12-30 Data columns (total 4 columns): JNJ 1762 non-null float64 JPM 1762 non-null float64 XOM 1762 non-null float64 DJIA 1762 non-null float64 dtypes: float64(4) memory usage: 68.8 KB None data.head() JNJ JPM XOM DJIA date 2010-01-04 64.68 42.85 69.15 10583.96 2010-01-05 63.93 43.68 69.42 10572.02 2010-01-06 64.45 43.92 70.02 10573.68 2010-01-07 63.99 44.79 69.80 10606.86 2010-01-08 64.21 44.68 69.52 10618.19 . . #### Plot performance difference vs benchmark index . You learned how to calculate and plot the performance difference of a stock in percentage points relative to a benchmark index. . Let’s compare the performance of Microsoft ( MSFT ) and Apple ( AAPL ) to the S&amp;P 500 over the last 10 years. . # Create tickers tickers = [&#39;MSFT&#39;, &#39;AAPL&#39;] # Import stock data here stocks = pd.read_csv(&#39;msft_aapl.csv&#39;, parse_dates=[&#39;date&#39;], index_col=&#39;date&#39;) # Import index here sp500 = pd.read_csv(&#39;sp500.csv&#39;, parse_dates=[&#39;date&#39;], index_col=&#39;date&#39;) # Concatenate stocks and index here data = pd.concat([stocks, sp500], axis=1).dropna() # Normalize data normalized = data.div(data.iloc[0]).mul(100) # Subtract the normalized index from the normalized stock prices, and plot the result normalized[tickers].sub(normalized[&#39;SP500&#39;], axis=0).plot() plt.show() . . Now you can compare these stocks to the overall market so you can more easily spot trends and outliers. . . 2.2 Changing the time series frequency: resampling . . #### Convert monthly to weekly data . You have learned how to use .reindex() to conform an existing time series to a DateTimeIndex at a different frequency. . Let’s practice this method by creating monthly data and then converting this data to weekly frequency while applying various fill logic options. . # Set start and end dates start = &#39;2016-1-1&#39; end = &#39;2016-2-29&#39; # Create monthly_dates here monthly_dates = pd.date_range(start=start, end=end, freq=&#39;M&#39;) # Create and print monthly here monthly = pd.Series(data=[1,2], index=monthly_dates) print(monthly) 2016-01-31 1 2016-02-29 2 Freq: M, dtype: int64 . # Create weekly_dates here weekly_dates = pd.date_range(start=start, end=end, freq=&#39;W&#39;) weekly_dates # DatetimeIndex([&#39;2016-01-03&#39;, &#39;2016-01-10&#39;, &#39;2016-01-17&#39;, &#39;2016-01-24&#39;, &#39;2016-01-31&#39;, &#39;2016-02-07&#39;, &#39;2016-02-14&#39;, &#39;2016-02-21&#39;, &#39;2016-02-28&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;W-SUN&#39;) # Print monthly, reindexed using weekly_dates print(monthly.reindex(weekly_dates)) 2016-01-03 NaN 2016-01-10 NaN 2016-01-17 NaN 2016-01-24 NaN 2016-01-31 1.0 2016-02-07 NaN 2016-02-14 NaN 2016-02-21 NaN 2016-02-28 NaN Freq: W-SUN, dtype: float64 print(monthly.reindex(weekly_dates, method=&#39;bfill&#39;)) 2016-01-03 1 2016-01-10 1 2016-01-17 1 2016-01-24 1 2016-01-31 1 2016-02-07 2 2016-02-14 2 2016-02-21 2 2016-02-28 2 Freq: W-SUN, dtype: int64 print(monthly.reindex(weekly_dates, method=&#39;ffill&#39;)) 2016-01-03 NaN 2016-01-10 NaN 2016-01-17 NaN 2016-01-24 NaN 2016-01-31 1.0 2016-02-07 1.0 2016-02-14 1.0 2016-02-21 1.0 2016-02-28 1.0 Freq: W-SUN, dtype: float64 . #### Create weekly from monthly unemployment data . The civilian US unemployment rate is reported monthly. You may need more frequent data, but that’s no problem because you just learned how to upsample a time series. . You’ll work with the time series data for the last 20 years, and apply a few options to fill in missing values before plotting the weekly series. . # Import data here data = pd.read_csv(&#39;unemployment.csv&#39;, parse_dates=[&#39;date&#39;], index_col=&#39;date&#39;) # Show first five rows of weekly series print(data.asfreq(&#39;W&#39;).head()) # Show first five rows of weekly series with bfill option print(data.asfreq(&#39;W&#39;, method=&#39;bfill&#39;).head()) # Create weekly series with ffill option and show first five rows weekly_ffill = data.asfreq(&#39;W&#39;, method=&#39;ffill&#39;) print(weekly_ffill.head()) # Plot weekly_fill starting 2015 here weekly_ffill[&#39;2015&#39;:].plot() plt.show() . data.head() UNRATE date 2000-01-01 4.0 2000-02-01 4.1 2000-03-01 4.0 2000-04-01 3.8 2000-05-01 4.0 print(data.asfreq(&#39;W&#39;).head()) UNRATE date 2000-01-02 NaN 2000-01-09 NaN 2000-01-16 NaN 2000-01-23 NaN 2000-01-30 NaN print(data.asfreq(&#39;W&#39;, method=&#39;bfill&#39;).head()) UNRATE date 2000-01-02 4.1 2000-01-09 4.1 2000-01-16 4.1 2000-01-23 4.1 2000-01-30 4.1 print(data.asfreq(&#39;W&#39;, method=&#39;ffill&#39;).head()) UNRATE date 2000-01-02 4.0 2000-01-09 4.0 2000-01-16 4.0 2000-01-23 4.0 2000-01-30 4.0 . . . 2.3 Upsampling &amp; interpolation with .resample() . . #### Use interpolation to create weekly employment data . You have recently used the civilian US unemployment rate, and converted it from monthly to weekly frequency using simple forward or backfill methods. . Compare your previous approach to the new .interpolate() method. . monthly.head() UNRATE DATE 2010-01-01 9.8 2010-02-01 9.8 2010-03-01 9.9 2010-04-01 9.9 2010-05-01 9.6 . # Create weekly dates weekly_dates = pd.date_range(monthly.index.min(), monthly.index.max(), freq=&#39;W&#39;) # Reindex monthly to weekly data weekly = monthly.reindex(weekly_dates) # Create ffill and interpolated columns weekly[&#39;ffill&#39;] = weekly.UNRATE.ffill() weekly[&#39;interpolated&#39;] = weekly.UNRATE.interpolate() # Plot weekly weekly.plot() plt.show() . weekly.tail() UNRATE ffill interpolated 2016-12-04 NaN 4.7 4.788571 2016-12-11 NaN 4.7 4.791429 2016-12-18 NaN 4.7 4.794286 2016-12-25 NaN 4.7 4.797143 2017-01-01 4.8 4.8 4.800000 . . Interpolating is a useful way to create smoother time series when resampling. . #### Interpolate debt/GDP and compare to unemployment . Since you have learned how to interpolate time series, you can now apply this new skill to the quarterly debt/GDP series, and compare the result to the monthly unemployment rate. . data.head() Debt/GDP Unemployment date 2010-01-01 87.00386 9.8 2010-02-01 NaN 9.8 2010-03-01 NaN 9.9 2010-04-01 88.67047 9.9 2010-05-01 NaN 9.6 . # Import &amp; inspect data here data = pd.read_csv(&#39;debt_unemployment.csv&#39;, parse_dates=[&#39;date&#39;], index_col=&#39;date&#39;) # Interpolate and inspect here interpolated = data.interpolate() # Plot interpolated data here interpolated.plot(secondary_y=&#39;Unemployment&#39;) plt.show() . . interpolated.head() Debt/GDP Unemployment date 2010-01-01 87.003860 9.8 2010-02-01 87.559397 9.8 2010-03-01 88.114933 9.9 2010-04-01 88.670470 9.9 2010-05-01 89.135103 9.6 . . 2.4 Downsampling &amp; aggregation . . #### Compare weekly, monthly and annual ozone trends for NYC &amp; LA . You’ll apply this new skill to ozone data for both NYC and LA since 2000 to compare the air quality trend at weekly, monthly and annual frequencies and explore how different resampling periods impact the visualization. . # Import and inspect data here ozone = pd.read_csv(&#39;ozone.csv&#39;, parse_dates=[&#39;date&#39;], index_col=&#39;date&#39;) print(ozone.info()) # Calculate and plot the weekly average ozone trend ozone.resample(&#39;W&#39;).mean().plot() plt.title(&#39;Ozone Weekly&#39;) plt.show() # Calculate and plot the monthly average ozone trend ozone.resample(&#39;M&#39;).mean().plot() plt.title(&#39;Ozone Monthly&#39;) plt.show() # Calculate and plot the annual average ozone trend ozone.resample(&#39;A&#39;).mean().plot() plt.title(&#39;Ozone Annualy&#39;) plt.show() . . #### Compare monthly average stock prices for Facebook and Google . # Import and inspect data here stocks = pd.read_csv(&#39;stocks.csv&#39;, parse_dates=[&#39;date&#39;], index_col=&#39;date&#39;) print(stocks.info()) # Calculate and plot the monthly averages monthly_average = stocks.resample(&#39;M&#39;).mean() monthly_average.plot(subplots=True) plt.show() . . How do the two stock price series compare? . #### Compare quarterly GDP growth rate and stock returns . With your new skill to downsample and aggregate time series, you can compare higher-frequency stock price series to lower-frequency economic time series. . As a first example, let’s compare the quarterly GDP growth rate to the quarterly rate of return on the (resampled) Dow Jones Industrial index of 30 large US stocks. . GDP growth is reported at the beginning of each quarter for the previous quarter. To calculate matching stock returns, you’ll resample the stock index to quarter start frequency using the alias &#39;QS&#39; , and aggregating using the .first() observations. . gdp_growth.head() gdp_growth date 2007-01-01 0.2 2007-04-01 3.1 2007-07-01 2.7 2007-10-01 1.4 2008-01-01 -2.7 djia.head() djia date 2007-06-29 13408.62 2007-07-02 13535.43 2007-07-03 13577.30 2007-07-04 NaN 2007-07-05 13565.84 . # Import and inspect gdp_growth here gdp_growth = pd.read_csv(&#39;gdp_growth.csv&#39;, parse_dates=[&#39;date&#39;], index_col=&#39;date&#39;) print(gdp_growth.info()) # Import and inspect djia here djia = pd.read_csv(&#39;djia.csv&#39;, parse_dates=[&#39;date&#39;], index_col=&#39;date&#39;) print(djia.info()) # Calculate djia quarterly returns here djia_quarterly = djia.resample(&#39;QS&#39;).first() djia_quarterly_return = djia_quarterly.pct_change().mul(100) # Concatenate, rename and plot djia_quarterly_return and gdp_growth here data = pd.concat([gdp_growth, djia_quarterly_return], axis=1) data.columns = [&#39;gdp&#39;, &#39;djia&#39;] data.plot() plt.show() . . #### Visualize monthly mean, median and standard deviation of S&amp;P500 returns . You have also learned how to calculate several aggregate statistics from upsampled data. . Let’s use this to explore how the monthly mean, median and standard deviation of daily S&amp;P500 returns have trended over the last 10 years. . # Import data here sp500 = pd.read_csv(&#39;sp500.csv&#39;, parse_dates=[&#39;date&#39;], index_col=&#39;date&#39;) # Calculate daily returns here daily_returns = sp500.squeeze().pct_change() # Resample and calculate statistics stats = daily_returns.resample(&#39;M&#39;).agg([&#39;mean&#39;, &#39;median&#39;, &#39;std&#39;]) # Plot stats here stats.plot() plt.show() . sp500.head() SP500 date 2007-06-29 1503.35 2007-07-02 1519.43 2007-07-03 1524.87 2007-07-05 1525.40 2007-07-06 1530.44 daily_returns.head() date 2007-06-29 NaN 2007-07-02 0.010696 2007-07-03 0.003580 2007-07-05 0.000348 2007-07-06 0.003304 Name: SP500, dtype: float64 stats.head() mean median std date 2007-06-30 NaN NaN NaN 2007-07-31 -0.001490 0.000921 0.010908 2007-08-31 0.000668 0.001086 0.015261 2007-09-30 0.001900 0.000202 0.010000 2007-10-31 0.000676 -0.000265 0.008719 . . 3. Window Functions: Rolling &amp; Expanding Metrics . . 3.1 Rolling window functions with pandas . . #### Rolling average air quality since 2010 for new york city . The last video was about rolling window functions. To practice this new tool, you’ll start with air quality trends for New York City since 2010. In particular, you’ll be using the daily Ozone concentration levels provided by the Environmental Protection Agency to calculate &amp; plot the 90 and 360 day rolling average. . # Import and inspect ozone data here data = pd.read_csv(&#39;ozone.csv&#39;, parse_dates=[&#39;date&#39;], index_col=&#39;date&#39;) print(data.info()) # Calculate 90d and 360d rolling mean for the last price data[&#39;90D&#39;] = data.Ozone.rolling(&#39;90D&#39;).mean() data[&#39;360D&#39;] = data.Ozone.rolling(&#39;360D&#39;).mean() # Plot data data[&#39;2010&#39;:].plot() plt.title(&#39;New York City&#39;) plt.show() . data.head() Ozone 90D 360D date 2000-01-01 0.004032 0.004032 0.004032 2000-01-02 0.009486 0.006759 0.006759 2000-01-03 0.005580 0.006366 0.006366 2000-01-04 0.008717 0.006954 0.006954 2000-01-05 0.013754 0.008314 0.008314 data.tail() Ozone 90D 360D date 2017-03-27 0.005640 0.021992 0.026629 2017-03-28 0.013870 0.021999 0.026583 2017-03-29 0.034341 0.022235 0.026584 2017-03-30 0.026059 0.022334 0.026599 2017-03-31 0.035983 0.022467 0.026607 . . Do the different rolling windows help you see any long term trends that are hard to spot in the original data. . #### Rolling 360-day median &amp; std. deviation for nyc ozone data since 2000 . The last video also showed you how to calculate several rolling statistics using the .agg() method, similar to .groupby() . . Let’s take a closer look at the air quality history of NYC using the Ozone data you have seen before. The daily data are very volatile, so using a longer term rolling average can help reveal a longer term trend. . You’ll be using a 360 day rolling window, and .agg() to calculate the rolling median and standard deviation for the daily average ozone values since 2000. . # Import and inspect ozone data here data = pd.read_csv(&#39;ozone.csv&#39;, parse_dates=[&#39;date&#39;], index_col=&#39;date&#39;).dropna() # Calculate the rolling mean and std here rolling_stats = data.Ozone.rolling(360).agg([&#39;mean&#39;, &#39;std&#39;]) # Join rolling_stats with ozone data stats = data.join(rolling_stats) # Plot stats stats.plot(subplots=True) plt.show() . . #### Rolling quantiles for daily air quality in nyc . You learned in the last video how to calculate rolling quantiles to describe changes in the dispersion of a time series over time in a way that is less sensitive to outliers than using the mean and standard deviation. . Let’s calculate rolling quantiles – at 10%, 50% (median) and 90% – of the distribution of daily average ozone concentration in NYC using a 360-day rolling window. . # before interpolate &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; DatetimeIndex: 6291 entries, 2000-01-01 to 2017-03-31 Data columns (total 1 columns): Ozone 6167 non-null float64 dtypes: float64(1) memory usage: 98.3 KB None # after interpolate &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; DatetimeIndex: 6300 entries, 2000-01-01 to 2017-03-31 Freq: D Data columns (total 1 columns): Ozone 6300 non-null float64 dtypes: float64(1) memory usage: 98.4 KB None . # Resample, interpolate and inspect ozone data here print(data.info()) data = data.resample(&#39;D&#39;).interpolate() print(data.info()) # Create the rolling window rolling = data.Ozone.rolling(360) # Insert the rolling quantiles to the monthly returns data[&#39;q10&#39;] = rolling.quantile(0.1) data[&#39;q50&#39;] = rolling.quantile(0.5) data[&#39;q90&#39;] = rolling.quantile(0.9) # Plot monthly returns data.plot() plt.show() . . The rolling quantiles help show the volatility of the series. . . 3.2 Expanding window functions with pandas . . #### Cumulative sum vs .diff() . You have learned about expanding windows that allow you to run cumulative calculations. . The cumulative sum method has in fact the opposite effect of the .diff() method that you came across in chapter 1. . To illustrate this, let’s use the Google stock price time series, create the differences between prices, and reconstruct the series using the cumulative sum. . data.head() Close Date 2014-01-02 556.00 2014-01-03 551.95 2014-01-06 558.10 2014-01-07 568.86 2014-01-08 570.04 . # Calculate differences differences = data.diff().dropna() # Select start price start_price = data.first(&#39;D&#39;) # Calculate cumulative sum cumulative_sum = start_price.append(differences).cumsum() # Validate cumulative sum equals data print(data.equals(cumulative_sum)) # True . differences.head() Close Date 2014-01-03 -4.05 2014-01-06 6.15 2014-01-07 10.76 2014-01-08 1.18 2014-01-09 -5.49 start_price Close Date 2014-01-02 556.0 . The .cumsum() method allows you to reconstruct the original data from the differences. . #### Cumulative return on $1,000 invested in google vs apple I . To put your new ability to do cumulative return calculations to practical use, let’s compare how much $1,000 would be worth if invested in Google ( &#39;GOOG&#39; ) or Apple ( &#39;AAPL&#39; ) in 2010. . data.tail() AAPL GOOG Date 2017-05-24 153.34 954.96 2017-05-25 153.87 969.54 2017-05-26 153.61 971.47 2017-05-30 153.67 975.88 2017-05-31 152.76 964.86 returns.tail() AAPL GOOG Date 2017-05-24 -0.002991 0.006471 2017-05-25 0.003456 0.015268 2017-05-26 -0.001690 0.001991 2017-05-30 0.000391 0.004540 2017-05-31 -0.005922 -0.011292 . # Define your investment investment = 1000 # Calculate the daily returns here returns = data.pct_change() # Calculate the cumulative returns here returns_plus_one = returns + 1 cumulative_return = returns_plus_one.cumprod() # Calculate and plot the investment return here cumulative_return.mul(investment).plot() plt.show() . . #### Cumulative return on $1,000 invested in google vs apple II . Apple outperformed Google over the entire period, but this may have been different over various 1-year sub periods, so that switching between the two stocks might have yielded an even better result. . To analyze this, calculate that cumulative return for rolling 1-year periods, and then plot the returns to see when each stock was superior. . # Import numpy import numpy as np # Define a multi_period_return function def multi_period_return(period_returns): return np.prod(period_returns + 1) - 1 # Calculate daily returns daily_returns = data.pct_change() # Calculate rolling_annual_returns rolling_annual_returns = daily_returns.rolling(&#39;360D&#39;).apply(multi_period_return) # Plot rolling_annual_returns rolling_annual_returns.mul(100).plot() plt.show() . . . 3.3 Case study: S&amp;P500 price simulation . . #### Random walk I . In the last video, you have seen how to generate a random walk of returns, and how to convert this random return series into a random stock price path. . In this exercise, you’ll build your own random walk by drawing random numbers from the normal distribution with the help of numpy . . import pandas as pd from numpy.random import normal, random import matplotlib.pyplot as plt # Set seed here seed = 42 # Create random_walk random_walk = normal(loc=0.001, scale=0.01, size=2500) # Convert random_walk to pd.series random_walk = pd.Series(random_walk) # Create random_prices random_prices = random_walk.add(1).cumprod() # Plot random_prices here random_prices.mul(1000).plot() plt.show() . . #### Random walk II . In the last video, you have also seen how to create a random walk of returns by sampling from actual returns, and how to use this random sample to create a random stock price path. . In this exercise, you’ll build a random walk using historical returns from Facebook’s stock price since IPO through the end of May 31, 2017. Then you’ll simulate an alternative random price path in the next exercise. . fb.head() date 2012-05-17 38.00 2012-05-18 38.23 2012-05-21 34.03 2012-05-22 31.00 2012-05-23 32.00 Name: 1, dtype: float64 . import pandas as pd from numpy.random import choice, random import matplotlib.pyplot as plt import seaborn as sns # Set seed here seed = 42 # Calculate daily_returns here daily_returns = fb.pct_change().dropna() # Get n_obs n_obs = daily_returns.count() # Create random_walk random_walk = choice(daily_returns, size=n_obs) # Convert random_walk to pd.series random_walk = pd.Series(random_walk) # Plot random_walk distribution sns.distplot(random_walk) plt.show() . . #### Random walk III . In this exercise, you’ll complete your random walk simulation using Facebook stock returns over the last five years. You’ll start off with a random sample of returns like the one you’ve generated during the last exercise and use it to create a random stock price path. . # Select fb start price here start = fb.price.first(&#39;D&#39;) # Add 1 to random walk and append to start random_walk = random_walk.add(1) random_price = start.append(random_walk) # Calculate cumulative product here random_price = random_price.cumprod() # Insert into fb and plot fb[&#39;random&#39;] = random_price fb.plot() plt.show() . . . 3.4 Relationships between time series: correlation . #### Annual return correlations among several stocks . You have seen in the video how to calculate correlations, and visualize the result. . In this exercise, we have provided you with the historical stock prices for Apple ( AAPL ), Amazon ( AMZN ), IBM ( IBM ), WalMart ( WMT ), and Exxon Mobile ( XOM ) for the last 4,000 trading days from July 2001 until the end of May 2017. . You’ll calculate the year-end returns, the pairwise correlations among all stocks, and visualize the result as an annotated heatmap. . # Inspect data here print(data.info()) # Calculate year-end prices here annual_prices = data.resample(&#39;A&#39;).last() # Calculate annual returns here annual_returns = annual_prices.pct_change() # Calculate and print the correlation matrix here correlations = annual_returns.corr() print(correlations) # Visualize the correlations as heatmap here sns.heatmap(correlations, annot=True) plt.show() . . 4. Putting it all together: Building a value-weighted index . . 4.1 Select index components &amp; import data . . #### Explore and clean company listing information . To get started with the construction of a market-value based index, you’ll work with the combined listing info for the three largest US stock exchanges, the NYSE, the NASDAQ and the AMEX. . In this and the next exercise, you will calculate market-cap weights for these stocks. . We have already imported pandas as pd , and loaded the listings data set with listings information from the NYSE, NASDAQ, and AMEX. The column &#39;Market Capitalization&#39; is already measured in USD mn. . listings.head(3) Exchange Stock Symbol Company Name Last Sale Market Capitalization IPO Year Sector Industry 0 amex XXII 22nd Century Group, Inc 1.33 120.628490 NaN Consumer Non-Durables Farming/Seeds/Milling 1 amex FAX Aberdeen Asia-Pacific Income Fund Inc 5.00 1266.332595 1986.0 NaN NaN 2 amex IAF Aberdeen Australia Equity Fund Inc 6.15 139.865305 NaN NaN NaN listings.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 6674 entries, 0 to 6673 Data columns (total 8 columns): Exchange 6674 non-null object Stock Symbol 6674 non-null object Company Name 6674 non-null object Last Sale 6590 non-null float64 Market Capitalization 6674 non-null float64 IPO Year 2852 non-null float64 Sector 5182 non-null object Industry 5182 non-null object dtypes: float64(3), object(5) memory usage: 417.2+ KB . # Move &#39;stock symbol&#39; into the index listings.set_index(&#39;Stock Symbol&#39;, inplace=True) # Drop rows with missing &#39;sector&#39; data listings.dropna(subset=[&#39;Sector&#39;], inplace=True) # Select companies with IPO Year before 2019 listings = listings.loc[listings[&#39;IPO Year&#39;] &lt; 2019] # Inspect the new listings data print(listings.info()) # Show the number of companies per sector print(listings.groupby(&#39;Sector&#39;).size().sort_values(ascending=False)) . # after cleaning &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 2349 entries, ACU to ZTO Data columns (total 7 columns): Exchange 2349 non-null object Company Name 2349 non-null object Last Sale 2349 non-null float64 Market Capitalization 2349 non-null float64 IPO Year 2349 non-null float64 Sector 2349 non-null object Industry 2349 non-null object dtypes: float64(3), object(4) memory usage: 146.8+ KB None Sector Health Care 445 Consumer Services 402 Technology 386 Finance 351 Energy 144 Capital Goods 143 Public Utilities 104 Basic Industries 104 Consumer Non-Durables 89 Miscellaneous 68 Transportation 58 Consumer Durables 55 dtype: int64 . Your data is squeaky clean now! . #### Select and inspect index components . Now that you have imported and cleaned the listings data, you can proceed to select the index components as the largest company for each sector by market capitalization. . You’ll also have the opportunity to take a closer look at the components, their last market value, and last price. . # Select largest company for each sector components = listings.groupby(&#39;Sector&#39;)[&#39;Market Capitalization&#39;].nlargest(1) # Print components, sorted by market cap print(components.sort_values(ascending=False)) . Sector Stock Symbol Technology AAPL 740,024.47 Consumer Services AMZN 422,138.53 Miscellaneous MA 123,330.09 Health Care AMGN 118,927.21 Transportation UPS 90,180.89 Finance GS 88,840.59 Basic Industries RIO 70,431.48 Public Utilities TEF 54,609.81 Consumer Non-Durables EL 31,122.51 Capital Goods ILMN 25,409.38 Energy PAA 22,223.00 Consumer Durables CPRT 13,620.92 Name: Market Capitalization, dtype: float64 . # Select stock symbols and print the result tickers = components.index.get_level_values(&#39;Stock Symbol&#39;) print(tickers) # Index([&#39;RIO&#39;, &#39;ILMN&#39;, &#39;CPRT&#39;, &#39;EL&#39;, &#39;AMZN&#39;, &#39;PAA&#39;, &#39;GS&#39;, &#39;AMGN&#39;, &#39;MA&#39;, &#39;TEF&#39;, &#39;AAPL&#39;, &#39;UPS&#39;], dtype=&#39;object&#39;, name=&#39;Stock Symbol&#39;) # Print company name, market cap, and last price for each component info_cols = [&#39;Company Name&#39;, &#39;Market Capitalization&#39;, &#39;Last Sale&#39;] info_cols = listings.loc[tickers, info_cols].sort_values(&#39;Market Capitalization&#39;, ascending=False) print(info_cols) . Company Name Market Capitalization Last Sale Stock Symbol AAPL Apple Inc. 740,024.47 141.05 AMZN Amazon.com, Inc. 422,138.53 884.67 MA Mastercard Incorporated 123,330.09 111.22 AMGN Amgen Inc. 118,927.21 161.61 UPS United Parcel Service, Inc. 90,180.89 103.74 GS Goldman Sachs Group, Inc. (The) 88,840.59 223.32 RIO Rio Tinto Plc 70,431.48 38.94 TEF Telefonica SA 54,609.81 10.84 EL Estee Lauder Companies, Inc. (The) 31,122.51 84.94 ILMN Illumina, Inc. 25,409.38 173.68 PAA Plains All American Pipeline, L.P. 22,223.00 30.72 CPRT Copart, Inc. 13,620.92 29.65 . #### Import index component price information . Now you’ll use the stock symbols for the companies you selected in the last exercise to calculate returns for each company. . # Print tickers print(tickers) [&#39;RIO&#39;, &#39;ILMN&#39;, &#39;CPRT&#39;, &#39;EL&#39;, &#39;AMZN&#39;, &#39;PAA&#39;, &#39;GS&#39;, &#39;AMGN&#39;, &#39;MA&#39;, &#39;TEF&#39;, &#39;AAPL&#39;, &#39;UPS&#39;] # Import prices and inspect result stock_prices = pd.read_csv(&#39;stock_prices.csv&#39;, parse_dates=[&#39;Date&#39;], index_col=&#39;Date&#39;) print(stock_prices.info()) # Calculate the returns price_return = stock_prices.iloc[-1].div(stock_prices.iloc[0]).sub(1).mul(100) # Plot horizontal bar chart of sorted price_return price_return.sort_values().plot(kind=&#39;barh&#39;, title=&#39;Stock Price Returns&#39;) plt.show() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; DatetimeIndex: 1762 entries, 2010-01-04 to 2016-12-30 Data columns (total 12 columns): AAPL 1761 non-null float64 AMGN 1761 non-null float64 AMZN 1761 non-null float64 CPRT 1761 non-null float64 EL 1762 non-null float64 GS 1762 non-null float64 ILMN 1761 non-null float64 MA 1762 non-null float64 PAA 1762 non-null float64 RIO 1762 non-null float64 TEF 1762 non-null float64 UPS 1762 non-null float64 dtypes: float64(12) memory usage: 179.0 KB None . . . 4.2 Build a market-cap weighted index . . #### Calculate number of shares outstanding . The next step towards building a value-weighted index is to calculate the number of shares for each index component. . The number of shares will allow you to calculate the total market capitalization for each component given the historical price series in the next exercise. . # Inspect listings and print tickers print(listings.info()) &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 1015 entries, ACU to YPF Data columns (total 7 columns): Exchange 1015 non-null object Company Name 1015 non-null object Last Sale 1015 non-null float64 Market Capitalization 1015 non-null float64 IPO Year 1015 non-null float64 Sector 1015 non-null object Industry 1015 non-null object dtypes: float64(3), object(4) memory usage: 103.4+ KB listings.head(2) Exchange Company Name Last Sale Market Capitalization IPO Year Sector Industry Stock Symbol ACU amex Acme United Corporation. 27.39 91.138992 1988.0 Capital Goods Industrial Machinery/Components ROX amex Castle Brands, Inc. 1.46 237.644444 2006.0 Consumer Non-Durables Beverages (Production/Distribution) print(tickers) [&#39;RIO&#39;, &#39;ILMN&#39;, &#39;CPRT&#39;, &#39;EL&#39;, &#39;AMZN&#39;, &#39;PAA&#39;, &#39;GS&#39;, &#39;AMGN&#39;, &#39;MA&#39;, &#39;TEF&#39;, &#39;AAPL&#39;, &#39;UPS&#39;] . # Select components and relevant columns from listings components = listings.loc[tickers, [&#39;Market Capitalization&#39;, &#39;Last Sale&#39;]] # Print the first rows of components print(components.head()) Market Capitalization Last Sale Stock Symbol RIO 70431.476895 38.94 ILMN 25409.384000 173.68 CPRT 13620.922869 29.65 EL 31122.510011 84.94 AMZN 422138.530626 884.67 . # Calculate the number of shares here no_shares = components[&#39;Market Capitalization&#39;].div(components[&#39;Last Sale&#39;]) # Print the sorted no_shares print(no_shares.sort_values(ascending=False)) Stock Symbol AAPL 5246.540000 TEF 5037.804990 RIO 1808.717948 MA 1108.884100 UPS 869.297154 AMGN 735.890171 PAA 723.404994 AMZN 477.170618 CPRT 459.390316 GS 397.817439 EL 366.405816 ILMN 146.300000 dtype: float64 . #### Create time series of market value . You can now use the number of shares to calculate the total market capitalization for each component and trading date from the historical price series. . The result will be the key input to construct the value-weighted stock index, which you will complete in the next exercise. . components.head() Market Capitalization Last Sale Number of Shares Stock Symbol RIO 70431.476895 38.94 1808.717948 ILMN 25409.384000 173.68 146.300000 CPRT 13620.922869 29.65 459.390316 EL 31122.510011 84.94 366.405816 AMZN 422138.530626 884.67 477.170618 . # Select the number of shares no_shares = components[&#39;Number of Shares&#39;] print(no_shares.sort_values()) Stock Symbol ILMN 146.300000 EL 366.405816 GS 397.817439 CPRT 459.390316 AMZN 477.170618 PAA 723.404994 AMGN 735.890171 UPS 869.297154 MA 1108.884100 RIO 1808.717948 TEF 5037.804990 AAPL 5246.540000 Name: Number of Shares, dtype: float64 . stock_prices.head() AAPL AMGN AMZN CPRT EL ... MA PAA RIO TEF UPS Date ... 2010-01-04 30.57 57.72 133.90 4.55 24.27 ... 25.68 27.00 56.03 28.55 58.18 2010-01-05 30.63 57.22 134.69 4.55 24.18 ... 25.61 27.30 56.90 28.53 58.28 2010-01-06 30.14 56.79 132.25 4.53 24.25 ... 25.56 27.29 58.64 28.23 57.85 2010-01-07 30.08 56.27 130.00 4.50 24.56 ... 25.39 26.96 58.65 27.75 57.41 2010-01-08 30.28 56.77 133.52 4.52 24.66 ... 25.40 27.05 59.30 27.57 60.17 [5 rows x 12 columns] # Create the series of market cap per ticker market_cap = stock_prices.mul(no_shares) market_cap.head() AAPL AMGN AMZN CPRT EL ... MA PAA RIO TEF UPS Date ... 2010-01-04 160386.7278 42475.580670 63893.145750 2090.225938 8892.669154 ... 28476.143688 19531.934838 101342.466626 143829.332465 50575.708420 2010-01-05 160701.5202 42107.635585 64270.110538 2090.225938 8859.692631 ... 28398.521801 19748.956336 102916.051241 143728.576365 50662.638135 2010-01-06 158130.7156 41791.202811 63105.814231 2081.038131 8885.341038 ... 28343.077596 19741.722286 106063.220471 142217.234868 50288.840359 2010-01-07 157815.9232 41408.539922 62032.180340 2067.256422 8998.926841 ... 28154.567299 19502.998638 106081.307650 139799.088473 49906.349611 2010-01-08 158865.2312 41776.485008 63711.820915 2076.444228 9035.567423 ... 28165.656140 19568.105088 107256.974316 138892.283574 52305.609756 [5 rows x 12 columns] . # Select first and last market cap here first_value = market_cap.iloc[0] last_value = market_cap.iloc[-1] # Concatenate and plot first and last market cap here pd.concat([first_value, last_value], axis=1).plot(kind=&#39;barh&#39;) plt.show() . . You’ve made one of the essential ingredients of the index. . ### Calculate &amp; plot the composite index . By now you have all ingredients that you need to calculate the aggregate stock performance for your group of companies. . Use the time series of market capitalization that you created in the last exercise to aggregate the market value for each period, and then normalize this series to convert it to an index. . market_cap_series.head(2) AAPL AMGN AMZN CPRT EL ... MA PAA RIO TEF UPS Date ... 2010-01-04 160386.7278 42475.580670 63893.145750 2090.225938 8892.669154 ... 28476.143688 19531.934838 101342.466626 143829.332465 50575.708420 2010-01-05 160701.5202 42107.635585 64270.110538 2090.225938 8859.692631 ... 28398.521801 19748.956336 102916.051241 143728.576365 50662.638135 [2 rows x 12 columns] raw_index.head(2) Date 2010-01-04 694817.642691 2010-01-05 697995.697475 dtype: float64 . # Aggregate and print the market cap per trading day raw_index = market_cap_series.sum(axis=1) print(raw_index) Date 2010-01-04 6.948176e+05 2010-01-05 6.979957e+05 ... 2016-12-29 1.589422e+06 2016-12-30 1.574862e+06 Length: 1761, dtype: float64 # Normalize the aggregate market cap here index = raw_index.div(raw_index.iloc[0]).mul(100) # Plot the index here index.plot(title=&#39;Market-Cap Weighted Index&#39;) plt.show() . . . 4.3 Evaluate index performance . . #### Calculate the contribution of each stock to the index . You have successfully built the value-weighted index. Let’s now explore how it performed over the 2010-2016 period. . Let’s also determine how much each stock has contributed to the index return. . index.head() Date 1/4/10 100.000000 1/5/10 100.457394 1/6/10 99.981005 1/7/10 99.485328 1/8/10 100.148231 Name: Unnamed: 1, dtype: float64 components.head(1) Market Capitalization Last Sale Number of Shares Stock Symbol RIO 70431.476895 38.94 1808.717948 . # Calculate and print the index return here index_return = (index[-1] / index[0] - 1) * 100 print(index_return) # 126.65826659999996 # Select the market capitalization market_cap = components[&#39;Market Capitalization&#39;] # Calculate the total market cap total_market_cap = market_cap.sum() # 1800858.8762796503 # Calculate the component weights, and print the result weights = market_cap / total_market_cap print(weights.sort_values()) Stock Symbol CPRT 0.007564 PAA 0.012340 ... AMZN 0.234410 AAPL 0.410929 Name: Market Capitalization, dtype: float64 # Calculate and plot the contribution by component weights.mul(index_return).sort_values().plot(kind=&#39;barh&#39;) plt.show() . . return contribution by component . The next step is to take a look at how your index stacks up against a benchmark! . #### Compare index performance against benchmark I . The next step in analyzing the performance of your index is to compare it against a benchmark. . In the video, we used the S&amp;P 500 as benchmark. You can also use the Dow Jones Industrial Average, which contains the 30 largest stocks, and would also be a reasonable benchmark for the largest stocks from all sectors across the three exchanges. . djia.head() DATE 2010-01-04 100.000000 2010-01-05 99.887188 2010-01-06 99.902872 2010-01-07 100.216365 2010-01-08 100.323414 Name: DJIA, dtype: float64 index.head() Date 2010-01-04 100.000000 2010-01-05 100.457394 2010-01-06 99.981005 2010-01-07 99.485328 2010-01-08 100.148231 Name: Unnamed: 1, dtype: float64 . # Convert index series to dataframe here data = index.to_frame(&#39;Index&#39;) # Normalize djia series and add as new column to data djia = djia.div(djia.iloc[0]).mul(100) data[&#39;DJIA&#39;] = djia # Show total return for both index and djia print((data.iloc[-1] / data.iloc[0] -1 ) * 100) Index 126.658267 DJIA 86.722172 dtype: float64 # Plot both series data.plot() plt.show() . . #### Compare index performance against benchmark II . The next step in analyzing the performance of your index is to compare it against a benchmark. . In the video, we have use the S&amp;P 500 as benchmark. You can also use the Dow Jones Industrial Average, which contains the 30 largest stocks, and would also be a reasonable benchmark for the largest stocks from all sectors across the three exchanges. . # Inspect data print(data.info()) print(data.head()) &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; DatetimeIndex: 1761 entries, 2010-01-04 to 2016-12-30 Data columns (total 2 columns): Index 1761 non-null float64 DJIA 1761 non-null float64 dtypes: float64(2) memory usage: 41.3 KB None Index DJIA Date 2010-01-04 100.000000 100.000000 2010-01-05 100.457394 99.887188 2010-01-06 99.981005 99.902872 2010-01-07 99.485328 100.216365 2010-01-08 100.148231 100.323414 . # Create multi_period_return function here def multi_period_return(r): return (np.prod(r + 1) - 1) * 100 # Calculate rolling_return_360 rolling_return_360 = data.pct_change().rolling(&#39;360D&#39;).apply(multi_period_return) # Plot rolling_return_360 here rolling_return_360.plot(title=&#39;Rolling 360D Return&#39;) plt.show() . . . ### 4.4 Index correlation &amp; exporting to excel . #### Visualize your index constituent correlations . To better understand the characteristics of your index constituents, you can calculate the return correlations. . Use the daily stock prices or your index companies, and show a heatmap of the daily return correlations! . stock_prices.head(3) AAPL AMGN AMZN CPRT EL ... MA PAA RIO TEF UPS Date ... 2010-01-04 30.57 57.72 133.90 4.55 24.27 ... 25.68 27.00 56.03 28.55 58.18 2010-01-05 30.63 57.22 134.69 4.55 24.18 ... 25.61 27.30 56.90 28.53 58.28 2010-01-06 30.14 56.79 132.25 4.53 24.25 ... 25.56 27.29 58.64 28.23 57.85 [3 rows x 12 columns] . # Calculate the daily returns returns = stock_prices.pct_change() # Calculate and print the pairwise correlations correlations = returns.corr() print(correlations) # Plot a heatmap of daily return correlations sns.heatmap(correlations, annot=True) plt.title(&#39;Daily Return Correlations&#39;) plt.show() . . #### Save your analysis to multiple excel worksheets . Now that you have completed your analysis, you may want to save all results into a single Excel workbook. . Let’s practice exporting various DataFrame to multiple Excel worksheets. . # Inspect index and stock_prices print(index.info()) print(stock_prices.info()) # Join index to stock_prices, and inspect the result data = stock_prices.join(index) print(data.info()) # Create index &amp; stock price returns returns = data.pct_change() # Export data and data as returns to excel with pd.ExcelWriter(&#39;data.xls&#39;) as writer: data.to_excel(writer, sheet_name=&#39;data&#39;) returns.to_excel(writer, sheet_name=&#39;returns&#39;) . The End. . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/manipulating-time-series-data-in-python.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/manipulating-time-series-data-in-python.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Machine Learning for Time Series Data in Python",
            "content": "Machine Learning for Time Series Data in Python . This is the memo of the 9th course (23 courses in all) of ‘Machine Learning Scientist with Python’ skill track. . You can find the original course HERE . . ### Course Description . Time series data is ubiquitous. Whether it be stock market fluctuations, sensor data recording climate change, or activity in the brain, any signal that changes over time can be described as a time series. Machine learning has emerged as a powerful method for leveraging complexity in data in order to generate predictions and insights into the problem one is trying to solve. This course is an intersection between these two worlds of machine learning and time series data, and covers feature engineering, spectograms, and other advanced techniques in order to classify heartbeat sounds and predict stock prices. . ### . Time Series and Machine Learning Primer | Time Series as Inputs to a Model | Predicting Time Series Data | Validating and Inspecting Time Series Models | 1. Time Series and Machine Learning Primer . . 1.1 Timeseries kinds and applications . . 1.1.1 Plotting a time series (I) . In this exercise, you’ll practice plotting the values of two time series without the time component. . Two DataFrames, data and data2 are available in your workspace. . Unless otherwise noted, assume that all required packages are loaded with their common aliases throughout this course. . Note This course assumes some familiarity with time series data, as well as how to use them in data analytics pipelines. For an introduction to time series, we recommend the Introduction to Time Series Analysis in Python and Visualizing Time Series Data with Python courses. # Plot the time series in each dataset fig, axs = plt.subplots(2, 1, figsize=(5, 10)) data.iloc[:1000].plot(y=&#39;data_values&#39;, ax=axs[0]) data2.iloc[:1000].plot(y=&#39;data_values&#39;, ax=axs[1]) plt.show() . . 1.1.2 Plotting a time series (II) . You’ll now plot both the datasets again, but with the included time stamps for each (stored in the column called &quot;time&quot; ). Let’s see if this gives you some more context for understanding each time series data. . # Plot the time series in each dataset fig, axs = plt.subplots(2, 1, figsize=(5, 10)) data.iloc[:1000].plot(x=&#39;time&#39;, y=&#39;data_values&#39;, ax=axs[0]) data2.iloc[:1000].plot(x=&#39;time&#39;, y=&#39;data_values&#39;, ax=axs[1]) plt.show() . . As you can now see, each time series has a very different sampling frequency (the amount of time between samples). The first is daily stock market data, and the second is an audio waveform. . . 1.2 Machine learning basics . . 1.2.1 Fitting a simple model: classification . In this exercise, you’ll use the iris dataset (representing petal characteristics of a number of flowers) to practice using the scikit-learn API to fit a classification model. You can see a sample plot of the data to the right. . Note This course assumes some familiarity with Machine Learning and scikit-learn . For an introduction to scikit-learn, we recommend the Supervised Learning with Scikit-Learn and Preprocessing for Machine Learning in Python courses. . from sklearn.svm import LinearSVC # Construct data for the model X = data[[&#39;petal length (cm)&#39;,&#39;petal width (cm)&#39;]] y = data[[&#39;target&#39;]] # Fit the model model = LinearSVC() model.fit(X, y) . 1.2.2 Predicting using a classification model . Now that you have fit your classifier, let’s use it to predict the type of flower (or class) for some newly-collected flowers. . Information about petal width and length for several new flowers is stored in the variable targets . Using the classifier you fit, you’ll predict the type of each flower. . # Create input array X_predict = targets[[&#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;]] # Predict with the model predictions = model.predict(X_predict) print(predictions) # [2 2 2 1 1 2 2 2 2 1 2 1 1 2 1 1 2 1 2 2] # Visualize predictions and actual values plt.scatter(X_predict[&#39;petal length (cm)&#39;], X_predict[&#39;petal width (cm)&#39;], c=predictions, cmap=plt.cm.coolwarm) plt.title(&quot;Predicted class values&quot;) plt.show() . . 1.2.3 Fitting a simple model: regression . In this exercise, you’ll practice fitting a regression model using data from the Boston housing market. A DataFrame called boston is available in your workspace. It contains many variables of data (stored as columns). Can you find a relationship between the following two variables? . &quot;AGE&quot; proportion of owner-occupied units built prior to 1940 | &quot;RM&quot; average number of rooms per dwelling | . . from sklearn import linear_model # Prepare input and output DataFrames X = boston[[&#39;AGE&#39;]] y = boston[[&#39;RM&#39;]] # Fit the model model = linear_model.LinearRegression() model.fit(X,y) . 1.2.4 Predicting using a regression model . Now that you’ve fit a model with the Boston housing data, lets see what predictions it generates on some new data. You can investigate the underlying relationship that the model has found between inputs and outputs by feeding in a range of numbers as inputs and seeing what the model predicts for each input. . A 1-D array new_inputs consisting of 100 “new” values for &quot;AGE&quot; (proportion of owner-occupied units built prior to 1940) is available in your workspace along with the model you fit in the previous exercise. . # Generate predictions with the model using those inputs predictions = model.predict(new_inputs.reshape(-1,1)) # Visualize the inputs and predicted values plt.scatter(new_inputs, predictions, color=&#39;r&#39;, s=3) plt.xlabel(&#39;inputs&#39;) plt.ylabel(&#39;predictions&#39;) plt.show() . . Here the red line shows the relationship that your model found. As the proportion of pre-1940s houses gets larger, the average number of rooms gets slightly lower. . . 1.3 Machine learning and time series data . . . 1.3.1 Inspecting the classification data . In these final exercises of this chapter, you’ll explore the two datasets you’ll use in this course. . The first is a collection of heartbeat sounds. Hearts normally have a predictable sound pattern as they beat, but some disorders can cause the heart to beat abnormally . This dataset contains a training set with labels for each type of heartbeat, and a testing set with no labels. You’ll use the testing set to validate your models. . As you have labeled data, this dataset is ideal for classification . In fact, it was originally offered as a part of a public Kaggle competition . . import librosa as lr from glob import glob # List all the wav files in the folder audio_files = glob(data_dir + &#39;/*.wav&#39;) # Read in the first audio file, create the time array audio, sfreq = lr.load(audio_files[0]) time = np.arange(0, len(audio)) / sfreq # Plot audio over time fig, ax = plt.subplots() ax.plot(time, audio) ax.set(xlabel=&#39;Time (s)&#39;, ylabel=&#39;Sound Amplitude&#39;) plt.show() . audio_files [&#39;./files/murmur__201108222248.wav&#39;, &#39;./files/murmur__201108222242.wav&#39;, &#39;./files/murmur__201108222253.wav&#39;, ...] audio array([-0.00039537, -0.00043787, -0.00047949, ..., 0.00376802, 0.00299449, 0.00206312], dtype=float32) sfreq 22050 time array([ 0.00000000e+00, 4.53514739e-05, 9.07029478e-05, ..., 7.93546485e+00, 7.93551020e+00, 7.93555556e+00]) . . There are several seconds of heartbeat sounds in here, though note that most of this time is silence. A common procedure in machine learning is to separate the datapoints with lots of stuff happening from the ones that don’t. . 1.3.2 Inspecting the regression data . The next dataset contains information about company market value over several years of time. This is one of the most popular kind of time series data used for regression. If you can model the value of a company as it changes over time, you can make predictions about where that company will be in the future. This dataset was also originally provided as part of a public Kaggle competition . . In this exercise, you’ll plot the time series for a number of companies to get an understanding of how they are (or aren’t) related to one another. . # Read in the data data = pd.read_csv(&#39;prices.csv&#39;, index_col=0) # Convert the index of the DataFrame to datetime data.index = pd.to_datetime(data.index) print(data.head()) # Loop through each column, plot its values over time fig, ax = plt.subplots() for column in data.columns: data[column].plot(ax=ax, label=column) ax.legend() plt.show() . . Note that each company’s value is sometimes correlated with others, and sometimes not. Also note there are a lot of ‘jumps’ in there – what effect do you think these jumps would have on a predictive model? . 2. Time Series as Inputs to a Model . . 2.1 Classifying a time series . . 2.1.1 Many repetitions of sounds . In this exercise, you’ll start with perhaps the simplest classification technique: averaging across dimensions of a dataset and visually inspecting the result. . You’ll use the heartbeat data described in the last chapter. Some recordings are normal heartbeat activity, while others are abnormal activity. Let’s see if you can spot the difference. . Two DataFrames, normal and abnormal , each with the shape of (n_times_points, n_audio_files) containing the audio for several heartbeats are available in your workspace. Also, the sampling frequency is loaded into a variable called sfreq . A convenience plotting function show_plot_and_make_titles() is also available in your workspace. . normal.shape # (8820, 3) normal.head() 3 4 6 time 0.000000 -0.000995 0.000281 0.002953 0.000454 -0.003381 0.000381 0.003034 0.000907 -0.000948 0.000063 0.000292 0.001361 -0.000766 0.000026 -0.005916 0.001814 0.000469 -0.000432 -0.005307 . fig, axs = plt.subplots(3, 2, figsize=(15, 7), sharex=True, sharey=True) # Calculate the time array time = np.arange(normal.shape[0]) / sfreq # Stack the normal/abnormal audio so you can loop and plot stacked_audio = np.hstack([normal, abnormal]).T # Loop through each audio file / ax object and plot # .T.ravel() transposes the array, then unravels it into a 1-D vector for looping for iaudio, ax in zip(stacked_audio, axs.T.ravel()): ax.plot(time, iaudio) show_plot_and_make_titles() . . As you can see there is a lot of variability in the raw data, let’s see if you can average out some of that noise to notice a difference. . 2.1.2 Invariance in time . While you should always start by visualizing your raw data, this is often uninformative when it comes to discriminating between two classes of data points. Data is usually noisy or exhibits complex patterns that aren’t discoverable by the naked eye. . Another common technique to find simple differences between two sets of data is to average across multiple instances of the same class. This may remove noise and reveal underlying patterns (or, it may not). . In this exercise, you’ll average across many instances of each class of heartbeat sound. . The two DataFrames ( normal and abnormal ) and the time array ( time ) from the previous exercise are available in your workspace. . normal.shape # (8820, 10) . # Average across the audio files of each DataFrame mean_normal = np.mean(normal, axis=1) mean_abnormal = np.mean(abnormal, axis=1) # Plot each average over time fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3), sharey=True) ax1.plot(time, mean_normal) ax1.set(title=&quot;Normal Data&quot;) ax2.plot(time, mean_abnormal) ax2.set(title=&quot;Abnormal Data&quot;) plt.show() . . Do you see a noticeable difference between the two? Maybe, but it’s quite noisy. Let’s see how you can dig into the data a bit further. . 2.1.3 Build a classification model . While eye-balling differences is a useful way to gain an intuition for the data, let’s see if you can operationalize things with a model. In this exercise, you will use each repetition as a datapoint, and each moment in time as a feature to fit a classifier that attempts to predict abnormal vs. normal heartbeats using only the raw data . . We’ve split the two DataFrames ( normal and abnormal ) into X_train , X_test , y_train , and y_test . . from sklearn.svm import LinearSVC # Initialize and fit the model model = LinearSVC() model.fit(X_train,y_train) # Generate predictions and score them manually predictions = model.predict(X_test) print(sum(predictions == y_test.squeeze()) / len(y_test)) # 0.555555555556 . Note that your predictions didn’t do so well. That’s because the features you’re using as inputs to the model (raw data) aren’t very good at differentiating classes. Next, you’ll explore how to calculate some more complex features that may improve the results. . . 2.2 Improving features for classification . . row . abs . roll . #### 2.2.1 Calculating the envelope of sound . One of the ways you can improve the features available to your model is to remove some of the noise present in the data. In audio data, a common way to do this is to smooth the data and then rectify it so that the total amount of sound energy over time is more distinguishable. You’ll do this in the current exercise. . audio.head() time 0.000000 -0.024684 0.000454 -0.060429 0.000907 -0.070080 0.001361 -0.084212 0.001814 -0.085111 Name: 0, dtype: float32 . # Plot the raw data first audio.plot(figsize=(10, 5)) plt.show() . . # Rectify the audio signal audio_rectified = audio.apply(np.abs) # Plot the result audio_rectified.plot(figsize=(10, 5)) plt.show() . . # Smooth by applying a rolling mean audio_rectified_smooth = audio_rectified.rolling(50).mean() # Plot the result audio_rectified_smooth.plot(figsize=(10, 5)) plt.show() . . By calculating the envelope of each sound and smoothing it, you’ve eliminated much of the noise and have a cleaner signal to tell you when a heartbeat is happening. . 2.2.2 Calculating features from the envelope . Now that you’ve removed some of the noisier fluctuations in the audio, let’s see if this improves your ability to classify. . model LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True, intercept_scaling=1, loss=&#39;squared_hinge&#39;, max_iter=1000, multi_class=&#39;ovr&#39;, penalty=&#39;l2&#39;, random_state=None, tol=0.0001, verbose=0) . # Calculate stats means = np.mean(audio_rectified_smooth, axis=0) stds = np.std(audio_rectified_smooth, axis=0) maxs = np.max(audio_rectified_smooth, axis=0) # Create the X and y arrays X = np.column_stack([means, stds, maxs]) y = labels.reshape([-1, 1]) # Fit the model and score on testing data from sklearn.model_selection import cross_val_score percent_score = cross_val_score(model, X, y, cv=5) print(np.mean(percent_score)) # 0.716666666667 . This model is both simpler (only 3 features) and more understandable (features are simple summary statistics of the data). . 2.2.3 Derivative features: The tempogram . One benefit of cleaning up your data is that it lets you compute more sophisticated features. For example, the envelope calculation you performed is a common technique in computing tempo and rhythm features. In this exercise, you’ll use librosa to compute some tempo and rhythm features for heartbeat data, and fit a model once more. . Note that librosa functions tend to only operate on numpy arrays instead of DataFrames, so we’ll access our Pandas data as a Numpy array with the .values attribute. . # Calculate the tempo of the sounds tempos = [] for col, i_audio in audio.items(): tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None)) # Convert the list to an array so you can manipulate it more easily tempos = np.array(tempos) # Calculate statistics of each tempo tempos_mean = tempos.mean(axis=-1) tempos_std = tempos.std(axis=-1) tempos_max = tempos.max(axis=-1) # Create the X and y arrays X = np.column_stack([means, stds, maxs, tempos_mean, tempos_std, tempos_max]) y = labels.reshape([-1, 1]) # Fit the model and score on testing data percent_score = cross_val_score(model, X, y, cv=5) print(np.mean(percent_score)) # 0.516666666667 . Note that your predictive power may not have gone up (because this dataset is quite small), but you now have a more rich feature representation of audio that your model can use! . . 2.3 The spectrogram . . 2.3.1 Spectrograms of heartbeat audio . Spectral engineering is one of the most common techniques in machine learning for time series data. The first step in this process is to calculate a spectrogram of sound. This describes what spectral content (e.g., low and high pitches) are present in the sound over time. In this exercise, you’ll calculate a spectrogram of a heartbeat audio file. . # Import the stft function from librosa.core import stft # Prepare the STFT HOP_LENGTH = 2**4 spec = stft(audio, hop_length=HOP_LENGTH, n_fft=2**7) . from librosa.core import amplitude_to_db from librosa.display import specshow # Convert into decibels spec_db = amplitude_to_db(spec) # Compare the raw audio to the spectrogram of the audio fig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True) axs[0].plot(time, audio) specshow(spec_db, sr=sfreq, x_axis=&#39;time&#39;, y_axis=&#39;hz&#39;, hop_length=HOP_LENGTH) plt.show() . . Do you notice that the heartbeats come in pairs, as seen by the vertical lines in the spectrogram? . 2.3.2 Engineering spectral features . As you can probably tell, there is a lot more information in a spectrogram compared to a raw audio file. By computing the spectral features, you have a much better idea of what’s going on. As such, there are all kinds of spectral features that you can compute using the spectrogram as a base. In this exercise, you’ll look at a few of these features. . import librosa as lr # Calculate the spectral centroid and bandwidth for the spectrogram bandwidths = lr.feature.spectral_bandwidth(S=spec)[0] centroids = lr.feature.spectral_centroid(S=spec)[0] . from librosa.core import amplitude_to_db from librosa.display import specshow # Convert spectrogram to decibels for visualization spec_db = amplitude_to_db(spec) # Display these features on top of the spectrogram fig, ax = plt.subplots(figsize=(10, 5)) ax = specshow(spec_db, x_axis=&#39;time&#39;, y_axis=&#39;hz&#39;, hop_length=HOP_LENGTH) ax.plot(times_spec, centroids) ax.fill_between(times_spec, centroids - bandwidths / 2, centroids + bandwidths / 2, alpha=.5) ax.set(ylim=[None, 6000]) plt.show() . . As you can see, the spectral centroid and bandwidth characterize the spectral content in each sound over time. They give us a summary of the spectral content that we can use in a classifier. . 2.3.3 Combining many features in a classifier . You’ve spent this lesson engineering many features from the audio data – some contain information about how the audio changes in time, others contain information about the spectral content that is present. . The beauty of machine learning is that it can handle all of these features at the same time. If there is different information present in each feature, it should improve the classifier’s ability to distinguish the types of audio. Note that this often requires more advanced techniques such as regularization, which we’ll cover in the next chapter. . For the final exercise in the chapter, we’ve loaded many of the features that you calculated before. Combine all of them into an array that can be fed into the classifier, and see how it does. . # Loop through each spectrogram bandwidths = [] centroids = [] for spec in spectrograms: # Calculate the mean spectral bandwidth this_mean_bandwidth = np.mean(lr.feature.spectral_bandwidth(S=spec)) # Calculate the mean spectral centroid this_mean_centroid = np.mean(lr.feature.spectral_centroid(S=spec)) # Collect the values bandwidths.append(this_mean_bandwidth) centroids.append(this_mean_centroid) # Create X and y arrays X = np.column_stack([means, stds, maxs, tempo_mean, tempo_max, tempo_std, bandwidths, centroids]) y = labels.reshape([-1, 1]) # Fit the model and score on testing data percent_score = cross_val_score(model, X, y, cv=5) print(np.mean(percent_score)) # 0.533333333333 . You calculated many different features of the audio, and combined each of them under the assumption that they provide independent information that can be used in classification. You may have noticed that the accuracy of your models varied a lot when using different set of features. This chapter was focused on creating new “features” from raw data and not obtaining the best accuracy. To improve the accuracy, you want to find the right features that provide relevant information and also build models on much larger data. . 3. Predicting Time Series Data . . 3.1 Predicting data over time . . 3.1.1 Introducing the dataset . As mentioned in the video, you’ll deal with stock market prices that fluctuate over time. In this exercise you’ve got historical prices from two tech companies ( Ebay and Yahoo ) in the DataFrame prices . You’ll visualize the raw data for the two companies, then generate a scatter plot showing how the values for each company compare with one another. Finally, you’ll add in a “time” dimension to your scatter plot so you can see how this relationship changes over time. . # Plot the raw values over time prices.plot() plt.show() . . # Scatterplot with one company per axis prices.plot.scatter(&#39;EBAY&#39;, &#39;YHOO&#39;) plt.show() . . # Scatterplot with color relating to time prices.plot.scatter(&#39;EBAY&#39;, &#39;YHOO&#39;, c=&#39;date&#39;, cmap=plt.cm.viridis, colorbar=False) plt.show() . . As you can see, these two time series seem somewhat related to each other, though its a complex relationship that changes over time. . 3.1.2 Fitting a simple regression model . Now we’ll look at a larger number of companies. Recall that we have historical price values for many companies. Let’s use data from several companies to predict the value of a test company. You’ll attempt to predict the value of the Apple stock price using the values of NVidia, Ebay, and Yahoo. . all_prices.head() symbol AAPL ABT AIG AMAT ARNC BAC date 2010-01-04 214.009998 54.459951 29.889999 14.30 16.650013 15.690000 2010-01-05 214.379993 54.019953 29.330000 14.19 16.130013 16.200001 2010-01-06 210.969995 54.319953 29.139999 14.16 16.970013 16.389999 2010-01-07 210.580000 54.769952 28.580000 14.01 16.610014 16.930000 2010-01-08 211.980005 55.049952 29.340000 14.55 17.020014 16.780001 . from sklearn.linear_model import Ridge from sklearn.model_selection import cross_val_score # Use stock symbols to extract training data X = all_prices[[&quot;EBAY&quot;,&quot;NVDA&quot;,&quot;YHOO&quot;]] y = all_prices[[&quot;AAPL&quot;]] # Fit and score the model with cross-validation scores = cross_val_score(Ridge(), X, y, cv=3) print(scores) # [-6.09050633 -0.3179172 -3.72957284] . As you can see, fitting a model with raw data doesn’t give great results. . 3.1.3 Visualizing predicted values . When dealing with time series data, it’s useful to visualize model predictions on top of the “actual” values that are used to test the model. . In this exercise, after splitting the data (stored in the variables X and y ) into training and test sets, you’ll build a model and then visualize the model’s predictions on top of the testing data in order to estimate the model’s performance. . from sklearn.model_selection import train_test_split from sklearn.metrics import r2_score # Split our data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8, shuffle=False, random_state=1) # Fit our model and generate predictions model = Ridge() model.fit(X_train, y_train) predictions = model.predict(X_test) score = r2_score(y_test, predictions) print(score) # -5.70939901949 . # Visualize our predictions along with the &quot;true&quot; values, and print the score fig, ax = plt.subplots(figsize=(15, 5)) ax.plot(y_test, color=&#39;k&#39;, lw=3) ax.plot(predictions, color=&#39;r&#39;, lw=2) plt.show() . . Now you have an explanation for your poor score. The predictions clearly deviate from the true time series values. . . 3.2 Advanced time series prediction . . 3.2.1 Visualizing messy data . Let’s take a look at a new dataset – this one is a bit less-clean than what you’ve seen before. . As always, you’ll first start by visualizing the raw data. Take a close look and try to find datapoints that could be problematic for fitting models. . # Visualize the dataset prices.plot(legend=False) plt.tight_layout() plt.show() # Count the missing values of each time series missing_values = prices.isna().sum() print(missing_values) . . In the plot, you can see there are clearly missing chunks of time in your data. There also seem to be a few ‘jumps’ in the data. How can you deal with this? . 3.2.2 Imputing missing values . When you have missing data points, how can you fill them in? . In this exercise, you’ll practice using different interpolation methods to fill in some missing values, visualizing the result each time. But first, you will create the function ( interpolate_and_plot() ) you’ll use to interpolate missing data points and plot them. . # Create a function we&#39;ll use to interpolate and plot def interpolate_and_plot(prices, interpolation): # Create a boolean mask for missing values missing_values = prices.isna() # Interpolate the missing values prices_interp = prices.interpolate(interpolation) # Plot the results, highlighting the interpolated values in black fig, ax = plt.subplots(figsize=(10, 5)) prices_interp.plot(color=&#39;k&#39;, alpha=.6, ax=ax, legend=False) # Now plot the interpolated values on top in red prices_interp[missing_values].plot(ax=ax, color=&#39;r&#39;, lw=3, legend=False) plt.show() . # Interpolate using the latest non-missing value interpolation_type = &#39;zero&#39; interpolate_and_plot(prices, interpolation_type) . . # Interpolate linearly interpolation_type = &#39;linear&#39; interpolate_and_plot(prices, interpolation_type) . . # Interpolate with a quadratic function interpolation_type = &#39;quadratic&#39; interpolate_and_plot(prices, interpolation_type) . . When you interpolate, the pre-existing data is used to infer the values of missing data. As you can see, the method you use for this has a big effect on the outcome. . 3.2.3 Transforming raw data . In the last chapter, you calculated the rolling mean. In this exercise, you will define a function that calculates the percent change of the latest data point from the mean of a window of previous data points. This function will help you calculate the percent change over a rolling window. . This is a more stable kind of time series that is often useful in machine learning. . # Your custom function def percent_change(series): # Collect all *but* the last value of this window, then the final value previous_values = series[:-1] last_value = series[-1] # Calculate the % difference between the last value and the mean of earlier values percent_change = (last_value - np.mean(previous_values)) / np.mean(previous_values) return percent_change # Apply your custom function and plot prices_perc = prices.rolling(20).aggregate(percent_change) prices_perc.loc[&quot;2014&quot;:&quot;2015&quot;].plot() plt.show() . . You’ve converted the data so it’s easier to compare one time point to another. This is a cleaner representation of the data. . 3.2.4 Handling outliers . In this exercise, you’ll handle outliers – data points that are so different from the rest of your data, that you treat them differently from other “normal-looking” data points. You’ll use the output from the previous exercise (percent change over time) to detect the outliers. First you will write a function that replaces outlier data points with the median value from the entire time series. . def replace_outliers(series): # Calculate the absolute difference of each timepoint from the series mean absolute_differences_from_mean = np.abs(series - np.mean(series)) # Calculate a mask for the differences that are &gt; 3 standard deviations from zero this_mask = absolute_differences_from_mean &gt; (np.std(series) * 3) # Replace these values with the median accross the data series[this_mask] = np.nanmedian(series) return series # Apply your preprocessing function to the timeseries and plot the results prices_perc = prices_perc.apply(replace_outliers) prices_perc.loc[&quot;2014&quot;:&quot;2015&quot;].plot() plt.show() . . Since you’ve converted the data to % change over time, it was easier to spot and correct the outliers. . . 3.3 Creating features over time . . 3.3.1 Engineering multiple rolling features at once . Now that you’ve practiced some simple feature engineering, let’s move on to something more complex. You’ll calculate a collection of features for your time series data and visualize what they look like over time. This process resembles how many other time series models operate. . # Define a rolling window with Pandas, excluding the right-most datapoint of the window prices_perc_rolling = prices_perc.rolling(20, min_periods=5, closed=&#39;right&#39;) # Define the features you&#39;ll calculate for each window features_to_calculate = [np.min, np.max, np.mean, np.std] # Calculate these features for your rolling window object features = prices_perc_rolling.agg(features_to_calculate) # Plot the results ax = features.loc[:&quot;2011-01&quot;].plot() prices_perc.loc[:&quot;2011-01&quot;].plot(ax=ax, color=&#39;k&#39;, alpha=.2, lw=3) ax.legend(loc=(1.01, .6)) plt.show() . . 3.3.2 Percentiles and partial functions . In this exercise, you’ll practice how to pre-choose arguments of a function so that you can pre-configure how it runs. You’ll use this to calculate several percentiles of your data using the same percentile() function in numpy . . # Import partial from functools from functools import partial percentiles = [1, 10, 25, 50, 75, 90, 99] # Use a list comprehension to create a partial function for each quantile percentile_functions = [partial(np.percentile, q=percentile) for percentile in percentiles] # Calculate each of these quantiles on the data using a rolling window prices_perc_rolling = prices_perc.rolling(20, min_periods=5, closed=&#39;right&#39;) features_percentiles = prices_perc_rolling.agg(percentile_functions) # Plot a subset of the result ax = features_percentiles.loc[:&quot;2011-01&quot;].plot(cmap=plt.cm.viridis) ax.legend(percentiles, loc=(1.01, .5)) plt.show() . . 3.3.3 Using “date” information . It’s easy to think of timestamps as pure numbers, but don’t forget they generally correspond to things that happen in the real world. That means there’s often extra information encoded in the data such as “is it a weekday?” or “is it a holiday?”. This information is often useful in predicting timeseries data. . prices_perc.head() EBAY date 2014-01-02 0.017938 2014-01-03 0.002268 2014-01-06 -0.027365 2014-01-07 -0.006665 2014-01-08 -0.017206 . # Extract date features from the data, add them as columns prices_perc[&#39;day_of_week&#39;] = prices_perc.index.dayofweek prices_perc[&#39;week_of_year&#39;] = prices_perc.index.weekofyear prices_perc[&#39;month_of_year&#39;] = prices_perc.index.month # Print prices_perc print(prices_perc) . EBAY day_of_week week_of_year month_of_year date 2014-01-02 0.017938 3 1 1 2014-01-03 0.002268 4 1 1 2014-01-06 -0.027365 0 2 1 2014-01-07 -0.006665 1 2 1 ... . This concludes the third chapter. In the next chapter, you will learn advanced techniques to validate and inspect your time series models. . 4. Validating and Inspecting Time Series Models . . 4.1 Creating features from the past . . 4.1.1 Creating time-shifted features . In machine learning for time series, it’s common to use information about previous time points to predict a subsequent time point. . In this exercise, you’ll “shift” your raw data and visualize the results. You’ll use the percent change time series that you calculated in the previous chapter, this time with a very short window. A short window is important because, in a real-world scenario, you want to predict the day-to-day fluctuations of a time series, not its change over a longer window of time. . # These are the &quot;time lags&quot; shifts = np.arange(1, 11).astype(int) # Use a dictionary comprehension to create name: value pairs, one pair per shift shifted_data = {&quot;lag_{}_day&quot;.format(day_shift): prices_perc.shift(day_shift) for day_shift in shifts} # Convert into a DataFrame for subsequent use prices_perc_shifted = pd.DataFrame(shifted_data) # Plot the first 100 samples of each ax = prices_perc_shifted.iloc[:100].plot(cmap=plt.cm.viridis) prices_perc.iloc[:100].plot(color=&#39;r&#39;, lw=2) ax.legend(loc=&#39;best&#39;) plt.show() . . 4.1.2 Special case: Auto-regressive models . Now that you’ve created time-shifted versions of a single time series, you can fit an auto-regressive model. This is a regression model where the input features are time-shifted versions of the output time series data. You are using previous values of a timeseries to predict current values of the same timeseries (thus, it is auto-regressive). . By investigating the coefficients of this model, you can explore any repetitive patterns that exist in a timeseries, and get an idea for how far in the past a data point is predictive of the future. . # Replace missing values with the median for each column X = prices_perc_shifted.fillna(np.nanmedian(prices_perc_shifted)) y = prices_perc.fillna(np.nanmedian(prices_perc)) # Fit the model model = Ridge() model.fit(X, y) . X.head(1) lag_1_day lag_2_day lag_3_day lag_4_day lag_5_day lag_6_day date 2010-01-04 0.000756 0.000756 0.000756 0.000756 0.000756 0.000756 lag_7_day lag_8_day lag_9_day lag_10_day date 2010-01-04 0.000756 0.000756 0.000756 0.000756 y.head(1) date 2010-01-04 0.000756 Name: AAPL, dtype: float64 . You’ve filled in the missing values with the median so that it behaves well with scikit-learn. Now let’s take a look at what your model found. . 4.1.3 Visualize regression coefficients . Now that you’ve fit the model, let’s visualize its coefficients. This is an important part of machine learning because it gives you an idea for how the different features of a model affect the outcome. . The shifted time series DataFrame ( prices_perc_shifted ) and the regression model ( model ) are available in your workspace. . In this exercise, you will create a function that, given a set of coefficients and feature names, visualizes the coefficient values. . def visualize_coefficients(coefs, names, ax): # Make a bar plot for the coefficients, including their names on the x-axis ax.bar(names, coefs) ax.set(xlabel=&#39;Coefficient name&#39;, ylabel=&#39;Coefficient value&#39;) # Set formatting so it looks nice plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment=&#39;right&#39;) return ax . # Visualize the output data up to &quot;2011-01&quot; fig, axs = plt.subplots(2, 1, figsize=(10, 5)) y.loc[:&#39;2011-01&#39;].plot(ax=axs[0]) # Run the function to visualize model&#39;s coefficients visualize_coefficients(model.coef_, prices_perc_shifted.columns, ax=axs[1]) plt.show() . . When you use time-lagged features on the raw data, you see that the highest coefficient by far is the first one. This means that the N-1th time point is useful in predicting the Nth timepoint, but no other points are useful. . 4.1.4 Auto-regression with a smoother time series . Now, let’s re-run the same procedure using a smoother signal. You’ll use the same percent change algorithm as before, but this time use a much larger window (40 instead of 20). As the window grows, the difference between neighboring timepoints gets smaller, resulting in a smoother signal. What do you think this will do to the auto-regressive model? . prices_perc_shifted and model (updated to use a window of 40) are available in your workspace. . # Visualize the output data up to &quot;2011-01&quot; fig, axs = plt.subplots(2, 1, figsize=(10, 5)) y.loc[:&#39;2011-01&#39;].plot(ax=axs[0]) # Run the function to visualize model&#39;s coefficients visualize_coefficients(model.coef_, prices_perc_shifted.columns, ax=axs[1]) plt.show() . . As you can see here, by transforming your data with a larger window, you’ve also changed the relationship between each timepoint and the ones that come just before it. This model’s coefficients gradually go down to zero, which means that the signal itself is smoother over time. Be careful when you see something like this, as it means your data is not i.i.d . . . 4.2 Cross-validating time series data . . 4.2.1 Cross-validation with shuffling . As you’ll recall, cross-validation is the process of splitting your data into training and test sets multiple times. Each time you do this, you choose a different training and test set. In this exercise, you’ll perform a traditional ShuffleSplit cross-validation on the company value data from earlier. Later we’ll cover what changes need to be made for time series data. The data we’ll use is the same historical price data for several large companies. . An instance of the Linear regression object ( model ) is available in your workspace along with the function r2_score() for scoring. Also, the data is stored in arrays X and y . We’ve also provided a helper function ( visualize_predictions() ) to help visualize the results. . # Import ShuffleSplit and create the cross-validation object from sklearn.model_selection import ShuffleSplit cv = ShuffleSplit(n_splits=10, random_state=1) # Iterate through CV splits results = [] for tr, tt in cv.split(X, y): # Fit the model on training data model.fit(X[tr], y[tr]) # Generate predictions on the test data, score the predictions, and collect prediction = model.predict(X[tt]) score = r2_score(y[tt], prediction) results.append((prediction, score, tt)) # Custom function to quickly visualize predictions visualize_predictions(results) . . You’ve correctly constructed and fit the model. If you look at the plot to the right, see that the order of datapoints in the test set is scrambled. Let’s see how it looks when we shuffle the data in blocks. . 4.2.2 Cross-validation without shuffling . Now, re-run your model fit using block cross-validation (without shuffling all datapoints). In this case, neighboring time-points will be kept close to one another. How do you think the model predictions will look in each cross-validation loop? . An instance of the Linear regression model object is available in your workspace. Also, the arrays X and y (training data) are available too. . # Create KFold cross-validation object from sklearn.model_selection import KFold cv = KFold(n_splits=10, shuffle=False, random_state=1) # Iterate through CV splits results = [] for tr, tt in cv.split(X, y): # Fit the model on training data model.fit(X[tr], y[tr]) # Generate predictions on the test data and collect prediction = model.predict(X[tt]) results.append((prediction, tt)) # Custom function to quickly visualize predictions visualize_predictions(results) . . This time, the predictions generated within each CV loop look ‘smoother’ than they were before – they look more like a real time series because you didn’t shuffle the data. This is a good sanity check to make sure your CV splits are correct. . 4.2.3 Time-based cross-validation . Finally, let’s visualize the behavior of the time series cross-validation iterator in scikit-learn. Use this object to iterate through your data one last time, visualizing the training data used to fit the model on each iteration. . An instance of the Linear regression model object is available in your workpsace. Also, the arrays X and y (training data) are available too. . # Import TimeSeriesSplit from sklearn.model_selection import TimeSeriesSplit # Create time-series cross-validation object cv = TimeSeriesSplit(n_splits=10) # Iterate through CV splits fig, ax = plt.subplots() for ii, (tr, tt) in enumerate(cv.split(X, y)): # Plot the training data on each iteration, to see the behavior of the CV ax.plot(tr, ii + y[tr]) ax.set(title=&#39;Training data on each CV iteration&#39;, ylabel=&#39;CV iteration&#39;) plt.show() . . Note that the size of the training set grew each time when you used the time series cross-validation object. This way, the time points you predict are always after the timepoints we train on. . . 4.3 Stationarity and stability . . 4.3.1 Stationarity . First, let’s confirm what we know about stationarity. Take a look at these time series. . Which of the following time series do you think are not stationary? . . B and C . C begins to trend upward partway through, while B shows a large increase in variance mid-way through, making both of them non-stationary. . 4.3.2 Bootstrapping a confidence interval . A useful tool for assessing the variability of some data is the bootstrap. In this exercise, you’ll write your own bootstrapping function that can be used to return a bootstrapped confidence interval. . This function takes three parameters: a 2-D array of numbers ( data ), a list of percentiles to calculate ( percentiles ), and the number of boostrap iterations to use ( n_boots ). It uses the resample function to generate a bootstrap sample, and then repeats this many times to calculate the confidence interval. . from sklearn.utils import resample def bootstrap_interval(data, percentiles=(2.5, 97.5), n_boots=100): &quot;&quot;&quot;Bootstrap a confidence interval for the mean of columns of a 2-D dataset.&quot;&quot;&quot; # Create our empty array to fill the results bootstrap_means = np.zeros([n_boots, data.shape[-1]]) for ii in range(n_boots): # Generate random indices for our data *with* replacement, then take the sample mean random_sample = resample(data) bootstrap_means[ii] = random_sample.mean(axis=0) # Compute the percentiles of choice for the bootstrapped means percentiles = np.percentile(bootstrap_means, percentiles, axis=0) return percentiles . You can use this function to assess the variability of your model coefficients. . 4.3.3 Calculating variability in model coefficients . In this lesson, you’ll re-run the cross-validation routine used before, but this time paying attention to the model’s stability over time. You’ll investigate the coefficients of the model, as well as the uncertainty in its predictions. . Begin by assessing the stability (or uncertainty) of a model’s coefficients across multiple CV splits. Remember, the coefficients are a reflection of the pattern that your model has found in the data. . An instance of the Linear regression object ( model ) is available in your workpsace. Also, the arrays X and y (the data) are available too. . # Iterate through CV splits n_splits = 100 cv = TimeSeriesSplit(n_splits=n_splits) # Create empty array to collect coefficients coefficients = np.zeros([n_splits, X.shape[1]]) for ii, (tr, tt) in enumerate(cv.split(X, y)): # Fit the model on training data and collect the coefficients model.fit(X[tr], y[tr]) coefficients[ii] = model.coef_ . # Calculate a confidence interval around each coefficient bootstrapped_interval = bootstrap_interval(coefficients, (2.5,97.5)) # Plot it fig, ax = plt.subplots() ax.scatter(feature_names, bootstrapped_interval[0], marker=&#39;_&#39;, lw=3) ax.scatter(feature_names, bootstrapped_interval[1], marker=&#39;_&#39;, lw=3) ax.set(title=&#39;95% confidence interval for model coefficients&#39;) plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment=&#39;right&#39;) plt.show() . . You’ve calculated the variability around each coefficient, which helps assess which coefficients are more stable over time! . 4.3.4 Visualizing model score variability over time . Now that you’ve assessed the variability of each coefficient, let’s do the same for the performance (scores) of the model. Recall that the TimeSeriesSplit object will use successively-later indices for each test set. This means that you can treat the scores of your validation as a time series. You can visualize this over time in order to see how the model’s performance changes over time. . An instance of the Linear regression model object is stored in model , a cross-validation object in cv , and data in X and y . . from sklearn.model_selection import cross_val_score # Generate scores for each split to see how the model performs over time scores = cross_val_score(model, X, y, cv=cv, scoring=my_pearsonr) # Convert to a Pandas Series object scores_series = pd.Series(scores, index=times_scores, name=&#39;score&#39;) # Bootstrap a rolling confidence interval for the mean score scores_lo = scores_series.rolling(20).aggregate(partial(bootstrap_interval, percentiles=2.5)) scores_hi = scores_series.rolling(20).aggregate(partial(bootstrap_interval, percentiles=97.5)) . # Plot the results fig, ax = plt.subplots() scores_lo.plot(ax=ax, label=&quot;Lower confidence interval&quot;) scores_hi.plot(ax=ax, label=&quot;Upper confidence interval&quot;) ax.legend() plt.show() . . You plotted a rolling confidence interval for scores over time. This is useful in seeing when your model predictions are correct. . 4.3.5 Accounting for non-stationarity . In this exercise, you will again visualize the variations in model scores, but now for data that changes its statistics over time. . An instance of the Linear regression model object is stored in model , a cross-validation object in cv , and the data in X and y . . # Pre-initialize window sizes window_sizes = [25, 50, 75, 100] # Create an empty DataFrame to collect the stores all_scores = pd.DataFrame(index=times_scores) # Generate scores for each split to see how the model performs over time for window in window_sizes: # Create cross-validation object using a limited lookback window cv = TimeSeriesSplit(n_splits=100, max_train_size=window) # Calculate scores across all CV splits and collect them in a DataFrame this_scores = cross_val_score(model, X, y, cv=cv, scoring=my_pearsonr) all_scores[&#39;Length {}&#39;.format(window)] = this_scores . # Visualize the scores ax = all_scores.rolling(10).mean().plot(cmap=plt.cm.coolwarm) ax.set(title=&#39;Scores for multiple windows&#39;, ylabel=&#39;Correlation (r)&#39;) plt.show() . . Wonderful – notice how in some stretches of time, longer windows perform worse than shorter ones. This is because the statistics in the data have changed, and the longer window is now using outdated information. . . ### 4.4 Wrap-up . . . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/machine-learning-for-time-series-data-in-python.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/machine-learning-for-time-series-data-in-python.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "Linear Classifiers in Python",
            "content": "Linear Classifiers in Python . This is the memo of the 3rd course (5 courses in all) of ‘Machine Learning with Python’ skill track. . You can find the original course HERE . . . 1. Applying logistic regression and SVM . . 1.1 scikit-learn refresher . #### KNN classification . In this exercise you’ll explore a subset of the Large Movie Review Dataset . The variables X_train , X_test , y_train , and y_test are already loaded into the environment. The X variables contain features based on the words in the movie reviews, and the y variables contain labels for whether the review sentiment is positive (+1) or negative (-1). . This course touches on a lot of concepts you may have forgotten, so if you ever need a quick refresher, download the Scikit-Learn Cheat Sheet and keep it handy! . X_train.shape # (2000, 2500) X_test.shape # (2000, 2500) type(X_train) scipy.sparse.csr.csr_matrix X_train[0] &lt;1x2500 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39; with 73 stored elements in Compressed Sparse Row format&gt; y_train[-10:] array([-1., 1., 1., -1., -1., 1., -1., 1., 1., 1.]) . from sklearn.neighbors import KNeighborsClassifier # Create and fit the model knn = KNeighborsClassifier() knn.fit(X_train, y_train) # Predict on the test features, print the results pred = knn.predict(X_test)[0] print(&quot;Prediction for test example 0:&quot;, pred) # Prediction for test example 0: 1.0 . #### Comparing models . Compare k nearest neighbors classifiers with k=1 and k=5 on the handwritten digits data set, which is already loaded into the variables X_train , y_train , X_test , and y_test . . Which model has a higher test accuracy? . from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score knn = KNeighborsClassifier(n_neighbors=1) knn.fit(X_train, y_train) y_pred = knn.predict(X_test) print(accuracy_score(y_test, y_pred)) # 0.9888888888888889 knn = KNeighborsClassifier(n_neighbors=5) knn.fit(X_train, y_train) y_pred = knn.predict(X_test) print(accuracy_score(y_test, y_pred)) # 0.9933333333333333 . . 1.2 Applying logistic regression and SVM . #### Running LogisticRegression and SVC . In this exercise, you’ll apply logistic regression and a support vector machine to classify images of handwritten digits. . X_train[:2] array([[ 0., 0., 10., 16., 5., 0., 0., 0., 0., 1., 10., 14., 12., 0., 0., 0., 0., 0., 0., 9., 11., 0., 0., 0., 0., 0., 2., 11., 13., 3., 0., 0., 0., 0., 11., 16., 16., 16., 7., 0., 0., 0., 3., 16., 4., 5., 1., 0., 0., 0., 7., 13., 0., 0., 0., 0., 0., 0., 13., 6., 0., 0., 0., 0.], [ 0., 0., 3., 11., 13., 15., 3., 0., 0., 4., 16., 14., 11., 16., 8., 0., 0., 2., 5., 0., 14., 15., 1., 0., 0., 0., 0., 0., 16., 11., 0., 0., 0., 0., 0., 0., 11., 10., 0., 0., 0., 0., 0., 0., 8., 12., 0., 0., 0., 0., 8., 11., 15., 8., 0., 0., 0., 0., 2., 12., 14., 3., 0., 0.]]) y_train[:2] # array([7, 3]) X_train.shape # (1347, 64) . from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.model_selection import train_test_split from sklearn import datasets # load the data digits = datasets.load_digits() X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target) # Apply logistic regression and print scores lr = LogisticRegression() lr.fit(X_train, y_train) # score(self, X, y[, sample_weight]) # Returns the mean accuracy on the given test data and labels. print(lr.score(X_train, y_train)) print(lr.score(X_test, y_test)) # 0.9955456570155902 # 0.9622222222222222 # Apply SVM and print scores svm = SVC() svm.fit(X_train, y_train) # score(self, X, y[, sample_weight]) # Returns the mean accuracy on the given test data and labels. print(svm.score(X_train, y_train)) print(svm.score(X_test, y_test)) # 1.0 # 0.48 . Later in the course we’ll look at the similarities and differences of logistic regression vs. SVMs. . #### Sentiment analysis for movie reviews . In this exercise you’ll explore the probabilities outputted by logistic regression on a subset of the Large Movie Review Dataset . . The variables X and y are already loaded into the environment. X contains features based on the number of times words appear in the movie reviews, and y contains labels for whether the review sentiment is positive (+1) or negative (-1). . get_features? Signature: get_features(review) Docstring: &lt;no docstring&gt; File: /tmp/tmpn52ffwy5/&lt;ipython-input-1-33e0d8df8588&gt; Type: function review1 = &quot;LOVED IT! This movie was amazing. Top 10 this year.&quot; review1_features = get_features(review1) review1_features &lt;1x2500 sparse matrix of type &#39;&lt;class &#39;numpy.int64&#39;&gt;&#39; with 8 stored elements in Compressed Sparse Row format&gt; . # Instantiate logistic regression and train lr = LogisticRegression() lr.fit(X, y) # Predict sentiment for a glowing review review1 = &quot;LOVED IT! This movie was amazing. Top 10 this year.&quot; review1_features = get_features(review1) print(&quot;Review:&quot;, review1) print(&quot;Probability of positive review:&quot;, lr.predict_proba(review1_features)[0,1]) # Review: LOVED IT! This movie was amazing. Top 10 this year. # Probability of positive review: 0.8079007873616059 # Predict sentiment for a poor review review2 = &quot;Total junk! I&#39;ll never watch a film by that director again, no matter how good the reviews.&quot; review2_features = get_features(review2) print(&quot;Review:&quot;, review2) print(&quot;Probability of positive review:&quot;, lr.predict_proba(review2_features)[0,1]) # Review: Total junk! I&#39;ll never watch a film by that director again, no matter how good the reviews. # Probability of positive review: 0.5855117402793947 . . 1.3 Linear classifiers . #### Visualizing decision boundaries . In this exercise, you’ll visualize the decision boundaries of various classifier types. . A subset of scikit-learn ‘s built-in wine dataset is already loaded into X , along with binary labels in y . . X[:3] array([[11.45, 2.4 ], [13.62, 4.95], [13.88, 1.89]]) y[:3] array([ True, True, False]) plot_4_classifiers? Signature: plot_4_classifiers(X, y, clfs) Docstring: &lt;no docstring&gt; File: /usr/local/share/datasets/plot_classifier.py Type: function . from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC, LinearSVC from sklearn.neighbors import KNeighborsClassifier # Define the classifiers classifiers = [LogisticRegression(), LinearSVC(), SVC(), KNeighborsClassifier()] # Fit the classifiers for c in classifiers: c.fit(X, y) # Plot the classifiers plot_4_classifiers(X, y, classifiers) plt.show() . . As you can see, logistic regression and linear SVM are linear classifiers whereas the default SVM and KNN are not. . . 2. Loss functions . . 2.1 Linear classifiers: the coefficients . . #### Changing the model coefficients . # Set the coefficients model.coef_ = np.array([[-1,1]]) model.intercept_ = np.array([-3]) # Plot the data and decision boundary plot_classifier(X,y,model) # Print the number of errors num_err = np.sum(y != model.predict(X)) print(&quot;Number of errors:&quot;, num_err) # Number of errors: 0 . . model.coef_ = np.array([[-1,1]]) . model.intercept_ = np.array([-3]) . model.coef_ = np.array([[-1,1]]) . model.intercept_ = np.array([1]) . model.coef_ = np.array([[-1,1]]) . model.intercept_ = np.array([-3]) . model.coef_ = np.array([[-1,0]]) . model.intercept_ = np.array([-3]) . model.coef_ = np.array([[-1,1]]) . model.intercept_ = np.array([-3]) . model.coef_ = np.array([[-1,2]]) . model.intercept_ = np.array([-3]) . model.coef_ = np.array([[-1,1]]) . model.intercept_ = np.array([-3]) . model.coef_ = np.array([[0,1]]) . model.intercept_ = np.array([-3]) . As you can see, the coefficients determine the slope of the boundary and the intercept shifts it. . . 2.2 What is a loss function? . . . #### Minimizing a loss function . In this exercise you’ll implement linear regression “from scratch” using scipy.optimize.minimize . . We’ll train a model on the Boston housing price data set, which is already loaded into the variables X and y . For simplicity, we won’t include an intercept in our regression model. . X.shape (506, 13) X[:2] array([[6.3200e-03, 1.8000e+01, 2.3100e+00, 0.0000e+00, 5.3800e-01, 6.5750e+00, 6.5200e+01, 4.0900e+00, 1.0000e+00, 2.9600e+02, 1.5300e+01, 3.9690e+02, 4.9800e+00], [2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01, 6.4210e+00, 7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02, 1.7800e+01, 3.9690e+02, 9.1400e+00]]) y[:3] array([24. , 21.6, 34.7]) . from scipy.optimize import minimize # The squared error, summed over training examples def my_loss(w): s = 0 for i in range(y.size): # Get the true and predicted target values for example &#39;i&#39; y_i_true = y[i] y_i_pred = w@X[i] s = s + (y_i_true - y_i_pred)**2 return s # Returns the w that makes my_loss(w) smallest w_fit = minimize(my_loss, X[0]).x print(w_fit) # Compare with scikit-learn&#39;s LinearRegression coefficients lr = LinearRegression(fit_intercept=False).fit(X,y) print(lr.coef_) . [-9.16299112e-02 4.86754828e-02 -3.77698794e-03 2.85635998e+00 -2.88057050e+00 5.92521269e+00 -7.22470732e-03 -9.67992974e-01 1.70448714e-01 -9.38971600e-03 -3.92421893e-01 1.49830571e-02 -4.16973012e-01] [-9.16297843e-02 4.86751203e-02 -3.77930006e-03 2.85636751e+00 -2.88077933e+00 5.92521432e+00 -7.22447929e-03 -9.67995240e-01 1.70443393e-01 -9.38925373e-03 -3.92425680e-01 1.49832102e-02 -4.16972624e-01] . . 2.3 Loss function diagrams . . not good for classification because loss is large on the correct predict . #### Classification loss functions . Which of the four loss functions makes sense for classification? . . 2. . This loss is very similar to the hinge loss used in SVMs (just shifted slightly). . #### Comparing the logistic and hinge losses . In this exercise you’ll create a plot of the logistic and hinge losses using their mathematical expressions, which are provided to you. . The loss function diagram from the video is shown below. . # Mathematical functions for logistic and hinge losses def log_loss(raw_model_output): return np.log(1+np.exp(-raw_model_output)) def hinge_loss(raw_model_output): return np.maximum(0,1-raw_model_output) # Create a grid of values and plot grid = np.linspace(-2,2,1000) plt.plot(grid, log_loss(grid), label=&#39;logistic&#39;) plt.plot(grid, hinge_loss(grid), label=&#39;hinge&#39;) plt.legend() plt.show() . . As you can see, these match up with the loss function diagrams above. . #### Implementing logistic regression . This is very similar to the earlier exercise where you implemented linear regression “from scratch” using scipy.optimize.minimize . However, this time we’ll minimize the logistic loss and compare with scikit-learn’s LogisticRegression (we’ve set C to a large value to disable regularization; more on this in Chapter 3!). . The log_loss() function from the previous exercise is already defined in your environment, and the sklearn breast cancer prediction dataset (first 10 features, standardized) is loaded into the variables X and y . . X.shape (569, 10) X[:2] array([[ 1.09706398e+00, -2.07333501e+00, 1.26993369e+00, 9.84374905e-01, 1.56846633e+00, 3.28351467e+00, 2.65287398e+00, 2.53247522e+00, 2.21751501e+00, 2.25574689e+00], [ 1.82982061e+00, -3.53632408e-01, 1.68595471e+00, 1.90870825e+00, -8.26962447e-01, -4.87071673e-01, -2.38458552e-02, 5.48144156e-01, 1.39236330e-03, -8.68652457e-01]]) y[:2] array([-1, -1]) . # The logistic loss, summed over training examples def my_loss(w): s = 0 for i in range(y.shape[0]): raw_model_output = w@X[i] s = s + log_loss(raw_model_output * y[i]) return s # Returns the w that makes my_loss(w) smallest w_fit = minimize(my_loss, X[0]).x print(w_fit) # Compare with scikit-learn&#39;s LogisticRegression lr = LogisticRegression(fit_intercept=False, C=1000000).fit(X,y) print(lr.coef_) . [ 1.03592182 -1.65378492 4.08331342 -9.40923002 -1.06786489 0.07892114 -0.85110344 -2.44103305 -0.45285671 0.43353448] [[ 1.03731085 -1.65339037 4.08143924 -9.40788356 -1.06757746 0.07895582 -0.85072003 -2.44079089 -0.45271 0.43334997]] . minimize(my_loss, X[0]) fun: 73.43533837769074 hess_inv: array([[ 0.36738362, -0.00184266, -0.09662977, -0.38758529, -0.0212197 , -0.05640658, -0.03033375, 0.21477573, 0.01029659, -0.03659313], ... [-0.03659313, -0.00862774, 0.09674119, 0.03706539, 0.02197145, -0.16126887, 0.06496472, -0.09572242, 0.01406182, 0.0907421 ]]) jac: array([ 0.00000000e+00, 4.76837158e-06, 2.86102295e-06, 3.81469727e-06, -4.76837158e-06, -2.86102295e-06, -6.67572021e-06, -9.53674316e-07, 9.53674316e-07, -7.62939453e-06]) message: &#39;Optimization terminated successfully.&#39; nfev: 660 nit: 40 njev: 55 status: 0 success: True x: array([ 1.03592182, -1.65378492, 4.08331342, -9.40923002, -1.06786489, 0.07892114, -0.85110344, -2.44103305, -0.45285671, 0.43353448]) . As you can see, logistic regression is just minimizing the loss function we’ve been looking at. . . 3. Logistic regression . . 3.1 Logistic regression and regularization . #### Regularized logistic regression . In Chapter 1, you used logistic regression on the handwritten digits data set. Here, we’ll explore the effect of L2 regularization. . The handwritten digits dataset is already loaded, split, and stored in the variables X_train , y_train , X_valid , and y_valid . . X_train[:2] array([[ 0., 0., 7., 15., 15., 5., 0., 0., 0., 6., 16., 12., 16., 12., 0., 0., 0., 1., 7., 0., 16., 10., 0., 0., 0., 0., 0., 10., 15., 0., 0., 0., 0., 0., 1., 16., 7., 0., 0., 0., 0., 0., 10., 13., 1., 5., 1., 0., 0., 0., 12., 12., 13., 15., 3., 0., 0., 0., 10., 16., 13., 3., 0., 0.], [ 0., 0., 0., 10., 11., 0., 0., 0., 0., 0., 3., 16., 10., 0., 0., 0., 0., 0., 8., 16., 0., 0., 0., 0., 0., 0., 12., 14., 0., 0., 0., 0., 0., 0., 14., 16., 15., 6., 0., 0., 0., 0., 12., 16., 12., 15., 6., 0., 0., 0., 7., 16., 10., 13., 14., 0., 0., 0., 0., 9., 13., 11., 6., 0.]]) y_train[:2] array([2, 6]) . # Train and validaton errors initialized as empty list train_errs = list() valid_errs = list() # Loop over values of C_value for C_value in [0.001, 0.01, 0.1, 1, 10, 100, 1000]: # Create LogisticRegression object and fit lr = LogisticRegression(C=C_value) lr.fit(X_train, y_train) # Evaluate error rates and append to lists train_errs.append( 1.0 - lr.score(X_train, y_train) ) valid_errs.append( 1.0 - lr.score(X_valid, y_valid) ) # Plot results plt.semilogx(C_values, train_errs, C_values, valid_errs) plt.legend((&quot;train&quot;, &quot;validation&quot;)) plt.show() . . As you can see, too much regularization (small C ) doesn’t work well – due to underfitting – and too little regularization (large C ) doesn’t work well either – due to overfitting. . #### Logistic regression and feature selection . In this exercise we’ll perform feature selection on the movie review sentiment data set using L1 regularization. The features and targets are already loaded for you in X_train and y_train . . We’ll search for the best value of C using scikit-learn’s GridSearchCV() . . # Specify L1 regularization lr = LogisticRegression(penalty=&#39;l1&#39;) # Instantiate the GridSearchCV object and run the search searcher = GridSearchCV(lr, {&#39;C&#39;:[0.001, 0.01, 0.1, 1, 10]}) searcher.fit(X_train, y_train) # Report the best parameters print(&quot;Best CV params&quot;, searcher.best_params_) # Find the number of nonzero coefficients (selected features) best_lr = searcher.best_estimator_ coefs = best_lr.coef_ print(&quot;Total number of features:&quot;, coefs.size) print(&quot;Number of selected features:&quot;, np.count_nonzero(coefs)) . Best CV params {&#39;C&#39;: 1} Total number of features: 2500 Number of selected features: 1220 . #### Identifying the most positive and negative words . In this exercise we’ll try to interpret the coefficients of a logistic regression fit on the movie review sentiment dataset. The model object is already instantiated and fit for you in the variable lr . . In addition, the words corresponding to the different features are loaded into the variable vocab . . lr LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class=&#39;ovr&#39;, n_jobs=1, penalty=&#39;l2&#39;, random_state=None, solver=&#39;liblinear&#39;, tol=0.0001, verbose=0, warm_start=False) vocab.shape (2500,) vocab[:3] array([&#39;the&#39;, &#39;and&#39;, &#39;a&#39;], dtype=&#39;&lt;U14&#39;) vocab[-3:] array([&#39;birth&#39;, &#39;sorts&#39;, &#39;gritty&#39;], dtype=&#39;&lt;U14&#39;) . # Get the indices of the sorted cofficients inds_ascending = np.argsort(lr.coef_.flatten()) inds_ascending # array([1278, 427, 240, ..., 1458, 870, 493]) inds_descending = inds_ascending[::-1] inds_descending # array([ 493, 870, 1458, ..., 240, 427, 1278]) # Print the most positive words print(&quot;Most positive words: &quot;, end=&quot;&quot;) for i in range(5): print(vocab[inds_descending][i], end=&quot;, &quot;) print(&quot; n&quot;) # Most positive words: favorite, superb, noir, knowing, loved, # Print most negative words print(&quot;Most negative words: &quot;, end=&quot;&quot;) for i in range(5): print(vocab[inds_ascending][i], end=&quot;, &quot;) print(&quot; n&quot;) # Most negative words: disappointing, waste, worst, boring, lame, . . 3.2 Logistic regression and probabilities . . #### Regularization and probabilities . In this exercise, you will observe the effects of changing the regularization strength on the predicted probabilities. . A 2D binary classification dataset is already loaded into the environment as X and y . . X.shape (20, 2) X[:3] array([[ 1.78862847, 0.43650985], [ 0.09649747, -1.8634927 ], [-0.2773882 , -0.35475898]]) y[:3] array([-1, -1, -1]) . # Set the regularization strength model = LogisticRegression(C=1) # Fit and plot model.fit(X,y) plot_classifier(X,y,model,proba=True) # Predict probabilities on training points prob = model.predict_proba(X) print(&quot;Maximum predicted probability&quot;, np.max(prob)) . . C = 1 . Maximum predicted probability 0.9761229966765974 . C=0.1 . Maximum predicted probability 0.8990965659596716 . Smaller values of C lead to less confident predictions. . That’s because smaller C means more regularization, which in turn means smaller coefficients, which means raw model outputs closer to zero. . #### Visualizing easy and difficult examples . In this exercise, you’ll visualize the examples that the logistic regression model is most and least confident about by looking at the largest and smallest predicted probabilities. . The handwritten digits dataset is already loaded into the variables X and y . The show_digit function takes in an integer index and plots the corresponding image, with some extra information displayed above the image. . X.shape (1797, 64) X[0] array([ 0., 0., 5., 13., 9., 1., 0., 0., 0., 0., 13., 15., 10., 15., 5., 0., 0., 3., 15., 2., 0., 11., 8., 0., 0., 4., 12., 0., 0., 8., 8., 0., 0., 5., 8., 0., 0., 9., 8., 0., 0., 4., 11., 0., 1., 12., 7., 0., 0., 2., 14., 5., 10., 12., 0., 0., 0., 0., 6., 13., 10., 0., 0., 0.]) y[:3] array([0, 1, 2]) show_digit? Signature: show_digit(i, lr=None) Docstring: &lt;no docstring&gt; File: /tmp/tmp12h5q4tk/&lt;ipython-input-1-5d2049073a74&gt; Type: function . lr = LogisticRegression() lr.fit(X,y) # Get predicted probabilities proba = lr.predict_proba(X) # Sort the example indices by their maximum probability proba_inds = np.argsort(np.max(proba,axis=1)) # Show the most confident (least ambiguous) digit show_digit(proba_inds[-1], lr) # Show the least confident (most ambiguous) digit show_digit(proba_inds[0], lr) . . As you can see, the least confident example looks like a weird 4, and the most confident example looks like a very typical 0. . . 3.3 Multi-class logistic regression . . #### Counting the coefficients . If you fit a logistic regression model on a classification problem with 3 classes and 100 features, how many coefficients would you have, including intercepts? . 303 . 100 coefficients + 1 intercept for each binary classifier. (A, B), (B, C), (C, A) . 101 * 3 = 303 . #### Fitting multi-class logistic regression . In this exercise, you’ll fit the two types of multi-class logistic regression, one-vs-rest and softmax/multinomial, on the handwritten digits data set and compare the results. . # Fit one-vs-rest logistic regression classifier lr_ovr = LogisticRegression() lr_ovr.fit(X_train, y_train) print(&quot;OVR training accuracy:&quot;, lr_ovr.score(X_train, y_train)) print(&quot;OVR test accuracy :&quot;, lr_ovr.score(X_test, y_test)) # Fit softmax classifier lr_mn = LogisticRegression(multi_class=&#39;multinomial&#39;, solver=&#39;lbfgs&#39;) lr_mn.fit(X_train, y_train) print(&quot;Softmax training accuracy:&quot;, lr_mn.score(X_train, y_train)) print(&quot;Softmax test accuracy :&quot;, lr_mn.score(X_test, y_test)) . OVR training accuracy: 0.9948032665181886 OVR test accuracy : 0.9644444444444444 Softmax training accuracy: 1.0 Softmax test accuracy : 0.9688888888888889 . As you can see, the accuracies of the two methods are fairly similar on this data set. . #### Visualizing multi-class logistic regression . In this exercise we’ll continue with the two types of multi-class logistic regression, but on a toy 2D data set specifically designed to break the one-vs-rest scheme. . The data set is loaded into X_train and y_train . The two logistic regression objects, lr_mn and lr_ovr , are already instantiated (with C=100 ), fit, and plotted. . Notice that lr_ovr never predicts the dark blue class… yikes! Let’s explore why this happens by plotting one of the binary classifiers that it’s using behind the scenes. . . # Print training accuracies print(&quot;Softmax training accuracy:&quot;, lr_mn.score(X_train, y_train)) print(&quot;One-vs-rest training accuracy:&quot;, lr_ovr.score(X_train, y_train)) # Softmax training accuracy: 0.996 # One-vs-rest training accuracy: 0.916 # Create the binary classifier (class 1 vs. rest) lr_class_1 = LogisticRegression(C=100) lr_class_1.fit(X_train, y_train==1) # Plot the binary classifier (class 1 vs. rest) plot_classifier(X_train, y_train==1, lr_class_1) . . As you can see, the binary classifier incorrectly labels almost all points in class 1 (shown as red triangles in the final plot)! Thus, this classifier is not a very effective component of the one-vs-rest classifier. . In general, though, one-vs-rest often works well. . #### One-vs-rest SVM . As motivation for the next and final chapter on support vector machines, we’ll repeat the previous exercise with a non-linear SVM. . Instead of using LinearSVC , we’ll now use scikit-learn’s SVC object, which is a non-linear “kernel” SVM (much more on what this means in Chapter 4!). Again, your task is to create a plot of the binary classifier for class 1 vs. rest. . # We&#39;ll use SVC instead of LinearSVC from now on from sklearn.svm import SVC # Create/plot the binary classifier (class 1 vs. rest) svm_class_1 = SVC() svm_class_1.fit(X_train, y_train==1) plot_classifier(X_train, y_train==1, svm_class_1) . . The non-linear SVM works fine with one-vs-rest on this dataset because it learns to “surround” class 1. . . 4. Support Vector Machines . . 4.1 Support vectors . . #### Effect of removing examples . Support vectors are defined as training examples that influence the decision boundary. In this exercise, you’ll observe this behavior by removing non support vectors from the training set. . The wine quality dataset is already loaded into X and y (first two features only). (Note: we specify lims in plot_classifier() so that the two plots are forced to use the same axis limits and can be compared directly.) . X.shape (178, 2) X[:3] array([[14.23, 1.71], [13.2 , 1.78], [13.16, 2.36]]) set(y) {0, 1, 2} . # Train a linear SVM svm = SVC(kernel=&quot;linear&quot;) svm.fit(X, y) plot_classifier(X, y, svm, lims=(11,15,0,6)) # Make a new data set keeping only the support vectors print(&quot;Number of original examples&quot;, len(X)) print(&quot;Number of support vectors&quot;, len(svm.support_)) X_small = X[svm.support_] y_small = y[svm.support_] # Train a new SVM using only the support vectors svm_small = SVC(kernel=&quot;linear&quot;) svm_small.fit(X_small, y_small) plot_classifier(X_small, y_small, svm_small, lims=(11,15,0,6)) . . By the definition of support vectors, the decision boundaries of the two trained models are the same. . . 4.2 Kernel SVMs . . #### GridSearchCV warm-up . Increasing the RBF kernel hyperparameter gamma increases training accuracy. . In this exercise we’ll search for the gamma that maximizes cross-validation accuracy using scikit-learn’s GridSearchCV . . A binary version of the handwritten digits dataset, in which you’re just trying to predict whether or not an image is a “2”, is already loaded into the variables X and y . . set(y) {False, True} X.shape (898, 64) X[0] array([ 0., 1., 10., 15., 11., 1., 0., 0., 0., 3., 8., 8., 11., 12., 0., 0., 0., 0., 0., 5., 14., 15., 1., 0., 0., 0., 0., 11., 15., 2., 0., 0., 0., 0., 0., 4., 15., 2., 0., 0., 0., 0., 0., 0., 12., 10., 0., 0., 0., 0., 3., 4., 10., 16., 1., 0., 0., 0., 13., 16., 15., 10., 0., 0.]) . # Instantiate an RBF SVM svm = SVC() # Instantiate the GridSearchCV object and run the search parameters = {&#39;gamma&#39;:[0.00001, 0.0001, 0.001, 0.01, 0.1]} searcher = GridSearchCV(svm, param_grid=parameters) searcher.fit(X, y) # Report the best parameters print(&quot;Best CV params&quot;, searcher.best_params_) # Best CV params {&#39;gamma&#39;: 0.001} . Larger values of gamma are better for training accuracy, but cross-validation helped us find something different (and better!). . #### Jointly tuning gamma and C with GridSearchCV . In the previous exercise the best value of gamma was 0.001 using the default value of C , which is 1. In this exercise you’ll search for the best combination of C and gamma using GridSearchCV . . As in the previous exercise, the 2-vs-not-2 digits dataset is already loaded, but this time it’s split into the variables X_train , y_train , X_test , and y_test . . Even though cross-validation already splits the training set into parts, it’s often a good idea to hold out a separate test set to make sure the cross-validation results are sensible. . # Instantiate an RBF SVM svm = SVC() # Instantiate the GridSearchCV object and run the search parameters = {&#39;C&#39;:[0.1, 1, 10], &#39;gamma&#39;:[0.00001, 0.0001, 0.001, 0.01, 0.1]} searcher = GridSearchCV(svm, param_grid=parameters) searcher.fit(X_train, y_train) # Report the best parameters and the corresponding score print(&quot;Best CV params&quot;, searcher.best_params_) print(&quot;Best CV accuracy&quot;, searcher.best_score_) # Report the test accuracy using these best parameters print(&quot;Test accuracy of best grid search hypers:&quot;, searcher.score(X_test, y_test)) . Best CV params {&#39;C&#39;: 10, &#39;gamma&#39;: 0.0001} Best CV accuracy 0.9988864142538976 Test accuracy of best grid search hypers: 0.9988876529477196 . Note that the best value of gamma , 0.0001, is different from the value of 0.001 that we got in the previous exercise, when we fixed C=1 . Hyperparameters can affect each other! . . 4.3 Comparing logistic regression and SVM (and beyond) . . #### An advantage of SVMs . Having a limited number of support vectors makes kernel SVMs computationally efficient. . #### An advantage of logistic regression . It naturally outputs meaningful probabilities. . #### Using SGDClassifier . In this final coding exercise, you’ll do a hyperparameter search over the regularization type, regularization strength, and the loss (logistic regression vs. linear SVM) using SGDClassifier() . . X_train.shape (1257, 64) X_train[0] array([ 0., 0., 2., 10., 16., 11., 1., 0., 0., 0., 13., 13., 10., 16., 8., 0., 0., 4., 14., 1., 8., 14., 1., 0., 0., 4., 15., 12., 15., 8., 0., 0., 0., 0., 6., 7., 14., 5., 0., 0., 0., 1., 2., 0., 12., 5., 0., 0., 0., 8., 15., 6., 13., 4., 0., 0., 0., 0., 5., 11., 16., 3., 0., 0.]) set(y_train) {0, 1, 2, 3, 4, 5, 6, 7, 8, 9} . # We set random_state=0 for reproducibility linear_classifier = SGDClassifier(random_state=0) # Instantiate the GridSearchCV object and run the search parameters = {&#39;alpha&#39;:[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], &#39;loss&#39;:[&#39;hinge&#39;, &#39;log&#39;], &#39;penalty&#39;:[&#39;l1&#39;, &#39;l2&#39;]} searcher = GridSearchCV(linear_classifier, parameters, cv=10) searcher.fit(X_train, y_train) # Report the best parameters and the corresponding score print(&quot;Best CV params&quot;, searcher.best_params_) print(&quot;Best CV accuracy&quot;, searcher.best_score_) print(&quot;Test accuracy of best grid search hypers:&quot;, searcher.score(X_test, y_test)) . Best CV params {&#39;alpha&#39;: 0.0001, &#39;loss&#39;: &#39;hinge&#39;, &#39;penalty&#39;: &#39;l1&#39;} Best CV accuracy 0.94351630867144 Test accuracy of best grid search hypers: 0.9592592592592593 . One advantage of SGDClassifier is that it’s very fast – this would have taken a lot longer with LogisticRegression or LinearSVC . . The End. . Thank you for reading. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/linear-classifiers-in-python.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/linear-classifiers-in-python.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "Joining Data in SQL",
            "content": "Joining Data in SQL . This is the memo of the 18th course of ‘Data Scientist with Python’ track. . You can find the original course HERE . . Further Reading: . More dangerous subtleties of JOINs in SQL — Be careful when JOIN tables with duplications or NULLs . . Introduction to joins . ### Introduction to INNER JOIN . #### Inner join . . SELECT table_name FROM information_schema.tables -- Specify the correct table_schema value WHERE table_schema = &#39;public&#39;; table_name cities countries languages economies currencies populations . SELECT * FROM left_table INNER JOIN right_table ON left_table.id = right_table.id; . -- 1. Select name fields (with alias) and region SELECT cities.name AS city, countries.name AS country, region FROM cities INNER JOIN countries ON cities.country_code = countries.code; city country region Abidjan Cote d&#39;Ivoire Western Africa Abu Dhabi United Arab Emirates Middle East Abuja Nigeria Western Africa . #### Inner join (2) . SELECT c1.name AS city, c2.name AS country FROM cities AS c1 INNER JOIN countries AS c2 ON c1.country_code = c2.code; . -- 3. Select fields with aliases SELECT c.code AS country_code, name, year, inflation_rate FROM countries AS c -- 1. Join to economies (alias e) INNER JOIN economies AS e -- 2. Match on code ON c.code = e.code; . #### Inner join (3) . SELECT * FROM left_table INNER JOIN right_table ON left_table.id = right_table.id INNER JOIN another_table ON left_table.id = another_table.id; . -- 6. Select fields SELECT c.code, name, region, e.year, fertility_rate, unemployment_rate -- 1. From countries (alias as c) FROM countries AS c -- 2. Join to populations (as p) INNER JOIN populations AS p -- 3. Match on country code ON c.code = p.country_code -- 4. Join to economies (as e) INNER JOIN economies AS e -- 5. Match on country code ON c.code = e.code; . -- countries INNER JOIN populations table code name fertility_rate ABW Aruba 1.704 ABW Aruba 1.647 AFG Afghanistan 5.746 AFG Afghanistan 4.653 -- economies table econ_id code year 1 AFG 2010 2 AFG 2015 code name region year fertility_rate unemployment_rate AFG Afghanistan Southern and Central Asia 2010 4.653 null AFG Afghanistan Southern and Central Asia 2010 5.746 null AFG Afghanistan Southern and Central Asia 2015 4.653 null AFG Afghanistan Southern and Central Asia 2015 5.746 null AGO Angola Central Africa 2010 5.996 null . -- 6. Select fields SELECT c.code, name, region, e.year, fertility_rate, unemployment_rate -- 1. From countries (alias as c) FROM countries AS c -- 2. Join to populations (as p) INNER JOIN populations AS p -- 3. Match on country code ON c.code = p.country_code -- 4. Join to economies (as e) INNER JOIN economies AS e -- 5. Match on country code and year ON c.code = e.code AND p.year = e.year; code name region year fertility_rate unemployment_rate AFG Afghanistan Southern and Central Asia 2010 5.746 null AFG Afghanistan Southern and Central Asia 2015 4.653 null . ### INNER JOIN via USING . #### Inner join with using . SELECT * FROM countries INNER JOIN economies ON countries.code = economies.code -- is equal to SELECT * FROM countries INNER JOIN economies USING(code) . -- 4. Select fields SELECT c.name AS country, continent, l.name AS language, official -- 1. From countries (alias as c) FROM countries as c -- 2. Join to languages (as l) INNER JOIN languages as l -- 3. Match using code USING (code) country continent language official Afghanistan Asia Dari true Afghanistan Asia Pashto true . ### Self-ish joins, just in CASE . #### Self-join . . pop_id country_code year fertility_rate life_expectancy size 20 ABW 2010 1.704 74.9535 101597 19 ABW 2015 1.647 75.5736 103889 . -- 4. Select fields with aliases SELECT p1.country_code, p1.size AS size2010, p2.size AS size2015 -- 1. From populations (alias as p1) FROM populations AS p1 -- 2. Join to itself (alias as p2) INNER JOIN populations AS p2 -- 3. Match on country code ON p1.country_code = p2.country_code country_code size2010 size2015 ABW 101597 103889 ABW 101597 101597 ABW 103889 103889 ABW 103889 101597 . -- 5. Select fields with aliases SELECT p1.country_code, p1.size AS size2010, p2.size AS size2015 -- 1. From populations (alias as p1) FROM populations as p1 -- 2. Join to itself (alias as p2) INNER JOIN populations as p2 -- 3. Match on country code ON p1.country_code = p2.country_code -- 4. and year (with calculation) AND p1.year = p2.year - 5 country_code size2010 size2015 ABW 101597 103889 AFG 27962200 32526600 AGO 21220000 25022000 ALB 2913020 2889170 . -- With two numeric fields A and B, the percentage growth from A to B can be calculated as (B−A)/A∗100.0. SELECT p1.country_code, p1.size AS size2010, p2.size AS size2015, -- 1. calculate growth_perc ((p2.size - p1.size)/p1.size * 100.0) AS growth_perc -- 2. From populations (alias as p1) FROM populations AS p1 -- 3. Join to itself (alias as p2) INNER JOIN populations AS p2 -- 4. Match on country code ON p1.country_code = p2.country_code -- 5. and year (with calculation) AND p1.year = p2.year - 5; country_code size2010 size2015 growth_perc ABW 101597 103889 2.25597210228443 AFG 27962200 32526600 16.32329672575 AGO 21220000 25022000 17.9171919822693 ALB 2913020 2889170 -0.818874966353178 . #### Case when and then . SELECT name, continent, code, surface_area, -- 1. First case CASE WHEN surface_area &gt; 2000000 THEN &#39;large&#39; -- 2. Second case WHEN surface_area &gt; 350000 THEN &#39;medium&#39; -- 3. Else clause + end ELSE &#39;small&#39; END -- 4. Alias name AS geosize_group -- 5. From table FROM countries; name continent code surface_area geosize_group Afghanistan Asia AFG 652090 medium Netherlands Europe NLD 41526 small Albania Europe ALB 28748 small . #### Inner challenge . SELECT name, continent, code, surface_area, CASE WHEN surface_area &gt; 2000000 THEN &#39;large&#39; WHEN surface_area &gt; 350000 THEN &#39;medium&#39; ELSE &#39;small&#39; END AS geosize_group INTO countries_plus FROM countries; name continent code surface_area geosize_group Afghanistan Asia AFG 652090 medium Netherlands Europe NLD 41526 small Albania Europe ALB 28748 small Algeria Africa DZA 2381740 large . SELECT country_code, size, -- 1. First case CASE WHEN size &gt; 50000000 THEN &#39;large&#39; -- 2. Second case WHEN size &gt; 1000000 THEN &#39;medium&#39; -- 3. Else clause + end ELSE &#39;small&#39; END -- 4. Alias name AS popsize_group -- 5. From table FROM populations -- 6. Focus on 2015 WHERE year = 2015; country_code size popsize_group ABW 103889 small AFG 32526600 medium AGO 25022000 medium ALB 2889170 medium . SELECT country_code, size, CASE WHEN size &gt; 50000000 THEN &#39;large&#39; WHEN size &gt; 1000000 THEN &#39;medium&#39; ELSE &#39;small&#39; END AS popsize_group -- 1. Into table INTO pop_plus FROM populations WHERE year = 2015; -- 2. Select all columns of pop_plus SELECT * FROM pop_plus; country_code size popsize_group ABW 103889 small AFG 32526600 medium AGO 25022000 medium ALB 2889170 medium . SELECT country_code, size, CASE WHEN size &gt; 50000000 THEN &#39;large&#39; WHEN size &gt; 1000000 THEN &#39;medium&#39; ELSE &#39;small&#39; END AS popsize_group INTO pop_plus FROM populations WHERE year = 2015; -- 5. Select fields SELECT name, continent, geosize_group, popsize_group -- 1. From countries_plus (alias as c) FROM countries_plus AS c -- 2. Join to pop_plus (alias as p) INNER JOIN pop_plus AS p -- 3. Match on country code ON c.code = p.country_code -- 4. Order the table ORDER BY geosize_group; name continent geosize_group popsize_group India Asia large large United States North America large large Saudi Arabia Asia large medium China Asia large large . . Outer joins and cross joins . ### LEFT and RIGHT JOINs . . #### Left Join . -- Select the city name (with alias), the country code, -- the country name (with alias), the region, -- and the city proper population SELECT c1.name AS city, code, c2.name AS country, region, city_proper_pop -- From left table (with alias) FROM cities AS c1 -- Join to right table (with alias) INNER JOIN countries AS c2 -- Match on country code ON c1.country_code = c2.code -- Order by descending country code ORDER BY code DESC; city code country region city_proper_pop Harare ZWE Zimbabwe Eastern Africa 1606000 Lusaka ZMB Zambia Eastern Africa 1742980 Cape Town ZAF South Africa Southern Africa 3740030 -- 230 rows . SELECT c1.name AS city, code, c2.name AS country, region, city_proper_pop FROM cities AS c1 -- 1. Join right table (with alias) LEFT JOIN countries AS c2 -- 2. Match on country code ON c1.country_code = c2.code -- 3. Order by descending country code ORDER BY code DESC; city code country region city_proper_pop Taichung null null null 2752410 Tainan null null null 1885250 Kaohsiung null null null 2778920 Bucharest null null null 1883420 -- 236 rows . #### Left join (2) . /* 5. Select country name AS country, the country&#39;s local name, the language name AS language, and the percent of the language spoken in the country */ SELECT c.name AS country, local_name, l.name AS language, percent -- 1. From left table (alias as c) FROM countries AS c -- 2. Join to right table (alias as l) INNER JOIN languages AS l -- 3. Match on fields ON c.code = l.code -- 4. Order by descending country ORDER BY country DESC; country local_name language percent Zimbabwe Zimbabwe Shona null Zimbabwe Zimbabwe Tonga null Zimbabwe Zimbabwe Tswana null -- 914 rows . /* 5. Select country name AS country, the country&#39;s local name, the language name AS language, and the percent of the language spoken in the country */ SELECT c.name AS country, local_name, l.name AS language, percent -- 1. From left table (alias as c) FROM countries AS c -- 2. Join to right table (alias as l) LEFT JOIN languages AS l -- 3. Match on fields ON c.code = l.code -- 4. Order by descending country ORDER BY country DESC; country local_name language percent Zimbabwe Zimbabwe Chibarwe null Zimbabwe Zimbabwe Shona null Zimbabwe Zimbabwe Ndebele null Zimbabwe Zimbabwe English null -- 921 rows . #### Left join (3) . -- 5. Select name, region, and gdp_percapita SELECT name, region, gdp_percapita -- 1. From countries (alias as c) FROM countries AS c -- 2. Left join with economies (alias as e) LEFT JOIN economies AS e -- 3. Match on code fields ON c.code = e.code -- 4. Focus on 2010 WHERE year = 2010; name region gdp_percapita Afghanistan Southern and Central Asia 539.667 Angola Central Africa 3599.27 Albania Southern Europe 4098.13 United Arab Emirates Middle East 34628.6 . -- Select fields SELECT region, AVG(gdp_percapita) AS avg_gdp -- From countries (alias as c) FROM countries AS c -- Left join with economies (alias as e) LEFT JOIN economies AS e -- Match on code fields ON c.code = e.code -- Focus on 2010 WHERE year = 2010 -- Group by region GROUP BY region; region avg_gdp Southern Africa 5051.59797363281 Australia and New Zealand 44792.384765625 Southeast Asia 10547.1541320801 . -- Select fields SELECT region, AVG(gdp_percapita) AS avg_gdp -- From countries (alias as c) FROM countries AS c -- Left join with economies (alias as e) LEFT JOIN economies AS e -- Match on code fields ON c.code = e.code -- Focus on 2010 WHERE year = 2010 -- Group by region GROUP BY region -- Order by descending avg_gdp ORDER BY avg_gdp DESC; region avg_gdp Western Europe 58130.9614955357 Nordic Countries 57073.99765625 North America 47911.509765625 Australia and New Zealand 44792.384765625 . #### Right join . . -- convert this code to use RIGHT JOINs instead of LEFT JOINs /* SELECT cities.name AS city, urbanarea_pop, countries.name AS country, indep_year, languages.name AS language, percent FROM cities LEFT JOIN countries ON cities.country_code = countries.code LEFT JOIN languages ON countries.code = languages.code ORDER BY city, language; */ SELECT cities.name AS city, urbanarea_pop, countries.name AS country, indep_year, languages.name AS language, percent FROM languages RIGHT JOIN countries ON languages.code = countries.code RIGHT JOIN cities ON countries.code = cities.country_code ORDER BY city, language; city urbanarea_pop country indep_year language percent Abidjan 4765000 Cote d&#39;Ivoire 1960 French null Abidjan 4765000 Cote d&#39;Ivoire 1960 Other null Abu Dhabi 1145000 United Arab Emirates 1971 Arabic null . ### FULL JOINs . #### Full join . . SELECT name AS country, code, region, basic_unit -- 3. From countries FROM countries -- 4. Join to currencies FULL JOIN currencies -- 5. Match on code USING (code) -- 1. Where region is North America or null WHERE region = &#39;North America&#39; OR region IS NULL -- 2. Order by region ORDER BY region; country code region basic_unit Greenland GRL North America null null TMP null United States dollar null FLK null Falkland Islands pound null AIA null East Caribbean dollar null NIU null New Zealand dollar null ROM null Romanian leu null SHN null Saint Helena pound null SGS null British pound null TWN null New Taiwan dollar null WLF null CFP franc null MSR null East Caribbean dollar null IOT null United States dollar null CCK null Australian dollar null COK null New Zealand dollar . SELECT name AS country, code, region, basic_unit -- 1. From countries FROM countries -- 2. Join to currencies LEFT JOIN currencies -- 3. Match on code USING (code) -- 4. Where region is North America or null WHERE region = &#39;North America&#39; OR region IS NULL -- 5. Order by region ORDER BY region; country code region basic_unit Bermuda BMU North America Bermudian dollar Canada CAN North America Canadian dollar United States USA North America United States dollar Greenland GRL North America null . SELECT name AS country, code, region, basic_unit FROM countries -- 1. Join to currencies INNER JOIN currencies USING (code) -- 2. Where region is North America or null WHERE region = &#39;North America&#39; OR region IS NULL -- 3. Order by region ORDER BY region; country code region basic_unit Bermuda BMU North America Bermudian dollar Canada CAN North America Canadian dollar United States USA North America United States dollar . #### Full join (2) . -- FULL JOIN SELECT countries.name, code, languages.name AS language -- 3. From languages FROM languages -- 4. Join to countries FULL JOIN countries -- 5. Match on code USING (code) -- 1. Where countries.name starts with V or is null WHERE countries.name LIKE &#39;V%&#39; OR countries.name IS NULL -- 2. Order by ascending countries.name ORDER BY countries.name; name code language Vanuatu VUT Tribal Languages Vanuatu VUT English Vanuatu VUT French Vanuatu VUT Other -- 53 rows . -- LEFT JOIN SELECT countries.name, code, languages.name AS language FROM languages -- 1. Join to countries LEFT JOIN countries -- 2. Match using code USING (code) -- 3. Where countries.name starts with V or is null WHERE countries.name LIKE &#39;V%&#39; OR countries.name IS NULL ORDER BY countries.name; name code language Vanuatu VUT English Vanuatu VUT Other Vanuatu VUT French -- 51 rows . -- LEFT JOIN SELECT countries.name, code, languages.name AS language FROM languages -- 1. Join to countries INNER JOIN countries USING (code) -- 2. Where countries.name starts with V or is null WHERE countries.name LIKE &#39;V%&#39; OR countries.name IS NULL ORDER BY countries.name; name code language Vanuatu VUT Tribal Languages Vanuatu VUT Bislama Vanuatu VUT English -- 10 rows . #### Full join (3) . -- 7. Select fields (with aliases) SELECT c1.name AS country, region, l.name AS language, basic_unit, frac_unit -- 1. From countries (alias as c1) FROM countries AS c1 -- 2. Join with languages (alias as l) FULL JOIN languages AS l -- 3. Match on code USING (code) -- 4. Join with currencies (alias as c2) FULL JOIN currencies AS c2 -- 5. Match on code USING (code) -- 6. Where region like Melanesia and Micronesia WHERE region LIKE &#39;M%esia&#39;; country region language basic_unit frac_unit Kiribati Micronesia English Australian dollar Cent Kiribati Micronesia Kiribati Australian dollar Cent Marshall Islands Micronesia Other United States dollar Cent Marshall Islands Micronesia Marshallese United States dollar Cent . ### CROSSing the rubicon . . #### A table of two cities . CROSS JOIN . -- 4. Select fields SELECT c.name AS city, l.name AS language -- 1. From cities (alias as c) FROM cities AS c -- 2. Join to languages (alias as l) CROSS JOIN languages AS l -- 3. Where c.name like Hyderabad WHERE c.name LIKE &#39;Hyder%&#39;; city language Hyderabad (India) Dari Hyderabad Dari Hyderabad (India) Pashto Hyderabad Pashto . -- 5. Select fields SELECT c.name AS city, l.name AS language -- 1. From cities (alias as c) FROM cities AS c -- 2. Join to languages (alias as l) INNER JOIN languages AS l -- 3. Match on country code ON c.country_code = l.code -- 4. Where c.name like Hyderabad WHERE c.name LIKE &#39;Hyder%&#39;; city language Hyderabad (India) Hindi Hyderabad (India) Bengali Hyderabad (India) Telugu Hyderabad (India) Marathi . #### Outer challenge . -- Select fields SELECT c.name AS country, region, life_expectancy AS life_exp -- From countries (alias as c) FROM countries as c -- Join to populations (alias as p) LEFT JOIN populations as p -- Match on country code ON c.code = p.country_code -- Focus on 2010 WHERE year = 2010 -- Order by life_exp ORDER BY life_exp -- Limit to 5 records LIMIT 5; . . Set theory clauses . ### State of the UNION . . #### Union . . -- Select fields from 2010 table SELECT * -- From 2010 table FROM economies2010 -- Set theory clause UNION -- Select fields from 2015 table SELECT * -- From 2015 table FROM economies2015 -- Order by code and year ORDER BY code, year; code year income_group gross_savings AFG 2010 Low income 37.133 AFG 2015 Low income 21.466 AGO 2010 Upper middle income 23.534 AGO 2015 Upper middle income -0.425 . #### Union (2) . -- Select field SELECT country_code -- From cities FROM cities -- Set theory clause UNION -- Select field SELECT code AS country_code -- From currencies FROM currencies -- Order by country_code ORDER BY country_code; country_code ABW AFG AGO AIA . #### Union all . . -- Select fields SELECT code, year -- From economies FROM economies -- Set theory clause UNION ALL -- Select fields SELECT country_code AS code, year -- From populations FROM populations -- Order by code, year ORDER BY code, year; code year ABW 2010 ABW 2015 AFG 2010 AFG 2010 . ### INTERSECTional data science . . #### Intersect . -- Select fields SELECT code, year -- From economies FROM economies -- Set theory clause INTERSECT -- Select fields SELECT country_code AS code, year -- From populations FROM populations -- Order by code and year ORDER BY code, year; code year AFG 2010 AFG 2015 AGO 2010 . #### Intersect (2) . -- Select fields SELECT name -- From countries FROM countries -- Set theory clause INTERSECT -- Select fields SELECT name -- From cities FROM cities; name Singapore Hong Kong . Hong Kong is part of China, but it appears separately here because it has its own ISO country code. Depending upon your analysis, treating Hong Kong separately could be useful or a mistake. Always check your dataset closely before you perform an analysis! . ### EXCEPTional . . #### Except . -- Get the names of cities in cities which are not noted as capital cities in countries as a single field result. -- Select field SELECT name -- From cities FROM cities -- Set theory clause EXCEPT -- Select field SELECT capital -- From countries FROM countries -- Order by result ORDER BY name; name Abidjan Ahmedabad Alexandria . #### Except (2) . -- Determine the names of capital cities that are not listed in the cities table. -- Select field SELECT capital -- From countries FROM countries -- Set theory clause EXCEPT -- Select field SELECT name -- From cities FROM cities -- Order by ascending capital ORDER BY capital; capital Agana Amman Amsterdam ... . ### Semi-joins and Anti-joins . . #### Semi-join . -- You are now going to use the concept of a semi-join to identify languages spoken in the Middle East. -- Select distinct fields SELECT DISTINCT name -- From languages FROM languages -- Where in statement WHERE code IN -- Subquery (SELECT code FROM countries WHERE region = &#39;Middle East&#39;) -- Order by name ORDER BY name; . #### Relating semi-join to a tweaked inner join . SELECT DISTINCT name FROM languages WHERE code IN (SELECT code FROM countries WHERE region = &#39;Middle East&#39;) ORDER BY name; -- is equal to SELECT DISTINCT languages.name AS language FROM languages INNER JOIN countries ON languages.code = countries.code WHERE region = &#39;Middle East&#39; ORDER BY language; . #### Diagnosing problems using anti-join . Your goal is to identify the currencies used in Oceanian countries! . -- Begin by determining the number of countries in countries that are listed in Oceania using SELECT, FROM, and WHERE. -- Select statement SELECT COUNT(*) -- From countries FROM countries -- Where continent is Oceania WHERE continent = &#39;Oceania&#39;; count 19 . -- 5. Select fields (with aliases) SELECT c1.code, name, basic_unit AS currency -- 1. From countries (alias as c1) FROM countries AS c1 -- 2. Join with currencies (alias as c2) INNER JOIN currencies c2 -- 3. Match on code USING (code) -- 4. Where continent is Oceania WHERE continent = &#39;Oceania&#39;; code name currency AUS Australia Australian dollar PYF French Polynesia CFP franc KIR Kiribati Australian dollar . -- 3. Select fields SELECT code, name -- 4. From Countries FROM countries -- 5. Where continent is Oceania WHERE continent = &#39;Oceania&#39; -- 1. And code not in AND code NOT IN -- 2. Subquery (SELECT code FROM currencies); code name ASM American Samoa FJI Fiji Islands GUM Guam FSM Micronesia, Federated States of MNP Northern Mariana Islands . #### Set theory challenge . Identify the country codes that are included in either economies or currencies but not in populations . | Use that result to determine the names of cities in the countries that match the specification in the previous instruction. | . -- Select the city name SELECT name -- Alias the table where city name resides FROM cities AS c1 -- Choose only records matching the result of multiple set theory clauses WHERE country_code IN ( -- Select appropriate field from economies AS e SELECT e.code FROM economies AS e -- Get all additional (unique) values of the field from currencies AS c2 UNION SELECT c2.code FROM currencies AS c2 -- Exclude those appearing in populations AS p EXCEPT SELECT p.country_code FROM populations AS p ); . . Subqueries . ### Subqueries inside WHERE and SELECT clauses . #### Subquery inside where . You’ll now try to figure out which countries had high average life expectancies (at the country level) in 2015. . -- Select average life_expectancy SELECT AVG(life_expectancy) -- From populations FROM populations -- Where year is 2015 WHERE year = 2015 avg 71.6763415481105 . -- Select fields SELECT * -- From populations FROM populations -- Where life_expectancy is greater than WHERE life_expectancy &gt; -- 1.15 * subquery 1.15 * (SELECT AVG(life_expectancy) FROM populations WHERE year = 2015) AND year = 2015; pop_id country_code year fertility_rate life_expectancy size 21 AUS 2015 1.833 82.4512 23789800 376 CHE 2015 1.54 83.1976 8281430 356 ESP 2015 1.32 83.3805 46444000 134 FRA 2015 2.01 82.6707 66538400 . #### Subquery inside where (2) . -- 2. Select fields SELECT name, country_code, urbanarea_pop -- 3. From cities FROM cities -- 4. Where city name in the field of capital cities WHERE name IN -- 1. Subquery (SELECT capital FROM countries) ORDER BY urbanarea_pop DESC; name country_code urbanarea_pop Beijing CHN 21516000 Dhaka BGD 14543100 Tokyo JPN 13513700 . #### Subquery inside select . The code selects the top 9 countries in terms of number of cities appearing in the cities table. . SELECT countries.name AS country, COUNT(*) AS cities_num FROM cities INNER JOIN countries ON countries.code = cities.country_code GROUP BY country ORDER BY cities_num DESC, country LIMIT 9; -- is equal to SELECT countries.name AS country, (SELECT COUNT(*) FROM cities WHERE countries.code = cities.country_code) AS cities_num FROM countries ORDER BY cities_num DESC, country LIMIT 9; country cities_num China 36 India 18 Japan 11 . ### Subquery inside FROM clause . . #### Subquery inside from . You will use this to determine the number of languages spoken for each country, identified by the country’s local name! . -- Select fields (with aliases) SELECT code, COUNT(*) AS lang_num -- From languages From languages -- Group by code GROUP BY code; code lang_num BLZ 9 BGD 2 ITA 4 . -- Select fields SELECT local_name, subquery.lang_num -- From countries FROM countries, -- Subquery (alias as subquery) (SELECT code, COUNT(*) AS lang_num From languages GROUP BY code) AS subquery -- Where codes match WHERE countries.code = subquery.code -- Order by descending number of languages ORDER BY lang_num DESC; local_name lang_num Zambia 19 Zimbabwe 16 YeItyop´iya 16 Bharat/India 14 . #### Advanced subquery . You can also nest multiple subqueries to answer even more specific questions. . In this exercise, for each of the six continents listed in 2015, you’ll identify which country had the maximum inflation rate (and how high it was) using multiple subqueries. The table result of your query in Task 3 should look something like the following, where anything between &lt; &gt; will be filled in with appropriate values: . +++-+ | name | continent | inflation_rate | |++-| | &lt;country1&gt; | North America | &lt;max_inflation1&gt; | | &lt;country2&gt; | Africa | &lt;max_inflation2&gt; | | &lt;country3&gt; | Oceania | &lt;max_inflation3&gt; | | &lt;country4&gt; | Europe | &lt;max_inflation4&gt; | | &lt;country5&gt; | South America | &lt;max_inflation5&gt; | | &lt;country6&gt; | Asia | &lt;max_inflation6&gt; | +++-+ . Again, there are multiple ways to get to this solution using only joins, but the focus here is on showing you an introduction into advanced subqueries. . -- step 1 -- Select fields SELECT name, continent, inflation_rate -- From countries FROM countries -- Join to economies INNER JOIN economies -- Match on code USING (code) -- Where year is 2015 WHERE year = 2015; name continent inflation_rate Afghanistan Asia -1.549 Angola Africa 10.287 Albania Europe 1.896 United Arab Emirates Asia 4.07 . -- step 2 -- Select fields SELECT MAX(inflation_rate) AS max_inf -- Subquery using FROM (alias as subquery) FROM ( SELECT name, continent, inflation_rate FROM countries INNER JOIN economies USING (code) WHERE year = 2015) AS subquery -- Group by continent GROUP BY continent; max_inf 48.684 9.784 39.403 21.858 . -- step 3 -- Select fields SELECT name, continent, inflation_rate -- From countries FROM countries -- Join to economies INNER JOIN economies -- Match on code ON countries.code = economies.code -- Where year is 2015 WHERE year = 2015 AND inflation_rate -- And inflation rate in subquery (alias as subquery) IN ( SELECT MAX(inflation_rate) AS max_inf FROM ( SELECT name, continent, inflation_rate FROM countries INNER JOIN economies ON countries.code = economies.code WHERE year = 2015) AS subquery GROUP BY continent); name continent inflation_rate Haiti North America 7.524 Malawi Africa 21.858 Nauru Oceania 9.784 . #### Subquery challenge . Let’s test your understanding of the subqueries with a challenge problem! Use a subquery to get 2015 economic data for countries that do not have . gov_form of &#39;Constitutional Monarchy&#39; or | &#39;Republic&#39; in their gov_form . | . Here, gov_form stands for the form of the government for each country. Review the different entries for gov_form in the countries table. . -- Select fields SELECT code, inflation_rate, unemployment_rate -- From economies FROM economies -- Where year is 2015 and code is not in WHERE year = 2015 AND code NOT IN -- Subquery (SELECT code FROM countries WHERE (gov_form = &#39;Constitutional Monarchy&#39; OR gov_form LIKE &#39;%Republic%&#39;)) -- Order by inflation rate ORDER BY inflation_rate; code inflation_rate unemployment_rate AFG -1.549 null CHE -1.14 3.178 PRI -0.751 12 ROU -0.596 6.812 . ### Course review . . . . . . . . #### Final challenge . In this exercise, you’ll need to get the country names and other 2015 data in the economies table and the countries table for Central American countries with an official language . . -- Select fields SELECT DISTINCT c.name, e.total_investment, e.imports -- From table (with alias) FROM countries AS c -- Join with table (with alias) LEFT JOIN economies AS e -- Match on code ON (c.code = e.code -- and code in Subquery AND c.code IN ( SELECT l.code FROM languages AS l WHERE official = &#39;true&#39; ) ) -- Where region and year are correct WHERE region = &#39;Central America&#39; AND year = 2015 -- Order by field ORDER BY name; name total_investment imports Belize 22.014 6.743 Costa Rica 20.218 4.629 El Salvador 13.983 8.193 . #### Final challenge (2) . Let’s ease up a bit and calculate the average fertility rate for each region in 2015. . -- Select fields SELECT region, continent, AVG(fertility_rate) AS avg_fert_rate -- From left table FROM countries AS c -- Join to right table INNER JOIN populations AS p -- Match on join condition ON c.code = p.country_code -- Where specific records matching some condition WHERE year = 2015 -- Group appropriately GROUP BY region, continent -- Order appropriately ORDER BY avg_fert_rate; region continent avg_fert_rate Southern Europe Europe 1.42610000371933 Eastern Europe Europe 1.49088890022702 Baltic Countries Europe 1.60333331425985 Eastern Asia Asia 1.62071430683136 . #### Final challenge (3) . You are now tasked with determining the top 10 capital cities in Europe and the Americas in terms of a calculated percentage using city_proper_pop and metroarea_pop in cities . . -- Select fields SELECT name, country_code, city_proper_pop, metroarea_pop, -- Calculate city_perc city_proper_pop / metroarea_pop * 100 AS city_perc -- From appropriate table FROM cities -- Where WHERE name IN -- Subquery (SELECT capital FROM countries WHERE (continent = &#39;Europe&#39; OR continent LIKE &#39;%America&#39;)) AND metroarea_pop IS NOT NULL -- Order appropriately ORDER BY city_perc DESC -- Limit amount LIMIT 10; name country_code city_proper_pop metroarea_pop city_perc Lima PER 8852000 10750000 82.3441863059998 Bogota COL 7878780 9800000 80.3957462310791 Moscow RUS 12197600 16170000 75.4334926605225 . This is the memo of the 18th course of ‘Data Scientist with Python’ track. . You can find the original course HERE . . Further Reading: . More dangerous subtleties of JOINs in SQL — Be careful when JOIN tables with duplications or NULLs . . Introduction to joins . ### Introduction to INNER JOIN . #### Inner join . . SELECT table_name FROM information_schema.tables -- Specify the correct table_schema value WHERE table_schema = &#39;public&#39;; table_name cities countries languages economies currencies populations . SELECT * FROM left_table INNER JOIN right_table ON left_table.id = right_table.id; . -- 1. Select name fields (with alias) and region SELECT cities.name AS city, countries.name AS country, region FROM cities INNER JOIN countries ON cities.country_code = countries.code; city country region Abidjan Cote d&#39;Ivoire Western Africa Abu Dhabi United Arab Emirates Middle East Abuja Nigeria Western Africa . #### Inner join (2) . SELECT c1.name AS city, c2.name AS country FROM cities AS c1 INNER JOIN countries AS c2 ON c1.country_code = c2.code; . -- 3. Select fields with aliases SELECT c.code AS country_code, name, year, inflation_rate FROM countries AS c -- 1. Join to economies (alias e) INNER JOIN economies AS e -- 2. Match on code ON c.code = e.code; . #### Inner join (3) . SELECT * FROM left_table INNER JOIN right_table ON left_table.id = right_table.id INNER JOIN another_table ON left_table.id = another_table.id; . -- 6. Select fields SELECT c.code, name, region, e.year, fertility_rate, unemployment_rate -- 1. From countries (alias as c) FROM countries AS c -- 2. Join to populations (as p) INNER JOIN populations AS p -- 3. Match on country code ON c.code = p.country_code -- 4. Join to economies (as e) INNER JOIN economies AS e -- 5. Match on country code ON c.code = e.code; . -- countries INNER JOIN populations table code name fertility_rate ABW Aruba 1.704 ABW Aruba 1.647 AFG Afghanistan 5.746 AFG Afghanistan 4.653 -- economies table econ_id code year 1 AFG 2010 2 AFG 2015 code name region year fertility_rate unemployment_rate AFG Afghanistan Southern and Central Asia 2010 4.653 null AFG Afghanistan Southern and Central Asia 2010 5.746 null AFG Afghanistan Southern and Central Asia 2015 4.653 null AFG Afghanistan Southern and Central Asia 2015 5.746 null AGO Angola Central Africa 2010 5.996 null . -- 6. Select fields SELECT c.code, name, region, e.year, fertility_rate, unemployment_rate -- 1. From countries (alias as c) FROM countries AS c -- 2. Join to populations (as p) INNER JOIN populations AS p -- 3. Match on country code ON c.code = p.country_code -- 4. Join to economies (as e) INNER JOIN economies AS e -- 5. Match on country code and year ON c.code = e.code AND p.year = e.year; code name region year fertility_rate unemployment_rate AFG Afghanistan Southern and Central Asia 2010 5.746 null AFG Afghanistan Southern and Central Asia 2015 4.653 null . ### INNER JOIN via USING . #### Inner join with using . SELECT * FROM countries INNER JOIN economies ON countries.code = economies.code -- is equal to SELECT * FROM countries INNER JOIN economies USING(code) . -- 4. Select fields SELECT c.name AS country, continent, l.name AS language, official -- 1. From countries (alias as c) FROM countries as c -- 2. Join to languages (as l) INNER JOIN languages as l -- 3. Match using code USING (code) country continent language official Afghanistan Asia Dari true Afghanistan Asia Pashto true . ### Self-ish joins, just in CASE . #### Self-join . . pop_id country_code year fertility_rate life_expectancy size 20 ABW 2010 1.704 74.9535 101597 19 ABW 2015 1.647 75.5736 103889 . -- 4. Select fields with aliases SELECT p1.country_code, p1.size AS size2010, p2.size AS size2015 -- 1. From populations (alias as p1) FROM populations AS p1 -- 2. Join to itself (alias as p2) INNER JOIN populations AS p2 -- 3. Match on country code ON p1.country_code = p2.country_code country_code size2010 size2015 ABW 101597 103889 ABW 101597 101597 ABW 103889 103889 ABW 103889 101597 . -- 5. Select fields with aliases SELECT p1.country_code, p1.size AS size2010, p2.size AS size2015 -- 1. From populations (alias as p1) FROM populations as p1 -- 2. Join to itself (alias as p2) INNER JOIN populations as p2 -- 3. Match on country code ON p1.country_code = p2.country_code -- 4. and year (with calculation) AND p1.year = p2.year - 5 country_code size2010 size2015 ABW 101597 103889 AFG 27962200 32526600 AGO 21220000 25022000 ALB 2913020 2889170 . -- With two numeric fields A and B, the percentage growth from A to B can be calculated as (B−A)/A∗100.0. SELECT p1.country_code, p1.size AS size2010, p2.size AS size2015, -- 1. calculate growth_perc ((p2.size - p1.size)/p1.size * 100.0) AS growth_perc -- 2. From populations (alias as p1) FROM populations AS p1 -- 3. Join to itself (alias as p2) INNER JOIN populations AS p2 -- 4. Match on country code ON p1.country_code = p2.country_code -- 5. and year (with calculation) AND p1.year = p2.year - 5; country_code size2010 size2015 growth_perc ABW 101597 103889 2.25597210228443 AFG 27962200 32526600 16.32329672575 AGO 21220000 25022000 17.9171919822693 ALB 2913020 2889170 -0.818874966353178 . #### Case when and then . SELECT name, continent, code, surface_area, -- 1. First case CASE WHEN surface_area &gt; 2000000 THEN &#39;large&#39; -- 2. Second case WHEN surface_area &gt; 350000 THEN &#39;medium&#39; -- 3. Else clause + end ELSE &#39;small&#39; END -- 4. Alias name AS geosize_group -- 5. From table FROM countries; name continent code surface_area geosize_group Afghanistan Asia AFG 652090 medium Netherlands Europe NLD 41526 small Albania Europe ALB 28748 small . #### Inner challenge . SELECT name, continent, code, surface_area, CASE WHEN surface_area &gt; 2000000 THEN &#39;large&#39; WHEN surface_area &gt; 350000 THEN &#39;medium&#39; ELSE &#39;small&#39; END AS geosize_group INTO countries_plus FROM countries; name continent code surface_area geosize_group Afghanistan Asia AFG 652090 medium Netherlands Europe NLD 41526 small Albania Europe ALB 28748 small Algeria Africa DZA 2381740 large . SELECT country_code, size, -- 1. First case CASE WHEN size &gt; 50000000 THEN &#39;large&#39; -- 2. Second case WHEN size &gt; 1000000 THEN &#39;medium&#39; -- 3. Else clause + end ELSE &#39;small&#39; END -- 4. Alias name AS popsize_group -- 5. From table FROM populations -- 6. Focus on 2015 WHERE year = 2015; country_code size popsize_group ABW 103889 small AFG 32526600 medium AGO 25022000 medium ALB 2889170 medium . SELECT country_code, size, CASE WHEN size &gt; 50000000 THEN &#39;large&#39; WHEN size &gt; 1000000 THEN &#39;medium&#39; ELSE &#39;small&#39; END AS popsize_group -- 1. Into table INTO pop_plus FROM populations WHERE year = 2015; -- 2. Select all columns of pop_plus SELECT * FROM pop_plus; country_code size popsize_group ABW 103889 small AFG 32526600 medium AGO 25022000 medium ALB 2889170 medium . SELECT country_code, size, CASE WHEN size &gt; 50000000 THEN &#39;large&#39; WHEN size &gt; 1000000 THEN &#39;medium&#39; ELSE &#39;small&#39; END AS popsize_group INTO pop_plus FROM populations WHERE year = 2015; -- 5. Select fields SELECT name, continent, geosize_group, popsize_group -- 1. From countries_plus (alias as c) FROM countries_plus AS c -- 2. Join to pop_plus (alias as p) INNER JOIN pop_plus AS p -- 3. Match on country code ON c.code = p.country_code -- 4. Order the table ORDER BY geosize_group; name continent geosize_group popsize_group India Asia large large United States North America large large Saudi Arabia Asia large medium China Asia large large . . Outer joins and cross joins . ### LEFT and RIGHT JOINs . . #### Left Join . -- Select the city name (with alias), the country code, -- the country name (with alias), the region, -- and the city proper population SELECT c1.name AS city, code, c2.name AS country, region, city_proper_pop -- From left table (with alias) FROM cities AS c1 -- Join to right table (with alias) INNER JOIN countries AS c2 -- Match on country code ON c1.country_code = c2.code -- Order by descending country code ORDER BY code DESC; city code country region city_proper_pop Harare ZWE Zimbabwe Eastern Africa 1606000 Lusaka ZMB Zambia Eastern Africa 1742980 Cape Town ZAF South Africa Southern Africa 3740030 -- 230 rows . SELECT c1.name AS city, code, c2.name AS country, region, city_proper_pop FROM cities AS c1 -- 1. Join right table (with alias) LEFT JOIN countries AS c2 -- 2. Match on country code ON c1.country_code = c2.code -- 3. Order by descending country code ORDER BY code DESC; city code country region city_proper_pop Taichung null null null 2752410 Tainan null null null 1885250 Kaohsiung null null null 2778920 Bucharest null null null 1883420 -- 236 rows . #### Left join (2) . /* 5. Select country name AS country, the country&#39;s local name, the language name AS language, and the percent of the language spoken in the country */ SELECT c.name AS country, local_name, l.name AS language, percent -- 1. From left table (alias as c) FROM countries AS c -- 2. Join to right table (alias as l) INNER JOIN languages AS l -- 3. Match on fields ON c.code = l.code -- 4. Order by descending country ORDER BY country DESC; country local_name language percent Zimbabwe Zimbabwe Shona null Zimbabwe Zimbabwe Tonga null Zimbabwe Zimbabwe Tswana null -- 914 rows . /* 5. Select country name AS country, the country&#39;s local name, the language name AS language, and the percent of the language spoken in the country */ SELECT c.name AS country, local_name, l.name AS language, percent -- 1. From left table (alias as c) FROM countries AS c -- 2. Join to right table (alias as l) LEFT JOIN languages AS l -- 3. Match on fields ON c.code = l.code -- 4. Order by descending country ORDER BY country DESC; country local_name language percent Zimbabwe Zimbabwe Chibarwe null Zimbabwe Zimbabwe Shona null Zimbabwe Zimbabwe Ndebele null Zimbabwe Zimbabwe English null -- 921 rows . #### Left join (3) . -- 5. Select name, region, and gdp_percapita SELECT name, region, gdp_percapita -- 1. From countries (alias as c) FROM countries AS c -- 2. Left join with economies (alias as e) LEFT JOIN economies AS e -- 3. Match on code fields ON c.code = e.code -- 4. Focus on 2010 WHERE year = 2010; name region gdp_percapita Afghanistan Southern and Central Asia 539.667 Angola Central Africa 3599.27 Albania Southern Europe 4098.13 United Arab Emirates Middle East 34628.6 . -- Select fields SELECT region, AVG(gdp_percapita) AS avg_gdp -- From countries (alias as c) FROM countries AS c -- Left join with economies (alias as e) LEFT JOIN economies AS e -- Match on code fields ON c.code = e.code -- Focus on 2010 WHERE year = 2010 -- Group by region GROUP BY region; region avg_gdp Southern Africa 5051.59797363281 Australia and New Zealand 44792.384765625 Southeast Asia 10547.1541320801 . -- Select fields SELECT region, AVG(gdp_percapita) AS avg_gdp -- From countries (alias as c) FROM countries AS c -- Left join with economies (alias as e) LEFT JOIN economies AS e -- Match on code fields ON c.code = e.code -- Focus on 2010 WHERE year = 2010 -- Group by region GROUP BY region -- Order by descending avg_gdp ORDER BY avg_gdp DESC; region avg_gdp Western Europe 58130.9614955357 Nordic Countries 57073.99765625 North America 47911.509765625 Australia and New Zealand 44792.384765625 . #### Right join . . -- convert this code to use RIGHT JOINs instead of LEFT JOINs /* SELECT cities.name AS city, urbanarea_pop, countries.name AS country, indep_year, languages.name AS language, percent FROM cities LEFT JOIN countries ON cities.country_code = countries.code LEFT JOIN languages ON countries.code = languages.code ORDER BY city, language; */ SELECT cities.name AS city, urbanarea_pop, countries.name AS country, indep_year, languages.name AS language, percent FROM languages RIGHT JOIN countries ON languages.code = countries.code RIGHT JOIN cities ON countries.code = cities.country_code ORDER BY city, language; city urbanarea_pop country indep_year language percent Abidjan 4765000 Cote d&#39;Ivoire 1960 French null Abidjan 4765000 Cote d&#39;Ivoire 1960 Other null Abu Dhabi 1145000 United Arab Emirates 1971 Arabic null . ### FULL JOINs . #### Full join . . SELECT name AS country, code, region, basic_unit -- 3. From countries FROM countries -- 4. Join to currencies FULL JOIN currencies -- 5. Match on code USING (code) -- 1. Where region is North America or null WHERE region = &#39;North America&#39; OR region IS NULL -- 2. Order by region ORDER BY region; country code region basic_unit Greenland GRL North America null null TMP null United States dollar null FLK null Falkland Islands pound null AIA null East Caribbean dollar null NIU null New Zealand dollar null ROM null Romanian leu null SHN null Saint Helena pound null SGS null British pound null TWN null New Taiwan dollar null WLF null CFP franc null MSR null East Caribbean dollar null IOT null United States dollar null CCK null Australian dollar null COK null New Zealand dollar . SELECT name AS country, code, region, basic_unit -- 1. From countries FROM countries -- 2. Join to currencies LEFT JOIN currencies -- 3. Match on code USING (code) -- 4. Where region is North America or null WHERE region = &#39;North America&#39; OR region IS NULL -- 5. Order by region ORDER BY region; country code region basic_unit Bermuda BMU North America Bermudian dollar Canada CAN North America Canadian dollar United States USA North America United States dollar Greenland GRL North America null . SELECT name AS country, code, region, basic_unit FROM countries -- 1. Join to currencies INNER JOIN currencies USING (code) -- 2. Where region is North America or null WHERE region = &#39;North America&#39; OR region IS NULL -- 3. Order by region ORDER BY region; country code region basic_unit Bermuda BMU North America Bermudian dollar Canada CAN North America Canadian dollar United States USA North America United States dollar . #### Full join (2) . -- FULL JOIN SELECT countries.name, code, languages.name AS language -- 3. From languages FROM languages -- 4. Join to countries FULL JOIN countries -- 5. Match on code USING (code) -- 1. Where countries.name starts with V or is null WHERE countries.name LIKE &#39;V%&#39; OR countries.name IS NULL -- 2. Order by ascending countries.name ORDER BY countries.name; name code language Vanuatu VUT Tribal Languages Vanuatu VUT English Vanuatu VUT French Vanuatu VUT Other -- 53 rows . -- LEFT JOIN SELECT countries.name, code, languages.name AS language FROM languages -- 1. Join to countries LEFT JOIN countries -- 2. Match using code USING (code) -- 3. Where countries.name starts with V or is null WHERE countries.name LIKE &#39;V%&#39; OR countries.name IS NULL ORDER BY countries.name; name code language Vanuatu VUT English Vanuatu VUT Other Vanuatu VUT French -- 51 rows . -- LEFT JOIN SELECT countries.name, code, languages.name AS language FROM languages -- 1. Join to countries INNER JOIN countries USING (code) -- 2. Where countries.name starts with V or is null WHERE countries.name LIKE &#39;V%&#39; OR countries.name IS NULL ORDER BY countries.name; name code language Vanuatu VUT Tribal Languages Vanuatu VUT Bislama Vanuatu VUT English -- 10 rows . #### Full join (3) . -- 7. Select fields (with aliases) SELECT c1.name AS country, region, l.name AS language, basic_unit, frac_unit -- 1. From countries (alias as c1) FROM countries AS c1 -- 2. Join with languages (alias as l) FULL JOIN languages AS l -- 3. Match on code USING (code) -- 4. Join with currencies (alias as c2) FULL JOIN currencies AS c2 -- 5. Match on code USING (code) -- 6. Where region like Melanesia and Micronesia WHERE region LIKE &#39;M%esia&#39;; country region language basic_unit frac_unit Kiribati Micronesia English Australian dollar Cent Kiribati Micronesia Kiribati Australian dollar Cent Marshall Islands Micronesia Other United States dollar Cent Marshall Islands Micronesia Marshallese United States dollar Cent . ### CROSSing the rubicon . . #### A table of two cities . CROSS JOIN . -- 4. Select fields SELECT c.name AS city, l.name AS language -- 1. From cities (alias as c) FROM cities AS c -- 2. Join to languages (alias as l) CROSS JOIN languages AS l -- 3. Where c.name like Hyderabad WHERE c.name LIKE &#39;Hyder%&#39;; city language Hyderabad (India) Dari Hyderabad Dari Hyderabad (India) Pashto Hyderabad Pashto . -- 5. Select fields SELECT c.name AS city, l.name AS language -- 1. From cities (alias as c) FROM cities AS c -- 2. Join to languages (alias as l) INNER JOIN languages AS l -- 3. Match on country code ON c.country_code = l.code -- 4. Where c.name like Hyderabad WHERE c.name LIKE &#39;Hyder%&#39;; city language Hyderabad (India) Hindi Hyderabad (India) Bengali Hyderabad (India) Telugu Hyderabad (India) Marathi . #### Outer challenge . -- Select fields SELECT c.name AS country, region, life_expectancy AS life_exp -- From countries (alias as c) FROM countries as c -- Join to populations (alias as p) LEFT JOIN populations as p -- Match on country code ON c.code = p.country_code -- Focus on 2010 WHERE year = 2010 -- Order by life_exp ORDER BY life_exp -- Limit to 5 records LIMIT 5; . . Set theory clauses . ### State of the UNION . . #### Union . . -- Select fields from 2010 table SELECT * -- From 2010 table FROM economies2010 -- Set theory clause UNION -- Select fields from 2015 table SELECT * -- From 2015 table FROM economies2015 -- Order by code and year ORDER BY code, year; code year income_group gross_savings AFG 2010 Low income 37.133 AFG 2015 Low income 21.466 AGO 2010 Upper middle income 23.534 AGO 2015 Upper middle income -0.425 . #### Union (2) . -- Select field SELECT country_code -- From cities FROM cities -- Set theory clause UNION -- Select field SELECT code AS country_code -- From currencies FROM currencies -- Order by country_code ORDER BY country_code; country_code ABW AFG AGO AIA . #### Union all . . -- Select fields SELECT code, year -- From economies FROM economies -- Set theory clause UNION ALL -- Select fields SELECT country_code AS code, year -- From populations FROM populations -- Order by code, year ORDER BY code, year; code year ABW 2010 ABW 2015 AFG 2010 AFG 2010 . ### INTERSECTional data science . . #### Intersect . -- Select fields SELECT code, year -- From economies FROM economies -- Set theory clause INTERSECT -- Select fields SELECT country_code AS code, year -- From populations FROM populations -- Order by code and year ORDER BY code, year; code year AFG 2010 AFG 2015 AGO 2010 . #### Intersect (2) . -- Select fields SELECT name -- From countries FROM countries -- Set theory clause INTERSECT -- Select fields SELECT name -- From cities FROM cities; name Singapore Hong Kong . Hong Kong is part of China, but it appears separately here because it has its own ISO country code. Depending upon your analysis, treating Hong Kong separately could be useful or a mistake. Always check your dataset closely before you perform an analysis! . ### EXCEPTional . . #### Except . -- Get the names of cities in cities which are not noted as capital cities in countries as a single field result. -- Select field SELECT name -- From cities FROM cities -- Set theory clause EXCEPT -- Select field SELECT capital -- From countries FROM countries -- Order by result ORDER BY name; name Abidjan Ahmedabad Alexandria . #### Except (2) . -- Determine the names of capital cities that are not listed in the cities table. -- Select field SELECT capital -- From countries FROM countries -- Set theory clause EXCEPT -- Select field SELECT name -- From cities FROM cities -- Order by ascending capital ORDER BY capital; capital Agana Amman Amsterdam ... . ### Semi-joins and Anti-joins . . #### Semi-join . -- You are now going to use the concept of a semi-join to identify languages spoken in the Middle East. -- Select distinct fields SELECT DISTINCT name -- From languages FROM languages -- Where in statement WHERE code IN -- Subquery (SELECT code FROM countries WHERE region = &#39;Middle East&#39;) -- Order by name ORDER BY name; . #### Relating semi-join to a tweaked inner join . SELECT DISTINCT name FROM languages WHERE code IN (SELECT code FROM countries WHERE region = &#39;Middle East&#39;) ORDER BY name; -- is equal to SELECT DISTINCT languages.name AS language FROM languages INNER JOIN countries ON languages.code = countries.code WHERE region = &#39;Middle East&#39; ORDER BY language; . #### Diagnosing problems using anti-join . Your goal is to identify the currencies used in Oceanian countries! . -- Begin by determining the number of countries in countries that are listed in Oceania using SELECT, FROM, and WHERE. -- Select statement SELECT COUNT(*) -- From countries FROM countries -- Where continent is Oceania WHERE continent = &#39;Oceania&#39;; count 19 . -- 5. Select fields (with aliases) SELECT c1.code, name, basic_unit AS currency -- 1. From countries (alias as c1) FROM countries AS c1 -- 2. Join with currencies (alias as c2) INNER JOIN currencies c2 -- 3. Match on code USING (code) -- 4. Where continent is Oceania WHERE continent = &#39;Oceania&#39;; code name currency AUS Australia Australian dollar PYF French Polynesia CFP franc KIR Kiribati Australian dollar . -- 3. Select fields SELECT code, name -- 4. From Countries FROM countries -- 5. Where continent is Oceania WHERE continent = &#39;Oceania&#39; -- 1. And code not in AND code NOT IN -- 2. Subquery (SELECT code FROM currencies); code name ASM American Samoa FJI Fiji Islands GUM Guam FSM Micronesia, Federated States of MNP Northern Mariana Islands . #### Set theory challenge . Identify the country codes that are included in either economies or currencies but not in populations . | Use that result to determine the names of cities in the countries that match the specification in the previous instruction. | . -- Select the city name SELECT name -- Alias the table where city name resides FROM cities AS c1 -- Choose only records matching the result of multiple set theory clauses WHERE country_code IN ( -- Select appropriate field from economies AS e SELECT e.code FROM economies AS e -- Get all additional (unique) values of the field from currencies AS c2 UNION SELECT c2.code FROM currencies AS c2 -- Exclude those appearing in populations AS p EXCEPT SELECT p.country_code FROM populations AS p ); . . Subqueries . ### Subqueries inside WHERE and SELECT clauses . #### Subquery inside where . You’ll now try to figure out which countries had high average life expectancies (at the country level) in 2015. . -- Select average life_expectancy SELECT AVG(life_expectancy) -- From populations FROM populations -- Where year is 2015 WHERE year = 2015 avg 71.6763415481105 . -- Select fields SELECT * -- From populations FROM populations -- Where life_expectancy is greater than WHERE life_expectancy &gt; -- 1.15 * subquery 1.15 * (SELECT AVG(life_expectancy) FROM populations WHERE year = 2015) AND year = 2015; pop_id country_code year fertility_rate life_expectancy size 21 AUS 2015 1.833 82.4512 23789800 376 CHE 2015 1.54 83.1976 8281430 356 ESP 2015 1.32 83.3805 46444000 134 FRA 2015 2.01 82.6707 66538400 . #### Subquery inside where (2) . -- 2. Select fields SELECT name, country_code, urbanarea_pop -- 3. From cities FROM cities -- 4. Where city name in the field of capital cities WHERE name IN -- 1. Subquery (SELECT capital FROM countries) ORDER BY urbanarea_pop DESC; name country_code urbanarea_pop Beijing CHN 21516000 Dhaka BGD 14543100 Tokyo JPN 13513700 . #### Subquery inside select . The code selects the top 9 countries in terms of number of cities appearing in the cities table. . SELECT countries.name AS country, COUNT(*) AS cities_num FROM cities INNER JOIN countries ON countries.code = cities.country_code GROUP BY country ORDER BY cities_num DESC, country LIMIT 9; -- is equal to SELECT countries.name AS country, (SELECT COUNT(*) FROM cities WHERE countries.code = cities.country_code) AS cities_num FROM countries ORDER BY cities_num DESC, country LIMIT 9; country cities_num China 36 India 18 Japan 11 . ### Subquery inside FROM clause . . #### Subquery inside from . You will use this to determine the number of languages spoken for each country, identified by the country’s local name! . -- Select fields (with aliases) SELECT code, COUNT(*) AS lang_num -- From languages From languages -- Group by code GROUP BY code; code lang_num BLZ 9 BGD 2 ITA 4 . -- Select fields SELECT local_name, subquery.lang_num -- From countries FROM countries, -- Subquery (alias as subquery) (SELECT code, COUNT(*) AS lang_num From languages GROUP BY code) AS subquery -- Where codes match WHERE countries.code = subquery.code -- Order by descending number of languages ORDER BY lang_num DESC; local_name lang_num Zambia 19 Zimbabwe 16 YeItyop´iya 16 Bharat/India 14 . #### Advanced subquery . You can also nest multiple subqueries to answer even more specific questions. . In this exercise, for each of the six continents listed in 2015, you’ll identify which country had the maximum inflation rate (and how high it was) using multiple subqueries. The table result of your query in Task 3 should look something like the following, where anything between &lt; &gt; will be filled in with appropriate values: . +++-+ | name | continent | inflation_rate | |++-| | &lt;country1&gt; | North America | &lt;max_inflation1&gt; | | &lt;country2&gt; | Africa | &lt;max_inflation2&gt; | | &lt;country3&gt; | Oceania | &lt;max_inflation3&gt; | | &lt;country4&gt; | Europe | &lt;max_inflation4&gt; | | &lt;country5&gt; | South America | &lt;max_inflation5&gt; | | &lt;country6&gt; | Asia | &lt;max_inflation6&gt; | +++-+ . Again, there are multiple ways to get to this solution using only joins, but the focus here is on showing you an introduction into advanced subqueries. . -- step 1 -- Select fields SELECT name, continent, inflation_rate -- From countries FROM countries -- Join to economies INNER JOIN economies -- Match on code USING (code) -- Where year is 2015 WHERE year = 2015; name continent inflation_rate Afghanistan Asia -1.549 Angola Africa 10.287 Albania Europe 1.896 United Arab Emirates Asia 4.07 . -- step 2 -- Select fields SELECT MAX(inflation_rate) AS max_inf -- Subquery using FROM (alias as subquery) FROM ( SELECT name, continent, inflation_rate FROM countries INNER JOIN economies USING (code) WHERE year = 2015) AS subquery -- Group by continent GROUP BY continent; max_inf 48.684 9.784 39.403 21.858 . -- step 3 -- Select fields SELECT name, continent, inflation_rate -- From countries FROM countries -- Join to economies INNER JOIN economies -- Match on code ON countries.code = economies.code -- Where year is 2015 WHERE year = 2015 AND inflation_rate -- And inflation rate in subquery (alias as subquery) IN ( SELECT MAX(inflation_rate) AS max_inf FROM ( SELECT name, continent, inflation_rate FROM countries INNER JOIN economies ON countries.code = economies.code WHERE year = 2015) AS subquery GROUP BY continent); name continent inflation_rate Haiti North America 7.524 Malawi Africa 21.858 Nauru Oceania 9.784 . #### Subquery challenge . Let’s test your understanding of the subqueries with a challenge problem! Use a subquery to get 2015 economic data for countries that do not have . gov_form of &#39;Constitutional Monarchy&#39; or | &#39;Republic&#39; in their gov_form . | . Here, gov_form stands for the form of the government for each country. Review the different entries for gov_form in the countries table. . -- Select fields SELECT code, inflation_rate, unemployment_rate -- From economies FROM economies -- Where year is 2015 and code is not in WHERE year = 2015 AND code NOT IN -- Subquery (SELECT code FROM countries WHERE (gov_form = &#39;Constitutional Monarchy&#39; OR gov_form LIKE &#39;%Republic%&#39;)) -- Order by inflation rate ORDER BY inflation_rate; code inflation_rate unemployment_rate AFG -1.549 null CHE -1.14 3.178 PRI -0.751 12 ROU -0.596 6.812 . ### Course review . . . . . . . . #### Final challenge . In this exercise, you’ll need to get the country names and other 2015 data in the economies table and the countries table for Central American countries with an official language . . -- Select fields SELECT DISTINCT c.name, e.total_investment, e.imports -- From table (with alias) FROM countries AS c -- Join with table (with alias) LEFT JOIN economies AS e -- Match on code ON (c.code = e.code -- and code in Subquery AND c.code IN ( SELECT l.code FROM languages AS l WHERE official = &#39;true&#39; ) ) -- Where region and year are correct WHERE region = &#39;Central America&#39; AND year = 2015 -- Order by field ORDER BY name; name total_investment imports Belize 22.014 6.743 Costa Rica 20.218 4.629 El Salvador 13.983 8.193 . #### Final challenge (2) . Let’s ease up a bit and calculate the average fertility rate for each region in 2015. . -- Select fields SELECT region, continent, AVG(fertility_rate) AS avg_fert_rate -- From left table FROM countries AS c -- Join to right table INNER JOIN populations AS p -- Match on join condition ON c.code = p.country_code -- Where specific records matching some condition WHERE year = 2015 -- Group appropriately GROUP BY region, continent -- Order appropriately ORDER BY avg_fert_rate; region continent avg_fert_rate Southern Europe Europe 1.42610000371933 Eastern Europe Europe 1.49088890022702 Baltic Countries Europe 1.60333331425985 Eastern Asia Asia 1.62071430683136 . #### Final challenge (3) . You are now tasked with determining the top 10 capital cities in Europe and the Americas in terms of a calculated percentage using city_proper_pop and metroarea_pop in cities . . -- Select fields SELECT name, country_code, city_proper_pop, metroarea_pop, -- Calculate city_perc city_proper_pop / metroarea_pop * 100 AS city_perc -- From appropriate table FROM cities -- Where WHERE name IN -- Subquery (SELECT capital FROM countries WHERE (continent = &#39;Europe&#39; OR continent LIKE &#39;%America&#39;)) AND metroarea_pop IS NOT NULL -- Order appropriately ORDER BY city_perc DESC -- Limit amount LIMIT 10; name country_code city_proper_pop metroarea_pop city_perc Lima PER 8852000 10750000 82.3441863059998 Bogota COL 7878780 9800000 80.3957462310791 Moscow RUS 12197600 16170000 75.4334926605225 . This is the memo of the 18th course of ‘Data Scientist with Python’ track. . You can find the original course HERE . . Further Reading: . More dangerous subtleties of JOINs in SQL — Be careful when JOIN tables with duplications or NULLs . . Introduction to joins . ### Introduction to INNER JOIN . #### Inner join . . SELECT table_name FROM information_schema.tables -- Specify the correct table_schema value WHERE table_schema = &#39;public&#39;; table_name cities countries languages economies currencies populations . SELECT * FROM left_table INNER JOIN right_table ON left_table.id = right_table.id; . -- 1. Select name fields (with alias) and region SELECT cities.name AS city, countries.name AS country, region FROM cities INNER JOIN countries ON cities.country_code = countries.code; city country region Abidjan Cote d&#39;Ivoire Western Africa Abu Dhabi United Arab Emirates Middle East Abuja Nigeria Western Africa . #### Inner join (2) . SELECT c1.name AS city, c2.name AS country FROM cities AS c1 INNER JOIN countries AS c2 ON c1.country_code = c2.code; . -- 3. Select fields with aliases SELECT c.code AS country_code, name, year, inflation_rate FROM countries AS c -- 1. Join to economies (alias e) INNER JOIN economies AS e -- 2. Match on code ON c.code = e.code; . #### Inner join (3) . SELECT * FROM left_table INNER JOIN right_table ON left_table.id = right_table.id INNER JOIN another_table ON left_table.id = another_table.id; . -- 6. Select fields SELECT c.code, name, region, e.year, fertility_rate, unemployment_rate -- 1. From countries (alias as c) FROM countries AS c -- 2. Join to populations (as p) INNER JOIN populations AS p -- 3. Match on country code ON c.code = p.country_code -- 4. Join to economies (as e) INNER JOIN economies AS e -- 5. Match on country code ON c.code = e.code; . -- countries INNER JOIN populations table code name fertility_rate ABW Aruba 1.704 ABW Aruba 1.647 AFG Afghanistan 5.746 AFG Afghanistan 4.653 -- economies table econ_id code year 1 AFG 2010 2 AFG 2015 code name region year fertility_rate unemployment_rate AFG Afghanistan Southern and Central Asia 2010 4.653 null AFG Afghanistan Southern and Central Asia 2010 5.746 null AFG Afghanistan Southern and Central Asia 2015 4.653 null AFG Afghanistan Southern and Central Asia 2015 5.746 null AGO Angola Central Africa 2010 5.996 null . -- 6. Select fields SELECT c.code, name, region, e.year, fertility_rate, unemployment_rate -- 1. From countries (alias as c) FROM countries AS c -- 2. Join to populations (as p) INNER JOIN populations AS p -- 3. Match on country code ON c.code = p.country_code -- 4. Join to economies (as e) INNER JOIN economies AS e -- 5. Match on country code and year ON c.code = e.code AND p.year = e.year; code name region year fertility_rate unemployment_rate AFG Afghanistan Southern and Central Asia 2010 5.746 null AFG Afghanistan Southern and Central Asia 2015 4.653 null . ### INNER JOIN via USING . #### Inner join with using . SELECT * FROM countries INNER JOIN economies ON countries.code = economies.code -- is equal to SELECT * FROM countries INNER JOIN economies USING(code) . -- 4. Select fields SELECT c.name AS country, continent, l.name AS language, official -- 1. From countries (alias as c) FROM countries as c -- 2. Join to languages (as l) INNER JOIN languages as l -- 3. Match using code USING (code) country continent language official Afghanistan Asia Dari true Afghanistan Asia Pashto true . ### Self-ish joins, just in CASE . #### Self-join . . pop_id country_code year fertility_rate life_expectancy size 20 ABW 2010 1.704 74.9535 101597 19 ABW 2015 1.647 75.5736 103889 . -- 4. Select fields with aliases SELECT p1.country_code, p1.size AS size2010, p2.size AS size2015 -- 1. From populations (alias as p1) FROM populations AS p1 -- 2. Join to itself (alias as p2) INNER JOIN populations AS p2 -- 3. Match on country code ON p1.country_code = p2.country_code country_code size2010 size2015 ABW 101597 103889 ABW 101597 101597 ABW 103889 103889 ABW 103889 101597 . -- 5. Select fields with aliases SELECT p1.country_code, p1.size AS size2010, p2.size AS size2015 -- 1. From populations (alias as p1) FROM populations as p1 -- 2. Join to itself (alias as p2) INNER JOIN populations as p2 -- 3. Match on country code ON p1.country_code = p2.country_code -- 4. and year (with calculation) AND p1.year = p2.year - 5 country_code size2010 size2015 ABW 101597 103889 AFG 27962200 32526600 AGO 21220000 25022000 ALB 2913020 2889170 . -- With two numeric fields A and B, the percentage growth from A to B can be calculated as (B−A)/A∗100.0. SELECT p1.country_code, p1.size AS size2010, p2.size AS size2015, -- 1. calculate growth_perc ((p2.size - p1.size)/p1.size * 100.0) AS growth_perc -- 2. From populations (alias as p1) FROM populations AS p1 -- 3. Join to itself (alias as p2) INNER JOIN populations AS p2 -- 4. Match on country code ON p1.country_code = p2.country_code -- 5. and year (with calculation) AND p1.year = p2.year - 5; country_code size2010 size2015 growth_perc ABW 101597 103889 2.25597210228443 AFG 27962200 32526600 16.32329672575 AGO 21220000 25022000 17.9171919822693 ALB 2913020 2889170 -0.818874966353178 . #### Case when and then . SELECT name, continent, code, surface_area, -- 1. First case CASE WHEN surface_area &gt; 2000000 THEN &#39;large&#39; -- 2. Second case WHEN surface_area &gt; 350000 THEN &#39;medium&#39; -- 3. Else clause + end ELSE &#39;small&#39; END -- 4. Alias name AS geosize_group -- 5. From table FROM countries; name continent code surface_area geosize_group Afghanistan Asia AFG 652090 medium Netherlands Europe NLD 41526 small Albania Europe ALB 28748 small . #### Inner challenge . SELECT name, continent, code, surface_area, CASE WHEN surface_area &gt; 2000000 THEN &#39;large&#39; WHEN surface_area &gt; 350000 THEN &#39;medium&#39; ELSE &#39;small&#39; END AS geosize_group INTO countries_plus FROM countries; name continent code surface_area geosize_group Afghanistan Asia AFG 652090 medium Netherlands Europe NLD 41526 small Albania Europe ALB 28748 small Algeria Africa DZA 2381740 large . SELECT country_code, size, -- 1. First case CASE WHEN size &gt; 50000000 THEN &#39;large&#39; -- 2. Second case WHEN size &gt; 1000000 THEN &#39;medium&#39; -- 3. Else clause + end ELSE &#39;small&#39; END -- 4. Alias name AS popsize_group -- 5. From table FROM populations -- 6. Focus on 2015 WHERE year = 2015; country_code size popsize_group ABW 103889 small AFG 32526600 medium AGO 25022000 medium ALB 2889170 medium . SELECT country_code, size, CASE WHEN size &gt; 50000000 THEN &#39;large&#39; WHEN size &gt; 1000000 THEN &#39;medium&#39; ELSE &#39;small&#39; END AS popsize_group -- 1. Into table INTO pop_plus FROM populations WHERE year = 2015; -- 2. Select all columns of pop_plus SELECT * FROM pop_plus; country_code size popsize_group ABW 103889 small AFG 32526600 medium AGO 25022000 medium ALB 2889170 medium . SELECT country_code, size, CASE WHEN size &gt; 50000000 THEN &#39;large&#39; WHEN size &gt; 1000000 THEN &#39;medium&#39; ELSE &#39;small&#39; END AS popsize_group INTO pop_plus FROM populations WHERE year = 2015; -- 5. Select fields SELECT name, continent, geosize_group, popsize_group -- 1. From countries_plus (alias as c) FROM countries_plus AS c -- 2. Join to pop_plus (alias as p) INNER JOIN pop_plus AS p -- 3. Match on country code ON c.code = p.country_code -- 4. Order the table ORDER BY geosize_group; name continent geosize_group popsize_group India Asia large large United States North America large large Saudi Arabia Asia large medium China Asia large large . . Outer joins and cross joins . ### LEFT and RIGHT JOINs . . #### Left Join . -- Select the city name (with alias), the country code, -- the country name (with alias), the region, -- and the city proper population SELECT c1.name AS city, code, c2.name AS country, region, city_proper_pop -- From left table (with alias) FROM cities AS c1 -- Join to right table (with alias) INNER JOIN countries AS c2 -- Match on country code ON c1.country_code = c2.code -- Order by descending country code ORDER BY code DESC; city code country region city_proper_pop Harare ZWE Zimbabwe Eastern Africa 1606000 Lusaka ZMB Zambia Eastern Africa 1742980 Cape Town ZAF South Africa Southern Africa 3740030 -- 230 rows . SELECT c1.name AS city, code, c2.name AS country, region, city_proper_pop FROM cities AS c1 -- 1. Join right table (with alias) LEFT JOIN countries AS c2 -- 2. Match on country code ON c1.country_code = c2.code -- 3. Order by descending country code ORDER BY code DESC; city code country region city_proper_pop Taichung null null null 2752410 Tainan null null null 1885250 Kaohsiung null null null 2778920 Bucharest null null null 1883420 -- 236 rows . #### Left join (2) . /* 5. Select country name AS country, the country&#39;s local name, the language name AS language, and the percent of the language spoken in the country */ SELECT c.name AS country, local_name, l.name AS language, percent -- 1. From left table (alias as c) FROM countries AS c -- 2. Join to right table (alias as l) INNER JOIN languages AS l -- 3. Match on fields ON c.code = l.code -- 4. Order by descending country ORDER BY country DESC; country local_name language percent Zimbabwe Zimbabwe Shona null Zimbabwe Zimbabwe Tonga null Zimbabwe Zimbabwe Tswana null -- 914 rows . /* 5. Select country name AS country, the country&#39;s local name, the language name AS language, and the percent of the language spoken in the country */ SELECT c.name AS country, local_name, l.name AS language, percent -- 1. From left table (alias as c) FROM countries AS c -- 2. Join to right table (alias as l) LEFT JOIN languages AS l -- 3. Match on fields ON c.code = l.code -- 4. Order by descending country ORDER BY country DESC; country local_name language percent Zimbabwe Zimbabwe Chibarwe null Zimbabwe Zimbabwe Shona null Zimbabwe Zimbabwe Ndebele null Zimbabwe Zimbabwe English null -- 921 rows . #### Left join (3) . -- 5. Select name, region, and gdp_percapita SELECT name, region, gdp_percapita -- 1. From countries (alias as c) FROM countries AS c -- 2. Left join with economies (alias as e) LEFT JOIN economies AS e -- 3. Match on code fields ON c.code = e.code -- 4. Focus on 2010 WHERE year = 2010; name region gdp_percapita Afghanistan Southern and Central Asia 539.667 Angola Central Africa 3599.27 Albania Southern Europe 4098.13 United Arab Emirates Middle East 34628.6 . -- Select fields SELECT region, AVG(gdp_percapita) AS avg_gdp -- From countries (alias as c) FROM countries AS c -- Left join with economies (alias as e) LEFT JOIN economies AS e -- Match on code fields ON c.code = e.code -- Focus on 2010 WHERE year = 2010 -- Group by region GROUP BY region; region avg_gdp Southern Africa 5051.59797363281 Australia and New Zealand 44792.384765625 Southeast Asia 10547.1541320801 . -- Select fields SELECT region, AVG(gdp_percapita) AS avg_gdp -- From countries (alias as c) FROM countries AS c -- Left join with economies (alias as e) LEFT JOIN economies AS e -- Match on code fields ON c.code = e.code -- Focus on 2010 WHERE year = 2010 -- Group by region GROUP BY region -- Order by descending avg_gdp ORDER BY avg_gdp DESC; region avg_gdp Western Europe 58130.9614955357 Nordic Countries 57073.99765625 North America 47911.509765625 Australia and New Zealand 44792.384765625 . #### Right join . . -- convert this code to use RIGHT JOINs instead of LEFT JOINs /* SELECT cities.name AS city, urbanarea_pop, countries.name AS country, indep_year, languages.name AS language, percent FROM cities LEFT JOIN countries ON cities.country_code = countries.code LEFT JOIN languages ON countries.code = languages.code ORDER BY city, language; */ SELECT cities.name AS city, urbanarea_pop, countries.name AS country, indep_year, languages.name AS language, percent FROM languages RIGHT JOIN countries ON languages.code = countries.code RIGHT JOIN cities ON countries.code = cities.country_code ORDER BY city, language; city urbanarea_pop country indep_year language percent Abidjan 4765000 Cote d&#39;Ivoire 1960 French null Abidjan 4765000 Cote d&#39;Ivoire 1960 Other null Abu Dhabi 1145000 United Arab Emirates 1971 Arabic null . ### FULL JOINs . #### Full join . . SELECT name AS country, code, region, basic_unit -- 3. From countries FROM countries -- 4. Join to currencies FULL JOIN currencies -- 5. Match on code USING (code) -- 1. Where region is North America or null WHERE region = &#39;North America&#39; OR region IS NULL -- 2. Order by region ORDER BY region; country code region basic_unit Greenland GRL North America null null TMP null United States dollar null FLK null Falkland Islands pound null AIA null East Caribbean dollar null NIU null New Zealand dollar null ROM null Romanian leu null SHN null Saint Helena pound null SGS null British pound null TWN null New Taiwan dollar null WLF null CFP franc null MSR null East Caribbean dollar null IOT null United States dollar null CCK null Australian dollar null COK null New Zealand dollar . SELECT name AS country, code, region, basic_unit -- 1. From countries FROM countries -- 2. Join to currencies LEFT JOIN currencies -- 3. Match on code USING (code) -- 4. Where region is North America or null WHERE region = &#39;North America&#39; OR region IS NULL -- 5. Order by region ORDER BY region; country code region basic_unit Bermuda BMU North America Bermudian dollar Canada CAN North America Canadian dollar United States USA North America United States dollar Greenland GRL North America null . SELECT name AS country, code, region, basic_unit FROM countries -- 1. Join to currencies INNER JOIN currencies USING (code) -- 2. Where region is North America or null WHERE region = &#39;North America&#39; OR region IS NULL -- 3. Order by region ORDER BY region; country code region basic_unit Bermuda BMU North America Bermudian dollar Canada CAN North America Canadian dollar United States USA North America United States dollar . #### Full join (2) . -- FULL JOIN SELECT countries.name, code, languages.name AS language -- 3. From languages FROM languages -- 4. Join to countries FULL JOIN countries -- 5. Match on code USING (code) -- 1. Where countries.name starts with V or is null WHERE countries.name LIKE &#39;V%&#39; OR countries.name IS NULL -- 2. Order by ascending countries.name ORDER BY countries.name; name code language Vanuatu VUT Tribal Languages Vanuatu VUT English Vanuatu VUT French Vanuatu VUT Other -- 53 rows . -- LEFT JOIN SELECT countries.name, code, languages.name AS language FROM languages -- 1. Join to countries LEFT JOIN countries -- 2. Match using code USING (code) -- 3. Where countries.name starts with V or is null WHERE countries.name LIKE &#39;V%&#39; OR countries.name IS NULL ORDER BY countries.name; name code language Vanuatu VUT English Vanuatu VUT Other Vanuatu VUT French -- 51 rows . -- LEFT JOIN SELECT countries.name, code, languages.name AS language FROM languages -- 1. Join to countries INNER JOIN countries USING (code) -- 2. Where countries.name starts with V or is null WHERE countries.name LIKE &#39;V%&#39; OR countries.name IS NULL ORDER BY countries.name; name code language Vanuatu VUT Tribal Languages Vanuatu VUT Bislama Vanuatu VUT English -- 10 rows . #### Full join (3) . -- 7. Select fields (with aliases) SELECT c1.name AS country, region, l.name AS language, basic_unit, frac_unit -- 1. From countries (alias as c1) FROM countries AS c1 -- 2. Join with languages (alias as l) FULL JOIN languages AS l -- 3. Match on code USING (code) -- 4. Join with currencies (alias as c2) FULL JOIN currencies AS c2 -- 5. Match on code USING (code) -- 6. Where region like Melanesia and Micronesia WHERE region LIKE &#39;M%esia&#39;; country region language basic_unit frac_unit Kiribati Micronesia English Australian dollar Cent Kiribati Micronesia Kiribati Australian dollar Cent Marshall Islands Micronesia Other United States dollar Cent Marshall Islands Micronesia Marshallese United States dollar Cent . ### CROSSing the rubicon . . #### A table of two cities . CROSS JOIN . -- 4. Select fields SELECT c.name AS city, l.name AS language -- 1. From cities (alias as c) FROM cities AS c -- 2. Join to languages (alias as l) CROSS JOIN languages AS l -- 3. Where c.name like Hyderabad WHERE c.name LIKE &#39;Hyder%&#39;; city language Hyderabad (India) Dari Hyderabad Dari Hyderabad (India) Pashto Hyderabad Pashto . -- 5. Select fields SELECT c.name AS city, l.name AS language -- 1. From cities (alias as c) FROM cities AS c -- 2. Join to languages (alias as l) INNER JOIN languages AS l -- 3. Match on country code ON c.country_code = l.code -- 4. Where c.name like Hyderabad WHERE c.name LIKE &#39;Hyder%&#39;; city language Hyderabad (India) Hindi Hyderabad (India) Bengali Hyderabad (India) Telugu Hyderabad (India) Marathi . #### Outer challenge . -- Select fields SELECT c.name AS country, region, life_expectancy AS life_exp -- From countries (alias as c) FROM countries as c -- Join to populations (alias as p) LEFT JOIN populations as p -- Match on country code ON c.code = p.country_code -- Focus on 2010 WHERE year = 2010 -- Order by life_exp ORDER BY life_exp -- Limit to 5 records LIMIT 5; . . Set theory clauses . ### State of the UNION . . #### Union . . -- Select fields from 2010 table SELECT * -- From 2010 table FROM economies2010 -- Set theory clause UNION -- Select fields from 2015 table SELECT * -- From 2015 table FROM economies2015 -- Order by code and year ORDER BY code, year; code year income_group gross_savings AFG 2010 Low income 37.133 AFG 2015 Low income 21.466 AGO 2010 Upper middle income 23.534 AGO 2015 Upper middle income -0.425 . #### Union (2) . -- Select field SELECT country_code -- From cities FROM cities -- Set theory clause UNION -- Select field SELECT code AS country_code -- From currencies FROM currencies -- Order by country_code ORDER BY country_code; country_code ABW AFG AGO AIA . #### Union all . . -- Select fields SELECT code, year -- From economies FROM economies -- Set theory clause UNION ALL -- Select fields SELECT country_code AS code, year -- From populations FROM populations -- Order by code, year ORDER BY code, year; code year ABW 2010 ABW 2015 AFG 2010 AFG 2010 . ### INTERSECTional data science . . #### Intersect . -- Select fields SELECT code, year -- From economies FROM economies -- Set theory clause INTERSECT -- Select fields SELECT country_code AS code, year -- From populations FROM populations -- Order by code and year ORDER BY code, year; code year AFG 2010 AFG 2015 AGO 2010 . #### Intersect (2) . -- Select fields SELECT name -- From countries FROM countries -- Set theory clause INTERSECT -- Select fields SELECT name -- From cities FROM cities; name Singapore Hong Kong . Hong Kong is part of China, but it appears separately here because it has its own ISO country code. Depending upon your analysis, treating Hong Kong separately could be useful or a mistake. Always check your dataset closely before you perform an analysis! . ### EXCEPTional . . #### Except . -- Get the names of cities in cities which are not noted as capital cities in countries as a single field result. -- Select field SELECT name -- From cities FROM cities -- Set theory clause EXCEPT -- Select field SELECT capital -- From countries FROM countries -- Order by result ORDER BY name; name Abidjan Ahmedabad Alexandria . #### Except (2) . -- Determine the names of capital cities that are not listed in the cities table. -- Select field SELECT capital -- From countries FROM countries -- Set theory clause EXCEPT -- Select field SELECT name -- From cities FROM cities -- Order by ascending capital ORDER BY capital; capital Agana Amman Amsterdam ... . ### Semi-joins and Anti-joins . . #### Semi-join . -- You are now going to use the concept of a semi-join to identify languages spoken in the Middle East. -- Select distinct fields SELECT DISTINCT name -- From languages FROM languages -- Where in statement WHERE code IN -- Subquery (SELECT code FROM countries WHERE region = &#39;Middle East&#39;) -- Order by name ORDER BY name; . #### Relating semi-join to a tweaked inner join . SELECT DISTINCT name FROM languages WHERE code IN (SELECT code FROM countries WHERE region = &#39;Middle East&#39;) ORDER BY name; -- is equal to SELECT DISTINCT languages.name AS language FROM languages INNER JOIN countries ON languages.code = countries.code WHERE region = &#39;Middle East&#39; ORDER BY language; . #### Diagnosing problems using anti-join . Your goal is to identify the currencies used in Oceanian countries! . -- Begin by determining the number of countries in countries that are listed in Oceania using SELECT, FROM, and WHERE. -- Select statement SELECT COUNT(*) -- From countries FROM countries -- Where continent is Oceania WHERE continent = &#39;Oceania&#39;; count 19 . -- 5. Select fields (with aliases) SELECT c1.code, name, basic_unit AS currency -- 1. From countries (alias as c1) FROM countries AS c1 -- 2. Join with currencies (alias as c2) INNER JOIN currencies c2 -- 3. Match on code USING (code) -- 4. Where continent is Oceania WHERE continent = &#39;Oceania&#39;; code name currency AUS Australia Australian dollar PYF French Polynesia CFP franc KIR Kiribati Australian dollar . -- 3. Select fields SELECT code, name -- 4. From Countries FROM countries -- 5. Where continent is Oceania WHERE continent = &#39;Oceania&#39; -- 1. And code not in AND code NOT IN -- 2. Subquery (SELECT code FROM currencies); code name ASM American Samoa FJI Fiji Islands GUM Guam FSM Micronesia, Federated States of MNP Northern Mariana Islands . #### Set theory challenge . Identify the country codes that are included in either economies or currencies but not in populations . | Use that result to determine the names of cities in the countries that match the specification in the previous instruction. | . -- Select the city name SELECT name -- Alias the table where city name resides FROM cities AS c1 -- Choose only records matching the result of multiple set theory clauses WHERE country_code IN ( -- Select appropriate field from economies AS e SELECT e.code FROM economies AS e -- Get all additional (unique) values of the field from currencies AS c2 UNION SELECT c2.code FROM currencies AS c2 -- Exclude those appearing in populations AS p EXCEPT SELECT p.country_code FROM populations AS p ); . . Subqueries . ### Subqueries inside WHERE and SELECT clauses . #### Subquery inside where . You’ll now try to figure out which countries had high average life expectancies (at the country level) in 2015. . -- Select average life_expectancy SELECT AVG(life_expectancy) -- From populations FROM populations -- Where year is 2015 WHERE year = 2015 avg 71.6763415481105 . -- Select fields SELECT * -- From populations FROM populations -- Where life_expectancy is greater than WHERE life_expectancy &gt; -- 1.15 * subquery 1.15 * (SELECT AVG(life_expectancy) FROM populations WHERE year = 2015) AND year = 2015; pop_id country_code year fertility_rate life_expectancy size 21 AUS 2015 1.833 82.4512 23789800 376 CHE 2015 1.54 83.1976 8281430 356 ESP 2015 1.32 83.3805 46444000 134 FRA 2015 2.01 82.6707 66538400 . #### Subquery inside where (2) . -- 2. Select fields SELECT name, country_code, urbanarea_pop -- 3. From cities FROM cities -- 4. Where city name in the field of capital cities WHERE name IN -- 1. Subquery (SELECT capital FROM countries) ORDER BY urbanarea_pop DESC; name country_code urbanarea_pop Beijing CHN 21516000 Dhaka BGD 14543100 Tokyo JPN 13513700 . #### Subquery inside select . The code selects the top 9 countries in terms of number of cities appearing in the cities table. . SELECT countries.name AS country, COUNT(*) AS cities_num FROM cities INNER JOIN countries ON countries.code = cities.country_code GROUP BY country ORDER BY cities_num DESC, country LIMIT 9; -- is equal to SELECT countries.name AS country, (SELECT COUNT(*) FROM cities WHERE countries.code = cities.country_code) AS cities_num FROM countries ORDER BY cities_num DESC, country LIMIT 9; country cities_num China 36 India 18 Japan 11 . ### Subquery inside FROM clause . . #### Subquery inside from . You will use this to determine the number of languages spoken for each country, identified by the country’s local name! . -- Select fields (with aliases) SELECT code, COUNT(*) AS lang_num -- From languages From languages -- Group by code GROUP BY code; code lang_num BLZ 9 BGD 2 ITA 4 . -- Select fields SELECT local_name, subquery.lang_num -- From countries FROM countries, -- Subquery (alias as subquery) (SELECT code, COUNT(*) AS lang_num From languages GROUP BY code) AS subquery -- Where codes match WHERE countries.code = subquery.code -- Order by descending number of languages ORDER BY lang_num DESC; local_name lang_num Zambia 19 Zimbabwe 16 YeItyop´iya 16 Bharat/India 14 . #### Advanced subquery . You can also nest multiple subqueries to answer even more specific questions. . In this exercise, for each of the six continents listed in 2015, you’ll identify which country had the maximum inflation rate (and how high it was) using multiple subqueries. The table result of your query in Task 3 should look something like the following, where anything between &lt; &gt; will be filled in with appropriate values: . +++-+ | name | continent | inflation_rate | |++-| | &lt;country1&gt; | North America | &lt;max_inflation1&gt; | | &lt;country2&gt; | Africa | &lt;max_inflation2&gt; | | &lt;country3&gt; | Oceania | &lt;max_inflation3&gt; | | &lt;country4&gt; | Europe | &lt;max_inflation4&gt; | | &lt;country5&gt; | South America | &lt;max_inflation5&gt; | | &lt;country6&gt; | Asia | &lt;max_inflation6&gt; | +++-+ . Again, there are multiple ways to get to this solution using only joins, but the focus here is on showing you an introduction into advanced subqueries. . -- step 1 -- Select fields SELECT name, continent, inflation_rate -- From countries FROM countries -- Join to economies INNER JOIN economies -- Match on code USING (code) -- Where year is 2015 WHERE year = 2015; name continent inflation_rate Afghanistan Asia -1.549 Angola Africa 10.287 Albania Europe 1.896 United Arab Emirates Asia 4.07 . -- step 2 -- Select fields SELECT MAX(inflation_rate) AS max_inf -- Subquery using FROM (alias as subquery) FROM ( SELECT name, continent, inflation_rate FROM countries INNER JOIN economies USING (code) WHERE year = 2015) AS subquery -- Group by continent GROUP BY continent; max_inf 48.684 9.784 39.403 21.858 . -- step 3 -- Select fields SELECT name, continent, inflation_rate -- From countries FROM countries -- Join to economies INNER JOIN economies -- Match on code ON countries.code = economies.code -- Where year is 2015 WHERE year = 2015 AND inflation_rate -- And inflation rate in subquery (alias as subquery) IN ( SELECT MAX(inflation_rate) AS max_inf FROM ( SELECT name, continent, inflation_rate FROM countries INNER JOIN economies ON countries.code = economies.code WHERE year = 2015) AS subquery GROUP BY continent); name continent inflation_rate Haiti North America 7.524 Malawi Africa 21.858 Nauru Oceania 9.784 . #### Subquery challenge . Let’s test your understanding of the subqueries with a challenge problem! Use a subquery to get 2015 economic data for countries that do not have . gov_form of &#39;Constitutional Monarchy&#39; or | &#39;Republic&#39; in their gov_form . | . Here, gov_form stands for the form of the government for each country. Review the different entries for gov_form in the countries table. . -- Select fields SELECT code, inflation_rate, unemployment_rate -- From economies FROM economies -- Where year is 2015 and code is not in WHERE year = 2015 AND code NOT IN -- Subquery (SELECT code FROM countries WHERE (gov_form = &#39;Constitutional Monarchy&#39; OR gov_form LIKE &#39;%Republic%&#39;)) -- Order by inflation rate ORDER BY inflation_rate; code inflation_rate unemployment_rate AFG -1.549 null CHE -1.14 3.178 PRI -0.751 12 ROU -0.596 6.812 . ### Course review . . . . . . . . #### Final challenge . In this exercise, you’ll need to get the country names and other 2015 data in the economies table and the countries table for Central American countries with an official language . . -- Select fields SELECT DISTINCT c.name, e.total_investment, e.imports -- From table (with alias) FROM countries AS c -- Join with table (with alias) LEFT JOIN economies AS e -- Match on code ON (c.code = e.code -- and code in Subquery AND c.code IN ( SELECT l.code FROM languages AS l WHERE official = &#39;true&#39; ) ) -- Where region and year are correct WHERE region = &#39;Central America&#39; AND year = 2015 -- Order by field ORDER BY name; name total_investment imports Belize 22.014 6.743 Costa Rica 20.218 4.629 El Salvador 13.983 8.193 . #### Final challenge (2) . Let’s ease up a bit and calculate the average fertility rate for each region in 2015. . -- Select fields SELECT region, continent, AVG(fertility_rate) AS avg_fert_rate -- From left table FROM countries AS c -- Join to right table INNER JOIN populations AS p -- Match on join condition ON c.code = p.country_code -- Where specific records matching some condition WHERE year = 2015 -- Group appropriately GROUP BY region, continent -- Order appropriately ORDER BY avg_fert_rate; region continent avg_fert_rate Southern Europe Europe 1.42610000371933 Eastern Europe Europe 1.49088890022702 Baltic Countries Europe 1.60333331425985 Eastern Asia Asia 1.62071430683136 . #### Final challenge (3) . You are now tasked with determining the top 10 capital cities in Europe and the Americas in terms of a calculated percentage using city_proper_pop and metroarea_pop in cities . . -- Select fields SELECT name, country_code, city_proper_pop, metroarea_pop, -- Calculate city_perc city_proper_pop / metroarea_pop * 100 AS city_perc -- From appropriate table FROM cities -- Where WHERE name IN -- Subquery (SELECT capital FROM countries WHERE (continent = &#39;Europe&#39; OR continent LIKE &#39;%America&#39;)) AND metroarea_pop IS NOT NULL -- Order appropriately ORDER BY city_perc DESC -- Limit amount LIMIT 10; name country_code city_proper_pop metroarea_pop city_perc Lima PER 8852000 10750000 82.3441863059998 Bogota COL 7878780 9800000 80.3957462310791 Moscow RUS 12197600 16170000 75.4334926605225 . This is the memo of the 18th course of ‘Data Scientist with Python’ track. . You can find the original course HERE . . Further Reading: . More dangerous subtleties of JOINs in SQL — Be careful when JOIN tables with duplications or NULLs . . Introduction to joins . ### Introduction to INNER JOIN . #### Inner join . . SELECT table_name FROM information_schema.tables -- Specify the correct table_schema value WHERE table_schema = &#39;public&#39;; table_name cities countries languages economies currencies populations . SELECT * FROM left_table INNER JOIN right_table ON left_table.id = right_table.id; . -- 1. Select name fields (with alias) and region SELECT cities.name AS city, countries.name AS country, region FROM cities INNER JOIN countries ON cities.country_code = countries.code; city country region Abidjan Cote d&#39;Ivoire Western Africa Abu Dhabi United Arab Emirates Middle East Abuja Nigeria Western Africa . #### Inner join (2) . SELECT c1.name AS city, c2.name AS country FROM cities AS c1 INNER JOIN countries AS c2 ON c1.country_code = c2.code; . -- 3. Select fields with aliases SELECT c.code AS country_code, name, year, inflation_rate FROM countries AS c -- 1. Join to economies (alias e) INNER JOIN economies AS e -- 2. Match on code ON c.code = e.code; . #### Inner join (3) . SELECT * FROM left_table INNER JOIN right_table ON left_table.id = right_table.id INNER JOIN another_table ON left_table.id = another_table.id; . -- 6. Select fields SELECT c.code, name, region, e.year, fertility_rate, unemployment_rate -- 1. From countries (alias as c) FROM countries AS c -- 2. Join to populations (as p) INNER JOIN populations AS p -- 3. Match on country code ON c.code = p.country_code -- 4. Join to economies (as e) INNER JOIN economies AS e -- 5. Match on country code ON c.code = e.code; . -- countries INNER JOIN populations table code name fertility_rate ABW Aruba 1.704 ABW Aruba 1.647 AFG Afghanistan 5.746 AFG Afghanistan 4.653 -- economies table econ_id code year 1 AFG 2010 2 AFG 2015 code name region year fertility_rate unemployment_rate AFG Afghanistan Southern and Central Asia 2010 4.653 null AFG Afghanistan Southern and Central Asia 2010 5.746 null AFG Afghanistan Southern and Central Asia 2015 4.653 null AFG Afghanistan Southern and Central Asia 2015 5.746 null AGO Angola Central Africa 2010 5.996 null . -- 6. Select fields SELECT c.code, name, region, e.year, fertility_rate, unemployment_rate -- 1. From countries (alias as c) FROM countries AS c -- 2. Join to populations (as p) INNER JOIN populations AS p -- 3. Match on country code ON c.code = p.country_code -- 4. Join to economies (as e) INNER JOIN economies AS e -- 5. Match on country code and year ON c.code = e.code AND p.year = e.year; code name region year fertility_rate unemployment_rate AFG Afghanistan Southern and Central Asia 2010 5.746 null AFG Afghanistan Southern and Central Asia 2015 4.653 null . ### INNER JOIN via USING . #### Inner join with using . SELECT * FROM countries INNER JOIN economies ON countries.code = economies.code -- is equal to SELECT * FROM countries INNER JOIN economies USING(code) . -- 4. Select fields SELECT c.name AS country, continent, l.name AS language, official -- 1. From countries (alias as c) FROM countries as c -- 2. Join to languages (as l) INNER JOIN languages as l -- 3. Match using code USING (code) country continent language official Afghanistan Asia Dari true Afghanistan Asia Pashto true . ### Self-ish joins, just in CASE . #### Self-join . . pop_id country_code year fertility_rate life_expectancy size 20 ABW 2010 1.704 74.9535 101597 19 ABW 2015 1.647 75.5736 103889 . -- 4. Select fields with aliases SELECT p1.country_code, p1.size AS size2010, p2.size AS size2015 -- 1. From populations (alias as p1) FROM populations AS p1 -- 2. Join to itself (alias as p2) INNER JOIN populations AS p2 -- 3. Match on country code ON p1.country_code = p2.country_code country_code size2010 size2015 ABW 101597 103889 ABW 101597 101597 ABW 103889 103889 ABW 103889 101597 . -- 5. Select fields with aliases SELECT p1.country_code, p1.size AS size2010, p2.size AS size2015 -- 1. From populations (alias as p1) FROM populations as p1 -- 2. Join to itself (alias as p2) INNER JOIN populations as p2 -- 3. Match on country code ON p1.country_code = p2.country_code -- 4. and year (with calculation) AND p1.year = p2.year - 5 country_code size2010 size2015 ABW 101597 103889 AFG 27962200 32526600 AGO 21220000 25022000 ALB 2913020 2889170 . -- With two numeric fields A and B, the percentage growth from A to B can be calculated as (B−A)/A∗100.0. SELECT p1.country_code, p1.size AS size2010, p2.size AS size2015, -- 1. calculate growth_perc ((p2.size - p1.size)/p1.size * 100.0) AS growth_perc -- 2. From populations (alias as p1) FROM populations AS p1 -- 3. Join to itself (alias as p2) INNER JOIN populations AS p2 -- 4. Match on country code ON p1.country_code = p2.country_code -- 5. and year (with calculation) AND p1.year = p2.year - 5; country_code size2010 size2015 growth_perc ABW 101597 103889 2.25597210228443 AFG 27962200 32526600 16.32329672575 AGO 21220000 25022000 17.9171919822693 ALB 2913020 2889170 -0.818874966353178 . #### Case when and then . SELECT name, continent, code, surface_area, -- 1. First case CASE WHEN surface_area &gt; 2000000 THEN &#39;large&#39; -- 2. Second case WHEN surface_area &gt; 350000 THEN &#39;medium&#39; -- 3. Else clause + end ELSE &#39;small&#39; END -- 4. Alias name AS geosize_group -- 5. From table FROM countries; name continent code surface_area geosize_group Afghanistan Asia AFG 652090 medium Netherlands Europe NLD 41526 small Albania Europe ALB 28748 small . #### Inner challenge . SELECT name, continent, code, surface_area, CASE WHEN surface_area &gt; 2000000 THEN &#39;large&#39; WHEN surface_area &gt; 350000 THEN &#39;medium&#39; ELSE &#39;small&#39; END AS geosize_group INTO countries_plus FROM countries; name continent code surface_area geosize_group Afghanistan Asia AFG 652090 medium Netherlands Europe NLD 41526 small Albania Europe ALB 28748 small Algeria Africa DZA 2381740 large . SELECT country_code, size, -- 1. First case CASE WHEN size &gt; 50000000 THEN &#39;large&#39; -- 2. Second case WHEN size &gt; 1000000 THEN &#39;medium&#39; -- 3. Else clause + end ELSE &#39;small&#39; END -- 4. Alias name AS popsize_group -- 5. From table FROM populations -- 6. Focus on 2015 WHERE year = 2015; country_code size popsize_group ABW 103889 small AFG 32526600 medium AGO 25022000 medium ALB 2889170 medium . SELECT country_code, size, CASE WHEN size &gt; 50000000 THEN &#39;large&#39; WHEN size &gt; 1000000 THEN &#39;medium&#39; ELSE &#39;small&#39; END AS popsize_group -- 1. Into table INTO pop_plus FROM populations WHERE year = 2015; -- 2. Select all columns of pop_plus SELECT * FROM pop_plus; country_code size popsize_group ABW 103889 small AFG 32526600 medium AGO 25022000 medium ALB 2889170 medium . SELECT country_code, size, CASE WHEN size &gt; 50000000 THEN &#39;large&#39; WHEN size &gt; 1000000 THEN &#39;medium&#39; ELSE &#39;small&#39; END AS popsize_group INTO pop_plus FROM populations WHERE year = 2015; -- 5. Select fields SELECT name, continent, geosize_group, popsize_group -- 1. From countries_plus (alias as c) FROM countries_plus AS c -- 2. Join to pop_plus (alias as p) INNER JOIN pop_plus AS p -- 3. Match on country code ON c.code = p.country_code -- 4. Order the table ORDER BY geosize_group; name continent geosize_group popsize_group India Asia large large United States North America large large Saudi Arabia Asia large medium China Asia large large . . Outer joins and cross joins . ### LEFT and RIGHT JOINs . . #### Left Join . -- Select the city name (with alias), the country code, -- the country name (with alias), the region, -- and the city proper population SELECT c1.name AS city, code, c2.name AS country, region, city_proper_pop -- From left table (with alias) FROM cities AS c1 -- Join to right table (with alias) INNER JOIN countries AS c2 -- Match on country code ON c1.country_code = c2.code -- Order by descending country code ORDER BY code DESC; city code country region city_proper_pop Harare ZWE Zimbabwe Eastern Africa 1606000 Lusaka ZMB Zambia Eastern Africa 1742980 Cape Town ZAF South Africa Southern Africa 3740030 -- 230 rows . SELECT c1.name AS city, code, c2.name AS country, region, city_proper_pop FROM cities AS c1 -- 1. Join right table (with alias) LEFT JOIN countries AS c2 -- 2. Match on country code ON c1.country_code = c2.code -- 3. Order by descending country code ORDER BY code DESC; city code country region city_proper_pop Taichung null null null 2752410 Tainan null null null 1885250 Kaohsiung null null null 2778920 Bucharest null null null 1883420 -- 236 rows . #### Left join (2) . /* 5. Select country name AS country, the country&#39;s local name, the language name AS language, and the percent of the language spoken in the country */ SELECT c.name AS country, local_name, l.name AS language, percent -- 1. From left table (alias as c) FROM countries AS c -- 2. Join to right table (alias as l) INNER JOIN languages AS l -- 3. Match on fields ON c.code = l.code -- 4. Order by descending country ORDER BY country DESC; country local_name language percent Zimbabwe Zimbabwe Shona null Zimbabwe Zimbabwe Tonga null Zimbabwe Zimbabwe Tswana null -- 914 rows . /* 5. Select country name AS country, the country&#39;s local name, the language name AS language, and the percent of the language spoken in the country */ SELECT c.name AS country, local_name, l.name AS language, percent -- 1. From left table (alias as c) FROM countries AS c -- 2. Join to right table (alias as l) LEFT JOIN languages AS l -- 3. Match on fields ON c.code = l.code -- 4. Order by descending country ORDER BY country DESC; country local_name language percent Zimbabwe Zimbabwe Chibarwe null Zimbabwe Zimbabwe Shona null Zimbabwe Zimbabwe Ndebele null Zimbabwe Zimbabwe English null -- 921 rows . #### Left join (3) . -- 5. Select name, region, and gdp_percapita SELECT name, region, gdp_percapita -- 1. From countries (alias as c) FROM countries AS c -- 2. Left join with economies (alias as e) LEFT JOIN economies AS e -- 3. Match on code fields ON c.code = e.code -- 4. Focus on 2010 WHERE year = 2010; name region gdp_percapita Afghanistan Southern and Central Asia 539.667 Angola Central Africa 3599.27 Albania Southern Europe 4098.13 United Arab Emirates Middle East 34628.6 . -- Select fields SELECT region, AVG(gdp_percapita) AS avg_gdp -- From countries (alias as c) FROM countries AS c -- Left join with economies (alias as e) LEFT JOIN economies AS e -- Match on code fields ON c.code = e.code -- Focus on 2010 WHERE year = 2010 -- Group by region GROUP BY region; region avg_gdp Southern Africa 5051.59797363281 Australia and New Zealand 44792.384765625 Southeast Asia 10547.1541320801 . -- Select fields SELECT region, AVG(gdp_percapita) AS avg_gdp -- From countries (alias as c) FROM countries AS c -- Left join with economies (alias as e) LEFT JOIN economies AS e -- Match on code fields ON c.code = e.code -- Focus on 2010 WHERE year = 2010 -- Group by region GROUP BY region -- Order by descending avg_gdp ORDER BY avg_gdp DESC; region avg_gdp Western Europe 58130.9614955357 Nordic Countries 57073.99765625 North America 47911.509765625 Australia and New Zealand 44792.384765625 . #### Right join . . -- convert this code to use RIGHT JOINs instead of LEFT JOINs /* SELECT cities.name AS city, urbanarea_pop, countries.name AS country, indep_year, languages.name AS language, percent FROM cities LEFT JOIN countries ON cities.country_code = countries.code LEFT JOIN languages ON countries.code = languages.code ORDER BY city, language; */ SELECT cities.name AS city, urbanarea_pop, countries.name AS country, indep_year, languages.name AS language, percent FROM languages RIGHT JOIN countries ON languages.code = countries.code RIGHT JOIN cities ON countries.code = cities.country_code ORDER BY city, language; city urbanarea_pop country indep_year language percent Abidjan 4765000 Cote d&#39;Ivoire 1960 French null Abidjan 4765000 Cote d&#39;Ivoire 1960 Other null Abu Dhabi 1145000 United Arab Emirates 1971 Arabic null . ### FULL JOINs . #### Full join . . SELECT name AS country, code, region, basic_unit -- 3. From countries FROM countries -- 4. Join to currencies FULL JOIN currencies -- 5. Match on code USING (code) -- 1. Where region is North America or null WHERE region = &#39;North America&#39; OR region IS NULL -- 2. Order by region ORDER BY region; country code region basic_unit Greenland GRL North America null null TMP null United States dollar null FLK null Falkland Islands pound null AIA null East Caribbean dollar null NIU null New Zealand dollar null ROM null Romanian leu null SHN null Saint Helena pound null SGS null British pound null TWN null New Taiwan dollar null WLF null CFP franc null MSR null East Caribbean dollar null IOT null United States dollar null CCK null Australian dollar null COK null New Zealand dollar . SELECT name AS country, code, region, basic_unit -- 1. From countries FROM countries -- 2. Join to currencies LEFT JOIN currencies -- 3. Match on code USING (code) -- 4. Where region is North America or null WHERE region = &#39;North America&#39; OR region IS NULL -- 5. Order by region ORDER BY region; country code region basic_unit Bermuda BMU North America Bermudian dollar Canada CAN North America Canadian dollar United States USA North America United States dollar Greenland GRL North America null . SELECT name AS country, code, region, basic_unit FROM countries -- 1. Join to currencies INNER JOIN currencies USING (code) -- 2. Where region is North America or null WHERE region = &#39;North America&#39; OR region IS NULL -- 3. Order by region ORDER BY region; country code region basic_unit Bermuda BMU North America Bermudian dollar Canada CAN North America Canadian dollar United States USA North America United States dollar . #### Full join (2) . -- FULL JOIN SELECT countries.name, code, languages.name AS language -- 3. From languages FROM languages -- 4. Join to countries FULL JOIN countries -- 5. Match on code USING (code) -- 1. Where countries.name starts with V or is null WHERE countries.name LIKE &#39;V%&#39; OR countries.name IS NULL -- 2. Order by ascending countries.name ORDER BY countries.name; name code language Vanuatu VUT Tribal Languages Vanuatu VUT English Vanuatu VUT French Vanuatu VUT Other -- 53 rows . -- LEFT JOIN SELECT countries.name, code, languages.name AS language FROM languages -- 1. Join to countries LEFT JOIN countries -- 2. Match using code USING (code) -- 3. Where countries.name starts with V or is null WHERE countries.name LIKE &#39;V%&#39; OR countries.name IS NULL ORDER BY countries.name; name code language Vanuatu VUT English Vanuatu VUT Other Vanuatu VUT French -- 51 rows . -- LEFT JOIN SELECT countries.name, code, languages.name AS language FROM languages -- 1. Join to countries INNER JOIN countries USING (code) -- 2. Where countries.name starts with V or is null WHERE countries.name LIKE &#39;V%&#39; OR countries.name IS NULL ORDER BY countries.name; name code language Vanuatu VUT Tribal Languages Vanuatu VUT Bislama Vanuatu VUT English -- 10 rows . #### Full join (3) . -- 7. Select fields (with aliases) SELECT c1.name AS country, region, l.name AS language, basic_unit, frac_unit -- 1. From countries (alias as c1) FROM countries AS c1 -- 2. Join with languages (alias as l) FULL JOIN languages AS l -- 3. Match on code USING (code) -- 4. Join with currencies (alias as c2) FULL JOIN currencies AS c2 -- 5. Match on code USING (code) -- 6. Where region like Melanesia and Micronesia WHERE region LIKE &#39;M%esia&#39;; country region language basic_unit frac_unit Kiribati Micronesia English Australian dollar Cent Kiribati Micronesia Kiribati Australian dollar Cent Marshall Islands Micronesia Other United States dollar Cent Marshall Islands Micronesia Marshallese United States dollar Cent . ### CROSSing the rubicon . . #### A table of two cities . CROSS JOIN . -- 4. Select fields SELECT c.name AS city, l.name AS language -- 1. From cities (alias as c) FROM cities AS c -- 2. Join to languages (alias as l) CROSS JOIN languages AS l -- 3. Where c.name like Hyderabad WHERE c.name LIKE &#39;Hyder%&#39;; city language Hyderabad (India) Dari Hyderabad Dari Hyderabad (India) Pashto Hyderabad Pashto . -- 5. Select fields SELECT c.name AS city, l.name AS language -- 1. From cities (alias as c) FROM cities AS c -- 2. Join to languages (alias as l) INNER JOIN languages AS l -- 3. Match on country code ON c.country_code = l.code -- 4. Where c.name like Hyderabad WHERE c.name LIKE &#39;Hyder%&#39;; city language Hyderabad (India) Hindi Hyderabad (India) Bengali Hyderabad (India) Telugu Hyderabad (India) Marathi . #### Outer challenge . -- Select fields SELECT c.name AS country, region, life_expectancy AS life_exp -- From countries (alias as c) FROM countries as c -- Join to populations (alias as p) LEFT JOIN populations as p -- Match on country code ON c.code = p.country_code -- Focus on 2010 WHERE year = 2010 -- Order by life_exp ORDER BY life_exp -- Limit to 5 records LIMIT 5; . . Set theory clauses . ### State of the UNION . . #### Union . . -- Select fields from 2010 table SELECT * -- From 2010 table FROM economies2010 -- Set theory clause UNION -- Select fields from 2015 table SELECT * -- From 2015 table FROM economies2015 -- Order by code and year ORDER BY code, year; code year income_group gross_savings AFG 2010 Low income 37.133 AFG 2015 Low income 21.466 AGO 2010 Upper middle income 23.534 AGO 2015 Upper middle income -0.425 . #### Union (2) . -- Select field SELECT country_code -- From cities FROM cities -- Set theory clause UNION -- Select field SELECT code AS country_code -- From currencies FROM currencies -- Order by country_code ORDER BY country_code; country_code ABW AFG AGO AIA . #### Union all . . -- Select fields SELECT code, year -- From economies FROM economies -- Set theory clause UNION ALL -- Select fields SELECT country_code AS code, year -- From populations FROM populations -- Order by code, year ORDER BY code, year; code year ABW 2010 ABW 2015 AFG 2010 AFG 2010 . ### INTERSECTional data science . . #### Intersect . -- Select fields SELECT code, year -- From economies FROM economies -- Set theory clause INTERSECT -- Select fields SELECT country_code AS code, year -- From populations FROM populations -- Order by code and year ORDER BY code, year; code year AFG 2010 AFG 2015 AGO 2010 . #### Intersect (2) . -- Select fields SELECT name -- From countries FROM countries -- Set theory clause INTERSECT -- Select fields SELECT name -- From cities FROM cities; name Singapore Hong Kong . Hong Kong is part of China, but it appears separately here because it has its own ISO country code. Depending upon your analysis, treating Hong Kong separately could be useful or a mistake. Always check your dataset closely before you perform an analysis! . ### EXCEPTional . . #### Except . -- Get the names of cities in cities which are not noted as capital cities in countries as a single field result. -- Select field SELECT name -- From cities FROM cities -- Set theory clause EXCEPT -- Select field SELECT capital -- From countries FROM countries -- Order by result ORDER BY name; name Abidjan Ahmedabad Alexandria . #### Except (2) . -- Determine the names of capital cities that are not listed in the cities table. -- Select field SELECT capital -- From countries FROM countries -- Set theory clause EXCEPT -- Select field SELECT name -- From cities FROM cities -- Order by ascending capital ORDER BY capital; capital Agana Amman Amsterdam ... . ### Semi-joins and Anti-joins . . #### Semi-join . -- You are now going to use the concept of a semi-join to identify languages spoken in the Middle East. -- Select distinct fields SELECT DISTINCT name -- From languages FROM languages -- Where in statement WHERE code IN -- Subquery (SELECT code FROM countries WHERE region = &#39;Middle East&#39;) -- Order by name ORDER BY name; . #### Relating semi-join to a tweaked inner join . SELECT DISTINCT name FROM languages WHERE code IN (SELECT code FROM countries WHERE region = &#39;Middle East&#39;) ORDER BY name; -- is equal to SELECT DISTINCT languages.name AS language FROM languages INNER JOIN countries ON languages.code = countries.code WHERE region = &#39;Middle East&#39; ORDER BY language; . #### Diagnosing problems using anti-join . Your goal is to identify the currencies used in Oceanian countries! . -- Begin by determining the number of countries in countries that are listed in Oceania using SELECT, FROM, and WHERE. -- Select statement SELECT COUNT(*) -- From countries FROM countries -- Where continent is Oceania WHERE continent = &#39;Oceania&#39;; count 19 . -- 5. Select fields (with aliases) SELECT c1.code, name, basic_unit AS currency -- 1. From countries (alias as c1) FROM countries AS c1 -- 2. Join with currencies (alias as c2) INNER JOIN currencies c2 -- 3. Match on code USING (code) -- 4. Where continent is Oceania WHERE continent = &#39;Oceania&#39;; code name currency AUS Australia Australian dollar PYF French Polynesia CFP franc KIR Kiribati Australian dollar . -- 3. Select fields SELECT code, name -- 4. From Countries FROM countries -- 5. Where continent is Oceania WHERE continent = &#39;Oceania&#39; -- 1. And code not in AND code NOT IN -- 2. Subquery (SELECT code FROM currencies); code name ASM American Samoa FJI Fiji Islands GUM Guam FSM Micronesia, Federated States of MNP Northern Mariana Islands . #### Set theory challenge . Identify the country codes that are included in either economies or currencies but not in populations . | Use that result to determine the names of cities in the countries that match the specification in the previous instruction. | . -- Select the city name SELECT name -- Alias the table where city name resides FROM cities AS c1 -- Choose only records matching the result of multiple set theory clauses WHERE country_code IN ( -- Select appropriate field from economies AS e SELECT e.code FROM economies AS e -- Get all additional (unique) values of the field from currencies AS c2 UNION SELECT c2.code FROM currencies AS c2 -- Exclude those appearing in populations AS p EXCEPT SELECT p.country_code FROM populations AS p ); . . Subqueries . ### Subqueries inside WHERE and SELECT clauses . #### Subquery inside where . You’ll now try to figure out which countries had high average life expectancies (at the country level) in 2015. . -- Select average life_expectancy SELECT AVG(life_expectancy) -- From populations FROM populations -- Where year is 2015 WHERE year = 2015 avg 71.6763415481105 . -- Select fields SELECT * -- From populations FROM populations -- Where life_expectancy is greater than WHERE life_expectancy &gt; -- 1.15 * subquery 1.15 * (SELECT AVG(life_expectancy) FROM populations WHERE year = 2015) AND year = 2015; pop_id country_code year fertility_rate life_expectancy size 21 AUS 2015 1.833 82.4512 23789800 376 CHE 2015 1.54 83.1976 8281430 356 ESP 2015 1.32 83.3805 46444000 134 FRA 2015 2.01 82.6707 66538400 . #### Subquery inside where (2) . -- 2. Select fields SELECT name, country_code, urbanarea_pop -- 3. From cities FROM cities -- 4. Where city name in the field of capital cities WHERE name IN -- 1. Subquery (SELECT capital FROM countries) ORDER BY urbanarea_pop DESC; name country_code urbanarea_pop Beijing CHN 21516000 Dhaka BGD 14543100 Tokyo JPN 13513700 . #### Subquery inside select . The code selects the top 9 countries in terms of number of cities appearing in the cities table. . SELECT countries.name AS country, COUNT(*) AS cities_num FROM cities INNER JOIN countries ON countries.code = cities.country_code GROUP BY country ORDER BY cities_num DESC, country LIMIT 9; -- is equal to SELECT countries.name AS country, (SELECT COUNT(*) FROM cities WHERE countries.code = cities.country_code) AS cities_num FROM countries ORDER BY cities_num DESC, country LIMIT 9; country cities_num China 36 India 18 Japan 11 . ### Subquery inside FROM clause . . #### Subquery inside from . You will use this to determine the number of languages spoken for each country, identified by the country’s local name! . -- Select fields (with aliases) SELECT code, COUNT(*) AS lang_num -- From languages From languages -- Group by code GROUP BY code; code lang_num BLZ 9 BGD 2 ITA 4 . -- Select fields SELECT local_name, subquery.lang_num -- From countries FROM countries, -- Subquery (alias as subquery) (SELECT code, COUNT(*) AS lang_num From languages GROUP BY code) AS subquery -- Where codes match WHERE countries.code = subquery.code -- Order by descending number of languages ORDER BY lang_num DESC; local_name lang_num Zambia 19 Zimbabwe 16 YeItyop´iya 16 Bharat/India 14 . #### Advanced subquery . You can also nest multiple subqueries to answer even more specific questions. . In this exercise, for each of the six continents listed in 2015, you’ll identify which country had the maximum inflation rate (and how high it was) using multiple subqueries. The table result of your query in Task 3 should look something like the following, where anything between &lt; &gt; will be filled in with appropriate values: . +++-+ | name | continent | inflation_rate | |++-| | &lt;country1&gt; | North America | &lt;max_inflation1&gt; | | &lt;country2&gt; | Africa | &lt;max_inflation2&gt; | | &lt;country3&gt; | Oceania | &lt;max_inflation3&gt; | | &lt;country4&gt; | Europe | &lt;max_inflation4&gt; | | &lt;country5&gt; | South America | &lt;max_inflation5&gt; | | &lt;country6&gt; | Asia | &lt;max_inflation6&gt; | +++-+ . Again, there are multiple ways to get to this solution using only joins, but the focus here is on showing you an introduction into advanced subqueries. . -- step 1 -- Select fields SELECT name, continent, inflation_rate -- From countries FROM countries -- Join to economies INNER JOIN economies -- Match on code USING (code) -- Where year is 2015 WHERE year = 2015; name continent inflation_rate Afghanistan Asia -1.549 Angola Africa 10.287 Albania Europe 1.896 United Arab Emirates Asia 4.07 . -- step 2 -- Select fields SELECT MAX(inflation_rate) AS max_inf -- Subquery using FROM (alias as subquery) FROM ( SELECT name, continent, inflation_rate FROM countries INNER JOIN economies USING (code) WHERE year = 2015) AS subquery -- Group by continent GROUP BY continent; max_inf 48.684 9.784 39.403 21.858 . -- step 3 -- Select fields SELECT name, continent, inflation_rate -- From countries FROM countries -- Join to economies INNER JOIN economies -- Match on code ON countries.code = economies.code -- Where year is 2015 WHERE year = 2015 AND inflation_rate -- And inflation rate in subquery (alias as subquery) IN ( SELECT MAX(inflation_rate) AS max_inf FROM ( SELECT name, continent, inflation_rate FROM countries INNER JOIN economies ON countries.code = economies.code WHERE year = 2015) AS subquery GROUP BY continent); name continent inflation_rate Haiti North America 7.524 Malawi Africa 21.858 Nauru Oceania 9.784 . #### Subquery challenge . Let’s test your understanding of the subqueries with a challenge problem! Use a subquery to get 2015 economic data for countries that do not have . gov_form of &#39;Constitutional Monarchy&#39; or | &#39;Republic&#39; in their gov_form . | . Here, gov_form stands for the form of the government for each country. Review the different entries for gov_form in the countries table. . -- Select fields SELECT code, inflation_rate, unemployment_rate -- From economies FROM economies -- Where year is 2015 and code is not in WHERE year = 2015 AND code NOT IN -- Subquery (SELECT code FROM countries WHERE (gov_form = &#39;Constitutional Monarchy&#39; OR gov_form LIKE &#39;%Republic%&#39;)) -- Order by inflation rate ORDER BY inflation_rate; code inflation_rate unemployment_rate AFG -1.549 null CHE -1.14 3.178 PRI -0.751 12 ROU -0.596 6.812 . ### Course review . . . . . . . . #### Final challenge . In this exercise, you’ll need to get the country names and other 2015 data in the economies table and the countries table for Central American countries with an official language . . -- Select fields SELECT DISTINCT c.name, e.total_investment, e.imports -- From table (with alias) FROM countries AS c -- Join with table (with alias) LEFT JOIN economies AS e -- Match on code ON (c.code = e.code -- and code in Subquery AND c.code IN ( SELECT l.code FROM languages AS l WHERE official = &#39;true&#39; ) ) -- Where region and year are correct WHERE region = &#39;Central America&#39; AND year = 2015 -- Order by field ORDER BY name; name total_investment imports Belize 22.014 6.743 Costa Rica 20.218 4.629 El Salvador 13.983 8.193 . #### Final challenge (2) . Let’s ease up a bit and calculate the average fertility rate for each region in 2015. . -- Select fields SELECT region, continent, AVG(fertility_rate) AS avg_fert_rate -- From left table FROM countries AS c -- Join to right table INNER JOIN populations AS p -- Match on join condition ON c.code = p.country_code -- Where specific records matching some condition WHERE year = 2015 -- Group appropriately GROUP BY region, continent -- Order appropriately ORDER BY avg_fert_rate; region continent avg_fert_rate Southern Europe Europe 1.42610000371933 Eastern Europe Europe 1.49088890022702 Baltic Countries Europe 1.60333331425985 Eastern Asia Asia 1.62071430683136 . #### Final challenge (3) . You are now tasked with determining the top 10 capital cities in Europe and the Americas in terms of a calculated percentage using city_proper_pop and metroarea_pop in cities . . -- Select fields SELECT name, country_code, city_proper_pop, metroarea_pop, -- Calculate city_perc city_proper_pop / metroarea_pop * 100 AS city_perc -- From appropriate table FROM cities -- Where WHERE name IN -- Subquery (SELECT capital FROM countries WHERE (continent = &#39;Europe&#39; OR continent LIKE &#39;%America&#39;)) AND metroarea_pop IS NOT NULL -- Order appropriately ORDER BY city_perc DESC -- Limit amount LIMIT 10; name country_code city_proper_pop metroarea_pop city_perc Lima PER 8852000 10750000 82.3441863059998 Bogota COL 7878780 9800000 80.3957462310791 Moscow RUS 12197600 16170000 75.4334926605225 . This is the memo of the 18th course of ‘Data Scientist with Python’ track. . You can find the original course HERE . . Further Reading: . More dangerous subtleties of JOINs in SQL — Be careful when JOIN tables with duplications or NULLs . . Introduction to joins . ### Introduction to INNER JOIN . #### Inner join . . SELECT table_name FROM information_schema.tables -- Specify the correct table_schema value WHERE table_schema = &#39;public&#39;; table_name cities countries languages economies currencies populations . SELECT * FROM left_table INNER JOIN right_table ON left_table.id = right_table.id; . -- 1. Select name fields (with alias) and region SELECT cities.name AS city, countries.name AS country, region FROM cities INNER JOIN countries ON cities.country_code = countries.code; city country region Abidjan Cote d&#39;Ivoire Western Africa Abu Dhabi United Arab Emirates Middle East Abuja Nigeria Western Africa . #### Inner join (2) . SELECT c1.name AS city, c2.name AS country FROM cities AS c1 INNER JOIN countries AS c2 ON c1.country_code = c2.code; . -- 3. Select fields with aliases SELECT c.code AS country_code, name, year, inflation_rate FROM countries AS c -- 1. Join to economies (alias e) INNER JOIN economies AS e -- 2. Match on code ON c.code = e.code; . #### Inner join (3) . SELECT * FROM left_table INNER JOIN right_table ON left_table.id = right_table.id INNER JOIN another_table ON left_table.id = another_table.id; . -- 6. Select fields SELECT c.code, name, region, e.year, fertility_rate, unemployment_rate -- 1. From countries (alias as c) FROM countries AS c -- 2. Join to populations (as p) INNER JOIN populations AS p -- 3. Match on country code ON c.code = p.country_code -- 4. Join to economies (as e) INNER JOIN economies AS e -- 5. Match on country code ON c.code = e.code; . -- countries INNER JOIN populations table code name fertility_rate ABW Aruba 1.704 ABW Aruba 1.647 AFG Afghanistan 5.746 AFG Afghanistan 4.653 -- economies table econ_id code year 1 AFG 2010 2 AFG 2015 code name region year fertility_rate unemployment_rate AFG Afghanistan Southern and Central Asia 2010 4.653 null AFG Afghanistan Southern and Central Asia 2010 5.746 null AFG Afghanistan Southern and Central Asia 2015 4.653 null AFG Afghanistan Southern and Central Asia 2015 5.746 null AGO Angola Central Africa 2010 5.996 null . -- 6. Select fields SELECT c.code, name, region, e.year, fertility_rate, unemployment_rate -- 1. From countries (alias as c) FROM countries AS c -- 2. Join to populations (as p) INNER JOIN populations AS p -- 3. Match on country code ON c.code = p.country_code -- 4. Join to economies (as e) INNER JOIN economies AS e -- 5. Match on country code and year ON c.code = e.code AND p.year = e.year; code name region year fertility_rate unemployment_rate AFG Afghanistan Southern and Central Asia 2010 5.746 null AFG Afghanistan Southern and Central Asia 2015 4.653 null . ### INNER JOIN via USING . #### Inner join with using . SELECT * FROM countries INNER JOIN economies ON countries.code = economies.code -- is equal to SELECT * FROM countries INNER JOIN economies USING(code) . -- 4. Select fields SELECT c.name AS country, continent, l.name AS language, official -- 1. From countries (alias as c) FROM countries as c -- 2. Join to languages (as l) INNER JOIN languages as l -- 3. Match using code USING (code) country continent language official Afghanistan Asia Dari true Afghanistan Asia Pashto true . ### Self-ish joins, just in CASE . #### Self-join . . pop_id country_code year fertility_rate life_expectancy size 20 ABW 2010 1.704 74.9535 101597 19 ABW 2015 1.647 75.5736 103889 . -- 4. Select fields with aliases SELECT p1.country_code, p1.size AS size2010, p2.size AS size2015 -- 1. From populations (alias as p1) FROM populations AS p1 -- 2. Join to itself (alias as p2) INNER JOIN populations AS p2 -- 3. Match on country code ON p1.country_code = p2.country_code country_code size2010 size2015 ABW 101597 103889 ABW 101597 101597 ABW 103889 103889 ABW 103889 101597 . -- 5. Select fields with aliases SELECT p1.country_code, p1.size AS size2010, p2.size AS size2015 -- 1. From populations (alias as p1) FROM populations as p1 -- 2. Join to itself (alias as p2) INNER JOIN populations as p2 -- 3. Match on country code ON p1.country_code = p2.country_code -- 4. and year (with calculation) AND p1.year = p2.year - 5 country_code size2010 size2015 ABW 101597 103889 AFG 27962200 32526600 AGO 21220000 25022000 ALB 2913020 2889170 . -- With two numeric fields A and B, the percentage growth from A to B can be calculated as (B−A)/A∗100.0. SELECT p1.country_code, p1.size AS size2010, p2.size AS size2015, -- 1. calculate growth_perc ((p2.size - p1.size)/p1.size * 100.0) AS growth_perc -- 2. From populations (alias as p1) FROM populations AS p1 -- 3. Join to itself (alias as p2) INNER JOIN populations AS p2 -- 4. Match on country code ON p1.country_code = p2.country_code -- 5. and year (with calculation) AND p1.year = p2.year - 5; country_code size2010 size2015 growth_perc ABW 101597 103889 2.25597210228443 AFG 27962200 32526600 16.32329672575 AGO 21220000 25022000 17.9171919822693 ALB 2913020 2889170 -0.818874966353178 . #### Case when and then . SELECT name, continent, code, surface_area, -- 1. First case CASE WHEN surface_area &gt; 2000000 THEN &#39;large&#39; -- 2. Second case WHEN surface_area &gt; 350000 THEN &#39;medium&#39; -- 3. Else clause + end ELSE &#39;small&#39; END -- 4. Alias name AS geosize_group -- 5. From table FROM countries; name continent code surface_area geosize_group Afghanistan Asia AFG 652090 medium Netherlands Europe NLD 41526 small Albania Europe ALB 28748 small . #### Inner challenge . SELECT name, continent, code, surface_area, CASE WHEN surface_area &gt; 2000000 THEN &#39;large&#39; WHEN surface_area &gt; 350000 THEN &#39;medium&#39; ELSE &#39;small&#39; END AS geosize_group INTO countries_plus FROM countries; name continent code surface_area geosize_group Afghanistan Asia AFG 652090 medium Netherlands Europe NLD 41526 small Albania Europe ALB 28748 small Algeria Africa DZA 2381740 large . SELECT country_code, size, -- 1. First case CASE WHEN size &gt; 50000000 THEN &#39;large&#39; -- 2. Second case WHEN size &gt; 1000000 THEN &#39;medium&#39; -- 3. Else clause + end ELSE &#39;small&#39; END -- 4. Alias name AS popsize_group -- 5. From table FROM populations -- 6. Focus on 2015 WHERE year = 2015; country_code size popsize_group ABW 103889 small AFG 32526600 medium AGO 25022000 medium ALB 2889170 medium . SELECT country_code, size, CASE WHEN size &gt; 50000000 THEN &#39;large&#39; WHEN size &gt; 1000000 THEN &#39;medium&#39; ELSE &#39;small&#39; END AS popsize_group -- 1. Into table INTO pop_plus FROM populations WHERE year = 2015; -- 2. Select all columns of pop_plus SELECT * FROM pop_plus; country_code size popsize_group ABW 103889 small AFG 32526600 medium AGO 25022000 medium ALB 2889170 medium . SELECT country_code, size, CASE WHEN size &gt; 50000000 THEN &#39;large&#39; WHEN size &gt; 1000000 THEN &#39;medium&#39; ELSE &#39;small&#39; END AS popsize_group INTO pop_plus FROM populations WHERE year = 2015; -- 5. Select fields SELECT name, continent, geosize_group, popsize_group -- 1. From countries_plus (alias as c) FROM countries_plus AS c -- 2. Join to pop_plus (alias as p) INNER JOIN pop_plus AS p -- 3. Match on country code ON c.code = p.country_code -- 4. Order the table ORDER BY geosize_group; name continent geosize_group popsize_group India Asia large large United States North America large large Saudi Arabia Asia large medium China Asia large large . . Outer joins and cross joins . ### LEFT and RIGHT JOINs . . #### Left Join . -- Select the city name (with alias), the country code, -- the country name (with alias), the region, -- and the city proper population SELECT c1.name AS city, code, c2.name AS country, region, city_proper_pop -- From left table (with alias) FROM cities AS c1 -- Join to right table (with alias) INNER JOIN countries AS c2 -- Match on country code ON c1.country_code = c2.code -- Order by descending country code ORDER BY code DESC; city code country region city_proper_pop Harare ZWE Zimbabwe Eastern Africa 1606000 Lusaka ZMB Zambia Eastern Africa 1742980 Cape Town ZAF South Africa Southern Africa 3740030 -- 230 rows . SELECT c1.name AS city, code, c2.name AS country, region, city_proper_pop FROM cities AS c1 -- 1. Join right table (with alias) LEFT JOIN countries AS c2 -- 2. Match on country code ON c1.country_code = c2.code -- 3. Order by descending country code ORDER BY code DESC; city code country region city_proper_pop Taichung null null null 2752410 Tainan null null null 1885250 Kaohsiung null null null 2778920 Bucharest null null null 1883420 -- 236 rows . #### Left join (2) . /* 5. Select country name AS country, the country&#39;s local name, the language name AS language, and the percent of the language spoken in the country */ SELECT c.name AS country, local_name, l.name AS language, percent -- 1. From left table (alias as c) FROM countries AS c -- 2. Join to right table (alias as l) INNER JOIN languages AS l -- 3. Match on fields ON c.code = l.code -- 4. Order by descending country ORDER BY country DESC; country local_name language percent Zimbabwe Zimbabwe Shona null Zimbabwe Zimbabwe Tonga null Zimbabwe Zimbabwe Tswana null -- 914 rows . /* 5. Select country name AS country, the country&#39;s local name, the language name AS language, and the percent of the language spoken in the country */ SELECT c.name AS country, local_name, l.name AS language, percent -- 1. From left table (alias as c) FROM countries AS c -- 2. Join to right table (alias as l) LEFT JOIN languages AS l -- 3. Match on fields ON c.code = l.code -- 4. Order by descending country ORDER BY country DESC; country local_name language percent Zimbabwe Zimbabwe Chibarwe null Zimbabwe Zimbabwe Shona null Zimbabwe Zimbabwe Ndebele null Zimbabwe Zimbabwe English null -- 921 rows . #### Left join (3) . -- 5. Select name, region, and gdp_percapita SELECT name, region, gdp_percapita -- 1. From countries (alias as c) FROM countries AS c -- 2. Left join with economies (alias as e) LEFT JOIN economies AS e -- 3. Match on code fields ON c.code = e.code -- 4. Focus on 2010 WHERE year = 2010; name region gdp_percapita Afghanistan Southern and Central Asia 539.667 Angola Central Africa 3599.27 Albania Southern Europe 4098.13 United Arab Emirates Middle East 34628.6 . -- Select fields SELECT region, AVG(gdp_percapita) AS avg_gdp -- From countries (alias as c) FROM countries AS c -- Left join with economies (alias as e) LEFT JOIN economies AS e -- Match on code fields ON c.code = e.code -- Focus on 2010 WHERE year = 2010 -- Group by region GROUP BY region; region avg_gdp Southern Africa 5051.59797363281 Australia and New Zealand 44792.384765625 Southeast Asia 10547.1541320801 . -- Select fields SELECT region, AVG(gdp_percapita) AS avg_gdp -- From countries (alias as c) FROM countries AS c -- Left join with economies (alias as e) LEFT JOIN economies AS e -- Match on code fields ON c.code = e.code -- Focus on 2010 WHERE year = 2010 -- Group by region GROUP BY region -- Order by descending avg_gdp ORDER BY avg_gdp DESC; region avg_gdp Western Europe 58130.9614955357 Nordic Countries 57073.99765625 North America 47911.509765625 Australia and New Zealand 44792.384765625 . #### Right join . . -- convert this code to use RIGHT JOINs instead of LEFT JOINs /* SELECT cities.name AS city, urbanarea_pop, countries.name AS country, indep_year, languages.name AS language, percent FROM cities LEFT JOIN countries ON cities.country_code = countries.code LEFT JOIN languages ON countries.code = languages.code ORDER BY city, language; */ SELECT cities.name AS city, urbanarea_pop, countries.name AS country, indep_year, languages.name AS language, percent FROM languages RIGHT JOIN countries ON languages.code = countries.code RIGHT JOIN cities ON countries.code = cities.country_code ORDER BY city, language; city urbanarea_pop country indep_year language percent Abidjan 4765000 Cote d&#39;Ivoire 1960 French null Abidjan 4765000 Cote d&#39;Ivoire 1960 Other null Abu Dhabi 1145000 United Arab Emirates 1971 Arabic null . ### FULL JOINs . #### Full join . . SELECT name AS country, code, region, basic_unit -- 3. From countries FROM countries -- 4. Join to currencies FULL JOIN currencies -- 5. Match on code USING (code) -- 1. Where region is North America or null WHERE region = &#39;North America&#39; OR region IS NULL -- 2. Order by region ORDER BY region; country code region basic_unit Greenland GRL North America null null TMP null United States dollar null FLK null Falkland Islands pound null AIA null East Caribbean dollar null NIU null New Zealand dollar null ROM null Romanian leu null SHN null Saint Helena pound null SGS null British pound null TWN null New Taiwan dollar null WLF null CFP franc null MSR null East Caribbean dollar null IOT null United States dollar null CCK null Australian dollar null COK null New Zealand dollar . SELECT name AS country, code, region, basic_unit -- 1. From countries FROM countries -- 2. Join to currencies LEFT JOIN currencies -- 3. Match on code USING (code) -- 4. Where region is North America or null WHERE region = &#39;North America&#39; OR region IS NULL -- 5. Order by region ORDER BY region; country code region basic_unit Bermuda BMU North America Bermudian dollar Canada CAN North America Canadian dollar United States USA North America United States dollar Greenland GRL North America null . SELECT name AS country, code, region, basic_unit FROM countries -- 1. Join to currencies INNER JOIN currencies USING (code) -- 2. Where region is North America or null WHERE region = &#39;North America&#39; OR region IS NULL -- 3. Order by region ORDER BY region; country code region basic_unit Bermuda BMU North America Bermudian dollar Canada CAN North America Canadian dollar United States USA North America United States dollar . #### Full join (2) . -- FULL JOIN SELECT countries.name, code, languages.name AS language -- 3. From languages FROM languages -- 4. Join to countries FULL JOIN countries -- 5. Match on code USING (code) -- 1. Where countries.name starts with V or is null WHERE countries.name LIKE &#39;V%&#39; OR countries.name IS NULL -- 2. Order by ascending countries.name ORDER BY countries.name; name code language Vanuatu VUT Tribal Languages Vanuatu VUT English Vanuatu VUT French Vanuatu VUT Other -- 53 rows . -- LEFT JOIN SELECT countries.name, code, languages.name AS language FROM languages -- 1. Join to countries LEFT JOIN countries -- 2. Match using code USING (code) -- 3. Where countries.name starts with V or is null WHERE countries.name LIKE &#39;V%&#39; OR countries.name IS NULL ORDER BY countries.name; name code language Vanuatu VUT English Vanuatu VUT Other Vanuatu VUT French -- 51 rows . -- LEFT JOIN SELECT countries.name, code, languages.name AS language FROM languages -- 1. Join to countries INNER JOIN countries USING (code) -- 2. Where countries.name starts with V or is null WHERE countries.name LIKE &#39;V%&#39; OR countries.name IS NULL ORDER BY countries.name; name code language Vanuatu VUT Tribal Languages Vanuatu VUT Bislama Vanuatu VUT English -- 10 rows . #### Full join (3) . -- 7. Select fields (with aliases) SELECT c1.name AS country, region, l.name AS language, basic_unit, frac_unit -- 1. From countries (alias as c1) FROM countries AS c1 -- 2. Join with languages (alias as l) FULL JOIN languages AS l -- 3. Match on code USING (code) -- 4. Join with currencies (alias as c2) FULL JOIN currencies AS c2 -- 5. Match on code USING (code) -- 6. Where region like Melanesia and Micronesia WHERE region LIKE &#39;M%esia&#39;; country region language basic_unit frac_unit Kiribati Micronesia English Australian dollar Cent Kiribati Micronesia Kiribati Australian dollar Cent Marshall Islands Micronesia Other United States dollar Cent Marshall Islands Micronesia Marshallese United States dollar Cent . ### CROSSing the rubicon . . #### A table of two cities . CROSS JOIN . -- 4. Select fields SELECT c.name AS city, l.name AS language -- 1. From cities (alias as c) FROM cities AS c -- 2. Join to languages (alias as l) CROSS JOIN languages AS l -- 3. Where c.name like Hyderabad WHERE c.name LIKE &#39;Hyder%&#39;; city language Hyderabad (India) Dari Hyderabad Dari Hyderabad (India) Pashto Hyderabad Pashto . -- 5. Select fields SELECT c.name AS city, l.name AS language -- 1. From cities (alias as c) FROM cities AS c -- 2. Join to languages (alias as l) INNER JOIN languages AS l -- 3. Match on country code ON c.country_code = l.code -- 4. Where c.name like Hyderabad WHERE c.name LIKE &#39;Hyder%&#39;; city language Hyderabad (India) Hindi Hyderabad (India) Bengali Hyderabad (India) Telugu Hyderabad (India) Marathi . #### Outer challenge . -- Select fields SELECT c.name AS country, region, life_expectancy AS life_exp -- From countries (alias as c) FROM countries as c -- Join to populations (alias as p) LEFT JOIN populations as p -- Match on country code ON c.code = p.country_code -- Focus on 2010 WHERE year = 2010 -- Order by life_exp ORDER BY life_exp -- Limit to 5 records LIMIT 5; . . Set theory clauses . ### State of the UNION . . #### Union . . -- Select fields from 2010 table SELECT * -- From 2010 table FROM economies2010 -- Set theory clause UNION -- Select fields from 2015 table SELECT * -- From 2015 table FROM economies2015 -- Order by code and year ORDER BY code, year; code year income_group gross_savings AFG 2010 Low income 37.133 AFG 2015 Low income 21.466 AGO 2010 Upper middle income 23.534 AGO 2015 Upper middle income -0.425 . #### Union (2) . -- Select field SELECT country_code -- From cities FROM cities -- Set theory clause UNION -- Select field SELECT code AS country_code -- From currencies FROM currencies -- Order by country_code ORDER BY country_code; country_code ABW AFG AGO AIA . #### Union all . . -- Select fields SELECT code, year -- From economies FROM economies -- Set theory clause UNION ALL -- Select fields SELECT country_code AS code, year -- From populations FROM populations -- Order by code, year ORDER BY code, year; code year ABW 2010 ABW 2015 AFG 2010 AFG 2010 . ### INTERSECTional data science . . #### Intersect . -- Select fields SELECT code, year -- From economies FROM economies -- Set theory clause INTERSECT -- Select fields SELECT country_code AS code, year -- From populations FROM populations -- Order by code and year ORDER BY code, year; code year AFG 2010 AFG 2015 AGO 2010 . #### Intersect (2) . -- Select fields SELECT name -- From countries FROM countries -- Set theory clause INTERSECT -- Select fields SELECT name -- From cities FROM cities; name Singapore Hong Kong . Hong Kong is part of China, but it appears separately here because it has its own ISO country code. Depending upon your analysis, treating Hong Kong separately could be useful or a mistake. Always check your dataset closely before you perform an analysis! . ### EXCEPTional . . #### Except . -- Get the names of cities in cities which are not noted as capital cities in countries as a single field result. -- Select field SELECT name -- From cities FROM cities -- Set theory clause EXCEPT -- Select field SELECT capital -- From countries FROM countries -- Order by result ORDER BY name; name Abidjan Ahmedabad Alexandria . #### Except (2) . -- Determine the names of capital cities that are not listed in the cities table. -- Select field SELECT capital -- From countries FROM countries -- Set theory clause EXCEPT -- Select field SELECT name -- From cities FROM cities -- Order by ascending capital ORDER BY capital; capital Agana Amman Amsterdam ... . ### Semi-joins and Anti-joins . . #### Semi-join . -- You are now going to use the concept of a semi-join to identify languages spoken in the Middle East. -- Select distinct fields SELECT DISTINCT name -- From languages FROM languages -- Where in statement WHERE code IN -- Subquery (SELECT code FROM countries WHERE region = &#39;Middle East&#39;) -- Order by name ORDER BY name; . #### Relating semi-join to a tweaked inner join . SELECT DISTINCT name FROM languages WHERE code IN (SELECT code FROM countries WHERE region = &#39;Middle East&#39;) ORDER BY name; -- is equal to SELECT DISTINCT languages.name AS language FROM languages INNER JOIN countries ON languages.code = countries.code WHERE region = &#39;Middle East&#39; ORDER BY language; . #### Diagnosing problems using anti-join . Your goal is to identify the currencies used in Oceanian countries! . -- Begin by determining the number of countries in countries that are listed in Oceania using SELECT, FROM, and WHERE. -- Select statement SELECT COUNT(*) -- From countries FROM countries -- Where continent is Oceania WHERE continent = &#39;Oceania&#39;; count 19 . -- 5. Select fields (with aliases) SELECT c1.code, name, basic_unit AS currency -- 1. From countries (alias as c1) FROM countries AS c1 -- 2. Join with currencies (alias as c2) INNER JOIN currencies c2 -- 3. Match on code USING (code) -- 4. Where continent is Oceania WHERE continent = &#39;Oceania&#39;; code name currency AUS Australia Australian dollar PYF French Polynesia CFP franc KIR Kiribati Australian dollar . -- 3. Select fields SELECT code, name -- 4. From Countries FROM countries -- 5. Where continent is Oceania WHERE continent = &#39;Oceania&#39; -- 1. And code not in AND code NOT IN -- 2. Subquery (SELECT code FROM currencies); code name ASM American Samoa FJI Fiji Islands GUM Guam FSM Micronesia, Federated States of MNP Northern Mariana Islands . #### Set theory challenge . Identify the country codes that are included in either economies or currencies but not in populations . | Use that result to determine the names of cities in the countries that match the specification in the previous instruction. | . -- Select the city name SELECT name -- Alias the table where city name resides FROM cities AS c1 -- Choose only records matching the result of multiple set theory clauses WHERE country_code IN ( -- Select appropriate field from economies AS e SELECT e.code FROM economies AS e -- Get all additional (unique) values of the field from currencies AS c2 UNION SELECT c2.code FROM currencies AS c2 -- Exclude those appearing in populations AS p EXCEPT SELECT p.country_code FROM populations AS p ); . . Subqueries . ### Subqueries inside WHERE and SELECT clauses . #### Subquery inside where . You’ll now try to figure out which countries had high average life expectancies (at the country level) in 2015. . -- Select average life_expectancy SELECT AVG(life_expectancy) -- From populations FROM populations -- Where year is 2015 WHERE year = 2015 avg 71.6763415481105 . -- Select fields SELECT * -- From populations FROM populations -- Where life_expectancy is greater than WHERE life_expectancy &gt; -- 1.15 * subquery 1.15 * (SELECT AVG(life_expectancy) FROM populations WHERE year = 2015) AND year = 2015; pop_id country_code year fertility_rate life_expectancy size 21 AUS 2015 1.833 82.4512 23789800 376 CHE 2015 1.54 83.1976 8281430 356 ESP 2015 1.32 83.3805 46444000 134 FRA 2015 2.01 82.6707 66538400 . #### Subquery inside where (2) . -- 2. Select fields SELECT name, country_code, urbanarea_pop -- 3. From cities FROM cities -- 4. Where city name in the field of capital cities WHERE name IN -- 1. Subquery (SELECT capital FROM countries) ORDER BY urbanarea_pop DESC; name country_code urbanarea_pop Beijing CHN 21516000 Dhaka BGD 14543100 Tokyo JPN 13513700 . #### Subquery inside select . The code selects the top 9 countries in terms of number of cities appearing in the cities table. . SELECT countries.name AS country, COUNT(*) AS cities_num FROM cities INNER JOIN countries ON countries.code = cities.country_code GROUP BY country ORDER BY cities_num DESC, country LIMIT 9; -- is equal to SELECT countries.name AS country, (SELECT COUNT(*) FROM cities WHERE countries.code = cities.country_code) AS cities_num FROM countries ORDER BY cities_num DESC, country LIMIT 9; country cities_num China 36 India 18 Japan 11 . ### Subquery inside FROM clause . . #### Subquery inside from . You will use this to determine the number of languages spoken for each country, identified by the country’s local name! . -- Select fields (with aliases) SELECT code, COUNT(*) AS lang_num -- From languages From languages -- Group by code GROUP BY code; code lang_num BLZ 9 BGD 2 ITA 4 . -- Select fields SELECT local_name, subquery.lang_num -- From countries FROM countries, -- Subquery (alias as subquery) (SELECT code, COUNT(*) AS lang_num From languages GROUP BY code) AS subquery -- Where codes match WHERE countries.code = subquery.code -- Order by descending number of languages ORDER BY lang_num DESC; local_name lang_num Zambia 19 Zimbabwe 16 YeItyop´iya 16 Bharat/India 14 . #### Advanced subquery . You can also nest multiple subqueries to answer even more specific questions. . In this exercise, for each of the six continents listed in 2015, you’ll identify which country had the maximum inflation rate (and how high it was) using multiple subqueries. The table result of your query in Task 3 should look something like the following, where anything between &lt; &gt; will be filled in with appropriate values: . +++-+ | name | continent | inflation_rate | |++-| | &lt;country1&gt; | North America | &lt;max_inflation1&gt; | | &lt;country2&gt; | Africa | &lt;max_inflation2&gt; | | &lt;country3&gt; | Oceania | &lt;max_inflation3&gt; | | &lt;country4&gt; | Europe | &lt;max_inflation4&gt; | | &lt;country5&gt; | South America | &lt;max_inflation5&gt; | | &lt;country6&gt; | Asia | &lt;max_inflation6&gt; | +++-+ . Again, there are multiple ways to get to this solution using only joins, but the focus here is on showing you an introduction into advanced subqueries. . -- step 1 -- Select fields SELECT name, continent, inflation_rate -- From countries FROM countries -- Join to economies INNER JOIN economies -- Match on code USING (code) -- Where year is 2015 WHERE year = 2015; name continent inflation_rate Afghanistan Asia -1.549 Angola Africa 10.287 Albania Europe 1.896 United Arab Emirates Asia 4.07 . -- step 2 -- Select fields SELECT MAX(inflation_rate) AS max_inf -- Subquery using FROM (alias as subquery) FROM ( SELECT name, continent, inflation_rate FROM countries INNER JOIN economies USING (code) WHERE year = 2015) AS subquery -- Group by continent GROUP BY continent; max_inf 48.684 9.784 39.403 21.858 . -- step 3 -- Select fields SELECT name, continent, inflation_rate -- From countries FROM countries -- Join to economies INNER JOIN economies -- Match on code ON countries.code = economies.code -- Where year is 2015 WHERE year = 2015 AND inflation_rate -- And inflation rate in subquery (alias as subquery) IN ( SELECT MAX(inflation_rate) AS max_inf FROM ( SELECT name, continent, inflation_rate FROM countries INNER JOIN economies ON countries.code = economies.code WHERE year = 2015) AS subquery GROUP BY continent); name continent inflation_rate Haiti North America 7.524 Malawi Africa 21.858 Nauru Oceania 9.784 . #### Subquery challenge . Let’s test your understanding of the subqueries with a challenge problem! Use a subquery to get 2015 economic data for countries that do not have . gov_form of &#39;Constitutional Monarchy&#39; or | &#39;Republic&#39; in their gov_form . | . Here, gov_form stands for the form of the government for each country. Review the different entries for gov_form in the countries table. . -- Select fields SELECT code, inflation_rate, unemployment_rate -- From economies FROM economies -- Where year is 2015 and code is not in WHERE year = 2015 AND code NOT IN -- Subquery (SELECT code FROM countries WHERE (gov_form = &#39;Constitutional Monarchy&#39; OR gov_form LIKE &#39;%Republic%&#39;)) -- Order by inflation rate ORDER BY inflation_rate; code inflation_rate unemployment_rate AFG -1.549 null CHE -1.14 3.178 PRI -0.751 12 ROU -0.596 6.812 . ### Course review . . . . . . . . #### Final challenge . In this exercise, you’ll need to get the country names and other 2015 data in the economies table and the countries table for Central American countries with an official language . . -- Select fields SELECT DISTINCT c.name, e.total_investment, e.imports -- From table (with alias) FROM countries AS c -- Join with table (with alias) LEFT JOIN economies AS e -- Match on code ON (c.code = e.code -- and code in Subquery AND c.code IN ( SELECT l.code FROM languages AS l WHERE official = &#39;true&#39; ) ) -- Where region and year are correct WHERE region = &#39;Central America&#39; AND year = 2015 -- Order by field ORDER BY name; name total_investment imports Belize 22.014 6.743 Costa Rica 20.218 4.629 El Salvador 13.983 8.193 . #### Final challenge (2) . Let’s ease up a bit and calculate the average fertility rate for each region in 2015. . -- Select fields SELECT region, continent, AVG(fertility_rate) AS avg_fert_rate -- From left table FROM countries AS c -- Join to right table INNER JOIN populations AS p -- Match on join condition ON c.code = p.country_code -- Where specific records matching some condition WHERE year = 2015 -- Group appropriately GROUP BY region, continent -- Order appropriately ORDER BY avg_fert_rate; region continent avg_fert_rate Southern Europe Europe 1.42610000371933 Eastern Europe Europe 1.49088890022702 Baltic Countries Europe 1.60333331425985 Eastern Asia Asia 1.62071430683136 . #### Final challenge (3) . You are now tasked with determining the top 10 capital cities in Europe and the Americas in terms of a calculated percentage using city_proper_pop and metroarea_pop in cities . . -- Select fields SELECT name, country_code, city_proper_pop, metroarea_pop, -- Calculate city_perc city_proper_pop / metroarea_pop * 100 AS city_perc -- From appropriate table FROM cities -- Where WHERE name IN -- Subquery (SELECT capital FROM countries WHERE (continent = &#39;Europe&#39; OR continent LIKE &#39;%America&#39;)) AND metroarea_pop IS NOT NULL -- Order appropriately ORDER BY city_perc DESC -- Limit amount LIMIT 10; name country_code city_proper_pop metroarea_pop city_perc Lima PER 8852000 10750000 82.3441863059998 Bogota COL 7878780 9800000 80.3957462310791 Moscow RUS 12197600 16170000 75.4334926605225 .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/joining-data-in-sql.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/joining-data-in-sql.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "Introduction to TensorFlow in Python",
            "content": "Introduction to TensorFlow in Python . This is the memo of the 14th course (23 courses in all) of ‘Machine Learning Scientist with Python’ skill track. . You can find the original course HERE . . ### Course Description . Not long ago, cutting-edge computer vision algorithms couldn’t differentiate between images of cats and dogs. Today, a skilled data scientist equipped with nothing more than a laptop can classify tens of thousands of objects with greater accuracy than the human eye. In this course, you will use TensorFlow 2.0 to develop, train, and make predictions with the models that have powered major advances in recommendation systems, image classification, and FinTech. You will learn both high-level APIs, which will enable you to design and train deep learning models in 15 lines of code, and low-level APIs, which will allow you to move beyond off-the-shelf routines. You will also learn to accurately predict housing prices, credit card borrower defaults, and images of sign language gestures. . ### . Introduction to TensorFlow | Linear models | Neural Networks | High Level APIs | . 1. Introduction to TensorFlow . . 1.1 Constants and variables . . 1.1.1 Defining data as constants . Throughout this course, we will use tensorflow version 2.0 and will exclusively import the submodules needed to complete each exercise. This will usually be done for you, but you will do it in this exercise by importing constant from tensorflow . . After you have imported constant , you will use it to transform a numpy array, credit_numpy , into a tensorflow constant, credit_constant . This array contains feature columns from a dataset on credit card holders and is previewed in the image below. We will return to this dataset in later chapters. . Note that tensorflow version 2.0 allows you to use data as either a numpy array or a tensorflow constant object. Using a constant will ensure that any operations performed with that object are done in tensorflow . . . # Import constant from TensorFlow from tensorflow import constant # Convert the credit_numpy array into a tensorflow constant credit_constant = constant(credit_numpy) # Print constant datatype print(&#39;The datatype is:&#39;, credit_constant.dtype) # Print constant shape print(&#39;The shape is:&#39;, credit_constant.shape) . The datatype is: &lt;dtype: &#39;float64&#39;&gt; The shape is: (30000, 4) . credit_numpy array([[ 2.0000e+00, 1.0000e+00, 2.4000e+01, 3.9130e+03], [ 2.0000e+00, 2.0000e+00, 2.6000e+01, 2.6820e+03], [ 2.0000e+00, 2.0000e+00, 3.4000e+01, 2.9239e+04], ..., [ 2.0000e+00, 2.0000e+00, 3.7000e+01, 3.5650e+03], [ 3.0000e+00, 1.0000e+00, 4.1000e+01, -1.6450e+03], [ 2.0000e+00, 1.0000e+00, 4.6000e+01, 4.7929e+04]]) credit_constant &lt;tf.Tensor: id=0, shape=(30000, 4), dtype=float64, numpy= array([[ 2.0000e+00, 1.0000e+00, 2.4000e+01, 3.9130e+03], [ 2.0000e+00, 2.0000e+00, 2.6000e+01, 2.6820e+03], [ 2.0000e+00, 2.0000e+00, 3.4000e+01, 2.9239e+04], ..., [ 2.0000e+00, 2.0000e+00, 3.7000e+01, 3.5650e+03], [ 3.0000e+00, 1.0000e+00, 4.1000e+01, -1.6450e+03], [ 2.0000e+00, 1.0000e+00, 4.6000e+01, 4.7929e+04]])&gt; . Excellent! You now understand how constants are used in tensorflow . In the following exercise, you’ll practice defining variables. . 1.1.2 Defining variables . Unlike a constant, a variable’s value can be modified. This will be quite useful when we want to train a model by updating its parameters. Constants can’t be used for this purpose, so variables are the natural choice. . Let’s try defining and working with a variable. Note that Variable() , which is used to create a variable tensor, has been imported from tensorflow and is available to use in the exercise. . from tensorflow import Variable # Define the 1-dimensional variable A1 A1 = Variable([1, 2, 3, 4]) # Print the variable A1 print(A1) # Convert A1 to a numpy array and assign it to B1 B1 = A1.numpy() # Print B1 print(B1) . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; [1 2 3 4] . Nice work! In our next exercise, we’ll review how to check the properties of a tensor after it is already defined. . . 1.2 Basic operations . . 1.2.1 Performing element-wise multiplication . Element-wise multiplication in TensorFlow is performed using two tensors with identical shapes. This is because the operation multiplies elements in corresponding positions in the two tensors. An example of an element-wise multiplication, denoted by the ⊙ symbol, is shown below: . . In this exercise, you will perform element-wise multiplication, paying careful attention to the shape of the tensors you multiply. Note that multiply() , constant() , and ones_like() have been imported for you. . # Define tensors A1 and A23 as constants A1 = constant([1, 2, 3, 4]) A23 = constant([[1, 2, 3], [1, 6, 4]]) # Define B1 and B23 to have the correct shape B1 = ones_like(A1) B23 = ones_like(A23) # Perform element-wise multiplication C1 = multiply(A1,B1) C23 = multiply(A23,B23) # Print the tensors C1 and C23 print(&#39;C1: {}&#39;.format(C1.numpy())) print(&#39;C23: {}&#39;.format(C23.numpy())) . C1: [1 2 3 4] C23: [[1 2 3] [1 6 4]] . ones_like(A1) &lt;tf.Tensor: id=12, shape=(4,), dtype=int32, numpy=array([1, 1, 1, 1], dtype=int32)&gt; ones_like(A23) &lt;tf.Tensor: id=15, shape=(2, 3), dtype=int32, numpy= array([[1, 1, 1], [1, 1, 1]], dtype=int32)&gt; . Excellent work! Notice how performing element-wise multiplication with tensors of ones leaves the original tensors unchanged. . 1.2.2 Making predictions with matrix multiplication . In later chapters, you will learn to train linear regression models. This process will yield a vector of parameters that can be multiplied by the input data to generate predictions. In this exercise, you will use input data, features , and a target vector, bill , which are taken from a credit card dataset we will use later in the course. . . The matrix of input data, features , contains two columns: education level and age. The target vector, bill , is the size of the credit card borrower’s bill. . Since we have not trained the model, you will enter a guess for the values of the parameter vector, params . You will then use matmul() to perform matrix multiplication of features by params to generate predictions, billpred , which you will compare with bill . Note that we have imported matmul() and constant() . . # Define features, params, and bill as constants features = constant([[2, 24], [2, 26], [2, 57], [1, 37]]) params = constant([[1000], [150]]) bill = constant([[3913], [2682], [8617], [64400]]) # Compute billpred using features and params billpred = matmul(features,params) # Compute and print the error error = bill - billpred print(error.numpy()) . [[-1687] [-3218] [-1933] [57850]] billpred &lt;tf.Tensor: id=15, shape=(4, 1), dtype=int32, numpy= array([[ 5600], [ 5900], [10550], [ 6550]], dtype=int32)&gt; . Nice job! Understanding matrix multiplication will make things simpler when we start making predictions with linear models. . 1.2.3 Summing over tensor dimensions . You’ve been given a matrix, wealth . This contains the value of bond and stock wealth for five individuals in thousands of dollars. . . The first column corresponds to bonds and the second corresponds to stocks. Each row gives the bond and stock wealth for a single individual. Use wealth , reduce_sum() , and .numpy() to determine which statements are correct about wealth . . reduce_sum(wealth,0).numpy() # array([ 50, 122], dtype=int32) reduce_sum(wealth,1).numpy() # array([61, 9, 64, 3, 35], dtype=int32) reduce_sum(wealth).numpy() # 172 . Combined, the 5 individuals hold $50,000 in bonds. . Excellent work! Understanding how to sum over tensor dimensions will be helpful when preparing datasets and training models. . . 1.3 Advanced operations . . 1.3.1 Reshaping tensors . Later in the course, you will classify images of sign language letters using a neural network. In some cases, the network will take 1-dimensional tensors as inputs, but your data will come in the form of images, which will either be either 2- or 3-dimensional tensors, depending on whether they are grayscale or color images. . The figure below shows grayscale and color images of the sign language letter A. The two images have been imported for you and converted to the numpy arrays gray_tensor and color_tensor . Reshape these arrays into 1-dimensional vectors using the reshape operation, which has been imported for you from tensorflow . Note that the shape of gray_tensor is 28×28 and the shape of color_tensor is 28x28x3. . . # Reshape the grayscale image tensor into a vector gray_vector = reshape(gray_tensor, (28*28, 1)) # Reshape the color image tensor into a vector color_vector = reshape(color_tensor, (28*28*3, 1)) . Excellent work! Notice that there are 3 times as many elements in color_vector as there are in gray_vector , since color_tensor has 3 color channels. . 1.3.2 Optimizing with gradients . You are given a loss function, y=x2y=x2, which you want to minimize. You can do this by computing the slope using the GradientTape() operation at different values of x . If the slope is positive, you can decrease the loss by lowering x . If it is negative, you can decrease it by increasing x . This is how gradient descent works. . . In practice, you will use a high level tensorflow operation to perform gradient descent automatically. In this exercise, however, you will compute the slope at x values of -1, 1, and 0. The following operations are available: GradientTape() , multiply() , and Variable() . . def compute_gradient(x0): # Define x as a variable with an initial value of x0 x = Variable(x0) with GradientTape() as tape: tape.watch(x) # Define y using the multiply operation y = multiply(x,x) # Return the gradient of y with respect to x return tape.gradient(y, x).numpy() # Compute and print gradients at x = -1, 1, and 0 print(compute_gradient(-1.0)) # -2.0 print(compute_gradient(1.0)) # 2.0 print(compute_gradient(0.0)) # 0.0 . Excellent work! Notice that the slope is positive at x = 1, which means that we can lower the loss by reducing x . The slope is negative at x = -1, which means that we can lower the loss by increasing x . The slope at x = 0 is 0, which means that we cannot lower the loss by either increasing or decreasing x . This is because the loss is minimized at x = 0. . 1.3.3 Working with image data . You are given a black-and-white image of a letter, which has been encoded as a tensor, letter . You want to determine whether the letter is an X or a K. You don’t have a trained neural network, but you do have a simple model, model , which can be used to classify letter . . The 3×3 tensor, letter , and the 1×3 tensor, model , are available in the Python shell. You can determine whether letter is a K by multiplying letter by model , summing over the result, and then checking if it is equal to 1. As with more complicated models, such as neural networks, model is a collection of weights, arranged in a tensor. . Note that the functions reshape() , matmul() , and reduce_sum() have been imported from tensorflow and are available for use. . letter array([[1., 0., 1.], [1., 1., 0.], [1., 0., 1.]], dtype=float32) . # Reshape model from a 1x3 to a 3x1 tensor model = reshape(model, (3, 1)) # Multiply letter by model output = matmul(letter, model) # Sum over output and print prediction using the numpy method prediction = reduce_sum(output) print(prediction.numpy()) # 1.0 . Excellent work! Your model found that prediction =1.0 and correctly classified the letter as a K. In the coming chapters, you will use data to train a model, model , and then combine this with matrix multiplication, matmul(letter, model) , as we have done here, to make predictions about the classes of objects. . 2. Linear models . . 2.1 Input data . . 2.1.1 Load data using pandas . Before you can train a machine learning model, you must first import data. There are several valid ways to do this, but for now, we will use a simple one-liner from pandas : pd.read_csv() . Recall from the video that the first argument specifies the path or URL. All other arguments are optional. . In this exercise, you will import the King County housing dataset, which we will use to train a linear model later in the chapter. . # Import pandas under the alias pd import pandas as pd # Assign the path to a string variable named data_path data_path = &#39;kc_house_data.csv&#39; # Load the dataset as a dataframe named housing housing = pd.read_csv(data_path) # Print the price column of housing print(housing[&#39;price&#39;]) . Excellent work! Notice that you did not have to specify a delimiter with the sep parameter, since the dataset was stored in the default, comma-separated format. . 2.1.2 Setting the data type . In this exercise, you will both load data and set its type. Note that housing is available and pandas has been imported as pd . You will import numpy and tensorflow , and define tensors that are usable in tensorflow using columns in housing with a given data type. Recall that you can select the price column, for instance, from housing using housing[&#39;price&#39;] . . # Import numpy and tensorflow with their standard aliases import numpy as np import tensorflow as tf # Use a numpy array to define price as a 32-bit float price = np.array(housing[&#39;price&#39;], np.float) # Define waterfront as a Boolean using cast waterfront = tf.cast(housing[&#39;waterfront&#39;], tf.bool) # Print price and waterfront print(price) print(waterfront) . [221900. 538000. 180000. ... 402101. 400000. 325000.] tf.Tensor([False False False ... False False False], shape=(21613,), dtype=bool) . Great job! Notice that printing price yielded a numpy array; whereas printing waterfront yielded a tf.Tensor() . . . 2.2 Loss functions . . 2.2.1 Loss functions in TensorFlow . In this exercise, you will compute the loss using data from the King County housing dataset. You are given a target, price , which is a tensor of house prices, and predictions , which is a tensor of predicted house prices. You will evaluate the loss function and print out the value of the loss. . # Import the keras module from tensorflow from tensorflow import keras # Compute the mean squared error (mse) loss = keras.losses.mse(price, predictions) # Print the mean squared error (mse) print(loss.numpy()) # 141171604777.12717 # Compute the mean absolute error (mae) loss = keras.losses.mae(price, predictions) # Print the mean absolute error (mae) print(loss.numpy()) # 268827.99302087986 . Great work! You may have noticed that the MAE was much smaller than the MSE, even though price and predictions were the same. This is because the different loss functions penalize deviations of predictions from price differently. MSE does not like large deviations and punishes them harshly. . 2.2.2 Modifying the loss function . In the previous exercise, you defined a tensorflow loss function and then evaluated it once for a set of actual and predicted values. In this exercise, you will compute the loss within another function called loss_function() , which first generates predicted values from the data and variables. The purpose of this is to construct a function of the trainable model variables that returns the loss. You can then repeatedly evaluate this function for different variable values until you find the minimum. In practice, you will pass this function to an optimizer in tensorflow . Note that features and targets have been defined and are available. Additionally, Variable , float32 , and keras are available. . import tensorflow as tf from tensorflow import Variable from tensorflow import keras # Initialize a variable named scalar scalar = Variable(1.0, tf.float32) # Define the model def model(scalar, features = features): return scalar * features # Define a loss function def loss_function(scalar, features = features, targets = targets): # Compute the predicted values predictions = model(scalar, features) # Return the mean absolute error loss return keras.losses.mae(targets, predictions) # Evaluate the loss function and print the loss print(loss_function(scalar).numpy()) # 3.0 . Great work! As you will see in the following lessons, this exercise was the equivalent of evaluating the loss function for a linear regression where the intercept is 0. . . 2.3 Linear regression . . 2.3.1 Set up a linear regression . A univariate linear regression identifies the relationship between a single feature and the target tensor. In this exercise, we will use a property’s lot size and price. Just as we discussed in the video, we will take the natural logarithms of both tensors, which are available as price_log and size_log . . In this exercise, you will define the model and the loss function. You will then evaluate the loss function for two different values of intercept and slope . Remember that the predicted values are given by intercept + features*slope . Additionally, note that keras.losses.mse() is available for you. Furthermore, slope and intercept have been defined as variables. . # Define a linear regression model def linear_regression(intercept, slope, features = size_log): return intercept + slope*features # Set loss_function() to take the variables as arguments def loss_function(intercept, slope, features = size_log, targets = price_log): # Set the predicted values predictions = linear_regression(intercept, slope, features) # Return the mean squared error loss return keras.losses.mse(targets, predictions) # Compute the loss for different slope and intercept values print(loss_function(0.1, 0.1).numpy()) print(loss_function(0.1, 0.5).numpy()) # 145.44652 # 71.866 . Great work! In the next exercise, you will actually run the regression and train intercept and slope . . 2.3.2 Train a linear model . In this exercise, we will pick up where the previous exercise ended. The intercept and slope, intercept and slope , have been defined and initialized. Additionally, a function has been defined, loss_function(intercept, slope) , which computes the loss using the data and model variables. . You will now define an optimization operation as opt . You will then train a univariate linear model by minimizing the loss to find the optimal values of intercept and slope . Note that the opt operation will try to move closer to the optimum with each step, but will require many steps to find it. Thus, you must repeatedly execute the operation. . # Initialize an adam optimizer opt = keras.optimizers.Adam(0.5) for j in range(100): # Apply minimize, pass the loss function, and supply the variables opt.minimize(lambda: loss_function(intercept, slope), var_list=[intercept, slope]) # Print every 10th value of the loss if j % 10 == 0: print(loss_function(intercept, slope).numpy()) # Plot data and regression line plot_results(intercept, slope) . 9.669481 11.726705 1.1193314 1.6605749 0.7982892 0.8017315 0.6106562 0.59997994 0.5811015 0.5576157 . . Excellent! Notice that we printed loss_function(intercept, slope) every 10th execution for 100 executions. Each time, the loss got closer to the minimum as the optimizer moved the slope and intercept parameters closer to their optimal values. . 2.3.3 Multiple linear regression . In most cases, performing a univariate linear regression will not yield a model that is useful for making accurate predictions. In this exercise, you will perform a multiple regression, which uses more than one feature. . You will use price_log as your target and size_log and bedrooms as your features. Each of these tensors has been defined and is available. You will also switch from using the the mean squared error loss to the mean absolute error loss: keras.losses.mae() . Finally, the predicted values are computed as follows: params[0] + feature1*params[1] + feature2*params[2] . Note that we’ve defined a vector of parameters, params , as a variable, rather than using three variables. Here, params[0] is the intercept and params[1] and params[2] are the slopes. . # Define the linear regression model def linear_regression(params, feature1 = size_log, feature2 = bedrooms): return params[0] + feature1*params[1] + feature2*params[2] # Define the loss function def loss_function(params, targets = price_log, feature1 = size_log, feature2 = bedrooms): # Set the predicted values predictions = linear_regression(params, feature1, feature2) # Use the mean absolute error loss return keras.losses.mae(targets, predictions) # Define the optimize operation opt = keras.optimizers.Adam() # Perform minimization and print trainable variables for j in range(10): opt.minimize(lambda: loss_function(params), var_list=[params]) print_results(params) . loss: 12.418, intercept: 0.101, slope_1: 0.051, slope_2: 0.021 loss: 12.404, intercept: 0.102, slope_1: 0.052, slope_2: 0.022 loss: 12.391, intercept: 0.103, slope_1: 0.053, slope_2: 0.023 loss: 12.377, intercept: 0.104, slope_1: 0.054, slope_2: 0.024 loss: 12.364, intercept: 0.105, slope_1: 0.055, slope_2: 0.025 loss: 12.351, intercept: 0.106, slope_1: 0.056, slope_2: 0.026 loss: 12.337, intercept: 0.107, slope_1: 0.057, slope_2: 0.027 loss: 12.324, intercept: 0.108, slope_1: 0.058, slope_2: 0.028 loss: 12.311, intercept: 0.109, slope_1: 0.059, slope_2: 0.029 loss: 12.297, intercept: 0.110, slope_1: 0.060, slope_2: 0.030 . Great job! Note that params[2] tells us how much the price will increase in percentage terms if we add one more bedroom. You could train params[2] and the other model parameters by increasing the number of times we iterate over opt . . . 2.4 Batch training . . 2.4.1 Preparing to batch train . Before we can train a linear model in batches, we must first define variables, a loss function, and an optimization operation. In this exercise, we will prepare to train a model that will predict price_batch , a batch of house prices, using size_batch , a batch of lot sizes in square feet. In contrast to the previous lesson, we will do this by loading batches of data using pandas , converting it to numpy arrays, and then using it to minimize the loss function in steps. . Variable() , keras() , and float32 have been imported for you. Note that you should not set default argument values for either the model or loss function, since we will generate the data in batches during the training process. . # Define the intercept and slope intercept = Variable(10.0, float32) slope = Variable(0.5, float32) # Define the model def linear_regression(intercept, slope, features): # Define the predicted values return intercept + slope*features # Define the loss function def loss_function(intercept, slope, targets, features): # Define the predicted values predictions = linear_regression(intercept, slope, features) # Define the MSE loss return keras.losses.mse(targets, predictions) . Excellent work! Notice that we did not use default argument values for the input data, features and targets . This is because the input data has not been defined in advance. Instead, with batch training, we will load it during the training process. . 2.4.2 Training a linear model in batches . In this exercise, we will train a linear regression model in batches, starting where we left off in the previous exercise. We will do this by stepping through the dataset in batches and updating the model’s variables, intercept and slope , after each step. This approach will allow us to train with datasets that are otherwise too large to hold in memory. . Note that the loss function, loss_function(intercept, slope, targets, features) , has been defined for you. Additionally, keras has been imported for you and numpy is available as np . The trainable variables should be entered into var_list in the order in which they appear as loss function arguments. . # Initialize adam optimizer opt = keras.optimizers.Adam() # Load data in batches for batch in pd.read_csv(&#39;kc_house_data.csv&#39;, chunksize=100): size_batch = np.array(batch[&#39;sqft_lot&#39;], np.float32) # Extract the price values for the current batch price_batch = np.array(batch[&#39;price&#39;], np.float32) # Complete the loss, fill in the variable list, and minimize opt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch), var_list=[intercept, slope]) # Print trained parameters print(intercept.numpy(), slope.numpy()) # 10.217888 0.7016 . Great work! Batch training will be very useful when you train neural networks, which we will do next. . 3. Neural Networks . . 3.1 Dense layers . . 3.1.1 The linear algebra of dense layers . There are two ways to define a dense layer in tensorflow . The first involves the use of low-level, linear algebraic operations. The second makes use of high-level keras operations. In this exercise, we will use the first method to construct the network shown in the image below. . The input layer contains 3 features — education, marital status, and age — which are available as borrower_features . The hidden layer contains 2 nodes and the output layer contains a single node. . For each layer, you will take the previous layer as an input, initialize a set of weights, compute the product of the inputs and weights, and then apply an activation function. Note that Variable() , ones() , matmul() , and keras() have been imported from tensorflow . . # Initialize bias1 bias1 = Variable(1.0) # Initialize weights1 as 3x2 variable of ones weights1 = Variable(ones((3, 2))) # Perform matrix multiplication of borrower_features and weights1 product1 = matmul(borrower_features,weights1) # Apply sigmoid activation function to product1 + bias1 dense1 = keras.activations.sigmoid(product1 + bias1) # Print shape of dense1 print(&quot; n dense1&#39;s output shape: {}&quot;.format(dense1.shape)) # dense1&#39;s output shape: (1, 2) . # From previous step bias1 = Variable(1.0) weights1 = Variable(ones((3, 2))) product1 = matmul(borrower_features, weights1) dense1 = keras.activations.sigmoid(product1 + bias1) # Initialize bias2 and weights2 bias2 = Variable(1.0) weights2 = Variable(ones((2, 1))) # Perform matrix multiplication of dense1 and weights2 product2 = matmul(dense1, weights2) # Apply activation to product2 + bias2 and print the prediction prediction = keras.activations.sigmoid(product2 + bias2) print(&#39; n prediction: {}&#39;.format(prediction.numpy()[0,0])) print(&#39; n actual: 1&#39;) . prediction: 0.9525741338729858 actual: 1 . Excellent work! Our model produces predicted values in the interval between 0 and 1. For the example we considered, the actual value was 1 and the predicted value was a probability between 0 and 1. This, of course, is not meaningful, since we have not yet trained our model’s parameters. . 3.1.2 The low-level approach with multiple examples . In this exercise, we’ll build further intuition for the low-level approach by constructing the first dense hidden layer for the case where we have multiple examples. We’ll assume the model is trained and the first layer weights, weights1 , and bias, bias1 , are available. We’ll then perform matrix multiplication of the borrower_features tensor by the weights1 variable. Recall that the borrower_features tensor includes education, marital status, and age. Finally, we’ll apply the sigmoid function to the elements of products1 + bias1 , yielding dense1 . . . Note that matmul() and keras() have been imported from tensorflow . . # Compute the product of borrower_features and weights1 products1 = matmul(borrower_features,weights1) # Apply a sigmoid activation function to products1 + bias1 dense1 = keras.activations.sigmoid(products1+bias1) # Print the shapes of borrower_features, weights1, bias1, and dense1 print(&#39; n shape of borrower_features: &#39;, borrower_features.shape) # shape of borrower_features: (5, 3) print(&#39; n shape of weights1: &#39;, weights1.shape) # shape of weights1: (3, 2) print(&#39; n shape of bias1: &#39;, bias1.shape) # shape of bias1: (1,) print(&#39; n shape of dense1: &#39;, dense1.shape) # shape of dense1: (5, 2) . Good job! Note that our input data, borrower_features , is 5×3 because it consists of 5 examples for 3 features. The shape of weights1 is 3×2, as it was in the previous exercise, since it does not depend on the number of examples. Additionally, bias1 is a scalar. Finally, dense1 is 5×2, which means that we can multiply it by the following set of weights, weights2 , which we defined to be 2×1 in the previous exercise. . 3.1.3 Using the dense layer operation . We’ve now seen how to define dense layers in tensorflow using linear algebra. In this exercise, we’ll skip the linear algebra and let keras work out the details. This will allow us to construct the network below, which has 2 hidden layers and 10 features, using less code than we needed for the network with 1 hidden layer and 3 features. . . To construct this network, we’ll need to define three dense layers, each of which takes the previous layer as an input, multiplies it by weights, and applies an activation function. Note that input data has been defined and is available as a 100×10 tensor: borrower_features . Additionally, the keras.layers module is available. . # Define the first dense layer dense1 = keras.layers.Dense(7, activation=&#39;sigmoid&#39;)(borrower_features) # Define a dense layer with 3 output nodes dense2 = keras.layers.Dense(3, activation=&#39;sigmoid&#39;)(dense1) # Define a dense layer with 1 output node predictions = keras.layers.Dense(1, activation=&#39;sigmoid&#39;)(dense2) # Print the shapes of dense1, dense2, and predictions print(&#39; n shape of dense1: &#39;, dense1.shape) # shape of dense1: (100, 7) print(&#39; n shape of dense2: &#39;, dense2.shape) # shape of dense2: (100, 3) print(&#39; n shape of predictions: &#39;, predictions.shape) # shape of predictions: (100, 1) . Great work! With just 8 lines of code, you were able to define 2 dense hidden layers and an output layer. This is the advantage of using high-level operations in tensorflow . Note that each layer has 100 rows because the input data contains 100 examples. . . 3.2 Activation functions . . 3.2.1 Binary classification problems . In this exercise, you will again make use of credit card data. The target variable, default , indicates whether a credit card holder defaults on her payment in the following period. Since there are only two options–default or not–this is a binary classification problem. While the dataset has many features, you will focus on just three: the size of the three latest credit card bills. Finally, you will compute predictions from your untrained network, outputs , and compare those the target variable, default . . The tensor of features has been loaded and is available as bill_amounts . Additionally, the constant() , float32 , and keras.layers.Dense() operations are available. . # Construct input layer from features inputs = constant(bill_amounts) # Define first dense layer dense1 = keras.layers.Dense(3, activation=&#39;relu&#39;)(inputs) # Define second dense layer dense2 = keras.layers.Dense(2, activation=&#39;relu&#39;)(dense1) # Define output layer outputs = keras.layers.Dense(1, activation=&#39;sigmoid&#39;)(dense2) # Print error for first five examples error = default[:5] - outputs.numpy()[:5] print(error) . [[ 0.0000000e+00] [ 3.4570694e-05] [-1.0000000e+00] [-1.0000000e+00] [-1.0000000e+00]] . Excellent work! If you run the code several times, you’ll notice that the errors change each time. This is because you’re using an untrained model with randomly initialized parameters. Furthermore, the errors fall on the interval between -1 and 1 because default is a binary variable that takes on values of 0 and 1 and outputs is a probability between 0 and 1. . 3.2.2 Multiclass classification problems . In this exercise, we expand beyond binary classification to cover multiclass problems. A multiclass problem has targets that can take on three or more values. In the credit card dataset, the education variable can take on 6 different values, each corresponding to a different level of education. We will use that as our target in this exercise and will also expand the feature set from 3 to 10 columns. . As in the previous problem, you will define an input layer, dense layers, and an output layer. You will also print the untrained model’s predictions, which are probabilities assigned to the classes. The tensor of features has been loaded and is available as borrower_features . Additionally, the constant() , float32 , and keras.layers.Dense() operations are available. . import tensorflow as tf # Construct input layer from borrower features inputs = constant(borrower_features,tf.float32) # Define first dense layer dense1 = keras.layers.Dense(10, activation=&#39;sigmoid&#39;)(inputs) # Define second dense layer dense2 = keras.layers.Dense(8, activation=&#39;relu&#39;)(dense1) # Define output layer outputs = keras.layers.Dense(6, activation=&#39;softmax&#39;)(dense2) # Print first five predictions print(outputs.numpy()[:5]) . [[0.17133032 0.16293828 0.14702542 0.17789574 0.16075517 0.18005505] [0.15597914 0.17065835 0.1275746 0.2044413 0.16524555 0.17610106] [0.15597914 0.17065835 0.1275746 0.2044413 0.16524555 0.17610106] [0.17133032 0.16293828 0.14702542 0.17789574 0.16075517 0.18005505] [0.07605464 0.17264706 0.15399623 0.2247733 0.1516134 0.22091544]] . Great work! Notice that each row of outputs sums to one. This is because a row contains the predicted class probabilities for one example. As with the previous exercise, our predictions are not yet informative, since we are using an untrained model with randomly initialized parameters. This is why the model tends to assign similar probabilities to each class. . . 3.3 Optimizers . . 3.3.1 The dangers of local minima . Consider the plot of the following loss function, loss_function() , which contains a global minimum, marked by the dot on the right, and several local minima, including the one marked by the dot on the left. . . In this exercise, you will try to find the global minimum of loss_function() using keras.optimizers.SGD() . You will do this twice, each time with a different initial value of the input to loss_function() . First, you will use x_1 , which is a variable with an initial value of 6.0. Second, you will use x_2 , which is a variable with an initial value of 0.3. Note that loss_function() has been defined and is available. . # Initialize x_1 and x_2 x_1 = Variable(6.0,float32) x_2 = Variable(0.3,float32) # Define the optimization operation opt = keras.optimizers.SGD(learning_rate=0.01) for j in range(100): # Perform minimization using the loss function and x_1 opt.minimize(lambda: loss_function(x_1), var_list=[x_1]) # Perform minimization using the loss function and x_2 opt.minimize(lambda: loss_function(x_2), var_list=[x_2]) # Print x_1 and x_2 as numpy arrays print(x_1.numpy(), x_2.numpy()) # 4.3801394 0.42052683 . Great work! Notice that we used the same optimizer and loss function, but two different initial values. When we started at 6.0 with x_1 , we found the global minimum at 4.38, marked by the dot on the right. When we started at 0.3, we stopped around 0.42 with x_2 , the local minimum marked by a dot on the far left. . 3.3.2 Avoiding local minima . The previous problem showed how easy it is to get stuck in local minima. We had a simple optimization problem in one variable and gradient descent still failed to deliver the global minimum when we had to travel through local minima first. One way to avoid this problem is to use momentum, which allows the optimizer to break through local minima. We will again use the loss function from the previous problem, which has been defined and is available for you as loss_function() . . . Several optimizers in tensorflow have a momentum parameter, including SGD and RMSprop . You will make use of RMSprop in this exercise. Note that x_1 and x_2 have been initialized to the same value this time. Furthermore, keras.optimizers.RMSprop() has also been imported for you from tensorflow . . # Initialize x_1 and x_2 x_1 = Variable(0.05,float32) x_2 = Variable(0.05,float32) # Define the optimization operation for opt_1 and opt_2 opt_1 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.99) opt_2 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.00) for j in range(100): opt_1.minimize(lambda: loss_function(x_1), var_list=[x_1]) # Define the minimization operation for opt_2 opt_2.minimize(lambda: loss_function(x_2), var_list=[x_2]) # Print x_1 and x_2 as numpy arrays print(x_1.numpy(), x_2.numpy()) # 4.3150263 0.4205261 . Good work! Recall that the global minimum is approximately 4.38. Notice that opt_1 built momentum, bringing x_1 closer to the global minimum. To the contrary, opt_2 , which had a momentum parameter of 0.0, got stuck in the local minimum on the left. . . 3.4 Training a network in TensorFlow . . 3.4.1 Initialization in TensorFlow . A good initialization can reduce the amount of time needed to find the global minimum. In this exercise, we will initialize weights and biases for a neural network that will be used to predict credit card default decisions. To build intuition, we will use the low-level, linear algebraic approach, rather than making use of convenience functions and high-level keras operations. We will also expand the set of input features from 3 to 23. Several operations have been imported from tensorflow : Variable() , random() , and ones() . . # Define the layer 1 weights w1 = Variable(random.normal([23, 7])) # Initialize the layer 1 bias b1 = Variable(ones([7])) # Define the layer 2 weights w2 = Variable(random.normal([7, 1])) # Define the layer 2 bias b2 = Variable((0)) . Variable(random.normal([7, 1])) &lt;tf.Variable &#39;Variable:0&#39; shape=(7, 1) dtype=float32, numpy= array([[ 0.654808 ], [ 0.05108023], [-0.4015795 ], [ 0.17105988], [-0.71988714], [ 1.8440487 ], [-0.0194056 ]], dtype=float32)&gt; . Excellent work! In the next exercise, you will start where we’ve ended and will finish constructing the neural network. . 3.4.2 Defining the model and loss function . In this exercise, you will train a neural network to predict whether a credit card holder will default. The features and targets you will use to train your network are available in the Python shell as borrower_features and default . You defined the weights and biases in the previous exercise. . Note that the predictions layer is defined as σ(layer1∗w2+b2)σ(layer1∗w2+b2), where σσ is the sigmoid activation, layer1 is a tensor of nodes for the first hidden dense layer, w2 is a tensor of weights, and b2 is the bias tensor. . The trainable variables are w1 , b1 , w2 , and b2 . Additionally, the following operations have been imported for you: keras.activations.relu() and keras.layers.Dropout() . . # Define the model def model(w1, b1, w2, b2, features = borrower_features): # Apply relu activation functions to layer 1 layer1 = keras.activations.relu(matmul(features, w1) + b1) # Apply dropout dropout = keras.layers.Dropout(0.25)(layer1) return keras.activations.sigmoid(matmul(dropout, w2) + b2) # Define the loss function def loss_function(w1, b1, w2, b2, features = borrower_features, targets = default): predictions = model(w1, b1, w2, b2) # Pass targets and predictions to the cross entropy loss return keras.losses.binary_crossentropy(targets, predictions) . Nice work! One of the benefits of using tensorflow is that you have the option to customize models down to the linear algebraic-level, as we’ve shown in the last two exercises. If you print w1 , you can see that the objects we’re working with are simply tensors. . 3.4.3 Training neural networks with TensorFlow . In the previous exercise, you defined a model, model(w1, b1, w2, b2, features) , and a loss function, loss_function(w1, b1, w2, b2, features, targets) , both of which are available to you in this exercise. You will now train the model and then evaluate its performance by predicting default outcomes in a test set, which consists of test_features and test_targets and is available to you. The trainable variables are w1 , b1 , w2 , and b2 . Additionally, the following operations have been imported for you: keras.activations.relu() and keras.layers.Dropout() . . # Train the model for j in range(100): # Complete the optimizer opt.minimize(lambda: loss_function(w1, b1, w2, b2), var_list=[w1, b1, w2, b2]) # Make predictions with model model_predictions = model(w1, b1, w2, b2, test_features) # Construct the confusion matrix confusion_matrix(test_targets, model_predictions) . . Nice work! The diagram shown is called a “confusion matrix.” The diagonal elements show the number of correct predictions. The off-diagonal elements show the number of incorrect predictions. We can see that the model performs reasonably-well, but does so by overpredicting non-default. This suggests that we may need to train longer, tune the model’s hyperparameters, or change the model’s architecture. . 4. High Level APIs . . 4.1 Defining neural networks with Keras . . 4.1.1 The sequential model in Keras . In chapter 3, we used components of the keras API in tensorflow to define a neural network, but we stopped short of using its full capabilities to streamline model definition and training. In this exercise, you will use the keras sequential model API to define a neural network that can be used to classify images of sign language letters. You will also use the .summary() method to print the model’s architecture, including the shape and number of parameters associated with each layer. . Note that the images were reshaped from (28, 28) to (784,), so that they could be used as inputs to a dense layer. Additionally, note that keras has been imported from tensorflow for you. . # Define a Keras sequential model model = keras.Sequential() # Define the first dense layer model.add(keras.layers.Dense(16, activation=&#39;relu&#39;, input_shape=(784,))) # Define the second dense layer model.add(keras.layers.Dense(8, activation=&#39;relu&#39;)) # Define the output layer model.add(keras.layers.Dense(4, activation=&#39;softmax&#39;)) # Print the model architecture print(model.summary()) . Model: &quot;sequential&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 16) 12560 _________________________________________________________________ dense_1 (Dense) (None, 8) 136 _________________________________________________________________ dense_2 (Dense) (None, 4) 36 ================================================================= Total params: 12,732 Trainable params: 12,732 Non-trainable params: 0 _________________________________________________________________ None . Excellent work! Notice that we’ve defined a model, but we haven’t compiled it. The compilation step in keras allows us to set the optimizer, loss function, and other useful training parameters in a single line of code. Furthermore, the .summary() method allows us to view the model’s architecture. . 4.1.2 Compiling a sequential model . In this exercise, you will work towards classifying letters from the Sign Language MNIST dataset; however, you will adopt a different network architecture than what you used in the previous exercise. There will be fewer layers, but more nodes. You will also apply dropout to prevent overfitting. Finally, you will compile the model to use the adam optimizer and the categorical_crossentropy loss. You will also use a method in keras to summarize your model’s architecture. Note that keras has been imported from tensorflow for you and a sequential keras model has been defined as model . . # Define the first dense layer model.add(keras.layers.Dense(16, activation=&#39;sigmoid&#39;, input_shape=(784,))) # Apply dropout to the first layer&#39;s output model.add(keras.layers.Dropout(0.25)) # Define the output layer model.add(keras.layers.Dense(4, activation=&#39;softmax&#39;)) # Compile the model model.compile(&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;) # Print a model summary print(model.summary()) . Model: &quot;sequential&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 16) 12560 _________________________________________________________________ dense_1 (Dense) (None, 16) 272 _________________________________________________________________ dropout (Dropout) (None, 16) 0 _________________________________________________________________ dense_2 (Dense) (None, 4) 68 ================================================================= Total params: 12,900 Trainable params: 12,900 Non-trainable params: 0 _________________________________________________________________ None . Great work! You’ve now defined and compiled a neural network using the keras sequential model. Notice that printing the .summary() method shows the layer type, output shape, and number of parameters of each layer. . 4.1.3 Defining a multiple input model . In some cases, the sequential API will not be sufficiently flexible to accommodate your desired model architecture and you will need to use the functional API instead. If, for instance, you want to train two models with different architectures jointly, you will need to use the functional API to do this. In this exercise, we will see how to do this. We will also use the .summary() method to examine the joint model’s architecture. . Note that keras has been imported from tensorflow for you. Additionally, the input layers of the first and second models have been defined as m1_inputs and m2_inputs , respectively. Note that the two models have the same architecture, but one of them uses a sigmoid activation in the first layer and the other uses a relu . . # For model 1, pass the input layer to layer 1 and layer 1 to layer 2 m1_layer1 = keras.layers.Dense(12, activation=&#39;sigmoid&#39;)(m1_inputs) m1_layer2 = keras.layers.Dense(4, activation=&#39;softmax&#39;)(m1_layer1) # For model 2, pass the input layer to layer 1 and layer 1 to layer 2 m2_layer1 = keras.layers.Dense(12, activation=&#39;relu&#39;)(m2_inputs) m2_layer2 = keras.layers.Dense(4, activation=&#39;softmax&#39;)(m2_layer1) # Merge model outputs and define a functional model merged = keras.layers.add([m1_layer2, m2_layer2]) model = keras.Model(inputs=[m1_inputs, m2_inputs], outputs=merged) # Print a model summary print(model.summary()) . Model: &quot;model&quot; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 784)] 0 __________________________________________________________________________________________________ input_2 (InputLayer) [(None, 784)] 0 __________________________________________________________________________________________________ dense (Dense) (None, 12) 9420 input_1[0][0] __________________________________________________________________________________________________ dense_2 (Dense) (None, 12) 9420 input_2[0][0] __________________________________________________________________________________________________ dense_1 (Dense) (None, 4) 52 dense[0][0] __________________________________________________________________________________________________ dense_3 (Dense) (None, 4) 52 dense_2[0][0] __________________________________________________________________________________________________ add (Add) (None, 4) 0 dense_1[0][0] dense_3[0][0] ================================================================================================== Total params: 18,944 Trainable params: 18,944 Non-trainable params: 0 __________________________________________________________________________________________________ None . Nice work! Notice that the .summary() method yields a new column: connected to . This column tells you how layers connect to each other within the network. We can see that dense_2 , for instance, is connected to the input_2 layer. We can also see that the add layer, which merged the two models, connected to both dense_1 and dense_3 . . . 4.2 Training and validation with Keras . . 4.2.1 Training with Keras . In this exercise, we return to our sign language letter classification problem. We have 2000 images of four letters–A, B, C, and D–and we want to classify them with a high level of accuracy. We will complete all parts of the problem, including the model definition, compilation, and training. . Note that keras has been imported from tensorflow for you. Additionally, the features are available as sign_language_features and the targets are available as sign_language_labels . . # Define a sequential model model = keras.Sequential() # Define a hidden layer model.add(keras.layers.Dense(16, activation=&#39;relu&#39;, input_shape=(784,))) # Define the output layer model.add(keras.layers.Dense(4, activation=&#39;softmax&#39;)) # Compile the model model.compile(&#39;SGD&#39;, loss=&#39;categorical_crossentropy&#39;) # Complete the fitting operation model.fit(sign_language_features, sign_language_labels, epochs=5) . Train on 1999 samples Epoch 1/5 32/1999 [..............................] - ETA: 29s - loss: 1.6657 ... Epoch 5/5 ... 1999/1999 [==============================] - 0s 92us/sample - loss: 0.4493 . Great work! You probably noticed that your only measure of performance improvement was the value of the loss function in the training sample, which is not particularly informative. You will improve on this in the next exercise. . 4.2.2 Metrics and validation with Keras . We trained a model to predict sign language letters in the previous exercise, but it is unclear how successful we were in doing so. In this exercise, we will try to improve upon the interpretability of our results. Since we did not use a validation split, we only observed performance improvements within the training set; however, it is unclear how much of that was due to overfitting. Furthermore, since we did not supply a metric, we only saw decreases in the loss function, which do not have any clear interpretation. . Note that keras has been imported for you from tensorflow . . # Define sequential model model = keras.Sequential() # Define the first layer model.add(keras.layers.Dense(32, activation=&#39;sigmoid&#39;, input_shape=(784,))) # Add activation function to classifier model.add(keras.layers.Dense(4, activation=&#39;softmax&#39;)) # Set the optimizer, loss function, and metrics model.compile(optimizer=&#39;RMSprop&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) # Add the number of epochs and the validation split model.fit(sign_language_features, sign_language_labels, epochs=10, validation_split=0.1) . Train on 1799 samples, validate on 200 samples Epoch 1/10 32/1799 [..............................] - ETA: 43s - loss: 1.6457 - accuracy: 0.2500 ... Epoch 10/10 ... 1799/1799 [==============================] - 0s 119us/sample - loss: 0.1381 - accuracy: 0.9772 - val_loss: 0.1356 - val_accuracy: 0.9700 . Nice work! With the keras API, you only needed 14 lines of code to define, compile, train, and validate a model. You may have noticed that your model performed quite well. In just 10 epochs, we achieved a classification accuracy of around 98% in the validation sample! . 4.2.3 Overfitting detection . In this exercise, we’ll work with a small subset of the examples from the original sign language letters dataset. A small sample, coupled with a heavily-parameterized model, will generally lead to overfitting. This means that your model will simply memorize the class of each example, rather than identifying features that generalize to many examples. . You will detect overfitting by checking whether the validation sample loss is substantially higher than the training sample loss and whether it increases with further training. With a small sample and a high learning rate, the model will struggle to converge on an optimum. You will set a low learning rate for the optimizer, which will make it easier to identify overfitting. . Note that keras has been imported from tensorflow . . # Define sequential model model = keras.Sequential() # Define the first layer model.add(keras.layers.Dense(1024, activation=&#39;relu&#39;, input_shape=(784,))) # Add activation function to classifier model.add(keras.layers.Dense(4, activation=&#39;softmax&#39;)) # Finish the model compilation model.compile(optimizer=keras.optimizers.Adam(lr=0.01), loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) # Complete the model fit operation model.fit(sign_language_features, sign_language_labels, epochs=200, validation_split=0.5) . Train on 25 samples, validate on 25 samples Epoch 1/200 25/25 [==============================] - 1s 37ms/sample - loss: 1.5469 - accuracy: 0.2000 - val_loss: 48.8668 - val_accuracy: 0.2400 ... Epoch 200/200 25/25 [==============================] - 0s 669us/sample - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.5236 - val_accuracy: 0.8400 . Excellent work! You may have noticed that the validation loss, val_loss , was substantially higher than the training loss, loss . Furthermore, if val_loss started to increase before the training process was terminated, then we may have overfitted. When this happens, you will want to try decreasing the number of epochs. . 4.2.4 Evaluating models . Two models have been trained and are available: large_model , which has many parameters; and small_model , which has fewer parameters. Both models have been trained using train_features and train_labels , which are available to you. A separate test set, which consists of test_features and test_labels , is also available. . Your goal is to evaluate relative model performance and also determine whether either model exhibits signs of overfitting. You will do this by evaluating large_model and small_model on both the train and test sets. For each model, you can do this by applying the .evaluate(x, y) method to compute the loss for features x and labels y . You will then compare the four losses generated. . # Evaluate the small model using the train data small_train = small_model.evaluate(train_features, train_labels) # Evaluate the small model using the test data small_test = small_model.evaluate(test_features, test_labels) # Evaluate the large model using the train data large_train = large_model.evaluate(train_features, train_labels) # Evaluate the large model using the test data large_test = large_model.evaluate(test_features, test_labels) # Print losses print(&#39; n Small - Train: {}, Test: {}&#39;.format(small_train, small_test)) print(&#39;Large - Train: {}, Test: {}&#39;.format(large_train, large_test)) . Small - Train: 0.7137059640884399, Test: 0.8472499084472657 Large - Train: 0.036491363495588305, Test: 0.1792870020866394 . Great job! Notice that the gap between the test and train set losses is substantially higher for large_model , suggesting that overfitting may be an issue. Furthermore, both test and train set performance is better for large_model . This suggests that we may want to use large_model , but reduce the number of training epochs. . . 4.3 Training models with the Estimators API . . 4.3.1 Preparing to train with Estimators . For this exercise, we’ll return to the King County housing transaction dataset from chapter 2. We will again develop and train a machine learning model to predict house prices; however, this time, we’ll do it using the estimator API. . Rather than completing everything in one step, we’ll break this procedure down into parts. We’ll begin by defining the feature columns and loading the data. In the next exercise, we’ll define and train a premade estimator . Note that feature_column has been imported for you from tensorflow . Additionally, numpy has been imported as np , and the Kings County housing dataset is available as a pandas DataFrame : housing . . housing.columns Index([&#39;id&#39;, &#39;date&#39;, &#39;price&#39;, &#39;bedrooms&#39;, &#39;bathrooms&#39;, &#39;sqft_living&#39;, &#39;sqft_lot&#39;, &#39;floors&#39;, &#39;waterfront&#39;, &#39;view&#39;, &#39;condition&#39;, &#39;grade&#39;, &#39;sqft_above&#39;, &#39;sqft_basement&#39;, &#39;yr_built&#39;, &#39;yr_renovated&#39;, &#39;zipcode&#39;, &#39;lat&#39;, &#39;long&#39;, &#39;sqft_living15&#39;, &#39;sqft_lot15&#39;], dtype=&#39;object&#39;) housing.shape (21613, 21) . # Define feature columns for bedrooms and bathrooms bedrooms = feature_column.numeric_column(&quot;bedrooms&quot;) bathrooms = feature_column.numeric_column(&quot;bathrooms&quot;) # Define the list of feature columns feature_list = [bedrooms, bathrooms] def input_fn(): # Define the labels labels = np.array(housing[&#39;price&#39;]) # Define the features features = {&#39;bedrooms&#39;:np.array(housing[&#39;bedrooms&#39;]), &#39;bathrooms&#39;:np.array(housing[&#39;bathrooms&#39;])} return features, labels . Excellent work! In the next exercise, we’ll use the feature columns and data input function to define and train an estimator. . 4.3.2 Defining Estimators . In the previous exercise, you defined a list of feature columns, feature_list , and a data input function, input_fn() . In this exercise, you will build on that work by defining an estimator that makes use of input data. . # Define the model and set the number of steps model = estimator.DNNRegressor(feature_columns=feature_list, hidden_units=[2,2]) model.train(input_fn, steps=1) . INFO:tensorflow:Using default config. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpwdsztbla INFO:tensorflow:Using config: {&#39;_model_dir&#39;: &#39;/tmp/tmpwdsztbla&#39;, &#39;_tf_random_seed&#39;: None, &#39;_save_summary_steps&#39;: 100, &#39;_save_checkpoints_steps&#39;: None, &#39;_save_checkpoints_secs&#39;: 600, &#39;_session_config&#39;: allow_soft_placement: true graph_options { rewrite_options { meta_optimizer_iterations: ONE } } , &#39;_keep_checkpoint_max&#39;: 5, &#39;_keep_checkpoint_every_n_hours&#39;: 10000, &#39;_log_step_count_steps&#39;: 100, &#39;_train_distribute&#39;: None, &#39;_device_fn&#39;: None, &#39;_protocol&#39;: None, &#39;_eval_distribute&#39;: None, &#39;_experimental_distribute&#39;: None, &#39;_experimental_max_worker_delay_secs&#39;: None, &#39;_session_creation_timeout_secs&#39;: 7200, &#39;_service&#39;: None, &#39;_cluster_spec&#39;: ClusterSpec({}), &#39;_task_type&#39;: &#39;worker&#39;, &#39;_task_id&#39;: 0, &#39;_global_id_in_cluster&#39;: 0, &#39;_master&#39;: &#39;&#39;, &#39;_evaluation_master&#39;: &#39;&#39;, &#39;_is_chief&#39;: True, &#39;_num_ps_replicas&#39;: 0, &#39;_num_worker_replicas&#39;: 1} WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating: If using Keras pass *_constraint arguments to layers. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version. Instructions for updating: Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts. INFO:tensorflow:Calling model_fn. WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer&#39;s dtype of float32, which is new behavior in TensorFlow 2. The layer has dtype float32 because it&#39;s dtype defaults to floatx. If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2. To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx(&#39;float64&#39;)`. To change just this layer, pass dtype=&#39;float64&#39; to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/adagrad.py:103: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version. Instructions for updating: Call initializer instance with the dtype argument instead of passing it to the constructor INFO:tensorflow:Done calling model_fn. INFO:tensorflow:Create CheckpointSaverHook. INFO:tensorflow:Graph was finalized. INFO:tensorflow:Running local_init_op. INFO:tensorflow:Done running local_init_op. INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpwdsztbla/model.ckpt. INFO:tensorflow:loss = 426469720000.0, step = 0 INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpwdsztbla/model.ckpt. INFO:tensorflow:Loss for final step: 426469720000.0. . # Define the model and set the number of steps model = estimator.LinearRegressor(feature_columns=feature_list) model.train(input_fn, steps=2) . Great work! Note that you have other premade estimator options, such as BoostedTreesRegressor() , and can also create your own custom estimators. . . ### Congratulations! . . . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/introduction-to-tensorflow-in-python.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/introduction-to-tensorflow-in-python.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post19": {
            "title": "Introduction to Natural Language Processing in Python",
            "content": "Introduction to Natural Language Processing in Python . This is the memo of the 12th course (23 courses in all) of ‘Machine Learning Scientist with Python’ skill track. . You can find the original course HERE . . reference: Natural Language Toolkit . ### Course Description . In this course, you’ll learn natural language processing (NLP) basics, such as how to identify and separate words, how to extract topics in a text, and how to build your own fake news classifier. You’ll also learn how to use basic libraries such as NLTK, alongside libraries which utilize deep learning to solve common NLP problems. This course will give you the foundation to process and parse text as you move forward in your Python learning. . ### . Regular expressions &amp; word tokenization | Simple topic identification | Named-entity recognition | Building a “fake news” classifier | . 1. Regular expressions &amp; word tokenization . . 1.1 Introduction to regular expressions . . 1.1.1 Which pattern? . Which of the following Regex patterns results in the following text? . &gt;&gt;&gt; my_string = &quot;Let&#39;s write RegEx!&quot; &gt;&gt;&gt; re.findall(PATTERN, my_string) [&#39;Let&#39;, &#39;s&#39;, &#39;write&#39;, &#39;RegEx&#39;] . In the IPython Shell, try replacing PATTERN with one of the below options and observe the resulting output. The re module has been pre-imported for you and my_string is available in your namespace. . PATTERN = r&quot; w+&quot; In [2]: re.findall(PATTERN, my_string) Out[2]: [&#39;Let&#39;, &#39;s&#39;, &#39;write&#39;, &#39;RegEx&#39;] . 1.1.2 Practicing regular expressions: re.split() and re.findall() . Now you’ll get a chance to write some regular expressions to match digits, strings and non-alphanumeric characters. Take a look at my_string first by printing it in the IPython Shell, to determine how you might best match the different steps. . Note: It’s important to prefix your regex patterns with r to ensure that your patterns are interpreted in the way you want them to. Else, you may encounter problems to do with escape sequences in strings. For example, &quot; n&quot; in Python is used to indicate a new line, but if you use the r prefix, it will be interpreted as the raw string &quot; n&quot; – that is, the character &quot; &quot; followed by the character &quot;n&quot; – and not as a new line. . The regular expression module re has already been imported for you. . Remember from the video that the syntax for the regex library is to always to pass the pattern first , and then the string second . . my_string &quot;Let&#39;s write RegEx! Won&#39;t that be fun? I sure think so. Can you find 4 sentences? Or perhaps, all 19 words?&quot; . # Write a pattern to match sentence endings: sentence_endings sentence_endings = r&quot;[.?!]&quot; # Split my_string on sentence endings and print the result print(re.split(sentence_endings, my_string)) # [&quot;Let&#39;s write RegEx&quot;, &quot; Won&#39;t that be fun&quot;, &#39; I sure think so&#39;, &#39; Can you find 4 sentences&#39;, &#39; Or perhaps, all 19 words&#39;, &#39;&#39;] # Find all capitalized words in my_string and print the result capitalized_words = r&quot;[A-Z] w+&quot; print(re.findall(capitalized_words, my_string)) # [&#39;Let&#39;, &#39;RegEx&#39;, &#39;Won&#39;, &#39;Can&#39;, &#39;Or&#39;] # Split my_string on spaces and print the result spaces = r&quot; s+&quot; print(re.split(spaces, my_string)) # [&quot;Let&#39;s&quot;, &#39;write&#39;, &#39;RegEx!&#39;, &quot;Won&#39;t&quot;, &#39;that&#39;, &#39;be&#39;, &#39;fun?&#39;, &#39;I&#39;, &#39;sure&#39;, &#39;think&#39;, &#39;so.&#39;, &#39;Can&#39;, &#39;you&#39;, &#39;find&#39;, &#39;4&#39;, &#39;sentences?&#39;, &#39;Or&#39;, &#39;perhaps,&#39;, &#39;all&#39;, &#39;19&#39;, &#39;words?&#39;] # Find all digits in my_string and print the result digits = r&quot; d+&quot; print(re.findall(digits, my_string)) # [&#39;4&#39;, &#39;19&#39;] . . 1.2 Introduction to tokenization . . 1.2.1 Word tokenization with NLTK . Here, you’ll be using the first scene of Monty Python’s Holy Grail, which has been pre-loaded as scene_one . Feel free to check it out in the IPython Shell! . Your job in this exercise is to utilize word_tokenize and sent_tokenize from nltk.tokenize to tokenize both words and sentences from Python strings – in this case, the first scene of Monty Python’s Holy Grail. . scene_one &quot;SCENE 1: [wind] [clop clop clop] nKING ARTHUR: Whoa there! [clop clop clop] nSOLDIER #1: Halt! Who goes there? nARTHUR: It is I, Arthur, son of ... creeper! nSOLDIER #1: What, held under the dorsal guiding feathers? nSOLDIER #2: Well, why not? n&quot; . # Import necessary modules from nltk.tokenize import sent_tokenize from nltk.tokenize import word_tokenize # Split scene_one into sentences: sentences sentences = sent_tokenize(scene_one) # Use word_tokenize to tokenize the fourth sentence: tokenized_sent tokenized_sent = word_tokenize(sentences[3]) # Make a set of unique tokens in the entire scene: unique_tokens unique_tokens = set(word_tokenize(scene_one)) # Print the unique tokens result print(unique_tokens) . {&#39;ridden&#39;, &#39;bird&#39;, &#39;King&#39;, &#39;winter&#39;, &#39;right&#39;, &#39;under&#39;, &#39;needs&#39;, &#39;them&#39;, &#39;with&#39;, &#39;use&#39;, &#39;Mercea&#39;, &#39;simple&#39;, &#39;No&#39;, &#39;!&#39;, &#39;Ridden&#39;, &#39;Pendragon&#39;, ... &#39;minute&#39;, &#39;Whoa&#39;, &#39;...&#39;, &quot;&#39;m&quot;, &#39;[&#39;, &#39;#&#39;, &#39;will&#39;, &quot;&#39;ve&quot;, &#39;an&#39;, &#39;In&#39;, &#39;interested&#39;, &#39;England&#39;, &quot;&#39;re&quot;} . Excellent! Tokenization is fundamental to NLP, and you’ll end up using it a lot in text mining and information retrieval projects. . 1.2.2 More regex with re.search() . In this exercise, you’ll utilize re.search() and re.match() to find specific tokens. Both search and match expect regex patterns, similar to those you defined in an earlier exercise. You’ll apply these regex library methods to the same Monty Python text from the nltk corpora. . You have both scene_one and sentences available from the last exercise; now you can use them with re.search() and re.match() to extract and match more text. . # Search for the first occurrence of &quot;coconuts&quot; in scene_one: match match = re.search(&quot;coconuts&quot;, scene_one) # Print the start and end indexes of match print(match.start(), match.end()) # 580 588 # Write a regular expression to search for anything in square brackets: pattern1 pattern1 = r&quot; [.*]&quot; # Use re.search to find the first text in square brackets print(re.search(pattern1, scene_one)) # &lt;_sre.SRE_Match object; span=(9, 32), match=&#39;[wind] [clop clop clop]&#39;&gt; # Find the script notation at the beginning of the fourth sentence and print it pattern2 = r&quot;[ w s]+:&quot; print(re.match(pattern2, sentences[3])) # &lt;_sre.SRE_Match object; span=(0, 7), match=&#39;ARTHUR:&#39;&gt; . Fantastic work! Now that you’re familiar with the basics of tokenization and regular expressions, it’s time to learn about more advanced tokenization. . . 1.3 Advanced tokenization with NLTK and regex . . 1.3.1 Choosing a tokenizer . Given the following string, which of the below patterns is the best tokenizer? If possible, you want to retain sentence punctuation as separate tokens, but have &#39;#1&#39; remain a single token. . my_string = &quot;SOLDIER #1: Found them? In Mercea? The coconut&#39;s tropical!&quot; . The string is available in your workspace as my_string , and the patterns have been pre-loaded as pattern1 , pattern2 , pattern3 , and pattern4 , respectively. . Additionally, regexp_tokenize has been imported from nltk.tokenize . You can use regexp_tokenize(string, pattern) with my_string and one of the patterns as arguments to experiment for yourself and see which is the best tokenizer. . my_string # &quot;SOLDIER #1: Found them? In Mercea? The coconut&#39;s tropical!&quot; pattern2 # &#39;( w+|# d| ?|!)&#39; . regexp_tokenize(my_string, pattern2) [&#39;SOLDIER&#39;, &#39;#1&#39;, &#39;Found&#39;, &#39;them&#39;, &#39;?&#39;, &#39;In&#39;, &#39;Mercea&#39;, &#39;?&#39;, &#39;The&#39;, &#39;coconut&#39;, &#39;s&#39;, &#39;tropical&#39;, &#39;!&#39;] . 1.3.2 Regex with NLTK tokenization . Twitter is a frequently used source for NLP text and tasks. In this exercise, you’ll build a more complex tokenizer for tweets with hashtags and mentions using nltk and regex. The nltk.tokenize.TweetTokenizer class gives you some extra methods and attributes for parsing tweets. . Here, you’re given some example tweets to parse using both TweetTokenizer and regexp_tokenize from the nltk.tokenize module. These example tweets have been pre-loaded into the variable tweets . Feel free to explore it in the IPython Shell! . Unlike the syntax for the regex library, with nltk_tokenize() you pass the pattern as the second argument. . tweets[0] &#39;This is the best #nlp exercise ive found online! #python&#39; . # Import the necessary modules from nltk.tokenize import regexp_tokenize from nltk.tokenize import TweetTokenizer # Define a regex pattern to find hashtags: pattern1 pattern1 = r&quot;# w+&quot; # Use the pattern on the first tweet in the tweets list hashtags = regexp_tokenize(tweets[0], pattern1) print(hashtags) [&#39;#nlp&#39;, &#39;#python&#39;] . tweets[-1] &#39;Thanks @datacamp 🙂 #nlp #python&#39; . # Import the necessary modules from nltk.tokenize import regexp_tokenize from nltk.tokenize import TweetTokenizer # Write a pattern that matches both mentions (@) and hashtags pattern2 = r&quot;([@#] w+)&quot; # Use the pattern on the last tweet in the tweets list mentions_hashtags = regexp_tokenize(tweets[-1], pattern2) print(mentions_hashtags) # [&#39;@datacamp&#39;, &#39;#nlp&#39;, &#39;#python&#39;] . tweets [&#39;This is the best #nlp exercise ive found online! #python&#39;, &#39;#NLP is super fun! ❤ #learning&#39;, &#39;Thanks @datacamp 🙂 #nlp #python&#39;] . # Import the necessary modules from nltk.tokenize import regexp_tokenize from nltk.tokenize import TweetTokenizer # Use the TweetTokenizer to tokenize all tweets into one list tknzr = TweetTokenizer() all_tokens = [tknzr.tokenize(t) for t in tweets] print(all_tokens) . [[&#39;This&#39;, &#39;is&#39;, &#39;the&#39;, &#39;best&#39;, &#39;#nlp&#39;, &#39;exercise&#39;, &#39;ive&#39;, &#39;found&#39;, &#39;online&#39;, &#39;!&#39;, &#39;#python&#39;], [&#39;#NLP&#39;, &#39;is&#39;, &#39;super&#39;, &#39;fun&#39;, &#39;!&#39;, &#39;&lt;3&#39;, &#39;#learning&#39;], [&#39;Thanks&#39;, &#39;@datacamp&#39;, &#39;:)&#39;, &#39;#nlp&#39;, &#39;#python&#39;]] . 1.3.3 Non-ascii tokenization . In this exercise, you’ll practice advanced tokenization by tokenizing some non-ascii based text. You’ll be using German with emoji! . Here, you have access to a string called german_text , which has been printed for you in the Shell. Notice the emoji and the German characters! . The following modules have been pre-imported from nltk.tokenize : regexp_tokenize and word_tokenize . . Unicode ranges for emoji are: . (&#39; U0001F300&#39;-&#39; U0001F5FF&#39;) , (&#39; U0001F600- U0001F64F&#39;) , (&#39; U0001F680- U0001F6FF&#39;) , and (&#39; u2600&#39;- u26FF- u2700- u27BF&#39;) . . german_text &#39;Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕&#39; . # Tokenize and print all words in german_text all_words = word_tokenize(german_text) print(all_words) [&#39;Wann&#39;, &#39;gehen&#39;, &#39;wir&#39;, &#39;Pizza&#39;, &#39;essen&#39;, &#39;?&#39;, &#39;🍕&#39;, &#39;Und&#39;, &#39;fährst&#39;, &#39;du&#39;, &#39;mit&#39;, &#39;Über&#39;, &#39;?&#39;, &#39;🚕&#39;] # Tokenize and print only capital words capital_words = r&quot;[A-ZÜ] w+&quot; print(regexp_tokenize(german_text, capital_words)) [&#39;Wann&#39;, &#39;Pizza&#39;, &#39;Und&#39;, &#39;Über&#39;] # Tokenize and print only emoji emoji = &quot;[&#39; U0001F300- U0001F5FF&#39;|&#39; U0001F600- U0001F64F&#39;|&#39; U0001F680- U0001F6FF&#39;|&#39; u2600- u26FF u2700- u27BF&#39;]&quot; print(regexp_tokenize(german_text, emoji)) [&#39;🍕&#39;, &#39;🚕&#39;] . 1.4 Charting word length with NLTK . . Charting practice . Try using your new skills to find and chart the number of words per line in the script using matplotlib . The Holy Grail script is loaded for you, and you need to use regex to find the words per line. . Using list comprehensions here will speed up your computations. For example: my_lines = [tokenize(l) for l in lines] will call a function tokenize on each line in the list lines . The new transformed list will be saved in the my_lines variable. . You have access to the entire script in the variable holy_grail . Go for it! . holy_grail &quot;SCENE 1: [wind] [clop clop clop] nKING ARTHUR: Whoa there! [clop clop clop] nSOLDIER #1: Halt! Who goes there? nARTHUR: It is I, Arthur, ... along. nINSPECTOR: Everything? [squeak] nOFFICER #1: All right, sonny. That&#39;s enough. Just pack that in. [crash] nCAMERAMAN: Christ! n&quot; . # Split the script into lines: lines lines = holy_grail.split(&#39; n&#39;) # Replace all script lines for speaker pattern = &quot;[A-Z]{2,}( s)?(# d)?([A-Z]{2,})?:&quot; lines = [re.sub(pattern, &#39;&#39;, l) for l in lines] # Tokenize each line: tokenized_lines tokenized_lines = [regexp_tokenize(s, &#39; w+&#39;) for s in lines] # Make a frequency list of lengths: line_num_words line_num_words = [len(t_line) for t_line in tokenized_lines] # Plot a histogram of the line lengths plt.hist(line_num_words) # Show the plot plt.show() . . 2. Simple topic identification . . 2.1 Word counts with bag-of-words . . 2.1.1 Bag-of-words picker . It’s time for a quick check on your understanding of bag-of-words. Which of the below options, with basic nltk tokenization, map the bag-of-words for the following text? . “The cat is in the box. The cat box.” . (‘The’, 2), (‘box’, 2), (‘.’, 2), (‘cat’, 2), (‘is’, 1), (‘in’, 1), (‘the’, 1) . 2.1.2 Building a Counter with bag-of-words . In this exercise, you’ll build your first (in this course) bag-of-words counter using a Wikipedia article, which has been pre-loaded as article . Try doing the bag-of-words without looking at the full article text, and guessing what the topic is! If you’d like to peek at the title at the end, we’ve included it as article_title . Note that this article text has had very little preprocessing from the raw Wikipedia database entry. . word_tokenize has been imported for you. . # Import Counter from collections import Counter # Tokenize the article: tokens tokens = word_tokenize(article) # Convert the tokens into lowercase: lower_tokens lower_tokens = [t.lower() for t in tokens] # Create a Counter with the lowercase tokens: bow_simple bow_simple = Counter(lower_tokens) # Print the 10 most common tokens print(bow_simple.most_common(10)) # [(&#39;,&#39;, 151), (&#39;the&#39;, 150), (&#39;.&#39;, 89), (&#39;of&#39;, 81), (&quot;&#39;&#39;&quot;, 68), (&#39;to&#39;, 63), (&#39;a&#39;, 60), (&#39;in&#39;, 44), (&#39;and&#39;, 41), (&#39;debugging&#39;, 40)] . . 2.2 Simple text preprocessing . . 2.2.1 Text preprocessing steps . Which of the following are useful text preprocessing steps? . Lemmatization, lowercasing, removing unwanted tokens. . 2.2.2 Text preprocessing practice . Now, it’s your turn to apply the techniques you’ve learned to help clean up text for better NLP results. You’ll need to remove stop words and non-alphabetic characters, lemmatize, and perform a new bag-of-words on your cleaned text. . You start with the same tokens you created in the last exercise: lower_tokens . You also have the Counter class imported. . # Import WordNetLemmatizer from nltk.stem import WordNetLemmatizer # Retain alphabetic words: alpha_only alpha_only = [t for t in lower_tokens if t.isalpha()] # Remove all stop words: no_stops no_stops = [t for t in alpha_only if t not in english_stops] # Instantiate the WordNetLemmatizer wordnet_lemmatizer = WordNetLemmatizer() # Lemmatize all tokens into a new list: lemmatized lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops] # Create the bag-of-words: bow bow = Counter(lemmatized) # Print the 10 most common tokens print(bow.most_common(10)) . [(&#39;debugging&#39;, 40), (&#39;system&#39;, 25), (&#39;software&#39;, 16), (&#39;bug&#39;, 16), (&#39;problem&#39;, 15), (&#39;tool&#39;, 15), (&#39;computer&#39;, 14), (&#39;process&#39;, 13), (&#39;term&#39;, 13), (&#39;used&#39;, 12)] . . 2.3 Introduction to gensim . . 2.3.1 What are word vectors? . What are word vectors and how do they help with NLP? . Word vectors are multi-dimensional mathematical representations of words created using deep learning methods. They give us insight into relationships between words in a corpus. . 2.3.2 Creating and querying a corpus with gensim . It’s time to apply the methods you learned in the previous video to create your first gensim dictionary and corpus! . You’ll use these data structures to investigate word trends and potential interesting topics in your document set. To get started, we have imported a few additional messy articles from Wikipedia, which were preprocessed by lowercasing all words, tokenizing them, and removing stop words and punctuation. These were then stored in a list of document tokens called articles . You’ll need to do some light preprocessing and then generate the gensim dictionary and corpus. . # Import Dictionary from gensim.corpora.dictionary import Dictionary # Create a Dictionary from the articles: dictionary dictionary = Dictionary(articles) # Select the id for &quot;computer&quot;: computer_id computer_id = dictionary.token2id.get(&quot;computer&quot;) # Use computer_id with the dictionary to print the word print(dictionary.get(computer_id)) # computer # Create a MmCorpus: corpus corpus = [dictionary.doc2bow(article) for article in articles] # Print the first 10 word ids with their frequency counts from the fifth document print(corpus[4][:10]) # [(0, 88), (23, 11), (24, 2), (39, 1), (41, 2), (55, 22), (56, 1), (57, 1), (58, 1), (59, 3)] . 2.3.3 Gensim bag-of-words . Now, you’ll use your new gensim corpus and dictionary to see the most common terms per document and across all documents. You can use your dictionary to look up the terms. Take a guess at what the topics are and feel free to explore more documents in the IPython Shell! . You have access to the dictionary and corpus objects you created in the previous exercise, as well as the Python defaultdict and itertools to help with the creation of intermediate data structures for analysis. . defaultdict allows us to initialize a dictionary that will assign a default value to non-existent keys. By supplying the argument int , we are able to ensure that any non-existent keys are automatically assigned a default value of 0 . This makes it ideal for storing the counts of words in this exercise. | itertools.chain.from_iterable() allows us to iterate through a set of sequences as if they were one continuous sequence. Using this function, we can easily iterate through our corpus object (which is a list of lists). | . The fifth document from corpus is stored in the variable doc , which has been sorted in descending order. . # Save the fifth document: doc doc = corpus[4] # Sort the doc for frequency: bow_doc bow_doc = sorted(doc, key=lambda w: w[1], reverse=True) # Print the top 5 words of the document alongside the count for word_id, word_count in bow_doc[:5]: print(dictionary.get(word_id), word_count) # Create the defaultdict: total_word_count total_word_count = defaultdict(int) for word_id, word_count in itertools.chain.from_iterable(corpus): total_word_count[word_id] += word_count . engineering 91 &#39;&#39; 88 reverse 71 software 51 cite 26 total_word_count defaultdict(int, {0: 1042, 1: 1, 2: 1, ... 997: 1, 998: 1, 999: 22, ...}) . # Create a sorted list from the defaultdict: sorted_word_count sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) # Print the top 5 words across all documents alongside the count for word_id, word_count in sorted_word_count[:5]: print(dictionary.get(word_id), word_count) . &#39;&#39; 1042 computer 594 software 450 `` 345 cite 322 . . 2.4 Tf-idf with gensim . . 2.4.1 What is tf-idf? . You want to calculate the tf-idf weight for the word &quot;computer&quot; , which appears five times in a document containing 100 words. Given a corpus containing 200 documents, with 20 documents mentioning the word &quot;computer&quot; , tf-idf can be calculated by multiplying term frequency with inverse document frequency. . Term frequency = percentage share of the word compared to all tokens in the document Inverse document frequency = logarithm of the total number of documents in a corpora divided by the number of documents containing the term . Which of the below options is correct? . (5 / 100) * log(200 / 20) . 2.4.2 Tf-idf with Wikipedia . Now it’s your turn to determine new significant terms for your corpus by applying gensim ‘s tf-idf. You will again have access to the same corpus and dictionary objects you created in the previous exercises – dictionary , corpus , and doc . Will tf-idf make for more interesting results on the document level? . TfidfModel has been imported for you from gensim.models.tfidfmodel . . # Create a new TfidfModel using the corpus: tfidf tfidf = TfidfModel(corpus) # Calculate the tfidf weights of doc: tfidf_weights tfidf_weights = tfidf[doc] # Print the first five weights print(tfidf_weights[:5]) # [(24, 0.0022836332291091273), (39, 0.0043409401554717324), (41, 0.008681880310943465), (55, 0.011988285029371418), (56, 0.005482756770026296)] . corpus [[(0, 50), (1, 1), (2, 1), ... (10325, 1), (10326, 1), (10327, 1)]] tfidf &lt;gensim.models.tfidfmodel.TfidfModel at 0x7f10f5755978&gt; doc [(0, 88), (23, 11), (24, 2), ... (3627, 1), (3628, 2), ...] tfidf_weights [(24, 0.0022836332291091273), (39, 0.0043409401554717324), (41, 0.008681880310943465), (55, 0.011988285029371418), ... . # Sort the weights from highest to lowest: sorted_tfidf_weights sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True) # Print the top 5 weighted words for term_id, weight in sorted_tfidf_weights[:5]: print(dictionary.get(term_id), weight) reverse 0.4884961428651127 infringement 0.18674529210288995 engineering 0.16395041814479536 interoperability 0.12449686140192663 reverse-engineered 0.12449686140192663 . sorted_tfidf_weights [(1535, 0.4884961428651127), (3796, 0.18674529210288995), (350, 0.16395041814479536), (3804, 0.12449686140192663), ... . 3. Named-entity recognition . . 3.1 Named Entity Recognition . . 3.1.1 NER with NLTK . You’re now going to have some fun with named-entity recognition! A scraped news article has been pre-loaded into your workspace. Your task is to use nltk to find the named entities in this article. . What might the article be about, given the names you found? . Along with nltk , sent_tokenize and word_tokenize from nltk.tokenize have been pre-imported. . # Tokenize the article into sentences: sentences sentences = sent_tokenize(article) # Tokenize each sentence into words: token_sentences token_sentences = [word_tokenize(sent) for sent in sentences] # Tag each tokenized sentence into parts of speech: pos_sentences pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] # Create the named entity chunks: chunked_sentences chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True) # Test for stems of the tree with &#39;NE&#39; tags for sent in chunked_sentences: for chunk in sent: if hasattr(chunk, &quot;label&quot;) and chunk.label() == &quot;NE&quot;: print(chunk) . (NE Uber/NNP) (NE Beyond/NN) (NE Apple/NNP) (NE Uber/NNP) (NE Uber/NNP) (NE Travis/NNP Kalanick/NNP) (NE Tim/NNP Cook/NNP) (NE Apple/NNP) (NE Silicon/NNP Valley/NNP) (NE CEO/NNP) (NE Yahoo/NNP) (NE Marissa/NNP Mayer/NNP) . 3.1.2 Charting practice . In this exercise, you’ll use some extracted named entities and their groupings from a series of newspaper articles to chart the diversity of named entity types in the articles. . You’ll use a defaultdict called ner_categories , with keys representing every named entity group type, and values to count the number of each different named entity type. You have a chunked sentence list called chunked_sentences similar to the last exercise, but this time with non-binary category names. . You can use hasattr() to determine if each chunk has a &#39;label&#39; and then simply use the chunk’s .label() method as the dictionary key. . type(chunked_sentences) list chunked_sentences [Tree(&#39;S&#39;, [(&#39; ufeffImage&#39;, &#39;NN&#39;), (&#39;copyright&#39;, &#39;NN&#39;), Tree(&#39;ORGANIZATION&#39;, [(&#39;EPA&#39;, &#39;NNP&#39;), (&#39;Image&#39;, &#39;NNP&#39;)]), (&#39;caption&#39;, &#39;NN&#39;), (&#39;Uber&#39;, &#39;NNP&#39;), (&#39;has&#39;, &#39;VBZ&#39;), (&#39;been&#39;, &#39;VBN&#39;), (&#39;criticised&#39;, &#39;VBN&#39;), (&#39;many&#39;, &#39;JJ&#39;), (&#39;times&#39;, &#39;NNS&#39;), (&#39;over&#39;, &#39;IN&#39;), (&#39;the&#39;, &#39;DT&#39;), (&#39;way&#39;, &#39;NN&#39;), (&#39;it&#39;, &#39;PRP&#39;), (&#39;runs&#39;, &#39;VBZ&#39;), (&#39;its&#39;, &#39;PRP$&#39;), (&#39;business&#39;, &#39;NN&#39;), (&#39;Ride-sharing&#39;, &#39;JJ&#39;), (&#39;firm&#39;, &#39;NN&#39;), (&#39;Uber&#39;, &#39;NNP&#39;), (&#39;is&#39;, &#39;VBZ&#39;), (&#39;facing&#39;, &#39;VBG&#39;), (&#39;a&#39;, &#39;DT&#39;), (&#39;criminal&#39;, &#39;JJ&#39;), (&#39;investigation&#39;, &#39;NN&#39;), (&#39;by&#39;, &#39;IN&#39;), (&#39;the&#39;, &#39;DT&#39;), Tree(&#39;GPE&#39;, [(&#39;US&#39;, &#39;JJ&#39;)]), (&#39;government&#39;, &#39;NN&#39;), (&#39;.&#39;, &#39;.&#39;)]), . # Create the defaultdict: ner_categories ner_categories = defaultdict(int) # Create the nested for loop for sent in chunked_sentences: for chunk in sent: if hasattr(chunk, &#39;label&#39;): ner_categories[chunk.label()] += 1 # Create a list from the dictionary keys for the chart labels: labels labels = list(ner_categories.keys()) labels # [&#39;ORGANIZATION&#39;, &#39;GPE&#39;, &#39;PERSON&#39;, &#39;LOCATION&#39;, &#39;FACILITY&#39;] . # Create a list of the values: values values = [ner_categories.get(v) for v in labels] # Create the pie chart plt.pie(values, labels=labels, autopct=&#39;%1.1f%%&#39;, startangle=140) # Display the chart plt.show() . . 3.1.3 Stanford library with NLTK . When using the Stanford library with NLTK, what is needed to get started? . NLTK, the Stanford Java Libraries and some environment variables to help with integration. . . 3.2 Introduction to SpaCy . . 3.2.1 Comparing NLTK with spaCy NER . Using the same text you used in the first exercise of this chapter, you’ll now see the results using spaCy’s NER annotator. How will they compare? . The article has been pre-loaded as article . To minimize execution times, you’ll be asked to specify the keyword arguments tagger=False, parser=False, matcher=False when loading the spaCy model, because you only care about the entity in this exercise. . # Import spacy import spacy # Instantiate the English model: nlp nlp = spacy.load(&#39;en&#39;,tagger=False, parser=False, matcher=False) # Create a new document: doc doc = nlp(article) # Print all of the found entities and their labels for ent in doc.ents: print(ent.label_, ent.text) . ORG Uber ORG Uber ORG Apple ORG Uber ORG Uber PERSON Travis Kalanick ORG Uber PERSON Tim Cook ORG Apple CARDINAL Millions ORG Uber GPE drivers’ LOC Silicon Valley’s ORG Yahoo PERSON Marissa Mayer MONEY $186m . 3.2.2 spaCy NER Categories . Which are the extra categories that spacy uses compared to nltk in its named-entity recognition? . NORP, CARDINAL, MONEY, WORK OF ART, LANGUAGE, EVENT . . 3.3 Multilingual NER with polyglot . . #### 3.3.1 French NER with polyglot I . In this exercise and the next, you’ll use the polyglot library to identify French entities. The library functions slightly differently than spacy , so you’ll use a few of the new things you learned in the last video to display the named entity text and category. . You have access to the full article string in article . Additionally, the Text class of polyglot has been imported from polyglot.text . . # Create a new text object using Polyglot&#39;s Text class: txt txt = Text(article) # Print each of the entities found for ent in txt.entities: print(ent) # Print the type of ent print(type(ent)) . [&#39;Charles&#39;, &#39;Cuvelliez&#39;] [&#39;Charles&#39;, &#39;Cuvelliez&#39;] [&#39;Bruxelles&#39;] [&#39;l’IA&#39;] [&#39;Julien&#39;, &#39;Maldonato&#39;] [&#39;Deloitte&#39;] [&#39;Ethiquement&#39;] [&#39;l’IA&#39;] [&#39;.&#39;] &lt;class &#39;polyglot.text.Chunk&#39;&gt; . 3.3.2 French NER with polyglot II . Here, you’ll complete the work you began in the previous exercise. . Your task is to use a list comprehension to create a list of tuples, in which the first element is the entity tag, and the second element is the full string of the entity text. . # Create the list of tuples: entities entities = [(ent.tag, &#39; &#39;.join(ent)) for ent in txt.entities] # Print entities print(entities) . [(&#39;I-PER&#39;, &#39;Charles Cuvelliez&#39;), (&#39;I-PER&#39;, &#39;Charles Cuvelliez&#39;), (&#39;I-ORG&#39;, &#39;Bruxelles&#39;), (&#39;I-PER&#39;, &#39;l’IA&#39;), (&#39;I-PER&#39;, &#39;Julien Maldonato&#39;), (&#39;I-ORG&#39;, &#39;Deloitte&#39;), (&#39;I-PER&#39;, &#39;Ethiquement&#39;), (&#39;I-LOC&#39;, &#39;l’IA&#39;), (&#39;I-PER&#39;, &#39;.&#39;)] . 3.3.3 Spanish NER with polyglot . You’ll continue your exploration of polyglot now with some Spanish annotation. This article is not written by a newspaper, so it is your first example of a more blog-like text. How do you think that might compare when finding entities? . The Text object has been created as txt , and each entity has been printed, as you can see in the IPython Shell. . Your specific task is to determine how many of the entities contain the words &quot;Márquez&quot; or &quot;Gabo&quot; – these refer to the same person in different ways! . txt.entities is available. . # Calculate the proportion of txt.entities that # contains &#39;Márquez&#39; or &#39;Gabo&#39;: prop_ggm count = 0 for ent in txt.entities: if (&quot;Márquez&quot; in ent) or (&quot;Gabo&quot; in ent): count += 1 prop_ggm = count/len(txt.entities) print(prop_ggm) # 0.29591836734693877 . 4. Building a “fake news” classifier . . 4.1 Classifying fake news using supervised learning with NLP . . 4.1.1 Which possible features? . Which of the following are possible features for a text classification problem? . Number of words in a document. | Specific named entities. | Language. | . . 4.2 Building word count vectors with scikit-learn . . 4.2.1 CountVectorizer for text classification . It’s time to begin building your text classifier! The data has been loaded into a DataFrame called df . Explore it in the IPython Shell to investigate what columns you can use. The .head() method is particularly informative. . In this exercise, you’ll use pandas alongside scikit-learn to create a sparse text vectorizer you can use to train and test a simple supervised model. To begin, you’ll set up a CountVectorizer and investigate some of its features. . print(df.head()) Unnamed: 0 title 0 8476 You Can Smell Hillary’s Fear 1 10294 Watch The Exact Moment Paul Ryan Committed Pol... 2 3608 Kerry to go to Paris in gesture of sympathy 3 10142 Bernie supporters on Twitter erupt in anger ag... 4 875 The Battle of New York: Why This Primary Matters text label 0 Daniel Greenfield, a Shillman Journalism Fello... FAKE 1 Google Pinterest Digg Linkedin Reddit Stumbleu... FAKE 2 U.S. Secretary of State John F. Kerry said Mon... REAL 3 — Kaydee King (@KaydeeKing) November 9, 2016 T... FAKE 4 It&#39;s primary day in New York and front-runners... REAL . # Import the necessary modules from sklearn.feature_extraction.text import CountVectorizer from sklearn.model_selection import train_test_split # Print the head of df print(df.head()) # Create a series to store the labels: y y = df.label # Create training and test sets X_train, X_test, y_train, y_test = train_test_split(df[&#39;text&#39;],y,test_size=0.33,random_state=53) # Initialize a CountVectorizer object: count_vectorizer count_vectorizer = CountVectorizer(stop_words=&#39;english&#39;) # Transform the training data using only the &#39;text&#39; column values: count_train count_train = count_vectorizer.fit_transform(X_train) # Transform the test data using only the &#39;text&#39; column values: count_test count_test = count_vectorizer.transform(X_test) # Print the first 10 features of the count_vectorizer print(count_vectorizer.get_feature_names()[:10]) # [&#39;00&#39;, &#39;000&#39;, &#39;0000&#39;, &#39;00000031&#39;, &#39;000035&#39;, &#39;00006&#39;, &#39;0001&#39;, &#39;0001pt&#39;, &#39;000ft&#39;, &#39;000km&#39;] . 4.2.2 TfidfVectorizer for text classification . Similar to the sparse CountVectorizer created in the previous exercise, you’ll work on creating tf-idf vectors for your documents. You’ll set up a TfidfVectorizer and investigate some of its features. . In this exercise, you’ll use pandas and sklearn along with the same X_train , y_train and X_test , y_test DataFrames and Series you created in the last exercise. . # Import TfidfVectorizer from sklearn.feature_extraction.text import TfidfVectorizer # Initialize a TfidfVectorizer object: tfidf_vectorizer tfidf_vectorizer = TfidfVectorizer(stop_words=&#39;english&#39;, max_df=0.7) # Transform the training data: tfidf_train tfidf_train = tfidf_vectorizer.fit_transform(X_train) # Transform the test data: tfidf_test tfidf_test = tfidf_vectorizer.transform(X_test) # Print the first 10 features print(tfidf_vectorizer.get_feature_names()[:10]) # Print the first 5 vectors of the tfidf training data print(tfidf_train.A[:5]) . [&#39;00&#39;, &#39;000&#39;, &#39;001&#39;, &#39;008s&#39;, &#39;00am&#39;, &#39;00pm&#39;, &#39;01&#39;, &#39;01am&#39;, &#39;02&#39;, &#39;024&#39;] [[0. 0.01928563 0. ... 0. 0. 0. ] [0. 0. 0. ... 0. 0. 0. ] [0. 0.02895055 0. ... 0. 0. 0. ] [0. 0.03056734 0. ... 0. 0. 0. ] [0. 0. 0. ... 0. 0. 0. ]] . 4.2.3 Inspecting the vectors . To get a better idea of how the vectors work, you’ll investigate them by converting them into pandas DataFrames. . Here, you’ll use the same data structures you created in the previous two exercises ( count_train , count_vectorizer , tfidf_train , tfidf_vectorizer ) as well as pandas , which is imported as pd . . # Create the CountVectorizer DataFrame: count_df count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names()) # Create the TfidfVectorizer DataFrame: tfidf_df tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names()) # Print the head of count_df print(count_df.head()) # Print the head of tfidf_df print(tfidf_df.head()) # Calculate the difference in columns: difference difference = set(count_df.columns) - set(tfidf_df.columns) print(difference) # set() # Check whether the DataFrames are equal print(count_df.equals(tfidf_df)) # False . print(count_df.head()) 000 00am 0600 10 100 107 11 110 1100 12 ... younger 0 0 0 0 0 0 0 0 0 0 0 ... 0 1 0 0 0 3 0 0 0 0 0 0 ... 0 2 0 0 0 0 0 0 0 0 0 0 ... 0 3 0 0 0 0 0 0 0 0 0 0 ... 1 4 0 0 0 0 0 0 0 0 0 0 ... 0 youth youths youtube ypg yuan zawahiri zeitung zero zerohedge 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 [5 rows x 5111 columns] print(tfidf_df.head()) 000 00am 0600 10 100 107 11 110 1100 12 ... 0 0.0 0.0 0.0 0.000000 0.0 0.0 0.0 0.0 0.0 0.0 ... 1 0.0 0.0 0.0 0.105636 0.0 0.0 0.0 0.0 0.0 0.0 ... 2 0.0 0.0 0.0 0.000000 0.0 0.0 0.0 0.0 0.0 0.0 ... 3 0.0 0.0 0.0 0.000000 0.0 0.0 0.0 0.0 0.0 0.0 ... 4 0.0 0.0 0.0 0.000000 0.0 0.0 0.0 0.0 0.0 0.0 ... younger youth youths youtube ypg yuan zawahiri zeitung zero 0 0.000000 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.033579 1 0.000000 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.000000 2 0.000000 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.000000 3 0.015175 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.000000 4 0.000000 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.000000 zerohedge 0 0.0 1 0.0 2 0.0 3 0.0 4 0.0 [5 rows x 5111 columns] . . 4.3 Training and testing a classification model with scikit-learn . . 4.3.1 Text classification models . Which of the below is the most reasonable model to use when training a new supervised model using text vector data? . Naive Bayes . 4.3.2 Training and testing the “fake news” model with CountVectorizer . Now it’s your turn to train the “fake news” model using the features you identified and extracted. In this first exercise you’ll train and test a Naive Bayes model using the CountVectorizer data. . The training and test sets have been created, and count_vectorizer , count_train , and count_test have been computed. . # Import the necessary modules from sklearn.naive_bayes import MultinomialNB from sklearn import metrics # Instantiate a Multinomial Naive Bayes classifier: nb_classifier nb_classifier = MultinomialNB() # Fit the classifier to the training data nb_classifier.fit(count_train, y_train) # Create the predicted tags: pred pred = nb_classifier.predict(count_test) # Calculate the accuracy score: score score = metrics.accuracy_score(y_test,pred) print(score) # 0.893352462936394 # Calculate the confusion matrix: cm cm = metrics.confusion_matrix(y_test,pred,labels=[&#39;FAKE&#39;, &#39;REAL&#39;]) print(cm) #[[ 865 143] [ 80 1003]] . 4.3.3 Training and testing the “fake news” model with TfidfVectorizer . Now that you have evaluated the model using the CountVectorizer , you’ll do the same using the TfidfVectorizer with a Naive Bayes model. . The training and test sets have been created, and tfidf_vectorizer , tfidf_train , and tfidf_test have been computed. Additionally, MultinomialNB and metrics have been imported from, respectively, sklearn.naive_bayes and sklearn . . # Calculate the accuracy score and confusion matrix of # Multinomial Naive Bayes classifier predictions trained on # tfidf_train, y_train and tested against tfidf_test and # y_test # Instantiate a Multinomial Naive Bayes classifier: nb_classifier nb_classifier = MultinomialNB() # Fit the classifier to the training data nb_classifier.fit(tfidf_train, y_train) # Create the predicted tags: pred pred = nb_classifier.predict(tfidf_test) # Calculate the accuracy score: score score = metrics.accuracy_score(y_test,pred) print(score) # 0.8565279770444764 # Calculate the confusion matrix: cm cm = metrics.confusion_matrix(y_test,pred,labels=[&#39;FAKE&#39;, &#39;REAL&#39;]) print(cm) #[[ 739 269] [ 31 1052]] . Fantastic fake detection! The model correctly identifies fake news about 86% of the time. That’s a great start, but for a real world situation, you’d need to improve the score. . . 4.4 Simple NLP, complex problems . . 4.4.1 Improving the model . What are possible next steps you could take to improve the model? . Tweaking alpha levels. | Trying a new classification model. | Training on a larger dataset. | Improving text preprocessing. | . 4.4.2 Improving your model . Your job in this exercise is to test a few different alpha levels using the Tfidf vectors to determine if there is a better performing combination. . The training and test sets have been created, and tfidf_vectorizer , tfidf_train , and tfidf_test have been computed. . # Create the list of alphas: alphas alphas = np.arange(0,1,0.1) # Define train_and_predict() def train_and_predict(alpha): # Instantiate the classifier: nb_classifier nb_classifier = MultinomialNB(alpha=alpha) # Fit to the training data nb_classifier.fit(tfidf_train,y_train) # Predict the labels: pred pred = nb_classifier.predict(tfidf_test) # Compute accuracy: score score = metrics.accuracy_score(y_test,pred) return score # Iterate over the alphas and print the corresponding score for alpha in alphas: print(&#39;Alpha: &#39;, alpha) print(&#39;Score: &#39;, train_and_predict(alpha)) print() . Alpha: 0.0 Score: 0.8813964610234337 Alpha: 0.1 Score: 0.8976566236250598 Alpha: 0.2 Score: 0.8938307030129125 Alpha: 0.30000000000000004 Score: 0.8900047824007652 Alpha: 0.4 Score: 0.8857006217120995 Alpha: 0.5 Score: 0.8842659014825442 Alpha: 0.6000000000000001 Score: 0.874701099952176 Alpha: 0.7000000000000001 Score: 0.8703969392635102 Alpha: 0.8 Score: 0.8660927785748446 Alpha: 0.9 Score: 0.8589191774270684 . 4.4.3 Inspecting your model . Now that you have built a “fake news” classifier, you’ll investigate what it has learned. You can map the important vector weights back to actual words using some simple inspection techniques. . You have your well performing tfidf Naive Bayes classifier available as nb_classifier , and the vectors as tfidf_vectorizer . . # Get the class labels: class_labels class_labels = nb_classifier.classes_ # Extract the features: feature_names feature_names = tfidf_vectorizer.get_feature_names() # Zip the feature names together with the coefficient array and sort by weights: feat_with_weights feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names)) # Print the first class label and the top 20 feat_with_weights entries print(class_labels[0], feat_with_weights[:20]) # Print the second class label and the bottom 20 feat_with_weights entries print(class_labels[1], feat_with_weights[-20:]) . FAKE [(-12.641778440826338, &#39;0000&#39;), (-12.641778440826338, &#39;000035&#39;), (-12.641778440826338, &#39;0001&#39;), (-12.641778440826338, &#39;0001pt&#39;), (-12.641778440826338, &#39;000km&#39;), (-12.641778440826338, &#39;0011&#39;), (-12.641778440826338, &#39;006s&#39;), (-12.641778440826338, &#39;007&#39;), (-12.641778440826338, &#39;007s&#39;), (-12.641778440826338, &#39;008s&#39;), (-12.641778440826338, &#39;0099&#39;), (-12.641778440826338, &#39;00am&#39;), (-12.641778440826338, &#39;00p&#39;), (-12.641778440826338, &#39;00pm&#39;), (-12.641778440826338, &#39;014&#39;), (-12.641778440826338, &#39;015&#39;), (-12.641778440826338, &#39;018&#39;), (-12.641778440826338, &#39;01am&#39;), (-12.641778440826338, &#39;020&#39;), (-12.641778440826338, &#39;023&#39;)] REAL [(-6.790929954967984, &#39;states&#39;), (-6.765360557845786, &#39;rubio&#39;), (-6.751044290367751, &#39;voters&#39;), (-6.701050756752027, &#39;house&#39;), (-6.695547793099875, &#39;republicans&#39;), (-6.6701912490429685, &#39;bush&#39;), (-6.661945235816139, &#39;percent&#39;), (-6.589623788689862, &#39;people&#39;), (-6.559670340096453, &#39;new&#39;), (-6.489892292073901, &#39;party&#39;), (-6.452319082422527, &#39;cruz&#39;), (-6.452076515575875, &#39;state&#39;), (-6.397696648238072, &#39;republican&#39;), (-6.376343060363355, &#39;campaign&#39;), (-6.324397735392007, &#39;president&#39;), (-6.2546017970213645, &#39;sanders&#39;), (-6.144621899738043, &#39;obama&#39;), (-5.756817248152807, &#39;clinton&#39;), (-5.596085785733112, &#39;said&#39;), (-5.357523914504495, &#39;trump&#39;)] . . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/introduction-to-natural-language-processing-in-python.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/introduction-to-natural-language-processing-in-python.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post20": {
            "title": "Introduction to Linear Modeling in Python",
            "content": "Introduction to Linear Modeling in Python . This is the memo of the 3rd course (5 courses in all) of ‘Statistics Fundamentals with Python’ skill track. . You can find the original course HERE . . . ### . Exploring Linear Trends | Building Linear Models | Making Model Predictions | Estimating Model Parameters | 1. Exploring Linear Trends . . 1.1 Introduction to Modeling Data . . #### Reasons for Modeling: Interpolation . One common use of modeling is interpolation to determine a value “inside” or “in between” the measured data points. In this exercise, you will make a prediction for the value of the dependent variable distances for a given independent variable times that falls “in between” two measurements from a road trip, where the distances are those traveled for the given elapse times. . . times, distances [hours], [miles] 0.0, 0.00 1.0, 44.05 2.0, 107.16 3.0, 148.44 4.0, 196.40 5.0, 254.44 6.0, 300.00 . # Compute the total change in distance and change in time total_distance = distances[-1] - distances[0] total_time = times[-1] - times[0] # Estimate the slope of the data from the ratio of the changes average_speed = total_distance / total_time # Predict the distance traveled for a time not measured elapse_time = 2.5 distance_traveled = average_speed * elapse_time print(&quot;The distance traveled is {}&quot;.format(distance_traveled)) # The distance traveled is 125.0 . Notice that the answer distance is ‘inside’ that range of data values, so, less than the max(distances) but greater than the min(distances) . . #### Reasons for Modeling: Extrapolation . Another common use of modeling is extrapolation to estimate data values “outside” or “beyond” the range (min and max values of time ) of the measured data. In this exercise, we have measured distances for times 0 through 5 hours, but we are interested in estimating how far we’d go in 8 hours. Using the same data set from the previous exercise, we have prepared a linear model distance = model(time) . Use that model() to make a prediction about the distance traveled for a time much larger than the other times in the measurements. . . # Select a time not measured. time = 8 # Use the model to compute a predicted distance for that time. distance = model(time) # Inspect the value of the predicted distance traveled. print(distance) # Determine if you will make it without refueling. answer = (distance &lt;= 400) print(answer) . Notice that the car can travel just to the range limit of 400 miles, so you’d run out of gas just as you completed the trip . #### Reasons for Modeling: Estimating Relationships . Another common application of modeling is to compare two data sets by building models for each, and then comparing the models . In this exercise, you are given data for a road trip two cars took together. The cars stopped for gas every 50 miles, but each car did not need to fill up the same amount, because the cars do not have the same fuel efficiency (MPG). Complete the function efficiency_model(miles, gallons) to estimate efficiency as average miles traveled per gallons of fuel consumed. Use the provided dictionaries car1 and car2 , which both have keys car[&#39;miles&#39;] and car[&#39;gallons&#39;] . . . car1 {&#39;gallons&#39;: array([ 0.03333333, 1.69666667, 3.36 , 5.02333333, 6.68666667, 8.35 , 10.01333333, 11.67666667, 13.34 , 15.00333333, 16.66666667]), &#39;miles&#39;: array([ 1. , 50.9, 100.8, 150.7, 200.6, 250.5, 300.4, 350.3, 400.2, 450.1, 500. ])} . # Complete the function to model the efficiency. def efficiency_model(miles, gallons): return np.mean( miles / gallons ) # Use the function to estimate the efficiency for each car. car1[&#39;mpg&#39;] = efficiency_model(car1[&#39;miles&#39;] , car1[&#39;gallons&#39;] ) car2[&#39;mpg&#39;] = efficiency_model(car2[&#39;miles&#39;] , car2[&#39;gallons&#39;] ) # Finish the logic statement to compare the car efficiencies. if car1[&#39;mpg&#39;] &gt; car2[&#39;mpg&#39;] : print(&#39;car1 is the best&#39;) elif car1[&#39;mpg&#39;] &lt; car2[&#39;mpg&#39;] : print(&#39;car2 is the best&#39;) else: print(&#39;the cars have the same efficiency&#39;) # car2 is the best . Notice the original plot that visualized the raw data was plot gpm(), and the slope is 1/MPG and so car1 is steeper than car2, but if you call plot mpg(gallons, miles) the slope is MPG, and so car2 has a steeper slope than car1 . . 1.2 Visualizing Linear Relationships . #### Plotting the Data . Everything in python is an object, even modules. Your goal in this exercise is to review the use of the object oriented interfaces to the python library matplotlib in order to visualize measured data in a more flexible and extendable work flow. The general plotting work flow looks like this: . import matplotlib.pyplot as plt fig, axis = plt.subplots() axis.plot(x, y, color=&quot;green&quot;, linestyle=&quot;--&quot;, marker=&quot;s&quot;) plt.show() . . # Create figure and axis objects using subplots() fig, axis = plt.subplots() # Plot line using the axis.plot() method line = axis.plot(times , distances , linestyle=&quot; &quot;, marker=&quot;o&quot;, color=&quot;red&quot;) # Use the plt.show() method to display the figure plt.show() . . Notice how linestyle=’ ‘ means no line at all, just markers. . #### Plotting the Model on the Data . Continuing with the same measured data from the previous exercise, your goal is to use a predefined model() and measured data times and measured_distances to compute modeled distances, and then plot both measured and modeled data on the same axis. . . # Pass times and measured distances into model model_distances = model(times, measured_distances) # Create figure and axis objects and call axis.plot() twice to plot data and model distances versus times fig, axis = plt.subplots() axis.plot(times, measured_distances, linestyle=&quot; &quot;, marker=&quot;o&quot;, color=&quot;black&quot;, label=&quot;Measured&quot;) axis.plot(times, model_distances, linestyle=&quot;-&quot;, marker=None, color=&quot;red&quot;, label=&quot;Modeled&quot;) # Add grid lines and a legend to your plot, and then show to display axis.grid(True) axis.legend(loc=&quot;best&quot;) plt.show() . . Notice a subtlety of python. None is a special object that is often used as a place-holder to be replaced by default values, so linestyle=None does not mean no line, it means the default which is a solid line style, whereas marker=None triggers the default marker, which happens to be no marker at all. If you use color=None , the resulting color will be blue, the default line color for matplotlib . . #### Visually Estimating the Slope &amp; Intercept . Building linear models is an automated way of doing something we can roughly do “manually” with data visualization and a lot of trial-and-error. The visual method is not the most efficient or precise method, but it does illustrate the concepts very well, so let’s try it! . Given some measured data, your goal is to guess values for slope and intercept, pass them into the model, and adjust your guess until the resulting model fits the data. Use the provided data xd, yd , and the provided function model() to create model predictions. Compare the predictions and data using the provided plot_data_and_model() . . . # Look at the plot data and guess initial trial values trial_slope = 1.2 trial_intercept = 1.8 # input thoses guesses into the model function to compute the model values. xm, ym = model(trial_intercept, trial_slope) # Compare your your model to the data with the plot function fig = plot_data_and_model(xd, yd, xm, ym) plt.show() # Repeat the steps above until your slope and intercept guess makes the model line up with the data. final_slope = 1.2 final_intercept = 1.8 . . Notice that you did not have to get the best values, slope = 1 and intercept = 2 , just something close. Models almost NEVER match the data exactly, and a model created from slightly different model parameters might fit the data equally well. We’ll cover quantifying model performance and comparison in more detail later in this course! . . 1.3 Quantifying Linear Relationships . #### Mean, Deviation, &amp; Standard Deviation . The mean describes the center of the data. The standard deviation describes the spread of the data. But to compare two variables, it is convenient to normalize both. In this exercise, you are provided with two arrays of data, which are highly correlated, and you will compute and visualize the normalized deviations of each array. . . # Compute the deviations by subtracting the mean offset dx = x - np.mean(x) dy = y - np.mean(y) # Normalize the data by dividing the deviations by the standard deviation zx = dx / np.std(x) zy = dy / np.std(y) # Plot comparisons of the raw data and the normalized data fig = plot_cdfs(dx, dy, zx, zy) . . Notice how hard it is to compare dx and dy, versus comparing the normalized zx and zy. . #### Covariance vs Correlation . Covariance is a measure of whether two variables change (“vary”) together. It is calculated by computing the products, point-by-point, of the deviations seen in the previous exercise, dx[n]*dy[n] , and then finding the average of all those products. . Correlation is in essence the normalized covariance. In this exercise, you are provided with two arrays of data, which are highly correlated, and you will visualize and compute both the covariance and the correlation . . . # Compute the covariance from the deviations. dx = x - np.mean(x) dy = y - np.mean(y) covariance = np.mean(dx * dy) print(&quot;Covariance: &quot;, covariance) # Covariance: 69.6798182602 # Compute the correlation from the normalized deviations. zx = dx / np.std(x) zy = dy / np.std(y) correlation = np.mean(zx * zy) print(&quot;Correlation: &quot;, correlation) # Plot the normalized deviations for visual inspection. fig = plot_normalized_deviations(zx, zy) # Correlation: 0.982433369757 . . Notice that you’ve plotted the product of the normalized deviations, and labeled the plot with the correlation, a single value that is the mean of that product. The product is always positive and the mean is typical of how the two vary together. . #### Correlation Strength . Intuitively, we can look at the plots provided and “see” whether the two variables seem to “vary together”. . Data Set A: x and y change together and appear to have a strong relationship. | Data Set B: there is a rough upward trend; x and y appear only loosely related. | Data Set C: looks like random scatter; x an y do not appear to change together and are unrelated. | . . . . Recall that deviations differ from the mean, and we normalized by dividing the deviations by standard deviation. In this exercise you will compare the 3 data sets by computing correlation, and determining which data set has the most strongly correlated variables x and y. Use the provided data table data_sets , a dictionary of records, each having keys ‘name’, ‘x’, ‘y’, and ‘correlation’. . # Complete the function that will compute correlation. def correlation(x,y): x_dev = x - np.mean(x) y_dev = y - np.mean(y) x_norm = x_dev / np.std(x) y_norm = y_dev / np.std(y) return np.mean(x_norm * y_norm) # Compute and store the correlation for each data set in the list. for name, data in data_sets.items(): data[&#39;correlation&#39;] = correlation(data[&#39;x&#39;], data[&#39;y&#39;]) print(&#39;data set {} has correlation {:.2f}&#39;.format(name, data[&#39;correlation&#39;])) # Assign the data set with the best correlation. best_data = data_sets[&#39;A&#39;] # data set A has correlation 1.00 # data set B has correlation 0.54 # data set C has correlation 0.09 . Note that the strongest relationship is in Dataset A, with correlation closest to 1.0 and the weakest is Datatset C with correlation value closest to zero. . 2. Building Linear Models . . 2.1 What makes a model linear . . #### Model Components . In this exercise, you will implement a model function that returns model values for y , computed from input x data, and any input coefficients for the “zero-th” order term a0 , the “first-order” term a1 , and a quadratic term a2 of a model (see below). . y=a0+a1x+a2x2y=a0+a1x+a2x^2 . Recall that “first order” is linear, so we’ll set the defaults for this general linear model with a2=0 , but later, we will change this for comparison. . # Define the general model as a function def model(x, a0=3, a1=2, a2=0): return a0 + (a1*x) + (a2*x**2) # Generate array x, then predict y values for specific, non-default a0 and a1 x = np.linspace(-10, 10, 21) y = model(x) # Plot the results, y versus x fig = plot_prediction(x, y) . . Notice that we used model() to compute predicted values of y for given possibly measured values of x . The model takes the independent data and uses it to generate a model for the dependent variables corresponding values. . #### Model Parameters . Now that you’ve built a general model, let’s “optimize” or “fit” it to a new (preloaded) measured data set, xd, yd , by finding the specific values for model parameters a0, a1 for which the model data and the measured data line up on a plot. . This is an iterative visualization strategy, where we start with a guess for model parameters, pass them into the model() , over-plot the resulting modeled data on the measured data, and visually check that the line passes through the points. If it doesn’t, we change the model parameters and try again. . . # Complete the plotting function definition def plot_data_with_model(xd, yd, ym): fig = plot_data(xd, yd) # plot measured data fig.axes[0].plot(xd, ym, color=&#39;red&#39;) # over-plot modeled data plt.show() return fig # Select new model parameters a0, a1, and generate modeled `ym` from them. a0 = 150 a1 = 25 ym = model(xd, a0, a1) # Plot the resulting model to see whether it fits the data fig = plot_data_with_model(xd, yd, ym) . . Notice again that the measured x-axis data xd is used to generate the modeled y-axis data ym so to plot the model, you are plotting ym vs xd , which may seem counter-intuitive at first. But we are modeling the y response to a given x; we are not modeling x. . . 2.2 Interpreting Slope and Intercept . #### Linear Proportionality . The definition of temperature scales is related to the linear expansion of certain liquids, such as mercury and alcohol. Originally, these scales were literally rulers for measuring length of fluid in the narrow marked or “graduated” tube as a proxy for temperature. The alcohol starts in a bulb, and then expands linearly into the tube, in response to increasing temperature of the bulb or whatever surrounds it. . In this exercise, we will explore the conversion between the Fahrenheit and Celsius temperature scales as a demonstration of interpreting slope and intercept of a linear relationship within a physical context. . . # Complete the function to convert C to F def convert_scale(temps_C): (freeze_C, boil_C) = (0, 100) (freeze_F, boil_F) = (32, 212) change_in_C = boil_C - freeze_C change_in_F = boil_F - freeze_F slope = change_in_F / change_in_C intercept = freeze_F - freeze_C temps_F = intercept + (slope * temps_C) return temps_F # Use the convert function to compute values of F and plot them temps_C = np.linspace(0, 100, 101) temps_F = convert_scale(temps_C) fig = plot_temperatures(temps_C, temps_F) . . #### Slope and Rates-of-Change . In this exercise, you will model the motion of a car driving (roughly) constant velocity by computing the average velocity over the entire trip. The linear relationship modeled is between the time elapsed and the distance traveled. . In this case, the model parameter a1 , or slope, is approximated or “estimated”, as the mean velocity, or put another way, the “rate-of-change” of the distance (“rise”) divided by the time (“run”). . . # Compute an array of velocities as the slope between each point diff_distances = np.diff(distances) diff_times = np.diff(times) velocities = diff_distances / diff_times # Chracterize the center and spread of the velocities v_avg = np.mean(velocities) v_max = np.max(velocities) v_min = np.min(velocities) v_range = v_max - v_min # Plot the distribution of velocities fig = plot_velocity_timeseries(times[1:], velocities) . . Generally we might use the average velocity as the slope in our model. But notice that there is some random variation in the instantaneous velocity values when plotted as a time series. The range of values v_max - v_min is one measure of the scale of that variation, and the standard deviation of velocity values is another measure. We see the implications of this variation in a model parameter in the next chapter of this course when discussing inference. . #### Intercept and Starting Points . In this exercise, you will see the intercept and slope parameters in the context of modeling measurements taken of the volume of a solution contained in a large glass jug. The solution is composed of water, grains, sugars, and yeast. The total mass of both the solution and the glass container was also recorded, but the empty container mass was not noted. . Your job is to use the preloaded pandas DataFrame df , with data columns volumes and masses , to build a linear model that relates the masses (y-data) to the volumes (x-data). The slope will be an estimate of the density (change in mass / change in volume) of the solution, and the intercept will be an estimate of the empty container weight (mass when volume=0). . . # Import ols from statsmodels, and fit a model to the data from statsmodels.formula.api import ols model_fit = ols(formula=&quot;masses ~ volumes&quot;, data=df) model_fit = model_fit.fit() # Extract the model parameter values, and assign them to a0, a1 a0 = model_fit.params[&#39;Intercept&#39;] a1 = model_fit.params[&#39;volumes&#39;] # Print model parameter values with meaningful names, and compare to summary() print( &quot;container_mass = {:0.4f}&quot;.format(a0) ) print( &quot;solution_density = {:0.4f}&quot;.format(a1) ) print( model_fit.summary() ) # container_mass = 5.4349 # solution_density = 1.1029 . OLS Regression Results ============================================================================== Dep. Variable: masses R-squared: 0.999 Model: OLS Adj. R-squared: 0.999 Method: Least Squares F-statistic: 1.328e+05 Date: Wed, 25 Sep 2019 Prob (F-statistic): 1.19e-156 Time: 00:29:26 Log-Likelihood: 102.39 No. Observations: 101 AIC: -200.8 Df Residuals: 99 BIC: -195.5 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] Intercept 5.4349 0.023 236.805 0.000 5.389 5.480 volumes 1.1029 0.003 364.408 0.000 1.097 1.109 ============================================================================== Omnibus: 0.319 Durbin-Watson: 2.072 Prob(Omnibus): 0.852 Jarque-Bera (JB): 0.169 Skew: 0.100 Prob(JB): 0.919 Kurtosis: 3.019 Cond. No. 20.0 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. . Don’t worry about everything in the summary output at first glance. We’ll see more of it later. For now, it’s good enough to try to find the slope and intercept values. . . 2.3 Model Optimization . #### Residual Sum of the Squares . In a previous exercise, we saw that the altitude along a hiking trail was roughly fit by a linear model, and we introduced the concept of differences between the model and the data as a measure of model goodness . . In this exercise, you’ll work with the same measured data, and quantifying how well a model fits it by computing the sum of the square of the “differences”, also called “residuals”. . . # Load the data x_data, y_data = load_data() # Model the data with specified values for parameters a0, a1 y_model = model(x_data, a0=150, a1=25) # Compute the RSS value for this parameterization of the model rss = np.sum(np.square(y_data - y_model)) print(&quot;RSS = {}&quot;.format(rss)) # RSS = 14444.484117694472 . The value we compute for RSS is not meaningful by itself, but later it becomes meaningful in context when we compare it to other values of RSS computed for other parameterizations of the model. More on that next! . Some notes about code style; notice you could have done the RSS calculation in a single line of python code, but writing functions than can be re-used is good practice. Notice also that we could have defined a parameter dictionary dict(a0=150, a1=25) and passed it into the model as model(x, **parameters) which would make it easier to pass around all the parameters together if we needed them for other functions . #### Minimizing the Residuals . In this exercise, you will complete a function to visually compare model and data, and compute and print the RSS. You will call it more than once to see how RSS changes when you change values for a0 and a1 . We’ll see that the values for the parameters we found earlier are the ones needed to minimize the RSS. . # Complete function to load data, build model, compute RSS, and plot def compute_rss_and_plot_fit(a0, a1): xd, yd = load_data() ym = model(xd, a0, a1) residuals = ym - yd rss = np.sum(np.square(yd - ym)) summary = &quot;Parameters a0={}, a1={} yield RSS={:0.2f}&quot;.format(a0, a1, rss) fig = plot_data_with_model(xd, yd, ym, summary) return rss, summary # Chose model parameter values and pass them into RSS function rss, summary = compute_rss_and_plot_fit(a0=150, a1=25) print(summary) # Parameters a0=150, a1=25 yield RSS=14444.48 . . As stated earlier, the significance of RSS is in context of other values. More specifically, the minimum RSS is of value in identifying the specific set of parameter values for our model which yield the smallest residuals in an overall sense. . Visualizing the RSS Minima . In this exercise you will compute and visualize how RSS varies for different values of model parameters. Start by holding the intercept constant, but vary the slope: and for each slope value, you’ll compute the model values, and the resulting RSS. Once you have an array of RSS values, you will determine minimal RSS value, in code, and from that minimum, determine the slope that resulted in that minimal RSS. . . Use pre-loaded data arrays x_data , y_data , and empty container rss_list to get started. . # Loop over all trial values in a1_array, computing rss for each a1_array = np.linspace(15, 35, 101) for a1_trial in a1_array: y_model = model(x_data, a0=150, a1=a1_trial) rss_value = compute_rss(y_data, y_model) rss_list.append(rss_value) # Find the minimum RSS and the a1 value from whence it came rss_array = np.array(rss_list) best_rss = np.min(rss_array) best_a1 = a1_array[np.where(rss_array==best_rss)] print(&#39;The minimum RSS = {}, came from a1 = {}&#39;.format(best_rss, best_a1)) # The minimum RSS = 14411.193019771845, came from a1 = [ 24.8] # Plot your rss and a1 values to confirm answer fig = plot_rss_vs_a1(a1_array, rss_array) . . The best slope is the one out of an array of slopes than yielded the minimum RSS value out of an array of RSS values. Python tip: notice that we started with rss_list to make it easy to .append() but then later converted to numpy.array() to gain access to all the numpy methods. . . 2.4 Least-Squares Optimization . . #### Least-Squares with numpy . The formulae below are the result of working through the calculus discussed in the introduction. In this exercise, we’ll trust that the calculus correct, and implement these formulae in code. . # prepare the means and deviations of the two variables x_mean = np.mean(x) y_mean = np.mean(y) x_dev = x - x_mean y_dev = y - y_mean # Complete least-squares formulae to find the optimal a0, a1 a1 = np.sum(x_dev * y_dev) / np.sum( np.square(x_dev) ) a0 = y_mean - (a1 * x_mean) # Use the those optimal model parameters a0, a1 to build a model y_model = model(x, a0, a1) # plot to verify that the resulting y_model best fits the data y fig, rss = compute_rss_and_plot_fit(a0, a1) . . Notice that the optimal slope a1, according to least-squares, is a ratio of the covariance to the variance. Also, note that the values of the parameters obtained here are NOT exactly the ones used to generate the pre-loaded data (a1=25 and a0=150), but they are close to those. Least-squares does not guarantee zero error; there is no perfect solution, but in this case, least-squares is the best we can do. . #### Optimization with Scipy . It is possible to write a numpy implementation of the analytic solution to find the minimal RSS value. But for more complex models, finding analytic formulae is not possible, and so we turn to other methods. . In this exercise you will use scipy.optimize to employ a more general approach to solve the same optimization problem. . In so doing, you will see additional return values from the method that tell answer us “how good is best”. Here we will use the same measured data and parameters as seen in the last exercise for ease of comparison of the new scipy approach. . # Define a model function needed as input to scipy def model_func(x, a0, a1): return a0 + (a1*x) # Load the measured data you want to model x_data, y_data = load_data() # call curve_fit, passing in the model function and data; then unpack the results param_opt, param_cov = optimize.curve_fit(model_func, x_data, y_data) a0 = param_opt[0] # a0 is the intercept in y = a0 + a1*x a1 = param_opt[1] # a1 is the slope in y = a0 + a1*x # test that these parameters result in a model that fits the data fig, rss = compute_rss_and_plot_fit(a0, a1) . Notice that we passed the function object itself, model_func into curve_fit , rather than passing in the model data. The model function object was the input, because the optimization wants to know what form in general it’s solve for; had we passed in a model_func with more terms like an a2*x**2 term, we would have seen different results for the parameters output . #### Least-Squares with statsmodels . Several python libraries provide convenient abstracted interfaces so that you need not always be so explicit in handling the machinery of optimization of the model. . As an example, in this exercise, you will use the statsmodels library in a more high-level, generalized work-flow for building a model using least-squares optimization (minimization of RSS). . To help get you started, we’ve pre-loaded the data from x_data, y_data = load_data() and stored it in a pandas DataFrame with column names x_column and y_column using df = pd.DataFrame(dict(x_column=x_data, y_column=y_data)) . # Pass data and `formula` into ols(), use and `.fit()` the model to the data model_fit = ols(formula=&quot;y_column ~ x_column&quot;, data=df).fit() # Use .predict(df) to get y_model values, then over-plot y_data with y_model y_model = model_fit.predict(df) fig = plot_data_with_model(x_data, y_data, y_model) # Extract the a0, a1 values from model_fit.params a0 = model_fit.params[&#39;Intercept&#39;] a1 = model_fit.params[&#39;x_column&#39;] # Visually verify that these parameters a0, a1 give the minimum RSS fig, rss = compute_rss_and_plot_fit(a0, a1) . Note that the params container always uses ‘Intercept’ for the a0 key, but all higher order terms will have keys that match the column name from the pandas DataFrame that you passed into ols(). . 3. Making Model Predictions . . 3.1 Modeling Real Data . #### Linear Model in Anthropology . If you found part of a skeleton, from an adult human that lived thousands of years ago, how could you estimate the height of the person that it came from? This exercise is in part inspired by the work of forensic anthropologist Mildred Trotter, who built a regression model for the calculation of stature estimates from human “long bones” or femurs that is commonly used today. . In this exercise, you’ll use data from many living people, and the python library scikit-learn , to build a linear model relating the length of the femur (thigh bone) to the “stature” (overall height) of the person. Then, you’ll apply your model to make a prediction about the height of your ancient ancestor. . . # import the sklearn class LinearRegression and initialize the model from sklearn.linear_model import LinearRegression model = LinearRegression(fit_intercept=False) # Prepare the measured data arrays and fit the model to them legs = legs.reshape(len(legs),1) heights = heights.reshape(len(legs),1) model.fit(legs, heights) # Use the fitted model to make a prediction for the found femur fossil_leg = 50.7 fossil_height = model.predict(fossil_leg) print(&quot;Predicted fossil height = {:0.2f} cm&quot;.format(fossil_height[0,0])) # Predicted fossil height = 181.34 cm . Notice that we used the pre-loaded data to fit or “train” the model, and then applied that model to make a prediction about newly collected data that was not part of the data used to fit the model. Also notice that model.predict() returns the answer as an array of shape = (1,1) , so we had to index into it with the [0,0] syntax when printing. . This is an artifact of our overly simplified use of sklearn here: the details of this are beyond the scope of the current course, but relate to the number of samples and features that one might use in a more sophisticated, generalized model. . #### Linear Model in Oceanography . Time-series data provides a context in which the “slope” of the linear model represents a “rate-of-change”. . In this exercise, you will use measurements of sea level change from 1970 to 2010, build a linear model of that changing sea level and use it to make a prediction about the future sea level rise. . . # Import LinearRegression class, build a model, fit to the data from sklearn.linear_model import LinearRegression model = LinearRegression(fit_intercept=True) model.fit(years, levels) # Use model to make a prediction for one year, 2100 future_year = 2100 future_level = model.predict(future_year) print(&quot;Prediction: year = {}, level = {:.02f}&quot;.format(future_year, future_level[0,0])) # Use model to predict for many years, and over-plot with measured data years_forecast = np.linspace(1970, 2100, 131).reshape(-1, 1) levels_forecast = model.predict(years_forecast) fig = plot_data_and_forecast(years, levels, years_forecast, levels_forecast) . . Note that with scikit-learn , although we could extract a0 = model.intercept_[0] and a1 = model.coef_[0,0] , we do not need to do that in order to make predictions, we just call model.predict() . With more complex models, these parameters may not have easy physical interpretations. . Notice also that although our model is linear, the actual data appears to have an up-turn that might be better modeled by adding a quadratic or even exponential term to our model. The linear model forecast may be underestimating the rate of increase in sea level. . #### Linear Model in Cosmology . Less than 100 years ago, the universe appeared to be composed of a single static galaxy, containing perhaps a million stars. Today we have observations of hundreds of billions of galaxies, each with hundreds of billions of stars, all moving. . The beginnings of the modern physical science of cosmology came with the publication in 1929 by Edwin Hubble that included use of a linear model. . In this exercise, you will build a model whose slope will give Hubble’s Constant, which describes the velocity of galaxies as a linear function of distance from Earth. . . # Fit the model, based on the form of the formula model_fit = ols(formula=&quot;velocities ~ distances&quot;, data=df).fit() # Extract the model parameters and associated &quot;errors&quot; or uncertainties a0 = model_fit.params[&#39;Intercept&#39;] a1 = model_fit.params[&#39;distances&#39;] e0 = model_fit.bse[&#39;Intercept&#39;] e1 = model_fit.bse[&#39;distances&#39;] # Print the results print(&#39;For slope a1={:.02f}, the uncertainty in a1 is {:.02f}&#39;.format(a1, e1)) print(&#39;For intercept a0={:.02f}, the uncertainty in a0 is {:.02f}&#39;.format(a0, e0)) # For slope a1=454.16, the uncertainty in a1 is 75.24 # For intercept a0=-40.78, the uncertainty in a0 is 83.44 . Later in the course, we will spend more time with model uncertainty, and exploring how to compute it ourselves. Notice the ~ in the formula means “similar to” and is interpreted by statsmodels to mean that y ~ x have a linear relationship. . More recently, observed astrophysical data extend the veritical scale of measured data out further by almost a factor of 50. Using this new data to model gives a very different value for the slope, Hubble’s Constant, of about 72. Modeling with new data revealed a different slope, and this has big implications in the physics of the Universe. . . 3.2 The Limits of Prediction . . #### Interpolation: Inbetween Times . In this exercise, you will build a linear model by fitting monthly time-series data for the Dow Jones Industrial Average (DJIA) and then use that model to make predictions for daily data (in effect, an interpolation). Then you will compare that daily prediction to the real daily DJIA data. . A few notes on the data. “OHLC” stands for “Open-High-Low-Close”, which is usually daily data, for example the opening and closing prices, and the highest and lowest prices, for a stock in a given day. “DayCount” is an integer number of days from start of the data collection. . . df_monthly.head(3) Open High Low Close Date 2013-01-01 13104.299805 13969.990234 13104.299805 13860.580078 2013-02-01 13860.580078 14149.150391 13784.009766 14054.490234 2013-03-01 14054.490234 14585.099609 13937.599609 14578.540039 Adj Close Volume Jday DayCount Date 2013-01-01 13860.580078 2786680000 2456293.5 1827.0 2013-02-01 14054.490234 2487580000 2456324.5 1858.0 2013-03-01 14578.540039 2546320000 2456352.5 1886.0 df_daily.head(3) Open High Low Close Date 2013-01-02 13104.299805 13412.709961 13104.299805 13412.549805 2013-01-03 13413.009766 13430.599609 13358.299805 13391.360352 2013-01-04 13391.049805 13447.110352 13376.230469 13435.209961 Adj Close Volume Jday DayCount Date 2013-01-02 13412.549805 161430000 2456294.5 1827.0 2013-01-03 13391.360352 129630000 2456295.5 1828.0 2013-01-04 13435.209961 107590000 2456296.5 1829.0 . # build and fit a model to the df_monthly data model_fit = ols(&#39;Close ~ DayCount&#39;, data=df_monthly).fit() # Use the model FIT to the MONTHLY data to make a predictions for both monthly and daily data df_monthly[&#39;Model&#39;] = model_fit.predict(df_monthly.DayCount) df_daily[&#39;Model&#39;] = model_fit.predict(df_daily.DayCount) # Plot the monthly and daily data and model, compare the RSS values seen on the figures fig_monthly = plot_model_with_data(df_monthly) fig_daily = plot_model_with_data(df_daily) . . Notice the monthly data looked linear, but the daily data clearly has additional, nonlinear trends. Under-sampled data often misses real-world features in the data on smaller time or spatial scales. Using the model from the under-sampled data to make interpolations to the daily data can result is large residuals. Notice that the RSS value for the daily plot is more than 30 times worse than the monthly plot . #### Extrapolation: Going Over the Edge . In this exercise, we consider the perils of extrapolation. Shown here is the profile of a hiking trail on a mountain. One portion of the trail, marked in black, looks linear, and was used to build a model. But we see that the best fit line, shown in red, does not fit outside the original “domain”, as it extends into this new outside data, marked in blue. . If we want use the model to make predictions for the altitude, but still be accurate to within some tolerance, what are the smallest and largest values of independent variable x that we can allow ourselves to apply the model to?” . Here, use the preloaded x_data , y_data , y_model , and plot_data_model_tolerance() to complete your solution. . . # Compute the residuals, &quot;data - model&quot;, and determine where [residuals &lt; tolerance] residuals = np.abs(y_data - y_model) tolerance = 100 x_good = x_data[residuals &lt; tolerance] # Find the min and max of the &quot;good&quot; values, and plot y_data, y_model, and the tolerance range print(&#39;Minimum good x value = {}&#39;.format(np.min(x_good))) print(&#39;Maximum good x value = {}&#39;.format(np.max(x_good))) fig = plot_data_model_tolerance(x_data, y_data, y_model, tolerance) . . Notice the range of good values, which extends a little out into the new data, is marked in green on the plot. By comparing the residuals to a tolerance threshold, we can quantify how far out out extrapolation can go before the difference between model and data gets too large. . . 3.3 Goodness-of-Fit . . . . #### RMSE Step-by-step . In this exercise, you will quantify the over-all model “goodness-of-fit” of a pre-built model, by computing one of the most common quantitative measures of model quality, the RMSE, step-by-step. . Start with the pre-loaded data x_data and y_data , and use it with a predefined modeling function model_fit_and_predict() . . . # Build the model and compute the residuals &quot;model - data&quot; y_model = model_fit_and_predict(x_data, y_data) residuals = y_data - y_model # Compute the RSS, MSE, and RMSE and print the results RSS = np.sum(np.square(residuals)) MSE = RSS/len(residuals) RMSE = np.sqrt(MSE) print(&#39;RMSE = {:0.2f}, MSE = {:0.2f}, RSS = {:0.2f}&#39;.format(RMSE, MSE, RSS)) # RMSE = 26.23, MSE = 687.83, RSS = 14444.48 . Notice that instead of computing RSS and normalizing with division by len(residuals) to get the MSE, you could have just applied np.mean(np.square()) to the residuals . . Another useful point to help you remember; you can think of the MSE like a variance, but instead of differencing the data from its mean, you difference the data and the model. Similarly, think of RMSE as a standard deviation. . #### R-Squared . In this exercise you’ll compute another measure of goodness, R-squared . R-squared is the ratio of the variance of the residuals divided by the variance of the data we are modeling, and in so doing, is a measure of how much of the variance in your data is “explained” by your model, as expressed in the spread of the residuals. . Here we have pre-loaded the data x_data , y_data and the model predictions y_model for the best fit model; you’re goal is to compute the R-squared measure to quantify how much this linear model accounts for variation in the data. . . # Compute the residuals and the deviations residuals = y_model - y_data deviations = np.mean(y_data) - y_data # Compute the variance of the residuals and deviations var_residuals = np.sum(np.square(residuals)) var_deviations = np.sum(np.square(deviations)) # Compute r_squared as 1 - the ratio of RSS/Variance r_squared = 1 - (var_residuals / var_deviations) print(&#39;R-squared is {:0.2f}&#39;.format(r_squared)) # R-squared is 0.89 . Notice that R-squared varies from 0 to 1, where a value of 1 means that the model and the data are perfectly correlated and all variation in the data is predicted by the model. A value of zero would mean none of the variation in the data is predicted by the model. Here, the data points are close to the line, so R-squared is closer to 1.0 . . 3.4 Standard Error . . #### Variation Around the Trend . The data need not be perfectly linear, and there may be some random variation or “spread” in the measurements, and that does translate into variation of the model parameters. This variation is in the parameter is quantified by “standard error”, and interpreted as “uncertainty” in the estimate of the model parameter. . In this exercise, you will use ols from statsmodels to build a model and extract the standard error for each parameter of that model. . . # Store x_data and y_data, as times and distances, in df, and use ols() to fit a model to it. df = pd.DataFrame(dict(times=x_data, distances=y_data)) model_fit = ols(formula=&quot;distances ~ times&quot;, data=df).fit() # Extact the model parameters and their uncertainties a0 = model_fit.params[&#39;Intercept&#39;] e0 = model_fit.bse[&#39;Intercept&#39;] a1 = model_fit.params[&#39;times&#39;] e1 = model_fit.bse[&#39;times&#39;] # Print the results with more meaningful names print(&#39;Estimate of the intercept = {:0.2f}&#39;.format(a0)) print(&#39;Uncertainty of the intercept = {:0.2f}&#39;.format(e0)) print(&#39;Estimate of the slope = {:0.2f}&#39;.format(a1)) print(&#39;Uncertainty of the slope = {:0.2f}&#39;.format(e1)) # Estimate of the intercept = -0.81 # Uncertainty of the intercept = 1.29 # Estimate of the slope = 50.78 # Uncertainty of the slope = 1.11 . The size of the parameters standard error only makes sense in comparison to the parameter value itself. In fact the units are the same! So a1 and e1 both have units of velocity (meters/second), and a0 and e0 both have units of distance (meters). . #### Variation in Two Parts . Given two data sets of distance-versus-time data, one with very small velocity and one with large velocity. Notice that both may have the same standard error of slope, but different R-squared for the model overall, depending on the size of the slope (“effect size”) as compared to the standard error (“uncertainty”). . If we plot both data sets as scatter plots on the same axes, the contrast is clear. Variation due to the slope is different than variation due to the random scatter about the trend line. In this exercise, your goal is to compute the standard error and R-squared for two data sets and compare. . . # Build and fit two models, for columns distances1 and distances2 in df model_1 = ols(formula=&quot;distances1 ~ times&quot;, data=df).fit() model_2 = ols(formula=&quot;distances2 ~ times&quot;, data=df).fit() # Extract R-squared for each model, and the standard error for each slope se_1 = model_1.bse[&#39;times&#39;] se_2 = model_2.bse[&#39;times&#39;] rsquared_1 = model_1.rsquared rsquared_2 = model_2.rsquared # Print the results print(&#39;Model 1: SE = {:0.3f}, R-squared = {:0.3f}&#39;.format(se_1, rsquared_1)) print(&#39;Model 2: SE = {:0.3f}, R-squared = {:0.3f}&#39;.format(se_2, rsquared_2)) # Model 1: SE = 3.694, R-squared = 0.898 # Model 2: SE = 3.694, R-squared = 0.335 . Notice that the standard error is the same for both models, but the r-squared changes. The uncertainty in the estimates of the model parameters is indepedent from R-squred because that uncertainty is being driven not by the linear trend, but by the inherent randomness in the data. This serves as a transition into looking at statistical inference in linear models. . 4. Estimating Model Parameters . . 4.1 Inferential Statistics Concepts . #### Sample Statistics versus Population . In this exercise you will work with a preloaded population . You will construct a sample by drawing points at random from the population. You will compute the mean standard deviation of the sample taken from that population to test whether the sample is representative of the population. Your goal is to see where the sample statistics are the same or very close to the population statistics. . . # Compute the population statistics print(&quot;Population mean {:.1f}, stdev {:.2f}&quot;.format( population.mean(), population.std() )) # Set random seed for reproducibility np.random.seed(42) # Construct a sample by randomly sampling 31 points from the population sample = np.random.choice(population, size=31) # Compare sample statistics to the population statistics print(&quot; Sample mean {:.1f}, stdev {:.2f}&quot;.format( sample.mean(), sample.std() )) # Population mean 100.0, stdev 9.74 # Sample mean 102.1, stdev 9.34 . Notice that the sample statistics are similar to the population statistics, but not the identical. If you were to compute the len() of each array, it is very different, but the means are not that much different as you might expect. . #### Variation in Sample Statistics . If we create one sample of size=1000 by drawing that many points from a population. Then compute a sample statistic, such as the mean, a single value that summarizes the sample itself. . If you repeat that sampling process num_samples=100 times, you get 100 samples. Computing the sample statistic, like the mean, for each of the different samples, will result in a distribution of values of the mean. The goal then is to compute the mean of the means and standard deviation of the means. . Here you will use the preloaded population , num_samples , and num_pts , and note that the means and deviations arrays have been initialized to zero to give you containers to use for the for loop. . # Initialize two arrays of zeros to be used as containers means = np.zeros(num_samples) stdevs = np.zeros(num_samples) # For each iteration, compute and store the sample mean and sample stdev for ns in range(num_samples): sample = np.random.choice(population, num_pts) means[ns] = sample.mean() stdevs[ns] = sample.std() # Compute and print the mean() and std() for the sample statistic distributions print(&quot;Means: center={:&gt;6.2f}, spread={:&gt;6.2f}&quot;.format(means.mean(), means.std())) print(&quot;Stdevs: center={:&gt;6.2f}, spread={:&gt;6.2f}&quot;.format(stdevs.mean(), stdevs.std())) # Means: center=100.00, spread= 0.33 # Stdevs: center= 10.01, spread= 0.22 . If we only took one sample, instead of 100, there could be only a single mean and the standard deviation of that single value is zero. But each sample is different because of the randomness of the draws. The mean of the means is our estimate for the population mean, the stdev of the means is our measure of the uncertainty in our estimate of the population mean. This is the same concept as the standard error of the slope seen in linear regression. . #### Visualizing Variation of a Statistic . Previously, you have computed the variation of sample statistics. Now you’ll visualize that variation. . We’ll start with a preloaded population and a predefined function get_sample_statistics() to draw the samples, and return the sample statistics arrays. . Here we will use a predefined plot_hist() function that wraps the matplotlib method axis.hist() , which both bins and plots the array passed in. In this way you can see how the sample statistics have a distribution of values, not just a single value. . # Generate sample distribution and associated statistics means, stdevs = get_sample_statistics(population, num_samples=100, num_pts=1000) # Define the binning for the histograms mean_bins = np.linspace(97.5, 102.5, 51) std_bins = np.linspace(7.5, 12.5, 51) # Plot the distribution of means, and the distribution of stdevs fig = plot_hist(data=means, bins=mean_bins, data_name=&quot;Means&quot;, color=&#39;green&#39;) fig = plot_hist(data=stdevs, bins=std_bins, data_name=&quot;Stdevs&quot;, color=&#39;red&#39;) . . . 4.2 Model Estimation and Likelihood . . What is the probability that A occurs given B that occured . Given the model, what is the probability that the model outputs any particular data point . Given the data, what is the likelihood that a candidate model could output the particular data point . #### Estimation of Population Parameters . Imagine a constellation (“population”) of satellites orbiting for a full year, and the distance traveled in each hour is measured in kilometers. There is variation in the distances measured from hour-to-hour, due to unknown complications of orbital dynamics. Assume we cannot measure all the data for the year, but we wish to build a population model for the variations in orbital distance per hour (speed) based on a sample of measurements. . In this exercise, you will assume that the population of hourly distances are best modeled by a gaussian, and further assume that the parameters of that population model can be estimated from the sample statistics. Start with the preloaded sample_distances that was taken from a population of cars. . . . # Compute the mean and standard deviation of the sample_distances sample_mean = np.mean(sample_distances) sample_stdev = np.std(sample_distances) # Use the sample mean and stdev as estimates of the population model parameters mu and sigma population_model = gaussian_model(sample_distances, mu=sample_mean, sigma=sample_stdev) # Plot the model and data to see how they compare fig = plot_data_and_model(sample_distances, population_model) . . Notice in the plot that the data and the model do not line up exactly. This is to be expected because the sample is just a subset of the population, and any model built from it cannot be a prefect representation of the population. Also notice the vertical axis: it shows the normalize data bin counts, and the probability density of the model. Think of that as probability-per-bin, so that if summed all the bins, the total would be 1.0. . #### Maximizing Likelihood, Part 1 . Previously, we chose the sample mean as an estimate of the population model paramter mu . But how do we know that the sample mean is the best estimator? This is tricky, so let’s do it in two parts. . In Part 1, you will use a computational approach to compute the log-likelihood of a given estimate. Then, in Part 2, we will see that when you compute the log-likelihood for many possible guess values of the estimate, one guess will result in the maximum likelihood. . . # Compute sample mean and stdev, for use as model parameter value guesses mu_guess = np.mean(sample_distances) sigma_guess = np.std(sample_distances) # For each sample distance, compute the probability modeled by the parameter guesses probs = np.zeros(len(sample_distances)) for n, distance in enumerate(sample_distances): probs[n] = gaussian_model(distance, mu=mu_guess, sigma=sigma_guess) # Compute and print the log-likelihood as the sum() of the log() of the probabilities loglikelihood = np.sum(np.log(probs)) print(&#39;For guesses mu={:0.2f} and sigma={:0.2f}, the loglikelihood={:0.2f}&#39;.format(mu_guess, sigma_guess, loglikelihood)) # For guesses mu=26918.10 and sigma=224.88, the loglikelihood=-6834.53 . Although the likelihood (the product of the probabilities) is easier to interpret, the loglikelihood has better numerical properties. Products of small and large numbers can cause numerical artifacts, but sum of the logs usually doesnt suffer those same artifacts, and the “sum(log(things))” is closely related to the “product(things)” . #### Maximizing Likelihood, Part 2 . In Part 1, you computed a single log-likelihood for a single mu . In this Part 2, you will apply the predefined function compute_loglikelihood() to compute an array of log-likelihood values, one for each element in an array of possible mu values. . The goal then is to determine which single mu guess leads to the single maximum value of the loglikelihood array. . To get started, use the preloaded data sample_distances , sample_mean , sample_stdev and a helper function compute_loglikelihood() . . # Create an array of mu guesses, centered on sample_mean, spread out +/- by sample_stdev low_guess = sample_mean - 2*sample_stdev high_guess = sample_mean + 2*sample_stdev mu_guesses = np.linspace(low_guess, high_guess, 101) # Compute the loglikelihood for each model created from each guess value loglikelihoods = np.zeros(len(mu_guesses)) for n, mu_guess in enumerate(mu_guesses): loglikelihoods[n] = compute_loglikelihood(sample_distances, mu=mu_guess, sigma=sample_stdev) # Find the best guess by using logical indexing, the print and plot the result best_mu = mu_guesses[loglikelihoods==np.max(loglikelihoods)] print(&#39;Maximum loglikelihood found for best mu guess={}&#39;.format(best_mu)) fig = plot_loglikelihoods(mu_guesses, loglikelihoods) # Maximum loglikelihood found for best mu guess=[ 26918.39241406] . Notice that the guess for mu that gave the maximum likelihood is precisely the same value as the sample.mean() . The sample_mean is thus said to be the “Maximum Likelihood Estimator” of the population mean mu . We call that value of mu the “Maximum Likelihood Estimator” of the population mu because, of all the mu values tested, it results in a model population with the greatest likelihood of producing the sample data we have. . . . 4.3 Model Uncertainty and Sample Distributions . #### Bootstrap and Standard Error . Imagine a National Park where park rangers hike each day as part of maintaining the park trails. They don’t always take the same path, but they do record their final distance and time. We’d like to build a statistical model of the variations in daily distance traveled from a limited sample of data from one ranger. . Your goal is to use bootstrap resampling, computing one mean for each resample, to create a distribution of means, and then compute standard error as a way to quantify the “uncertainty” in the sample statistic as an estimator for the population statistic . . Use the preloaded sample_data array of 500 independent measurements of distance traveled. For now, we this is a simulated data set to simplify this lesson. Later, we’ll see more realistic data. . . # Use the sample_data as a model for the population population_model = sample_data # Resample the population_model 100 times, computing the mean each sample for nr in range(num_resamples): bootstrap_sample = np.random.choice(population_model, size=resample_size, replace=True) bootstrap_means[nr] = np.mean(bootstrap_sample) # Compute and print the mean, stdev of the resample distribution of means distribution_mean = np.mean(bootstrap_means) standard_error = np.std(bootstrap_means) print(&#39;Bootstrap Distribution: center={:0.1f}, spread={:0.1f}&#39;.format(distribution_mean, standard_error)) # Plot the bootstrap resample distribution of means fig = plot_data_hist(bootstrap_means) . . Notice that standard_error is just one measure of spread of the distribution of bootstrap resample means. You could have computed the confidence_interval using np.percentile(bootstrap_means, 0.95) and np.percentile(bootstrap_means, 0.05) to find the range distance values containing the inner 90% of the distribution of means. . #### Estimating Speed and Confidence . Let’s continue looking at the National Park hiking data. Notice that some distances are negative because they walked in the opposite direction from the trail head; the data are messy so let’s just focus on the overall trend. . In this exercise, you goal is to use boot-strap resampling to find the distribution of speed values for a linear model, and then from that distribution, compute the best estimate for the speed and the 90th percent confidence interval of that estimate. The speed here is the slope parameter from the linear regression model to fit distance as a function of time. . To get you started, we’ve preloaded distance and time data, together with a pre-defined least_squares() function to compute the speed value for each resample. . . # Resample each preloaded population, and compute speed distribution population_inds = np.arange(0, 99, dtype=int) for nr in range(num_resamples): sample_inds = np.random.choice(population_inds, size=100, replace=True) sample_inds.sort() sample_distances = distances[sample_inds] sample_times = times[sample_inds] a0, a1 = least_squares(sample_times, sample_distances) resample_speeds[nr] = a1 # Compute effect size and confidence interval, and print speed_estimate = np.mean(resample_speeds) ci_90 = np.percentile(resample_speeds, [5, 95]) print(&#39;Speed Estimate = {:0.2f}, 90% Confidence Interval: {:0.2f}, {:0.2f} &#39;.format(speed_estimate, ci_90[0], ci_90[1])) # Speed Estimate = 2.29, 90% Confidence Interval: 1.23, 3.35 . Notice that the speed estimate (the mean) falls inside the confidence interval (the 5th and 95th percentiles). Moreover, notice if you computed the standard error, it would also fit inside the confidence interval. Think of the standard error here as the ‘one sigma’ confidence interval. Note that this should be very similar to the summary output of a statsmodels ols() linear regression model, but here you can compute arbitrary percentiles because you have the entire speeds distribution. . #### Visualize the Bootstrap . Continuing where we left off earlier in this lesson, let’s visualize the bootstrap distribution of speeds estimated using bootstrap resampling, where we computed a least-squares fit to the slope for every sample to test the variation or uncertainty in our slope estimation. . To get you started, we’ve preloaded a function compute_resample_speeds(distances, times) to do the computation of generate the speed sample distribution. . . # Create the bootstrap distribution of speeds resample_speeds = compute_resample_speeds(distances, times) speed_estimate = np.mean(resample_speeds) percentiles = np.percentile(resample_speeds, [5, 95]) # Plot the histogram with the estimate and confidence interval fig, axis = plt.subplots() hist_bin_edges = np.linspace(0.0, 4.0, 21) axis.hist(resample_speeds, bins=hist_bin_edges, color=&#39;green&#39;, alpha=0.35, rwidth=0.8) axis.axvline(speed_estimate, label=&#39;Estimate&#39;, color=&#39;black&#39;) axis.axvline(percentiles[0], label=&#39; 5th&#39;, color=&#39;blue&#39;) axis.axvline(percentiles[1], label=&#39;95th&#39;, color=&#39;blue&#39;) axis.legend() plt.show() . . Notice that vertical lines marking the 5th (left) and 95th (right) percentiles mark the extent of the confidence interval, while the speed estimate (center line) is the mean of the distribution and falls between them. Note the speed estimate is the mean, not the median, which would be 50% percentile. . . 4.4 Model Errors and Randomness . . #### Test Statistics and Effect Size . How can we explore linear relationships with bootstrap resampling? Back to the trail! For each hike plotted as one point, we can see that there is a linear relationship between total distance traveled and time elapsed. It we treat the distance traveled as an “effect” of time elapsed, then we can explore the underlying connection between linear regression and statistical inference. . In this exercise, you will separate the data into two populations, or “categories”: early times and late times. Then you will look at the differences between the total distance traveled within each population. This difference will serve as a “test statistic”, and it’s distribution will test the effect of separating distances by times. . . # Create two poulations, sample_distances for early and late sample_times. # Then resample with replacement, taking 500 random draws from each population. group_duration_short = sample_distances[sample_times &lt; 5] group_duration_long = sample_distances[sample_times &gt; 5] resample_short = np.random.choice(group_duration_short, size=500, replace=True) resample_long = np.random.choice(group_duration_long, size=500, replace=True) # Difference the resamples to compute a test statistic distribution, then compute its mean and stdev test_statistic = resample_long - resample_short effect_size = np.mean(test_statistic) standard_error = np.std(test_statistic) # Print and plot the results print(&#39;Test Statistic: mean={:0.2f}, stdev={:0.2f}&#39;.format(effect_size, standard_error)) fig = plot_test_statistic(test_statistic) # Test Statistic: mean=10.01, stdev=4.62 . . Notice again, the test statistic is the difference between a distance drawn from short duration trips and one drawn from long duration trips. The distribution of difference values is built up from differencing each point in the early time range with one from the late time range. The mean of the test statistic is not zero and tells us that there is on average a difference in distance traveled when comparing short and long duration trips. Again, we call this the ‘effect size’. The time increase had an effect on distance traveled. The standard error of the test statistic distribution is not zero, so there is some spread in that distribution, or put another way, uncertainty in the size of the effect. . #### Null Hypothesis . In this exercise, we formulate the null hypothesis as . short and long time durations have no effect on total distance traveled. . We interpret the “zero effect size” to mean that if we shuffled samples between short and long times, so that two new samples each have a mix of short and long duration trips, and then compute the test statistic, on average it will be zero. . In this exercise, your goal is to perform the shuffling and resampling. Start with the predefined group_duration_short and group_duration_long which are the un-shuffled time duration groups. . # Shuffle the time-ordered distances, then slice the result into two populations. shuffle_bucket = np.concatenate((group_duration_short, group_duration_long)) np.random.shuffle(shuffle_bucket) slice_index = len(shuffle_bucket)//2 shuffled_half1 = shuffle_bucket[0:slice_index] shuffled_half2 = shuffle_bucket[slice_index:] # Create new samples from each shuffled population, and compute the test statistic resample_half1 = np.random.choice(shuffled_half1, size=500, replace=True) resample_half2 = np.random.choice(shuffled_half2, size=500, replace=True) test_statistic = resample_half2 - resample_half1 # Compute and print the effect size effect_size = np.mean(test_statistic) print(&#39;Test Statistic, after shuffling, mean = {}&#39;.format(effect_size)) # Test Statistic, after shuffling, mean = 0.09300205283002799 . Notice that your effect size is not exactly zero because there is noise in the data. But the effect size is much closer to zero than before shuffling. Notice that if you rerun your code, which will generate a new shuffle, you will get slightly different results each time for the effect size, but np.abs(test_statistic) should be less than about 1.0, due to the noise, as opposed to the slope, which was about 2.0 . #### Visualizing Test Statistics . In this exercise, you will approach the null hypothesis by comparing the distribution of a test statistic arrived at from two different ways. . First, you will examine two “populations”, grouped by early and late times, and computing the test statistic distribution. Second, shuffle the two populations, so the data is no longer time ordered, and each has a mix of early and late times, and then recompute the test statistic distribution. . To get you started, we’ve pre-loaded the two time duration groups, group_duration_short and group_duration_long , and two functions, shuffle_and_split() and plot_test_statistic() . . # From the unshuffled groups, compute the test statistic distribution resample_short = np.random.choice(group_duration_short, size=500, replace=True) resample_long = np.random.choice(group_duration_long, size=500, replace=True) test_statistic_unshuffled = resample_long - resample_short # Shuffle two populations, cut in half, and recompute the test statistic shuffled_half1, shuffled_half2 = shuffle_and_split(group_duration_short, group_duration_long) resample_half1 = np.random.choice(shuffled_half1, size=500, replace=True) resample_half2 = np.random.choice(shuffled_half2, size=500, replace=True) test_statistic_shuffled = resample_half2 - resample_half1 # Plot both the unshuffled and shuffled results and compare fig = plot_test_statistic(test_statistic_unshuffled, label=&#39;Unshuffled&#39;) fig = plot_test_statistic(test_statistic_shuffled, label=&#39;Shuffled&#39;) . . Notice that after you shuffle, the effect size went almost to zero and the spread increased, as measured by the standard deviation of the sample statistic, aka the ‘standard error’. So shuffling did indeed have an effect. The null hypothesis is disproven. Time ordering does in fact have a non-zero effect on distance traveled. Distance is correlated to time. . #### Visualizing the P-Value . In this exercise, you will visualize the p-value, the chance that the effect (or “speed”) we estimated, was the result of random variation in the sample. Your goal is to visualize this as the fraction of points in the shuffled test statistic distribution that fall to the right of the mean of the test statistic (“effect size”) computed from the unshuffled samples. . To get you started, we’ve preloaded the group_duration_short and group_duration_long and functions compute_test_statistic() , shuffle_and_split() , and plot_test_statistic_effect() . # Compute the test stat distribution and effect size for two population groups test_statistic_unshuffled = compute_test_statistic(group_duration_short, group_duration_long) effect_size = np.mean(test_statistic_unshuffled) # Randomize the two populations, and recompute the test stat distribution shuffled_half1, shuffled_half2 = shuffle_and_split(group_duration_short, group_duration_long) test_statistic_shuffled = compute_test_statistic(shuffled_half1, shuffled_half2) # Compute the p-value as the proportion of shuffled test stat values &gt;= the effect size condition = test_statistic_shuffled &gt;= effect_size p_value = len(test_statistic_shuffled[condition]) / len(test_statistic_shuffled) # Print p-value and overplot the shuffled and unshuffled test statistic distributions print(&quot;The p-value is = {}&quot;.format(p_value)) fig = plot_test_stats_and_pvalue(test_statistic_unshuffled, test_statistic_shuffled) # The p-value is = 0.126 . . Note that the entire point of this is compute a p-value to quantify the chance that our estimate for speed could have been obtained by random chance. On the plot, the unshuffle test stats are the distribution of speed values estimated from time-ordered distances. The shuffled test stats are the distribution of speed values computed from randomizing unordered distances. Values of the shuffled stats to the right of the mean non-shuffled effect size line are those that both (1) could have both occured randomly and (2) are at least as big as the estimate you want to use for speed. . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/introduction-to-linear-modeling-in-python.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/introduction-to-linear-modeling-in-python.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post21": {
            "title": "Network Analysis in Python (Part 1)",
            "content": "Network Analysis in Python (Part 1) . This is the memo of the 26th course of ‘Data Scientist with Python’ track. . You can find the original course HERE . . . 1. Introduction to networks . . 1.1 Introduction to networks . . #### Basics of NetworkX API, using Twitter network . To get you up and running with the NetworkX API, we will run through some basic functions that let you query a Twitter network that has been pre-loaded for you and is available in the IPython Shell as T . The Twitter network comes from KONECT , and shows a snapshot of a subset of Twitter users. It is an anonymized Twitter network with metadata. . You’re now going to use the NetworkX API to explore some basic properties of the network, and are encouraged to experiment with the data in the IPython Shell. . Wait for the IPython shell to indicate that the graph that has been preloaded under the variable name T (representing a T witter network), and then answer the following question: . What is the size of the graph T , the type of T.nodes() , and the data structure of the third element of the last edge listed in T.edges(data=True) ? . T &lt;networkx.classes.digraph.DiGraph at 0x7f0c553c89b0&gt; T.nodes() NodeView((1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, ... 23365, 23366, 23367, 23368, 23369, 23370)) T.nodes(data=True) NodeDataView({1: {&#39;category&#39;: &#39;I&#39;, &#39;occupation&#39;: &#39;politician&#39;}, 3: {&#39;category&#39;: &#39;D&#39;, &#39;occupation&#39;: &#39;celebrity&#39;}, 4: {&#39;category&#39;: &#39;I&#39;, &#39;occupation&#39;: &#39;politician&#39;}, 5: {&#39;category&#39;: &#39;I&#39;, &#39;occupation&#39;: &#39;scientist&#39;}, ... T.edges(data=True) OutEdgeDataView([(1, 3, {&#39;date&#39;: datetime.date(2012, 11, 16)}), (1, 4, {&#39;date&#39;: datetime.date(2013, 6, 7)}), (1, 5, {&#39;date&#39;: datetime.date(2009, 7, 27)}), ... (23324, 23335, {&#39;date&#39;: datetime.date(2012, 2, 1)}), (23324, 23336, {&#39;date&#39;: datetime.date(2010, 9, 20)})]) len(T) 23369 type(T.nodes()) networkx.classes.reportviews.NodeView list(T.edges(data=True))[3] (1, 6, {&#39;date&#39;: datetime.date(2014, 12, 18)}) . #### Basic drawing of a network using NetworkX . # Import necessary modules import matplotlib.pyplot as plt import networkx as nx # Draw the graph to screen nx.draw(T_sub) plt.show() T_sub # &lt;networkx.classes.digraph.DiGraph at 0x7f0c4988bd68&gt; . . #### Queries on a graph . Now that you know some basic properties of the graph and have practiced using NetworkX’s drawing facilities to visualize components of it, it’s time to explore how you can query it for nodes and edges. Specifically, you’re going to look for “nodes of interest” and “edges of interest”. . # Use a list comprehension to get the nodes of interest: noi noi = [n for n, d in T.nodes(data=True) if d[&#39;occupation&#39;] == &#39;scientist&#39;] # Use a list comprehension to get the edges of interest: eoi eoi = [(u, v) for u, v, d in T.edges(data=True) if d[&#39;date&#39;] &lt; date(2010, 1, 1)] . noi[:3] [5, 9, 13] eoi[:3] [(1, 5), (1, 9), (1, 13)] . . 1.2 Types of graphs . . #### Checking the un/directed status of a graph . Which type of graph do you think the Twitter network data you have been working with corresponds to? . type(T) networkx.classes.digraph.DiGraph . #### Specifying a weight on edges . Weights can be added to edges in a graph, typically indicating the “strength” of an edge. In NetworkX, the weight is indicated by the &#39;weight&#39; key in the metadata dictionary. . Before attempting the exercise, use the IPython Shell to access the dictionary metadata of T and explore it, for instance by running the commands T.edges[1, 10] and then T.edges[10, 1] . Note how there’s only one field, and now you’re going to add another field, called &#39;weight&#39; . . T.edges[1, 10] # {&#39;date&#39;: datetime.date(2012, 9, 8)} T.edges[10, 1] Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; T.edges[10, 1] File &quot;&lt;stdin&gt;&quot;, line 927, in __getitem__ return self._adjdict[u][v] KeyError: 1 . # Set the weight of the edge T.edges[1, 10][&#39;weight&#39;] = 2 # Iterate over all the edges (with metadata) for u, v, d in T.edges(data=True): # Check if node 293 is involved if 293 in [u, v]: # Set the weight to 1.1 T.edges[u, v][&#39;weight&#39;] = 1.1 T.edges[1, 10] # {&#39;date&#39;: datetime.date(2012, 9, 8), &#39;weight&#39;: 2} . #### Checking whether there are self-loops in the graph . NetworkX also allows edges that begin and end on the same node; while this would be non-intuitive for a social network graph, it is useful to model data such as trip networks, in which individuals begin at one location and end in another. . In this exercise as well as later ones, you’ll find the assert statement useful. An assert -ions checks whether the statement placed after it evaluates to True, otherwise it will throw an AssertionError . . To begin, use the .number_of_selfloops() method on T in the IPython Shell to get the number of edges that begin and end on the same node. A number of self-loops have been synthetically added to the graph. Your job in this exercise is to write a function that returns these edges. . T.number_of_selfloops() 42 . # Define find_selfloop_nodes() def find_selfloop_nodes(G): &quot;&quot;&quot; Finds all nodes that have self-loops in the graph G. &quot;&quot;&quot; nodes_in_selfloops = [] # Iterate over all the edges of G for u, v in G.edges(): # Check if node u and node v are the same if u == v: # Append node u to nodes_in_selfloops nodes_in_selfloops.append(u) return nodes_in_selfloops # Check whether number of self loops equals the number of nodes in self loops assert T.number_of_selfloops() == len(find_selfloop_nodes(T)) . . 1.3 Network visualization . . . #### Visualizing using Matrix plots . It is time to try your first “fancy” graph visualization method: a matrix plot. To do this, nxviz provides a MatrixPlot object. . nxviz is a package for visualizing graphs in a rational fashion. Under the hood, the MatrixPlot utilizes nx.to_numpy_matrix(G) , which returns the matrix form of the graph. Here, each node is one column and one row, and an edge between the two nodes is indicated by the value 1. In doing so, however, only the weight metadata is preserved; all other metadata is lost, as you’ll verify using an assert statement. . A corresponding nx.from_numpy_matrix(A) allows one to quickly create a graph from a NumPy matrix. The default graph type is Graph() ; if you want to make it a DiGraph() , that has to be specified using the create_using keyword argument, e.g. ( nx.from_numpy_matrix(A, create_using=nx.DiGraph) ). . import matplotlib.pyplot as plt import networkx as nx # Import nxviz import nxviz as nv # Create the MatrixPlot object: m m = nv.MatrixPlot(T) # Draw m to the screen m.draw() # Display the plot plt.show() # Convert T to a matrix format: A A = nx.to_numpy_matrix(T) # Convert A back to the NetworkX form as a directed graph: T_conv T_conv = nx.from_numpy_matrix(A, create_using=nx.DiGraph()) # Check that the `category` metadata field is lost from each node for n, d in T_conv.nodes(data=True): assert &#39;category&#39; not in d.keys() . m &lt;nxviz.plots.MatrixPlot at 0x7fdc9e02cf28&gt; A matrix([[ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], ..., [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.]]) T_conv &lt;networkx.classes.digraph.DiGraph at 0x7fdc9cdfc438&gt; T_conv.nodes(data=True) NodeDataView({0: {}, 1: {}, 2: {}, 3: {}, 4: {}, 5: {}, 6: {}, 7: {}, 8: {}, 9: {}, 10: {}, 11: {}, ... T_conv.edges(data=True) OutEdgeDataView([(4, 73, {&#39;weight&#39;: 1.0}), (5, 125, {&#39;weight&#39;: 1.0}), (6, 7, {&#39;weight&#39;: 1.0}), ... T.nodes(data=True) NodeDataView({27: {&#39;category&#39;: &#39;D&#39;, &#39;occupation&#39;: &#39;scientist&#39;}, 35: {&#39;category&#39;: &#39;P&#39;, &#39;occupation&#39;: &#39;scientist&#39;}, ... T.edges(data=True) OutEdgeDataView([(151, 5071, {&#39;date&#39;: datetime.date(2011, 2, 21)}), (180, 12678, {&#39;date&#39;: datetime.date(2013, 6, 7)}), ... . . #### Visualizing using Circos plots . Circos plots are a rational, non-cluttered way of visualizing graph data, in which nodes are ordered around the circumference in some fashion, and the edges are drawn within the circle that results, giving a beautiful as well as informative visualization about the structure of the network. . In this exercise, you’ll continue getting practice with the nxviz API, this time with the CircosPlot object. . # Import necessary modules import matplotlib.pyplot as plt from nxviz import CircosPlot # Create the CircosPlot object: c c = CircosPlot(T) # Draw c to the screen c.draw() # Display the plot plt.show() . . #### Visualizing using Arc plots . Following on what you’ve learned about the nxviz API, now try making an ArcPlot of the network. Two keyword arguments that you will try here are node_order=&#39;keyX&#39; and node_color=&#39;keyX&#39; , in which you specify a key in the node metadata dictionary to color and order the nodes by. . # Import necessary modules import matplotlib.pyplot as plt from nxviz import ArcPlot # Create the un-customized ArcPlot object: a a = ArcPlot(T) # Draw a to the screen a.draw() # Display the plot plt.show() # Create the customized ArcPlot object: a2 a2 = ArcPlot(T, node_order=&#39;category&#39;, node_color=&#39;category&#39;) # Draw a2 to the screen a2.draw() # Display the plot plt.show() . . Notice the node coloring in the customized ArcPlot compared to the uncustomized version. In the customized ArcPlot, the nodes in each of the categories – &#39;I&#39; , &#39;D&#39; , and &#39;P&#39; – have their own color. . . 2. Important nodes . . 2.1 Degree centrality . . #### Compute number of neighbors for each node . How do you evaluate whether a node is an important one or not? There are a few ways to do so, and here, you’re going to look at one metric: the number of neighbors that a node has. . Your job in this exercise is to write a function that returns all nodes that have m neighbors. . T.neighbors(19) &lt;dict_keyiterator at 0x7f301617f458&gt; list(T.neighbors(19)) [5, 8, 12035, ... 37, 5485, 48] . # Define nodes_with_m_nbrs() def nodes_with_m_nbrs(G, m): &quot;&quot;&quot; Returns all nodes in graph G that have m neighbors. &quot;&quot;&quot; nodes = set() # Iterate over all nodes in G for n in G.nodes(): # Check if the number of neighbors of n matches m if len(list(G.neighbors(n))) == m: # Add the node n to the set nodes.add(n) # Return the nodes with m neighbors return nodes # Compute and print all nodes in T that have 6 neighbors six_nbrs = nodes_with_m_nbrs(T, 6) print(six_nbrs) {22533, 1803, 11276, 11279, 6161, 4261, 10149, 3880, 16681, 5420, 14898, 64, 14539, 6862, 20430, 9689, 475, 1374, 6112, 9186, 17762, 14956, 2927, 11764, 4725} . Great work! The number of neighbors a node has is one way to identify important nodes. It looks like 25 nodes in graph T have 6 neighbors. . #### Compute degree distribution . The number of neighbors that a node has is called its “degree”, and it’s possible to compute the degree distribution across the entire graph. In this exercise, your job is to compute the degree distribution across T . . # Compute the degree of every node: degrees degrees = [len(list(T.neighbors(n))) for n in T.nodes()] # Print the degrees print(degrees) [47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 10, 27, 0, 0, 0, 0, 0, 0, 0, 0, 34, 0, 0, 0, 0, 0, 60, 0, 11, 4, 0, 12, 0, 0, 56, 53, 0, 30, 0, 0, 0, 0, 12, 0, 0 ... . #### Degree centrality distribution . The degree of a node is the number of neighbors that it has. The degree centrality is the number of neighbors divided by all possible neighbors that it could have. Depending on whether self-loops are allowed, the set of possible neighbors a node could have could also include the node itself. . # Import matplotlib.pyplot import matplotlib.pyplot as plt # Compute the degree centrality of the Twitter network: deg_cent deg_cent = nx.degree_centrality(T) # Plot a histogram of the degree centrality distribution of the graph. plt.figure() plt.hist(list(deg_cent.values())) plt.xlabel(&#39;centrality&#39;) plt.ylabel(&#39;count&#39;) plt.show() # Plot a histogram of the degree distribution of the graph plt.figure() plt.hist(degrees) plt.xlabel(&#39;neighbors&#39;) plt.ylabel(&#39;count&#39;) plt.show() # Plot a scatter plot of the centrality distribution and the degree distribution plt.figure() plt.scatter(degrees, list(deg_cent.values())) plt.xlabel(&#39;degrees&#39;) plt.ylabel(&#39;centrality&#39;) plt.show() . . . 2.2 Graph algorithms . . . #### Shortest Path I . You can leverage what you know about finding neighbors to try finding paths in a network. One algorithm for path-finding between two nodes is the “breadth-first search” (BFS) algorithm. In a BFS algorithm, you start from a particular node and iteratively search through its neighbors and neighbors’ neighbors until you find the destination node. . Pathfinding algorithms are important because they provide another way of assessing node importance; you’ll see this in a later exercise. . In this set of 3 exercises, you’re going to build up slowly to get to the final BFS algorithm. The problem has been broken into 3 parts that, if you complete in succession, will get you to a first pass implementation of the BFS algorithm. . # Define path_exists() def path_exists(G, node1, node2): &quot;&quot;&quot; This function checks whether a path exists between two nodes (node1, node2) in graph G. &quot;&quot;&quot; visited_nodes = set() # Initialize the queue of nodes to visit with the first node: queue queue = [node1] # Iterate over the nodes in the queue for node in queue: # Get neighbors of the node neighbors = G.neighbors(node) # Check to see if the destination node is in the set of neighbors if node2 in neighbors: print(&#39;Path exists between nodes {0} and {1}&#39;.format(node1, node2)) return True break . #### Shortest Path II . Now that you’ve got the code for checking whether the destination node is present in neighbors, next up, you’re going to extend the same function to write the code for the condition where the destination node is not present in the neighbors. . def path_exists(G, node1, node2): &quot;&quot;&quot; This function checks whether a path exists between two nodes (node1, node2) in graph G. &quot;&quot;&quot; visited_nodes = set() queue = [node1] for node in queue: neighbors = G.neighbors(node) if node2 in neighbors: print(&#39;Path exists between nodes {0} and {1}&#39;.format(node1, node2)) return True else: # Add current node to visited nodes visited_nodes.add(node) # Add neighbors of current node that have not yet been visited queue.extend([n for n in neighbors if n not in visited_nodes]) . #### Shortest Path III . This is the final exercise of this trio! You’re now going to complete the problem by writing the code that returns False if there’s no path between two nodes. . def path_exists(G, node1, node2): &quot;&quot;&quot; This function checks whether a path exists between two nodes (node1, node2) in graph G. &quot;&quot;&quot; visited_nodes = set() queue = [node1] for node in queue: neighbors = G.neighbors(node) if node2 in neighbors: print(&#39;Path exists between nodes {0} and {1}&#39;.format(node1, node2)) return True break else: visited_nodes.add(node) queue.extend([n for n in neighbors if n not in visited_nodes]) # Check to see if the final element of the queue has been reached if node == queue[-1]: print(&#39;Path does not exist between nodes {0} and {1}&#39;.format(node1, node2)) # Place the appropriate return statement return False . You’ve just written an implementation of the BFS algorithm! . . 2.3 Betweenness centrality . . #### NetworkX betweenness centrality on a social network . Betweenness centrality is a node importance metric that uses information about the shortest paths in a network. It is defined as the fraction of all possible shortest paths between any pair of nodes that pass through the node. . # Compute the betweenness centrality of T: bet_cen bet_cen = nx.betweenness_centrality(T) # Compute the degree centrality of T: deg_cen deg_cen = nx.degree_centrality(T) # Create a scatter plot of betweenness centrality and degree centrality plt.scatter(list(bet_cen.values()), list(deg_cen.values())) plt.xlabel(&#39;betweenness_centrality&#39;) plt.ylabel(&#39;degree_centrality&#39;) # Display the plot plt.show() . . Now that you know how to compute different metrics for node importance, you’re going to take a deep dive into the Twitter network. . #### Deep dive – Twitter network . You’re going to now take a deep dive into a Twitter network, which will help reinforce what you’ve learned earlier. . First, you’re going to find the nodes that can broadcast messages very efficiently to lots of people one degree of separation away. . # Define find_nodes_with_highest_deg_cent() def find_nodes_with_highest_deg_cent(G): # Compute the degree centrality of G: deg_cent deg_cent = nx.degree_centrality(G) # Compute the maximum degree centrality: max_dc max_dc = max(list(deg_cent.values())) nodes = set() # Iterate over the degree centrality dictionary for k, v in deg_cent.items(): # Check if the current value has the maximum degree centrality if v == max_dc: # Add the current node to the set of nodes nodes.add(k) return nodes # Find the node(s) that has the highest degree centrality in T: top_dc top_dc = find_nodes_with_highest_deg_cent(T) print(top_dc) # {11824} # Write the assertion statement for node in top_dc: assert nx.degree_centrality(T)[node] == max(nx.degree_centrality(T).values()) . It looks like node 11824 has the highest degree centrality. . #### Deep dive – Twitter network part II . Next, you’re going to do an analogous deep dive on betweenness centrality! . # Define find_node_with_highest_bet_cent() def find_node_with_highest_bet_cent(G): # Compute betweenness centrality: bet_cent bet_cent = nx.betweenness_centrality(G) # Compute maximum betweenness centrality: max_bc max_bc = max(list(bet_cent.values())) nodes = set() # Iterate over the betweenness centrality dictionary for k, v in bet_cent.items(): # Check if the current value has the maximum betweenness centrality if v == max_bc: # Add the current node to the set of nodes nodes.add(k) return nodes # Use that function to find the node(s) that has the highest betweenness centrality in the network: top_bc top_bc = find_node_with_highest_bet_cent(T) print(top_bc) # {1} # Write an assertion statement that checks that the node(s) is/are correctly identified. for node in top_bc: assert nx.betweenness_centrality(T)[node] == max(nx.betweenness_centrality(T).values()) . You have correctly identified that node 1 has the highest betweenness centrality! . . 3. Structures . . 3.1 Cliques &amp; communities . . #### Identifying triangle relationships . Now that you’ve learned about cliques, it’s time to try leveraging what you know to find structures in a network. Triangles are what you’ll go for first. We may be interested in triangles because they’re the simplest complex clique. Let’s write a few functions; these exercises will bring you through the fundamental logic behind network algorithms. . In the Twitter network, each node has an &#39;occupation&#39; label associated with it, in which the Twitter user’s work occupation is divided into celebrity , politician and scientist . One potential application of triangle-finding algorithms is to find out whether users that have similar occupations are more likely to be in a clique with one another. . # check if node 3 is connect with node 4 T.has_edge(3, 4) False T.has_edge(1, 3) True # create a iterator contains all combinations of 2 neighbors of node 1. combinations(T.neighbors(1), 2) &lt;itertools.combinations at 0x7f8450bebe58&gt; [x for x in combinations(T.neighbors(1), 2)] [(3, 4), (3, 5), (3, 6), ... . from itertools import combinations # Define is_in_triangle() def is_in_triangle(G, n): &quot;&quot;&quot; Checks whether a node `n` in graph `G` is in a triangle relationship or not. Returns a boolean. &quot;&quot;&quot; in_triangle = False # Iterate over all possible triangle relationship combinations for n1, n2 in combinations(G.neighbors(n), 2): # Check if an edge exists between n1 and n2 if G.has_edge(n1, n2): in_triangle = True break return in_triangle . #### Finding nodes involved in triangles . NetworkX provides an API for counting the number of triangles that every node is involved in: nx.triangles(G) . It returns a dictionary of nodes as the keys and number of triangles as the values. . Your job in this exercise is to modify the function defined earlier to extract all of the nodes involved in a triangle relationship with a given node. . from itertools import combinations # Write a function that identifies all nodes in a triangle relationship with a given node. def nodes_in_triangle(G, n): &quot;&quot;&quot; Returns the nodes in a graph `G` that are involved in a triangle relationship with the node `n`. &quot;&quot;&quot; triangle_nodes = set([n]) # Iterate over all possible triangle relationship combinations for n1, n2 in combinations(G.neighbors(n), 2): # Check if n1 and n2 have an edge between them if G.has_edge(n1, n2): # Add n1 to triangle_nodes triangle_nodes.add(n1) # Add n2 to triangle_nodes triangle_nodes.add(n2) return triangle_nodes # Write the assertion statement assert len(nodes_in_triangle(T, 1)) == 35 . Your function correctly identified that node 1 is in a triangle relationship with 35 other nodes. . #### Finding open triangles . Let us now move on to finding open triangles! Recall that they form the basis of friend recommendation systems; if “A” knows “B” and “A” knows “C”, then it’s probable that “B” also knows “C”. . from itertools import combinations # Define node_in_open_triangle() def node_in_open_triangle(G, n): &quot;&quot;&quot; Checks whether pairs of neighbors of node `n` in graph `G` are in an &#39;open triangle&#39; relationship with node `n`. &quot;&quot;&quot; in_open_triangle = False # Iterate over all possible triangle relationship combinations for n1, n2 in combinations(G.neighbors(n), 2): # Check if n1 and n2 do NOT have an edge between them if not G.has_edge(n1, n2): in_open_triangle = True break return in_open_triangle # Compute the number of open triangles in T num_open_triangles = 0 # Iterate over all the nodes in T for n in T.nodes(): # Check if the current node is in an open triangle if node_in_open_triangle(T, n): # Increment num_open_triangles num_open_triangles += 1 print(num_open_triangles) # 22 . It looks like 22 nodes in graph T are in open triangles! . . 3.2 Maximal cliques . . #### Finding all maximal cliques of size “n” . Now that you’ve explored triangles (and open triangles), let’s move on to the concept of maximal cliques. Maximal cliques are cliques that cannot be extended by adding an adjacent edge, and are a useful property of the graph when finding communities. NetworkX provides a function that allows you to identify the nodes involved in each maximal clique in a graph: nx.find_cliques(G) . . # Define maximal_cliques() def maximal_cliques(G, size): &quot;&quot;&quot; Finds all maximal cliques in graph `G` that are of size `size`. &quot;&quot;&quot; mcs = [] for clique in nx.find_cliques(G): if len(clique) == size: mcs.append(clique) return mcs # Check that there are 33 maximal cliques of size 3 in the graph T assert len(maximal_cliques(T, 3)) == 33 . nx.find_cliques(T) &lt;generator object find_cliques at 0x7f3fb875f938&gt; maximal_cliques(T, 3) [[1, 13, 19], [1, 16, 48], [1, 19, 8], ... . . 3.3 Subgraphs . #### Subgraphs I . There may be times when you just want to analyze a subset of nodes in a network. To do so, you can copy them out into another graph object using G.subgraph(nodes) , which returns a new graph object (of the same type as the original graph) that is comprised of the iterable of nodes that was passed in. . nodes_of_interest = [29, 38, 42] # Define get_nodes_and_nbrs() def get_nodes_and_nbrs(G, nodes_of_interest): &quot;&quot;&quot; Returns a subgraph of the graph `G` with only the `nodes_of_interest` and their neighbors. &quot;&quot;&quot; nodes_to_draw = [] # Iterate over the nodes of interest for n in nodes_of_interest: # Append the nodes of interest to nodes_to_draw nodes_to_draw.append(n) # Iterate over all the neighbors of node n for nbr in G.neighbors(n): # Append the neighbors of n to nodes_to_draw nodes_to_draw.append(nbr) return G.subgraph(nodes_to_draw) # Extract the subgraph with the nodes of interest: T_draw T_draw = get_nodes_and_nbrs(T, nodes_of_interest) # Draw the subgraph to the screen nx.draw(T_draw) plt.show() . . The subgraph consisting of the nodes of interest and their neighbors has 7 nodes. . #### Subgraphs II . Using a list comprehension, extract nodes that have the metadata &#39;occupation&#39; as &#39;celebrity&#39; alongside their neighbors: . # Extract the nodes of interest: nodes nodes = [n for n, d in T.nodes(data=True) if d[&#39;occupation&#39;] == &#39;celebrity&#39;] # Create the set of nodes: nodeset nodeset = set(nodes) # Iterate over nodes for n in nodes: # Compute the neighbors of n: nbrs nbrs = T.neighbors(n) # Compute the union of nodeset and nbrs: nodeset nodeset = nodeset.union(nbrs) # Compute the subgraph using nodeset: T_sub T_sub = T.subgraph(nodeset) # Draw T_sub to the screen nx.draw(T_sub) plt.show() . . You’re now ready to bring together all of the concepts you’ve learned and apply them to a case study! . . 4. Bringing it all together . . 4.1 Case study . . . #### Characterizing the network (I) . To start out, let’s do some basic characterization of the network, by looking at the number of nodes and number of edges in a network. . G.nodes(data=True) NodeDataView({&#39;u41&#39;: {&#39;bipartite&#39;: &#39;users&#39;, &#39;grouping&#39;: 0}, &#39;u69&#39;: {&#39;bipartite&#39;: &#39;users&#39;, &#39;grouping&#39;: 0}, ... , &#39;u14964&#39;: {&#39;bipartite&#39;: &#39;users&#39;, &#39;grouping&#39;: 1}}) G.edges(data=True) EdgeDataView([(&#39;u41&#39;, &#39;u2022&#39;, {}), (&#39;u41&#39;, &#39;u69&#39;, {}), (&#39;u41&#39;, &#39;u5082&#39;, {}), (&#39;u41&#39;, &#39;u298&#39;, {}), ... (&#39;u9866&#39;, &#39;u10603&#39;, {}), (&#39;u9866&#39;, &#39;u10340&#39;, {}), (&#39;u9997&#39;, &#39;u10500&#39;, {}), (&#39;u10340&#39;, &#39;u10603&#39;, {})]) . len(G.nodes()) 56519 len(G.edges()) 72900 . #### Characterizing the network (II) . Let’s continue recalling what you’ve learned before about node importances, by plotting the degree distribution of a network. This is the distribution of node degrees computed across all nodes in a network. . # Import necessary modules import matplotlib.pyplot as plt import networkx as nx # Plot the degree distribution of the GitHub collaboration network plt.hist(list(nx.degree_centrality(G).values())) plt.xlabel(&#39;degree_centrality&#39;) plt.ylabel(&#39;count&#39;) plt.show() . . #### Characterizing the network (III) . The last exercise was on degree centrality; this time round, let’s recall betweenness centrality! . # Import necessary modules import matplotlib.pyplot as plt import networkx as nx # Plot the degree distribution of the GitHub collaboration network plt.hist(list(nx.betweenness_centrality(G).values())) plt.xlabel(&#39;betweenness_centrality&#39;) plt.ylabel(&#39;count&#39;) plt.show() . . . 4.2 Case study part II: Visualization . . #### MatrixPlot . Let’s now practice making some visualizations. The first one will be the MatrixPlot. In a MatrixPlot, the matrix is the representation of the edges. . # Import necessary modules from nxviz import MatrixPlot import matplotlib.pyplot as plt # Calculate the largest connected component subgraph: largest_ccs largest_ccs = sorted(nx.connected_component_subgraphs(G), key=lambda x: len(x))[-1] # Create the customized MatrixPlot object: h h = MatrixPlot(graph=largest_ccs, node_grouping=&#39;grouping&#39;) # Draw the MatrixPlot to the screen h.draw() plt.show() . nx.connected_component_subgraphs(G) &lt;generator object connected_component_subgraphs at 0x7f758c7e4eb8&gt; list(nx.connected_component_subgraphs(G)) [&lt;networkx.classes.graph.Graph at 0x7f758c81bfd0&gt;, &lt;networkx.classes.graph.Graph at 0x7f758c81bef0&gt;] . . Recall that in a MatrixPlot, nodes are the rows and columns of the matrix, and cells are filled in according to whether an edge exists between the pairs of nodes. . #### ArcPlot . # Import necessary modules from nxviz.plots import ArcPlot import matplotlib.pyplot as plt # Iterate over all the nodes in G, including the metadata for n, d in G.nodes(data=True): # Calculate the degree of each node: G.node[n][&#39;degree&#39;] G.node[n][&#39;degree&#39;] = nx.degree(G, n) # Create the ArcPlot object: a a = ArcPlot(graph=G, node_order=&#39;degree&#39;) # Draw the ArcPlot to the screen a.draw() plt.show() . G.node[&#39;u41&#39;] {&#39;bipartite&#39;: &#39;users&#39;, &#39;grouping&#39;: 0} # get the value of degree nx.degree(G, &#39;u41&#39;) 5 # set the value &#39;degree&#39; to 5 G.node[&#39;u41&#39;][&#39;degree&#39;] = nx.degree(G, &#39;u41&#39;) # show the value G.node[&#39;u41&#39;] {&#39;bipartite&#39;: &#39;users&#39;, &#39;degree&#39;: 5, &#39;grouping&#39;: 0} G.node[&#39;u41&#39;][&#39;degree&#39;] 5 . . #### CircosPlot . # Import necessary modules from nxviz import CircosPlot import matplotlib.pyplot as plt # Iterate over all the nodes, including the metadata for n, d in G.nodes(data=True): # Calculate the degree of each node: G.node[n][&#39;degree&#39;] G.node[n][&#39;degree&#39;] = nx.degree(G, n) # Create the CircosPlot object: c c = CircosPlot(graph=G, node_order=&#39;degree&#39;, node_grouping=&#39;grouping&#39;, node_color=&#39;grouping&#39;) # Draw the CircosPlot object to the screen c.draw() plt.show() . . This CircosPlot provides a compact alternative to the ArcPlot. It is easy to see in this plot that most users belong to one group. . . 4.3 Case study part III: Cliques . . #### Finding cliques (I) . You’re now going to practice finding cliques in G . Recall that cliques are “groups of nodes that are fully connected to one another”, while a maximal clique is a clique that cannot be extended by adding another node in the graph. . # Calculate the maximal cliques in G: cliques cliques = nx.find_cliques(G) # Count and print the number of maximal cliques in G print(len(list(cliques))) # 19 list(cliques) [ [&#39;u4761&#39;, &#39;u2643&#39;, &#39;u4329&#39;, &#39;u1254&#39;, &#39;u2737&#39;, &#39;u2289&#39;], [&#39;u2022&#39;, &#39;u9866&#39;, &#39;u435&#39;, &#39;u10340&#39;, &#39;u7623&#39;, &#39;u322&#39;, &#39;u8135&#39;, &#39;u10603&#39;], ... [&#39;u655&#39;, &#39;u2906&#39;], [&#39;u655&#39;, &#39;u914&#39;]] . #### Finding cliques (II) . Let’s continue by finding a particular maximal clique, and then plotting that clique. . # Import necessary modules import networkx as nx from nxviz import CircosPlot import matplotlib.pyplot as plt # Find the author(s) that are part of the largest maximal clique: largest_clique largest_clique = sorted(nx.find_cliques(G), key=lambda x:len(x))[-1] # Create the subgraph of the largest_clique: G_lc G_lc = G.subgraph(largest_clique) # Create the CircosPlot object: c c = CircosPlot(G_lc) # Draw the CircosPlot to the screen c.draw() plt.show() . . The subgraph consisting of the largest maximal clique has 14 users. . . 4.4 Case study part IV: Final tasks . #### Finding important collaborators . You’ll now look at important nodes once more. Here, you’ll make use of the degree_centrality() and betweenness_centrality() functions in NetworkX to compute each of the respective centrality scores, and then use that information to find the “important nodes”. In other words, your job in this exercise is to find the user(s) that have collaborated with the most number of users. . # Compute the degree centralities of G: deg_cent deg_cent = nx.degree_centrality(G) # Compute the maximum degree centrality: max_dc max_dc = max(deg_cent.values()) # Find the user(s) that have collaborated the most: prolific_collaborators prolific_collaborators = [n for n, dc in deg_cent.items() if dc == max_dc] # Print the most prolific collaborator(s) print(prolific_collaborators) # [&#39;u741&#39;] . It looks like &#39;u741&#39; is the most prolific collaborator. . #### Characterizing editing communities . You’re now going to combine what you’ve learned about the BFS algorithm and concept of maximal cliques to visualize the network with an ArcPlot. . # Import necessary modules from nxviz import ArcPlot import matplotlib.pyplot as plt # Identify the largest maximal clique: largest_max_clique largest_max_clique = set(sorted(nx.find_cliques(G), key=lambda x: len(x))[-1]) # Create a subgraph from the largest_max_clique: G_lmc G_lmc = G.subgraph(largest_max_clique).copy() # Go out 1 degree of separation for node in list(G_lmc.nodes()): G_lmc.add_nodes_from(G.neighbors(node)) G_lmc.add_edges_from(zip([node]*len(list(G.neighbors(node))), G.neighbors(node))) # Record each node&#39;s degree centrality score for n in G_lmc.nodes(): G_lmc.node[n][&#39;degree centrality&#39;] = nx.degree_centrality(G_lmc)[n] # Create the ArcPlot object: a a = ArcPlot(graph=G_lmc, node_order=&#39;degree centrality&#39;) # Draw the ArcPlot to the screen a.draw() plt.show() . . #### Recommending co-editors who have yet to edit together . Finally, you’re going to leverage the concept of open triangles to recommend users on GitHub to collaborate! . # Import necessary modules from itertools import combinations from collections import defaultdict # Initialize the defaultdict: recommended recommended = defaultdict(int) # Iterate over all the nodes in G for n, d in G.nodes(data=True): # Iterate over all possible triangle relationship combinations for n1, n2 in combinations(G.neighbors(n), 2): # Check whether n1 and n2 do not have an edge if not G.has_edge(n1, n2): # Increment recommended recommended[(n1, n2)] += 1 # Identify the top 10 pairs of users all_counts = sorted(recommended.values()) top10_pairs = [pair for pair, count in recommended.items() if count &gt; all_counts[-10]] print(top10_pairs) . recommended defaultdict(int, {(&#39;u10090&#39;, &#39;u2737&#39;): 1, (&#39;u10090&#39;, &#39;u3243&#39;): 1, (&#39;u10090&#39;, &#39;u3658&#39;): 1, (&#39;u10090&#39;, &#39;u4329&#39;): 1, ... . You’ve identified pairs of users who should collaborate together, and in doing so, built your very own recommendation system! . The End. . Thank you for reading. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/network-analysis-in-python-(part-1).html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/network-analysis-in-python-(part-1).html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post22": {
            "title": "Machine Learning with Tree-Based Models in Python",
            "content": "Machine Learning with Tree-Based Models in Python . This is the memo of the 24th course of ‘Data Scientist with Python’ track. . You can find the original course HERE . . . 1. Classification and Regression Trees(CART) . . 1.1 Decision tree for classification . . #### Train your first classification tree . In this exercise you’ll work with the Wisconsin Breast Cancer Dataset from the UCI machine learning repository. You’ll predict whether a tumor is malignant or benign based on two features: the mean radius of the tumor ( radius_mean ) and its mean number of concave points ( concave points_mean ). . # Import DecisionTreeClassifier from sklearn.tree from sklearn.tree import DecisionTreeClassifier # Instantiate a DecisionTreeClassifier &#39;dt&#39; with a maximum depth of 6 dt = DecisionTreeClassifier(max_depth=6, random_state=SEED) # Fit dt to the training set dt.fit(X_train, y_train) # Predict test set labels y_pred = dt.predict(X_test) print(y_pred[0:5]) # [0 0 0 1 0] . #### Evaluate the classification tree . # Import accuracy_score from sklearn.metrics import accuracy_score # Predict test set labels y_pred = dt.predict(X_test) # Compute test set accuracy acc = accuracy_score(y_test, y_pred) print(&quot;Test set accuracy: {:.2f}&quot;.format(acc)) # Test set accuracy: 0.89 . Using only two features, your tree was able to achieve an accuracy of 89%! . #### Logistic regression vs classification tree . A classification tree divides the feature space into rectangular regions . In contrast, a linear model such as logistic regression produces only a single linear decision boundary dividing the feature space into two decision regions. . help(plot_labeled_decision_regions) Signature: plot_labeled_decision_regions(X, y, models) Docstring: Function producing a scatter plot of the instances contained in the 2D dataset (X,y) along with the decision regions of two trained classification models contained in the list &#39;models&#39;. Parameters - X: pandas DataFrame corresponding to two numerical features y: pandas Series corresponding the class labels models: list containing two trained classifiers File: /tmp/tmpzto071yc/&lt;ipython-input-1-9e70bec83095&gt; Type: function . # Import LogisticRegression from sklearn.linear_model from sklearn.linear_model import LogisticRegression # Instatiate logreg logreg = LogisticRegression(random_state=1) # Fit logreg to the training set logreg.fit(X_train, y_train) # Define a list called clfs containing the two classifiers logreg and dt clfs = [logreg, dt] # Review the decision regions of the two classifiers plot_labeled_decision_regions(X_test, y_test, clfs) . . Notice how the decision boundary produced by logistic regression is linear while the boundaries produced by the classification tree divide the feature space into rectangular regions. . . 1.2 Classification tree Learning . Terms . Decision Tree: data structure consisting of a hierarchy of nodes | Node: question or prediction | . Node . Root: no parent node, question giving rise to two children nodes | Internal node: one parent node, question giving rise to two children nodes | Leaf: one parent node, no children nodes –&gt; prediction | . . Information gain in decision trees . Gini impurity . #### Growing a classification tree . The existence of a node depends on the state of its predecessors. | The impurity of a node can be determined using different criteria such as entropy and the gini-index. | When the information gain resulting from splitting a node is null, the node is declared as a leaf. | When an internal node is split, the split is performed in such a way so that information gain is maximized. | . #### Using entropy as a criterion . In this exercise, you’ll train a classification tree on the Wisconsin Breast Cancer dataset using entropy as an information criterion. . # Import DecisionTreeClassifier from sklearn.tree from sklearn.tree import DecisionTreeClassifier # Instantiate dt_entropy, set &#39;entropy&#39; as the information criterion dt_entropy = DecisionTreeClassifier(max_depth=8, criterion=&#39;entropy&#39;, random_state=1) # Fit dt_entropy to the training set dt_entropy.fit(X_train, y_train) . #### Entropy vs Gini index . In this exercise you’ll compare the test set accuracy of dt_entropy to the accuracy of another tree named dt_gini . . # Import accuracy_score from sklearn.metrics from sklearn.metrics import accuracy_score # Use dt_entropy to predict test set labels y_pred= dt_entropy.predict(X_test) # Evaluate accuracy_entropy accuracy_entropy = accuracy_score(y_test, y_pred) # Print accuracy_entropy print(&#39;Accuracy achieved by using entropy: &#39;, accuracy_entropy) # Print accuracy_gini print(&#39;Accuracy achieved by using the gini index: &#39;, accuracy_gini) # Accuracy achieved by using entropy: 0.929824561404 # Accuracy achieved by using the gini index: 0.929824561404 . Notice how the two models achieve exactly the same accuracy. Most of the time, the gini index and entropy lead to the same results. The gini index is slightly faster to compute and is the default criterion used in the DecisionTreeClassifier model of scikit-learn. . . 1.3 Decision tree for regression . #### Train your first regression tree . In this exercise, you’ll train a regression tree to predict the mpg (miles per gallon) consumption of cars in the auto-mpg dataset using all the six available features. . # Import DecisionTreeRegressor from sklearn.tree from sklearn.tree import DecisionTreeRegressor # Instantiate dt dt = DecisionTreeRegressor(max_depth=8, min_samples_leaf=0.13, random_state=3) # Fit dt to the training set dt.fit(X_train, y_train) . #### Evaluate the regression tree . In this exercise, you will evaluate the test set performance of dt using the Root Mean Squared Error (RMSE) metric. The RMSE of a model measures, on average, how much the model’s predictions differ from the actual labels. . The RMSE of a model can be obtained by computing the square root of the model’s Mean Squared Error (MSE). . # Import mean_squared_error from sklearn.metrics as MSE from sklearn.metrics import mean_squared_error as MSE # Compute y_pred y_pred = dt.predict(X_test) # Compute mse_dt mse_dt = MSE(y_test, y_pred) # Compute rmse_dt rmse_dt = mse_dt ** 0.5 # Print rmse_dt print(&quot;Test set RMSE of dt: {:.2f}&quot;.format(rmse_dt)) # Test set RMSE of dt: 4.37 . #### Linear regression vs regression tree . # Predict test set labels y_pred_lr = lr.predict(X_test) # Compute mse_lr mse_lr = MSE(y_test, y_pred_lr) # Compute rmse_lr rmse_lr = mse_lr ** 0.5 # Print rmse_lr print(&#39;Linear Regression test set RMSE: {:.2f}&#39;.format(rmse_lr)) # Print rmse_dt print(&#39;Regression Tree test set RMSE: {:.2f}&#39;.format(rmse_dt)) # Linear Regression test set RMSE: 5.10 # Regression Tree test set RMSE: 4.37 . . . 2. The Bias-Variance Tradeoff . . 2.1 Generalization Error . Bias–variance tradeoff . . . #### Overfitting and underfitting . . A: complex = overfit = low bias = high variance . B: simple = underfit = high bias = low variance . . 2.2 Diagnose bias and variance problems . #### Instantiate the model . In the following set of exercises, you’ll diagnose the bias and variance problems of a regression tree. . # Import train_test_split from sklearn.model_selection from sklearn.model_selection import train_test_split # Set SEED for reproducibility SEED = 1 # Split the data into 70% train and 30% test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED) # Instantiate a DecisionTreeRegressor dt dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.26, random_state=SEED) . #### Evaluate the 10-fold CV error . In this exercise, you’ll evaluate the 10-fold CV Root Mean Squared Error (RMSE) achieved by the regression tree dt that you instantiated in the previous exercise. . # Compute the array containing the 10-folds CV MSEs MSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv=10, scoring=&#39;neg_mean_squared_error&#39;, n_jobs=-1) # Compute the 10-folds CV RMSE RMSE_CV = (MSE_CV_scores.mean())**(0.5) # Print RMSE_CV print(&#39;CV RMSE: {:.2f}&#39;.format(RMSE_CV)) # CV RMSE: 5.14 . A very good practice is to keep the test set untouched until you are confident about your model’s performance. . CV is a great technique to get an estimate of a model’s performance without affecting the test set. . #### Evaluate the training error . # Import mean_squared_error from sklearn.metrics as MSE from sklearn.metrics import mean_squared_error as MSE # Fit dt to the training set dt.fit(X_train, y_train) # Predict the labels of the training set y_pred_train = dt.predict(X_train) # Evaluate the training set RMSE of dt RMSE_train = (MSE(y_train, y_pred_train))**(0.5) # Print RMSE_train print(&#39;Train RMSE: {:.2f}&#39;.format(RMSE_train)) # Train RMSE: 5.15 . Notice how the training error is roughly equal to the 10-folds CV error you obtained in the previous exercice. . #### High bias or high variance? . In this exercise you’ll diagnose whether the regression tree dt you trained in the previous exercise suffers from a bias or a variance problem. . The training set RMSE ( RMSE_train ) and the CV RMSE ( RMSE_CV ) achieved by dt are available in your workspace. In addition, we have also loaded a variable called baseline_RMSE which corresponds to the root mean-squared error achieved by the regression-tree trained with the disp feature only (it is the RMSE achieved by the regression tree trained in chapter 1, lesson 3). . Here baseline_RMSE serves as the baseline RMSE. . When above baseline, the model is considered to be underfitting. . When below baseline, the model is considered ‘good enough’. . Does dt suffer from a high bias or a high variance problem? . RMSE_train = 5.15 RMSE_CV = 5.14 baseline_RMSE = 5.1 . dt suffers from high bias because RMSE_CV ≈ RMSE_train and both scores are greater than baseline_RMSE . . dt is indeed underfitting the training set as the model is too constrained to capture the nonlinear dependencies between features and labels. . . 2.3 Ensemble Learning . #### Define the ensemble . In the following set of exercises, you’ll work with the Indian Liver Patient Dataset from the UCI Machine learning repository. . In this exercise, you’ll instantiate three classifiers to predict whether a patient suffers from a liver disease using all the features present in the dataset. . from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier as KNN from sklearn.tree import DecisionTreeClassifier # Set seed for reproducibility SEED=1 # Instantiate lr lr = LogisticRegression(random_state=SEED) # Instantiate knn knn = KNN(n_neighbors=27) # Instantiate dt dt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state=SEED) # Define the list classifiers classifiers = [(&#39;Logistic Regression&#39;, lr), (&#39;K Nearest Neighbours&#39;, knn), (&#39;Classification Tree&#39;, dt)] . #### Evaluate individual classifiers . In this exercise you’ll evaluate the performance of the models in the list classifiers that we defined in the previous exercise. You’ll do so by fitting each classifier on the training set and evaluating its test set accuracy. . from sklearn.metrics import accuracy_score # Iterate over the pre-defined list of classifiers for clf_name, clf in classifiers: # Fit clf to the training set clf.fit(X_train, y_train) # Predict y_pred y_pred = clf.predict(X_test) # Calculate accuracy accuracy = accuracy_score(y_test, y_pred) # Evaluate clf&#39;s accuracy on the test set print(&#39;{:s} : {:.3f}&#39;.format(clf_name, accuracy)) . Logistic Regression : 0.747 K Nearest Neighbours : 0.724 Classification Tree : 0.730 . #### Better performance with a Voting Classifier . Finally, you’ll evaluate the performance of a voting classifier that takes the outputs of the models defined in the list classifiers and assigns labels by majority voting. . classifiers [(&#39;Logistic Regression&#39;, LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class=&#39;ovr&#39;, n_jobs=1, penalty=&#39;l2&#39;, random_state=1, solver=&#39;liblinear&#39;, tol=0.0001, verbose=0, warm_start=False)), (&#39;K Nearest Neighbours&#39;, KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=1, n_neighbors=27, p=2, weights=&#39;uniform&#39;)), (&#39;Classification Tree&#39;, DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=0.13, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=1, splitter=&#39;best&#39;))] . # Import VotingClassifier from sklearn.ensemble from sklearn.ensemble import VotingClassifier # Instantiate a VotingClassifier vc vc = VotingClassifier(estimators=classifiers) # Fit vc to the training set vc.fit(X_train, y_train) # Evaluate the test set predictions y_pred = vc.predict(X_test) # Calculate accuracy score accuracy = accuracy_score(y_test, y_pred) print(&#39;Voting Classifier: {:.3f}&#39;.format(accuracy)) # Voting Classifier: 0.753 . Notice how the voting classifier achieves a test set accuracy of 75.3%. This value is greater than that achieved by LogisticRegression . . . 3. Bagging and Random Forests . . 3.1 Bagging(bootstrap) . boostrap = sample with replacement . #### Define the bagging classifier . In the following exercises you’ll work with the Indian Liver Patient dataset from the UCI machine learning repository. Your task is to predict whether a patient suffers from a liver disease using 10 features including Albumin, age and gender. You’ll do so using a Bagging Classifier. . # Import DecisionTreeClassifier from sklearn.tree import DecisionTreeClassifier # Import BaggingClassifier from sklearn.ensemble import BaggingClassifier # Instantiate dt dt = DecisionTreeClassifier(random_state=1) # Instantiate bc bc = BaggingClassifier(base_estimator=dt, n_estimators=50, random_state=1) . #### Evaluate Bagging performance . Now that you instantiated the bagging classifier, it’s time to train it and evaluate its test set accuracy. . # Fit bc to the training set bc.fit(X_train, y_train) # Predict test set labels y_pred = bc.predict(X_test) # Evaluate acc_test acc_test = accuracy_score(y_test, y_pred) print(&#39;Test set accuracy of bc: {:.2f}&#39;.format(acc_test)) # Test set accuracy of bc: 0.71 . A single tree dt would have achieved an accuracy of 63% which is 8% lower than bc ‘s accuracy! . . 3.2 Out of Bag(OOB) Evaluation . . What is OOB evaluation? = similar to Cross Validation . How OOB evaluation works = use OOB samples to evaluate the model . OOB sample = training data which are NOT selected by boostrap . #### Prepare the ground . In the following exercises, you’ll compare the OOB accuracy to the test set accuracy of a bagging classifier trained on the Indian Liver Patient dataset. . # Import DecisionTreeClassifier from sklearn.tree import DecisionTreeClassifier # Import BaggingClassifier from sklearn.ensemble import BaggingClassifier # Instantiate dt dt = DecisionTreeClassifier(min_samples_leaf=8, random_state=1) # Instantiate bc bc = BaggingClassifier(base_estimator=dt, n_estimators=50, oob_score=True, random_state=1) . #### OOB Score vs Test Set Score . Now that you instantiated bc , you will fit it to the training set and evaluate its test set and OOB accuracies. . # Fit bc to the training set bc.fit(X_train, y_train) # Predict test set labels y_pred = bc.predict(X_test) # Evaluate test set accuracy acc_test = accuracy_score(y_test, y_pred) # Evaluate OOB accuracy acc_oob = bc.oob_score_ # Print acc_test and acc_oob print(&#39;Test set accuracy: {:.3f}, OOB accuracy: {:.3f}&#39;.format(acc_test, acc_oob)) # Test set accuracy: 0.698, OOB accuracy: 0.704 . . 3.3 Random Forests (RF) . #### Train an RF regressor . In the following exercises you’ll predict bike rental demand in the Capital Bikeshare program in Washington, D.C using historical weather data from the Bike Sharing Demand dataset available through Kaggle. . As a first step, you’ll define a random forests regressor and fit it to the training set. . X_train.head(3) hr holiday workingday temp hum windspeed instant mnth yr 1236 12 0 1 0.72 0.45 0.0000 14240 8 1 1349 5 0 0 0.64 0.89 0.1940 14353 8 1 327 15 0 0 0.80 0.55 0.1642 13331 7 1 Clear to partly cloudy Light Precipitation Misty 1236 1 0 0 1349 1 0 0 327 1 0 0 y_train.head(3) 1236 305 1349 16 327 560 Name: cnt, dtype: int64 . # Import RandomForestRegressor from sklearn.ensemble import RandomForestRegressor # Instantiate rf rf = RandomForestRegressor(n_estimators=25, random_state=2) # Fit rf to the training set rf.fit(X_train, y_train) . #### Evaluate the RF regressor . # Import mean_squared_error as MSE from sklearn.metrics import mean_squared_error as MSE # Predict the test set labels y_pred = rf.predict(X_test) # Evaluate the test set RMSE rmse_test = MSE(y_test, y_pred) ** 0.5 # Print rmse_test print(&#39;Test set RMSE of rf: {:.2f}&#39;.format(rmse_test)) # Test set RMSE of rf: 51.97 . The test set RMSE achieved by rf is significantly smaller than that achieved by a single CART! . #### Visualizing features importances . In this exercise, you’ll determine which features were the most predictive according to the random forests regressor rf that you trained in a previous exercise. . # Create a pd.Series of features importances importances = pd.Series(data=rf.feature_importances_, index= X_train.columns) # Sort importances importances_sorted = importances.sort_values() # Draw a horizontal barplot of importances_sorted importances_sorted.plot(kind=&#39;barh&#39;, color=&#39;lightgreen&#39;) plt.title(&#39;Features Importances&#39;) plt.show() . . Apparently, hr and workingday are the most important features according to rf . The importances of these two features add up to more than 90%! . . 4. Boosting . . 4.1 Adaboost(Adaptive Boosting) . . . #### Define the AdaBoost classifier . In the following exercises you’ll revisit the Indian Liver Patient dataset which was introduced in a previous chapter. . Your task is to predict whether a patient suffers from a liver disease using 10 features including Albumin, age and gender. However, this time, you’ll be training an AdaBoost ensemble to perform the classification task. . In addition, given that this dataset is imbalanced, you’ll be using the ROC AUC score as a metric instead of accuracy. . As a first step, you’ll start by instantiating an AdaBoost classifier. . X_train.head(1) Age Total_Bilirubin Direct_Bilirubin Alkaline_Phosphotase 150 56 1.1 0.5 180 Alamine_Aminotransferase Aspartate_Aminotransferase Total_Protiens 150 30 42 6.9 Albumin Albumin_and_Globulin_Ratio Is_male 150 3.8 1.2 1 y_train.head(3) 150 0 377 0 473 0 Name: Liver_disease, dtype: int64 . # Import DecisionTreeClassifier from sklearn.tree import DecisionTreeClassifier # Import AdaBoostClassifier from sklearn.ensemble import AdaBoostClassifier # Instantiate dt dt = DecisionTreeClassifier(max_depth=2, random_state=1) # Instantiate ada ada = AdaBoostClassifier(base_estimator=dt, n_estimators=180, random_state=1) . #### Train the AdaBoost classifier . Now that you’ve instantiated the AdaBoost classifier ada , it’s time train it. You will also predict the probabilities of obtaining the positive class in the test set. . # Fit ada to the training set ada.fit(X_train, y_train) # Compute the probabilities of obtaining the positive class y_pred_proba = ada.predict_proba(X_test)[:,1] . ada.predict_proba(X_test) array([[ 0.57664817, 0.42335183], [ 0.48575393, 0.51424607], [ 0.34361394, 0.65638606], [ 0.50742464, 0.49257536], . #### Evaluate the AdaBoost classifier . # Import roc_auc_score from sklearn.metrics import roc_auc_score # Evaluate test-set roc_auc_score ada_roc_auc = roc_auc_score(y_test, y_pred_proba) # Print roc_auc_score print(&#39;ROC AUC score: {:.2f}&#39;.format(ada_roc_auc)) # ROC AUC score: 0.71 . Not bad! This untuned AdaBoost classifier achieved a ROC AUC score of 0.71! . . 4.2 Gradient Boosting (GB) . . . . ### Define the GB regressor . You’ll now revisit the Bike Sharing Demand dataset that was introduced in the previous chapter. . Recall that your task is to predict the bike rental demand using historical weather data from the Capital Bikeshare program in Washington, D.C.. For this purpose, you’ll be using a gradient boosting regressor. . As a first step, you’ll start by instantiating a gradient boosting regressor which you will train in the next exercise. . X_train.head() hr holiday workingday temp hum windspeed instant mnth yr 1236 12 0 1 0.72 0.45 0.0000 14240 8 1 1349 5 0 0 0.64 0.89 0.1940 14353 8 1 327 15 0 0 0.80 0.55 0.1642 13331 7 1 104 8 0 1 0.80 0.49 0.1343 13108 7 1 850 10 0 0 0.80 0.59 0.4179 13854 8 1 Clear to partly cloudy Light Precipitation Misty 1236 1 0 0 1349 1 0 0 327 1 0 0 104 1 0 0 850 1 0 0 y_train.head() 1236 305 1349 16 327 560 104 550 850 364 Name: cnt, dtype: int64 . # Import GradientBoostingRegressor from sklearn.ensemble import GradientBoostingRegressor # Instantiate gb gb = GradientBoostingRegressor(max_depth=4, n_estimators=200, random_state=2) . #### Train the GB regressor . # Fit gb to the training set gb.fit(X_train, y_train) # Predict test set labels y_pred = gb.predict(X_test) . #### Evaluate the GB regressor . # Import mean_squared_error as MSE from sklearn.metrics import mean_squared_error as MSE # Compute MSE mse_test = MSE(y_test, y_pred) # Compute RMSE rmse_test = mse_test ** 0.5 # Print RMSE print(&#39;Test set RMSE of gb: {:.3f}&#39;.format(rmse_test)) # Test set RMSE of gb: 52.065 . . 4.3 Stochastic Gradient Boosting (SGB) . #### Regression with SGB . As in the exercises from the previous lesson, you’ll be working with the Bike Sharing Demand dataset. In the following set of exercises, you’ll solve this bike count regression problem using stochastic gradient boosting. . # Import GradientBoostingRegressor from sklearn.ensemble import GradientBoostingRegressor # Instantiate sgbr sgbr = GradientBoostingRegressor(max_depth=4, subsample=0.9, max_features=0.75, n_estimators=200, random_state=2) . #### Train the SGB regressor . # Fit sgbr to the training set sgbr.fit(X_train, y_train) # Predict test set labels y_pred = sgbr.predict(X_test) . #### Evaluate the SGB regressor . # Import mean_squared_error as MSE from sklearn.metrics import mean_squared_error as MSE # Compute test set MSE mse_test = MSE(y_test, y_pred) # Compute test set RMSE rmse_test = mse_test ** 0.5 # Print rmse_test print(&#39;Test set RMSE of sgbr: {:.3f}&#39;.format(rmse_test)) # Test set RMSE of sgbr: 49.979 . The stochastic gradient boosting regressor achieves a lower test set RMSE than the gradient boosting regressor (which was 52.065 )! . . 5. Model Tuning . . 5.1 Tuning a CART’s Hyperprameters . . #### show hyperparameters . print(dt.get_params) &lt;bound method BaseEstimator.get_params of DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=1, splitter=&#39;best&#39;)&gt; . #### Set the tree’s hyperparameter grid . # Define params_dt params_dt = { &#39;max_depth&#39;:[2,3,4], &#39;min_samples_leaf&#39;:[0.12, 0.14, 0.16, 0.18]} . #### Search for the optimal tree . In this exercise, you’ll perform grid search using 5-fold cross validation to find dt ‘s optimal hyperparameters. Note that because grid search is an exhaustive process, it may take a lot time to train the model. . # Import GridSearchCV from sklearn.model_selection import GridSearchCV # Instantiate grid_dt grid_dt = GridSearchCV(estimator=dt, param_grid=params_dt, scoring=&#39;roc_auc&#39;, cv=5, n_jobs=-1) . #### Evaluate the optimal tree . # Import roc_auc_score from sklearn.metrics from sklearn.metrics import roc_auc_score # Extract the best estimator best_model = grid_dt.best_estimator_ # Predict the test set probabilities of the positive class y_pred_proba = best_model.predict_proba(X_test)[:,1] # Compute test_roc_auc test_roc_auc = roc_auc_score(y_test, y_pred_proba) # Print test_roc_auc print(&#39;Test set ROC AUC score: {:.3f}&#39;.format(test_roc_auc)) # Test set ROC AUC score: 0.610 . best_model DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=3, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=0.12, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=1, splitter=&#39;best&#39;) . An untuned classification-tree would achieve a ROC AUC score of 0.54 ! . . 5.2 Tuning a RF’s Hyperparameters . #### Random forests hyperparameters . rf.get_params &lt;bound method BaseEstimator.get_params of RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1, oob_score=False, random_state=2, verbose=0, warm_start=False)&gt; . #### Set the hyperparameter grid of RF . # Define the dictionary &#39;params_rf&#39; params_rf = { &#39;n_estimators&#39;:[100, 350, 500], &#39;max_features&#39;:[&#39;log2&#39;, &#39;auto&#39;, &#39;sqrt&#39;], &#39;min_samples_leaf&#39;:[2, 10, 30]} . #### Search for the optimal forest . # Import GridSearchCV from sklearn.model_selection import GridSearchCV # Instantiate grid_rf grid_rf = GridSearchCV(estimator=rf, param_grid=params_rf, scoring=&#39;neg_mean_squared_error&#39;, cv=3, verbose=1, n_jobs=-1) . #### Evaluate the optimal forest . # Import mean_squared_error from sklearn.metrics as MSE from sklearn.metrics import mean_squared_error as MSE # Extract the best estimator best_model = grid_rf.best_estimator_ # Predict test set labels y_pred = best_model.predict(X_test) # Compute rmse_test rmse_test = MSE(y_test, y_pred) ** 0.5 # Print rmse_test print(&#39;Test RMSE of best model: {:.3f}&#39;.format(rmse_test)) # Test RMSE of best model: 50.569 . The End. . Thank you for reading. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/machine-learning-with-tree-based-models-in-python.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/machine-learning-with-tree-based-models-in-python.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post23": {
            "title": "Introduction to Deep Learning with Keras",
            "content": "Introduction to Deep Learning with Keras . This is the memo of the 16th course (23 courses in all) of ‘Machine Learning Scientist with Python’ skill track. . You can find the original course HERE . . reference url: https://tensorspace.org/index.html . ### Course Description . Deep learning is here to stay! It’s the go-to technique to solve complex problems that arise with unstructured data and an incredible tool for innovation. Keras is one of the frameworks that make it easier to start developing deep learning models, and it’s versatile enough to build industry-ready models in no time. In this course, you will learn regression and save the earth by predicting asteroid trajectories, apply binary classification to distinguish between real and fake dollar bills, use multiclass classification to decide who threw which dart at a dart board, learn to use neural networks to reconstruct noisy images and much more. Additionally, you will learn how to better control your models during training and how to tune them to boost their performance. . ### . Introducing Keras | Going Deeper | Improving Your Model Performance | Advanced Model Architectures | . 1. Introducing Keras . . 1.1 What is Keras? . . 1.1.1 Describing Keras . Which of the following statements about Keras is false ? . Keras is integrated into TensorFlow, that means you can call Keras from within TensorFlow and get the best of both worlds. | Keras can work well on its own without using a backend, like TensorFlow. (False) | Keras is an open source project started by François Chollet. | . You’re good at spotting lies! Keras is a wrapper around a backend, so a backend like TensorFlow, Theano, CNTK, etc must be provided. . 1.1.2 Would you use deep learning? . Imagine you’re building an app that allows you to take a picture of your clothes and then shows you a pair of shoes that would match well. This app needs a machine learning module that’s in charge of identifying the type of clothes you are wearing, as well as their color and texture. Would you use deep learning to accomplish this task? . I’d use deep learning, since we are dealing with tabular data and neural networks work well with images. | I’d use deep learning since we are dealing with unstructured data and neural networks work well with images.(True) | This task can be easily accomplished with other machine learning algorithms, so deep learning is not required. | . You’re right! Using deep learning would be the easiest way. The model would generalize well if enough clothing images are provided. . . 1.2 Your first neural network . . 1.2.1 Hello nets! . You’re going to build a simple neural network to get a feeling for how quickly it is to accomplish in Keras. . You will build a network that takes two numbers as input , passes them through a hidden layer of 10 neurons , and finally outputs a single non-constrained number . . A non-constrained output can be obtained by avoiding setting an activation function in the output layer . This is useful for problems like regression, when we want our output to be able to take any value. . # Import the Sequential model and Dense layer from keras.models import Sequential from keras.layers import Dense # Create a Sequential model model = Sequential() # Add an input layer and a hidden layer with 10 neurons model.add(Dense(10, input_shape=(2,), activation=&quot;relu&quot;)) # Add a 1-neuron output layer model.add(Dense(1)) # Summarise your model model.summary() . Model: &quot;sequential_1&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_1 (Dense) (None, 10) 30 _________________________________________________________________ dense_2 (Dense) (None, 1) 11 ================================================================= Total params: 41 Trainable params: 41 Non-trainable params: 0 _________________________________________________________________ . You’ve just build your first neural network with Keras, well done! . 1.2.2 Counting parameters . You’ve just created a neural network. Create a new one now and take some time to think about the weights of each layer. The Keras Dense layer and the Sequential model are already loaded for you to use. . This is the network you will be creating: . # Instantiate a new Sequential model model = Sequential() # Add a Dense layer with five neurons and three inputs model.add(Dense(5, input_shape=(3,), activation=&quot;relu&quot;)) # Add a final Dense layer with one neuron and no activation model.add(Dense(1)) # Summarize your model model.summary() . Model: &quot;sequential_1&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_1 (Dense) (None, 5) 20 _________________________________________________________________ dense_2 (Dense) (None, 1) 6 ================================================================= Total params: 26 Trainable params: 26 Non-trainable params: 0 _________________________________________________________________ . Given the model you just built, which answer is correct regarding the number of weights (parameters) in the hidden layer ? . There are 20 parameters, 15 from the connection of our input layer to our hidden layer and 5 from the bias weight of each neuron in the hidden layer. . Great! You certainly know where those parameters come from! . 1.2.3 Build as shown! . You will take on a final challenge before moving on to the next lesson. Build the network shown in the picture below. Prove your mastered Keras basics in no time! . from keras.models import Sequential from keras.layers import Dense # Instantiate a Sequential model model = Sequential() # Build the input and hidden layer model.add(Dense(3, input_shape=(2,))) # Add the ouput layer model.add(Dense(1)) . Perfect! You’ve shown you can already translate a visual representation of a neural network into Keras code. . . 1.3 Surviving a meteor strike . . 1.3.1 Specifying a model . You will build a simple regression model to forecast the orbit of the meteor! . Your training data consist of measurements taken at time steps from -10 minutes before the impact region to +10 minutes after . Each time step can be viewed as an X coordinate in our graph, which has an associated position Y for the meteor at that time step. . Note that you can view this problem as approximating a quadratic function via the use of neural networks. . This data is stored in two numpy arrays: one called time_steps , containing the features , and another called y_positions , with the labels . . Feel free to look at these arrays in the console anytime, then build your model! Keras Sequential model and Dense layers are available for you to use. . # Instantiate a Sequential model model = Sequential() # Add a Dense layer with 50 neurons and an input of 1 neuron model.add(Dense(50, input_shape=(1,), activation=&#39;relu&#39;)) # Add two Dense layers with 50 neurons and relu activation model.add(Dense(50,activation=&#39;relu&#39;)) model.add(Dense(50,activation=&#39;relu&#39;)) # End your model with a Dense layer and no activation model.add(Dense(1)) . You are closer to forecasting the meteor orbit! It’s important to note we aren’t using an activation function in our output layer since y_positions aren’t bounded and they can take any value. Your model is performing regression. . 1.3.2 Training . You’re going to train your first model in this course, and for a good cause! . Remember that before training your Keras models you need to compile them . This can be done with the .compile() method. The .compile() method takes arguments such as the optimizer , used for weight updating, and the loss function, which is what we want to minimize. Training your model is as easy as calling the .fit() method, passing on the features , labels and number of epochs to train for. . The model you built in the previous exercise is loaded for you to use, along with the time_steps and y_positions data. . # Compile your model model.compile(optimizer = &#39;adam&#39;, loss = &#39;mse&#39;) print(&quot;Training started..., this can take a while:&quot;) # Fit your model on your data for 30 epochs model.fit(time_steps,y_positions, epochs = 30) # Evaluate your model print(&quot;Final lost value:&quot;,model.evaluate(time_steps, y_positions)) . Training started..., this can take a while: Epoch 1/30 32/2000 [..............................] - ETA: 14s - loss: 2465.2439 928/2000 [============&gt;.................] - ETA: 0s - loss: 1820.2874 1856/2000 [==========================&gt;...] - ETA: 0s - loss: 1439.9186 2000/2000 [==============================] - 0s 177us/step - loss: 1369.6929 ... Epoch 30/30 32/2000 [..............................] - ETA: 0s - loss: 0.1844 896/2000 [============&gt;.................] - ETA: 0s - loss: 0.2483 1696/2000 [========================&gt;.....] - ETA: 0s - loss: 0.2292 2000/2000 [==============================] - 0s 62us/step - loss: 0.2246 . 32/2000 [..............................] - ETA: 1s 1536/2000 [======================&gt;.......] - ETA: 0s 2000/2000 [==============================] - 0s 44us/step Final lost value: 0.14062700100243092 . Amazing! You can check the console to see how the loss function decreased as epochs went by. Your model is now ready to make predictions. . 1.3.3 Predicting the orbit! . You’ve already trained a model that approximates the orbit of the meteor approaching earth and it’s loaded for you to use. . Since you trained your model for values between -10 and 10 minutes, your model hasn’t yet seen any other values for different time steps. You will visualize how your model behaves on unseen data. . To see the source code of plot_orbit , type the following print(inspect.getsource(plot_orbit)) in the console. . Remember np.arange(x,y) produces a range of values from x to y-1 . . Hurry up, you’re running out of time! . # Predict the twenty minutes orbit twenty_min_orbit = model.predict(np.arange(-10, 11)) # Plot the twenty minute orbit plot_orbit(twenty_min_orbit) . # Predict the twenty minutes orbit eighty_min_orbit = model.predict(np.arange(-40, 41)) # Plot the twenty minute orbit plot_orbit(eighty_min_orbit) . . Your model fits perfectly to the scientists trajectory for time values between -10 to +10, the region where the meteor crosses the impact region, so we won’t be hit! However, it starts to diverge when predicting for further values we haven’t trained for. This shows neural networks learn according to the data they are fed with. Data quality and diversity are very important. You’ve barely scratched the surface of what neural networks can do. Are you prepared for the next chapter? . 2. Going Deeper . . 2.1 Binary classification . . 2.1.1 Exploring dollar bills . You will practice building classification models in Keras with the Banknote Authentication dataset. . Your goal is to distinguish between real and fake dollar bills. In order to do this, the dataset comes with 4 variables: variance , skewness , curtosis and entropy . These variables are calculated by applying mathematical operations over the dollar bill images. The labels are found in the class variable. . The dataset is pre-loaded in your workspace as banknotes , let’s do some data exploration! . # Import seaborn import seaborn as sns # Use pairplot and set the hue to be our class sns.pairplot(banknotes, hue=&#39;class&#39;) # Show the plot plt.show() # Describe the data print(&#39;Dataset stats: n&#39;, banknotes.describe()) # Count the number of observations of each class print(&#39;Observations per class: n&#39;, banknotes[&#39;class&#39;].value_counts()) . Dataset stats: variance skewness curtosis entropy count 96.000000 96.000000 96.000000 96.000000 mean -0.057791 -0.102829 0.230412 0.081497 std 1.044960 1.059236 1.128972 0.975565 min -2.084590 -2.621646 -1.482300 -3.034187 25% -0.839124 -0.916152 -0.415294 -0.262668 50% -0.026748 -0.037559 -0.033603 0.394888 75% 0.871034 0.813601 0.978766 0.745212 max 1.869239 1.634072 3.759017 1.343345 Observations per class: real 53 fake 43 Name: class, dtype: int64 . . Your pairplot shows that there are variables for which the classes spread out noticeably. This gives us an intuition about our classes being separable. Let’s build a model to find out what it can do! . 2.1.2 A binary classification model . Now that you know what the Banknote Authentication dataset looks like, we’ll build a simple model to distinguish between real and fake bills. . You will perform binary classification by using a single neuron as an output. The input layer will have 4 neurons since we have 4 features in our dataset. The model output will be a value constrained between 0 and 1. . We will interpret this number as the probability of our input variables coming from a fake dollar bill, with 1 meaning we are certain it’s fake. . # Import the sequential model and dense layer from keras.models import Sequential from keras.layers import Dense # Create a sequential model model = Sequential() # Add a dense layer model.add(Dense(1, input_shape=(4,), activation=&#39;sigmoid&#39;)) # Compile your model model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;sgd&#39;, metrics=[&#39;accuracy&#39;]) # Display a summary of your model model.summary() . Model: &quot;sequential_2&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_2 (Dense) (None, 1) 5 ================================================================= Total params: 5 Trainable params: 5 Non-trainable params: 0 _________________________________________________________________ . That was fast! Let’s use this model to make predictions! . 2.1.3 Is this dollar bill fake ? . You are now ready to train your model and check how well it performs when classifying new bills! The dataset has already been partitioned as X_train , X_test , y_train and y_test . . # Train your model for 20 epochs model.fit(X_train, y_train, epochs=20) # Evaluate your model accuracy on the test set accuracy = model.evaluate(X_test, y_test)[1] # Print accuracy print(&#39;Accuracy:&#39;,accuracy) # Accuracy: 0.8252427167105443 . Alright! It looks like you are getting a high accuracy with this simple model! . . 2.2 Multi-class classification . . 2.2.1 A multi-class model . You’re going to build a model that predicts who threw which dart only based on where that dart landed! (That is the dart’s x and y coordinates.) . This problem is a multi-class classification problem since each dart can only be thrown by one of 4 competitors. So classes are mutually exclusive, and therefore we can build a neuron with as many output as competitors and use the softmax activation function to achieve a total sum of probabilities of 1 over all competitors. . Keras Sequential model and Dense layer are already loaded for you to use. . # Instantiate a sequential model model = Sequential() # Add 3 dense layers of 128, 64 and 32 neurons each model.add(Dense(128, input_shape=(2,), activation=&#39;relu&#39;)) model.add(Dense(64, activation=&#39;relu&#39;)) model.add(Dense(32, activation=&#39;relu&#39;)) # Add a dense layer with as many neurons as competitors model.add(Dense(4, activation=&#39;softmax&#39;)) # Compile your model using categorical_crossentropy loss model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) . Good job! Your models are getting deeper, just as your knowledge on neural networks! . 2.2.2 Prepare your dataset . In the console you can check that your labels, darts.competitor are not yet in a format to be understood by your network. They contain the names of the competitors as strings. You will first turn these competitors into unique numbers,then use the to_categorical() function from keras.utils to turn these numbers into their one-hot encoded representation. . This is useful for multi-class classification problems, since there are as many output neurons as classes and for every observation in our dataset we just want one of the neurons to be activated. . The dart’s dataset is loaded as darts . Pandas is imported as pd . Let’s prepare this dataset! . darts.head() xCoord yCoord competitor 0 0.196451 -0.520341 Steve 1 0.476027 -0.306763 Susan 2 0.003175 -0.980736 Michael 3 0.294078 0.267566 Kate 4 -0.051120 0.598946 Steve darts.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 800 entries, 0 to 799 Data columns (total 3 columns): xCoord 800 non-null float64 yCoord 800 non-null float64 competitor 800 non-null object dtypes: float64(2), object(1) memory usage: 18.8+ KB . # Transform into a categorical variable darts.competitor = pd.Categorical(darts.competitor) # Assign a number to each category (label encoding) darts.competitor = darts.competitor.cat.codes # Print the label encoded competitors print(&#39;Label encoded competitors: n&#39;,darts.competitor.head()) . Label encoded competitors: 0 2 1 3 2 1 3 0 4 2 Name: competitor, dtype: int8 . # Transform into a categorical variable darts.competitor = pd.Categorical(darts.competitor) # Assign a number to each category (label encoding) darts.competitor = darts.competitor.cat.codes # Import to_categorical from keras utils module from keras.utils import to_categorical # Use to_categorical on your labels coordinates = darts.drop([&#39;competitor&#39;], axis=1) competitors = to_categorical(darts.competitor) # Now print the to_categorical() result print(&#39;One-hot encoded competitors: n&#39;,competitors) . One-hot encoded competitors: [[0. 0. 1. 0.] [0. 0. 0. 1.] [0. 1. 0. 0.] ... [0. 1. 0. 0.] [0. 1. 0. 0.] [0. 0. 0. 1.]] . Great! Each competitor is now a vector of length 4, full of zeroes except for the position representing her or himself. . 2.2.3 Training on dart throwers . Your model is now ready, just as your dataset. It’s time to train! . The coordinates and competitors variables you just transformed have been partitioned into coord_train , competitors_train , coord_test and competitors_test . Your model is also loaded. Feel free to visualize your training data or model.summary() in the console. . # Train your model on the training data for 200 epochs model.fit(coord_train,competitors_train,epochs=200) # Evaluate your model accuracy on the test data accuracy = model.evaluate(coord_test, competitors_test)[1] # Print accuracy print(&#39;Accuracy:&#39;, accuracy) # Accuracy: 0.8375 . Your model just trained for 200 epochs! The accuracy on the test set is quite high. What do the predictions look like? . 2.2.4 Softmax predictions . Your recently trained model is loaded for you. This model is generalizing well!, that’s why you got a high accuracy on the test set. . Since you used the softmax activation function, for every input of 2 coordinates provided to your model there’s an output vector of 4 numbers. Each of these numbers encodes the probability of a given dart being thrown by one of the 4 possible competitors. . When computing accuracy with the model’s .evaluate() method, your model takes the class with the highest probability as the prediction. np.argmax() can help you do this since it returns the index with the highest value in an array. . Use the collection of test throws stored in coords_small_test and np.argmax() to check this out! . # Predict on coords_small_test preds = model.predict(coords_small_test) # Print preds vs true values print(&quot;{:45} | {}&quot;.format(&#39;Raw Model Predictions&#39;,&#39;True labels&#39;)) for i,pred in enumerate(preds): print(&quot;{} | {}&quot;.format(pred,competitors_small_test[i])) . Raw Model Predictions | True labels [0.34438723 0.00842557 0.63167274 0.01551455] | [0. 0. 1. 0.] [0.0989717 0.00530467 0.07537904 0.8203446 ] | [0. 0. 0. 1.] [0.33512568 0.00785374 0.28132284 0.37569773] | [0. 0. 0. 1.] [0.8547263 0.01328656 0.11279515 0.01919206] | [1. 0. 0. 0.] [0.3540977 0.00867271 0.6223853 0.01484426] | [0. 0. 1. 0.] . # Predict on coords_small_test preds = model.predict(coords_small_test) # Print preds vs true values print(&quot;{:45} | {}&quot;.format(&#39;Raw Model Predictions&#39;,&#39;True labels&#39;)) for i,pred in enumerate(preds): print(&quot;{} | {}&quot;.format(pred,competitors_small_test[i])) # Extract the indexes of the highest probable predictions preds = [np.argmax(pred) for pred in preds] # Print preds vs true values print(&quot;{:10} | {}&quot;.format(&#39;Rounded Model Predictions&#39;,&#39;True labels&#39;)) for i,pred in enumerate(preds): print(&quot;{:25} | {}&quot;.format(pred,competitors_small_test[i])) . Rounded Model Predictions | True labels 2 | [0. 0. 1. 0.] 3 | [0. 0. 0. 1.] 3 | [0. 0. 0. 1.] 0 | [1. 0. 0. 0.] 2 | [0. 0. 1. 0.] . Well done! As you’ve seen you can easily interpret the softmax output. This can also help you spot those observations where your network is less certain on which class to predict, since you can see the probability distribution among classes. . . 2.3 Multi-label classification . . 2.3.1 An irrigation machine . You’re going to automate the watering of parcels by making an intelligent irrigation machine. Multi-label classification problems differ from multi-class problems in that each observation can be labeled with zero or more classes. So classes are not mutually exclusive. . To account for this behavior what we do is have an output layer with as many neurons as classes but this time, unlike in multi-class problems, each output neuron has a sigmoid activation function. This makes the output layer able to output a number between 0 and 1 in any of its neurons. . Keras Sequential() model and Dense() layers are preloaded. It’s time to build an intelligent irrigation machine! . # Instantiate a Sequential model model = Sequential() # Add a hidden layer of 64 neurons and a 20 neuron&#39;s input model.add(Dense(64,input_shape=(20,), activation=&#39;relu&#39;)) # Add an output layer of 3 neurons with sigmoid activation model.add(Dense(3, activation=&#39;sigmoid&#39;)) # Compile your model with adam and binary crossentropy loss model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.summary() . Model: &quot;sequential_2&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_3 (Dense) (None, 64) 1344 _________________________________________________________________ dense_4 (Dense) (None, 3) 195 ================================================================= Total params: 1,539 Trainable params: 1,539 Non-trainable params: 0 _________________________________________________________________ . Great! You’ve already built 3 models for 3 different problems! . 2.3.2 Training with multiple labels . An output of your multi-label model could look like this: [0.76 , 0.99 , 0.66 ] . If we round up probabilities higher than 0.5, this observation will be classified as containing all 3 possible labels [1,1,1] . For this particular problem, this would mean watering all 3 parcels in your field is the right thing to do given the input sensor measurements. . You will now train and predict with the model you just built. sensors_train , parcels_train , sensors_test and parcels_test are already loaded for you to use. Let’s see how well your machine performs! . # Train for 100 epochs using a validation split of 0.2 model.fit(sensors_train, parcels_train, epochs = 100, validation_split = 0.2) # Predict on sensors_test and round up the predictions preds = model.predict(sensors_test) preds_rounded = np.round(preds) # Print rounded preds print(&#39;Rounded Predictions: n&#39;, preds_rounded) # Evaluate your model&#39;s accuracy on the test data accuracy = model.evaluate(sensors_test, parcels_test)[1] # Print accuracy print(&#39;Accuracy:&#39;, accuracy) . ... Epoch 100/100 32/1120 [..............................] - ETA: 0s - loss: 0.0439 - acc: 0.9896 1024/1120 [==========================&gt;...] - ETA: 0s - loss: 0.0320 - acc: 0.9935 1120/1120 [==============================] - 0s 62us/step - loss: 0.0320 - acc: 0.9935 - val_loss: 0.5132 - val_acc: 0.8702 Rounded Predictions: [[1. 1. 0.] [0. 1. 0.] [0. 1. 0.] ... [1. 1. 0.] [0. 1. 0.] [0. 1. 1.]] 32/600 [&gt;.............................] - ETA: 0s 600/600 [==============================] - 0s 26us/step Accuracy: 0.8844444648424784 . Great work on automating this farm! You can see how the validation_split argument is useful for evaluating how your model performs as it trains. . . 2.4 Keras callbacks . . 2.4.1 The history callback . The history callback is returned by default every time you train a model with the .fit() method. To access these metrics you can access the history dictionary inside the returned callback object and the corresponding keys. . The irrigation machine model you built in the previous lesson is loaded for you to train, along with its features and labels (X and y). This time you will store the model’s history callback and use the validation_data parameter as it trains. . You will plot the results stored in history with plot_accuracy() and plot_loss() , two simple matplotlib functions. You can check their code in the console by typing print(inspect.getsource(plot_loss)) . . Let’s see the behind the scenes of our training! . # Train your model and save it&#39;s history history = model.fit(X_train, y_train, epochs = 50, validation_data=(X_test, y_test)) # Plot train vs test loss during training plot_loss(history.history[&#39;loss&#39;], history.history[&#39;val_loss&#39;]) # Plot train vs test accuracy during training plot_accuracy(history.history[&#39;acc&#39;], history.history[&#39;val_acc&#39;]) . . Awesome! These graphs are really useful for detecting overfitting and to know if your neural network would benefit from more training data. More on this on the next chapter! . 2.4.2 Early stopping your model . The early stopping callback is useful since it allows for you to stop the model training if it no longer improves after a given number of epochs. To make use of this functionality you need to pass the callback inside a list to the model’s callback parameter in the .fit() method. . The model you built to detect fake dollar bills is loaded for you to train, this time with early stopping. X_train , y_train , X_test and y_test are also available for you to use. . # Import the early stopping callback from keras.callbacks import EarlyStopping # Define a callback to monitor val_acc monitor_val_acc = EarlyStopping(monitor=&#39;val_acc&#39;, patience=5) # Train your model using the early stopping callback model.fit(X_train, y_train, epochs=1000, validation_data=(X_test, y_test), callbacks=[monitor_val_acc]) . ... Epoch 26/1000 32/960 [&gt;.............................] - ETA: 0s - loss: 0.2096 - acc: 0.9688 800/960 [========================&gt;.....] - ETA: 0s - loss: 0.2079 - acc: 0.9563 960/960 [==============================] - 0s 94us/step - loss: 0.2091 - acc: 0.9531 - val_loss: 0.2116 - val_acc: 0.9417 . Great! Now you won’t ever fall short of epochs! . 2.4.3 A combination of callbacks . Deep learning models can take a long time to train, especially when you move to deeper architectures and bigger datasets. Saving your model every time it improves as well as stopping it when it no longer does allows you to worry less about choosing the number of epochs to train for. You can also restore a saved model anytime. . The model training and validation data are available in your workspace as X_train , X_test , y_train , and y_test . . Use the EarlyStopping() and the ModelCheckpoint() callbacks so that you can go eat a jar of cookies while you leave your computer to work! . # Import the EarlyStopping and ModelCheckpoint callbacks from keras.callbacks import EarlyStopping, ModelCheckpoint # Early stop on validation accuracy monitor_val_acc = EarlyStopping(monitor = &#39;val_acc&#39;, patience=3) # Save the best model as best_banknote_model.hdf5 modelCheckpoint = ModelCheckpoint(&#39;best_banknote_model.hdf5&#39;, save_best_only = True) # Fit your model for a stupid amount of epochs history = model.fit(X_train, y_train, epochs = 10000000, callbacks = [monitor_val_acc, modelCheckpoint], validation_data = (X_test, y_test)) . ... Epoch 4/10000000 32/960 [&gt;.............................] - ETA: 0s - loss: 0.2699 - acc: 0.9688 960/960 [==============================] - 0s 59us/step - loss: 0.2679 - acc: 0.9312 - val_loss: 0.2870 - val_acc: 0.9126 . This is a powerful callback combo! Now you always get the model that performed best, even if you early stopped at one that was already performing worse. . 3. Improving Your Model Performance . 3.1 Learning curves . . 3.1.1 Learning the digits . You’re going to build a model on the digits dataset , a sample dataset that comes pre-loaded with scikit learn. The digits dataset consist of 8×8 pixel handwritten digits from 0 to 9 : You want to distinguish between each of the 10 possible digits given an image, so we are dealing with multi-class classification . . The dataset has already been partitioned into X_train , y_train , X_test , and y_test using 30% of the data as testing data. The labels are one-hot encoded vectors, so you don’t need to use Keras to_categorical() function. . Let’s build this new model ! . # Instantiate a Sequential model model = Sequential() # Input and hidden layer with input_shape, 16 neurons, and relu model.add(Dense(16, input_shape = (8*8,), activation = &#39;relu&#39;)) # Output layer with 10 neurons (one per digit) and softmax model.add(Dense(10, activation = &#39;softmax&#39;)) # Compile your model model.compile(optimizer = &#39;adam&#39;, loss = &#39;categorical_crossentropy&#39;, metrics = [&#39;accuracy&#39;]) # Test if your model works and can process input data print(model.predict(X_train)) . Great! Predicting on training data inputs before training can help you quickly check that your model works as expected. . 3.1.2 Is the model overfitting? . Let’s train the model you just built and plot its learning curve to check out if it’s overfitting! You can make use of loaded function plot_loss() to plot training loss against validation loss, you can get both from the history callback. . If you want to inspect the plot_loss() function code, paste this in the console: print(inspect.getsource(plot_loss)) . # Train your model for 60 epochs, using X_test and y_test as validation data h_callback = model.fit(X_train, y_train, epochs=60, validation_data=(X_test, y_test), verbose=0) # Extract from the history object loss and val_loss to plot the learning curve plot_loss(h_callback.history[&#39;loss&#39;], h_callback.history[&#39;val_loss&#39;]) . . Just by looking at the overall picture, do you think the learning curve shows this model is overfitting after having trained for 60 epochs? . No, the test loss is not getting higher as the epochs go by. . Awesome choice! This graph doesn’t show overfitting but convergence. It looks like your model has learned all it could from the data and it no longer improves. . 3.1.3 Do we need more data? . It’s time to check whether the digits dataset model you built benefits from more training examples! . In order to keep code to a minimum, various things are already initialized and ready to use: . Themodelyou just built. | X_train, y_train, X_test, and y_test. | The initial_weights of your model, saved after using model.get_weights(). | A defined list of training sizes: training_sizes. | A defined EarlyStopping callback monitoring loss: early_stop. | Two empty lists to store the evaluation results: train_accs and test_accs. | . Train your model on the different training sizes and evaluate the results on X_test . End by plotting the results with plot_results(). . The full code for this exercise can be found on the slides! . train_sizes array([ 125, 502, 879, 1255]) . for size in training_sizes: # Get a fraction of training data (we only care about the training data) X_train_frac, y_train_frac = X_train[:size], y_train[:size] # Reset the model to the initial weights and train it on the new training data fraction model.set_weights(initial_weights) model.fit(X_train_frac, y_train_frac, epochs=50, callbacks=[early_stop]) # Evaluate and store both: the training data fraction and the complete test set results train_accs.append(model.evaluate(X_train_frac, y_train_frac)[1]) test_accs.append(model.evaluate(X_test, y_test)[1]) # Plot train vs test accuracies plot_results(train_accs, test_accs) . . Great job, that was a lot of code to understand! The results shows that your model would not benefit a lot from more training data, since the test set is starting to flatten in accuracy already. . 3.2 Activation functions . . 3.2.1 Different activation functions . tanh(hyperbolic tangent) . The sigmoid() , tanh() , ReLU() , and leaky_ReLU() functions have been defined and ready for you to use. Each function receives an input number X and returns its corresponding Y value. . Which of the statements below is false ? . The sigmoid() takes a value of 0.5 when X = 0 whilst tanh() takes a value of 0 . | The leaky-ReLU() takes a value of -0.01 when X = -1 whilst ReLU() takes a value of 0 . | **The sigmoid() and tanh() both take values close to -1 for big negative numbers.(false)** | . Great! For big negative numbers the sigmoid approaches 0 not -1 whilst the tanh() does take values close to -1 . . 3.2.2 Comparing activation functions . Comparing activation functions involves a bit of coding, but nothing you can’t do! . You will try out different activation functions on the multi-label model you built for your irrigation machine in chapter 2. The function get_model() returns a copy of this model and applies the activation function, passed on as a parameter, to its hidden layer. . You will build a loop that goes through several activation functions, generates a new model for each and trains it. Storing the history callback in a dictionary will allow you to compare and visualize which activation function performed best in the next exercise! . # Set a seed np.random.seed(27) # Activation functions to try activations = [&#39;relu&#39;, &#39;leaky_relu&#39;, &#39;sigmoid&#39;, &#39;tanh&#39;] # Loop over the activation functions activation_results = {} for act in activations: # Get a new model with the current activation model = get_model(act_function=act) # Fit the model and store the history results h_callback = model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=20, verbose=0) activation_results[act] = h_callback . Finishing with relu ... Finishing with leaky_relu ... Finishing with sigmoid ... Finishing with tanh ... . Awesome job! You’ve trained 4 models with 4 different activation functions, let’s see how well they performed! . 3.2.3 Comparing activation functions II . The code used in the previous exercise has been executed to obtain the activation_results with the difference that 100 epochs instead of 20 are used. That way you’ll have more epochs to further compare how the training evolves per activation function. . For every history callback of each activation function in activation_results : . The history.history[&#39;val_loss&#39;] has been extracted. | The history.history[&#39;val_acc&#39;] has been extracted. | Both are saved in two dictionaries: val_loss_per_function and val_acc_per_function . | . Pandas is also loaded for you to use as pd . Let’s plot some quick comparison validation loss and accuracy charts with pandas! . # Create a dataframe from val_loss_per_function val_loss= pd.DataFrame(val_loss_per_function) # Call plot on the dataframe val_loss.plot() plt.show() # Create a dataframe from val_acc_per_function val_acc = pd.DataFrame(val_acc_per_function) # Call plot on the dataframe val_acc.plot() plt.show() . . 3.3 Batch size and batch normalization . . 3.3.1 Changing batch sizes . You’ve seen models are usually trained in batches of a fixed size. The smaller a batch size, the more weight updates per epoch, but at a cost of a more unstable gradient descent. Specially if the batch size is too small and it’s not representative of the entire training set. . Let’s see how different batch sizes affect the accuracy of a binary classification model that separates red from blue dots. . You’ll use a batch size of one, updating the weights once per sample in your training set for each epoch. Then you will use the entire dataset, updating the weights only once per epoch. . # Get a fresh new model with get_model model = get_model() # Train your model for 5 epochs with a batch size of 1 model.fit(X_train, y_train, epochs=5, batch_size=1) print(&quot; n The accuracy when using a batch of size 1 is: &quot;, model.evaluate(X_test, y_test)[1]) # The accuracy when using a batch of size 1 is: 0.9733333333333334 . model = get_model() # Fit your model for 5 epochs with a batch of size the training set model.fit(X_train, y_train, epochs=5, batch_size=X_train.shape[0]) print(&quot; n The accuracy when using the whole training set as a batch was: &quot;, model.evaluate(X_test, y_test)[1]) # The accuracy when using the whole training set as a batch was: 0.553333334128062 . 3.3.2 Batch normalizing a familiar model . Remember the digits dataset you trained in the first exercise of this chapter? . . A multi-class classification problem that you solved using softmax and 10 neurons in your output layer. . You will now build a new deeper model consisting of 3 hidden layers of 50 neurons each, using batch normalization in between layers. The kernel_initializer parameter is used to initialize weights in a similar way. . # Import batch normalization from keras layers from keras.layers import BatchNormalization # Build your deep network batchnorm_model = Sequential() batchnorm_model.add(Dense(50, input_shape=(64,), activation=&#39;relu&#39;, kernel_initializer=&#39;normal&#39;)) batchnorm_model.add(BatchNormalization()) batchnorm_model.add(Dense(50, activation=&#39;relu&#39;, kernel_initializer=&#39;normal&#39;)) batchnorm_model.add(BatchNormalization()) batchnorm_model.add(Dense(50, activation=&#39;relu&#39;, kernel_initializer=&#39;normal&#39;)) batchnorm_model.add(BatchNormalization()) batchnorm_model.add(Dense(10, activation=&#39;softmax&#39;, kernel_initializer=&#39;normal&#39;)) # Compile your model with sgd batchnorm_model.compile(optimizer=&#39;sgd&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . Congratulations! That was a deep model indeed. Let’s compare how it performs against this very same model without batch normalization! . 3.3.3 Batch normalization effects . Batch normalization tends to increase the learning speed of our models and make their learning curves more stable. Let’s see how two identical models with and without batch normalization compare. . The model you just built batchnorm_model is loaded for you to use. An exact copy of it without batch normalization: standard_model , is available as well. You can check their summary() in the console. X_train , y_train , X_test , and y_test are also loaded so that you can train both models. . You will compare the accuracy learning curves for both models plotting them with compare_histories_acc() . . You can check the function pasting print(inspect.getsource(compare_histories_acc)) in the console. . # Train your standard model, storing its history callback h1_callback = standard_model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=10, verbose=0) # Train the batch normalized model you recently built, store its history callback h2_callback = batchnorm_model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=10, verbose=0) # Call compare_histories_acc passing in both model histories compare_histories_acc(h1_callback, h2_callback) . . Outstanding! You can see that for this deep model batch normalization proved to be useful, helping the model obtain high accuracy values just over the first 10 training epochs. . 3.4 Hyperparameter tuning . . 3.4.1 Preparing a model for tuning . Let’s tune the hyperparameters of a binary classification model that does well classifying the breast cancer dataset . . You’ve seen that the first step to turn a model into a sklearn estimator is to build a function that creates it. This function is important since you can play with the parameters it receives to achieve the different models you’d like to try out. . Build a simple create_model() function that receives a learning rate and an activation function as parameters. The Adam optimizer has been imported as an object from keras.optimizers so that you can change its learning rate parameter. . # Creates a model given an activation and learning rate def create_model(learning_rate, activation): # Create an Adam optimizer with the given learning rate opt = Adam(lr = learning_rate) # Create your binary classification model model = Sequential() model.add(Dense(128, input_shape = (30,), activation = activation)) model.add(Dense(256, activation = activation)) model.add(Dense(1, activation = &#39;sigmoid&#39;)) # Compile your model with your optimizer, loss, and metrics model.compile(optimizer = opt, loss = &#39;binary_crossentropy&#39;, metrics = [&#39;accuracy&#39;]) return model . Well done! With this function ready you can now create a sklearn estimator and perform hyperparameter tuning! . 3.4.2 Tuning the model parameters . It’s time to try out different parameters on your model and see how well it performs! . The create_model() function you built in the previous exercise is loaded for you to use. . Since fitting the RandomizedSearchCV would take too long, the results you’d get are printed in the show_results() function. You could try random_search.fit(X,y) in the console yourself to check it does work after you have built everything else, but you will probably timeout your exercise (so copy your code first if you try it!). . You don’t need to use the optional epochs and batch_size parameters when building your KerasClassifier since you are passing them as params to the random search and this works as well. . # Import KerasClassifier from keras scikit learn wrappers from keras.wrappers.scikit_learn import KerasClassifier # Create a KerasClassifier model = KerasClassifier(build_fn = create_model) # Define the parameters to try out params = {&#39;activation&#39;: [&#39;relu&#39;, &#39;tanh&#39;], &#39;batch_size&#39;: [32, 128, 256], &#39;epochs&#39;: [50, 100, 200], &#39;learning_rate&#39;: [0.1, 0.01, 0.001]} # Create a randomize search cv object passing in the parameters to try random_search = RandomizedSearchCV(model, param_distributions = params, cv = KFold(3)) # Running random_search.fit(X,y) would start the search,but it takes too long! show_results() . Best: 0.975395 using {learning_rate: 0.001, epochs: 50, batch_size: 128, activation: relu} 0.956063 (0.013236) with: {learning_rate: 0.1, epochs: 200, batch_size: 32, activation: tanh} 0.970123 (0.019838) with: {learning_rate: 0.1, epochs: 50, batch_size: 256, activation: tanh} 0.971880 (0.006524) with: {learning_rate: 0.01, epochs: 100, batch_size: 128, activation: tanh} 0.724077 (0.072993) with: {learning_rate: 0.1, epochs: 50, batch_size: 32, activation: relu} 0.588752 (0.281793) with: {learning_rate: 0.1, epochs: 100, batch_size: 256, activation: relu} 0.966608 (0.004892) with: {learning_rate: 0.001, epochs: 100, batch_size: 128, activation: tanh} 0.952548 (0.019734) with: {learning_rate: 0.1, epochs: 50, batch_size: 256, activation: relu} 0.971880 (0.006524) with: {learning_rate: 0.001, epochs: 200, batch_size: 128, activation: relu} 0.968366 (0.004239) with: {learning_rate: 0.01, epochs: 100, batch_size: 32, activation: relu} 0.910369 (0.055824) with: {learning_rate: 0.1, epochs: 100, batch_size: 128, activation: relu} . That was great! I’m glad that the server is still working. Now that we have a better idea of which parameters are performing best, let’s use them! . 3.4.3 Training with cross-validation . Time to train your model with the best parameters found: 0.001 for the learning rate , 50 epochs , a 128 batch_size and relu activations . . The create_model() function has been redefined so that it now creates a model with those parameters. X and y are loaded for you to use as features and labels. . In this exercise you do pass the best epochs and batch size values found for your model to the KerasClassifier object so that they are used when performing cross validation. . End this chapter by training an awesome tuned model on the breast cancer dataset ! . # Import KerasClassifier from keras wrappers from keras.wrappers.scikit_learn import KerasClassifier # Create a KerasClassifier model = KerasClassifier(build_fn = create_model(learning_rate = 0.001, activation = &#39;relu&#39;), epochs = 50, batch_size = 128, verbose = 0) # Calculate the accuracy score for each fold kfolds = cross_val_score(model, X, y, cv = 3) # Print the mean accuracy print(&#39;The mean accuracy was:&#39;, kfolds.mean()) # Print the accuracy standard deviation print(&#39;With a standard deviation of:&#39;, kfolds.std()) . The mean accuracy was: 0.9718834066666666 With a standard deviation of: 0.002448915612216046 . Amazing! Now you can more reliably test out different parameters on your networks and find better models! . 4. Advanced Model Architectures . 4.1 Tensors, layers, and autoencoders . . 4.1.1 It’s a flow of tensors . If you have already built a model, you can use the model.layers and the keras.backend to build functions that, provided with a valid input tensor, return the corresponding output tensor. . This is a useful tool when trying to understand what is going on inside the layers of a neural network. . For instance, if you get the input and output from the first layer of a network, you can build an inp_to_out function that returns the result of carrying out forward propagation through only the first layer for a given input tensor. . So that’s what you’re going to do right now! . X_test from the Banknote Authentication dataset and its model are preloaded. Type model.summary() in the console to check it. . # Import keras backend import keras.backend as K # Input tensor from the 1st layer of the model inp = model.layers[0].input # Output tensor from the 1st layer of the model out = model.layers[0].output # Define a function from inputs to outputs inp_to_out = K.function([inp],[out]) # Print the results of passing X_test through the 1st layer print(inp_to_out([X_test])) . [array([[7.77682841e-01, 0.00000000e+00], [0.00000000e+00, 0.00000000e+00], [0.00000000e+00, 1.50813460e+00], [0.00000000e+00, 1.34600031e+00], ... . Nice job! Let’s use this function for something more interesting. . 4.1.2 Neural separation . Neurons learn by updating their weights to output values that help them distinguish between the input classes. So put on your gloves because you’re going to perform brain surgery! . You will make use of the inp_to_out() function you just built to visualize the output of two neurons in the first layer of the Banknote Authentication model as epochs go by. Plotting the outputs of both of these neurons against each other will show you the difference in output depending on whether each bill was real or fake. . The model you built in chapter 2 is ready for you to use, just like X_test and y_test . Copy print(inspect.getsource(plot)) in the console if you want to check plot() . . You’re performing heavy duty, once it’s done, take a look at the graphs to watch the separation live! . print(inspect.getsource(plot)) def plot(): fig, ax = plt.subplots() plt.scatter(layer_output[:, 0], layer_output[:, 1],c=y_test,edgecolors=&#39;none&#39;) plt.title(&#39;Epoch: {}, Test Acc: {:3.1f} %&#39;.format(i+1, test_accuracy * 100.0)) plt.show() . for i in range(0, 21): # Train model for 1 epoch h = model.fit(X_train, y_train, batch_size=16, epochs=1,verbose=0) if i%4==0: # Get the output of the first layer layer_output = inp_to_out([X_test])[0] # Evaluate model accuracy for this epoch test_accuracy = model.evaluate(X_test, y_test)[1] # Plot 1st vs 2nd neuron output plot() . . That took a while! If you take a look at the graphs you can see how the neurons are learning to spread out the classes during training. Click in between graphs fast, it’s like a movie! . 4.1.3 Building an autoencoder . Autoencoders have several interesting applications like anomaly detection or image denoising. They aim at producing an output identical to its inputs. The input will be compressed into a lower dimensional space, encoded . The model then learns to decode it back to its original form. . You will encode and decode the MNIST dataset of handwritten digits, the hidden layer will encode a 32-dimensional representation of the image, which originally consists of 784 pixels. . The Sequential model and Dense layers are ready for you to use. . Let’s build an autoencoder! . # Start with a sequential model autoencoder = Sequential() # Add a dense layer with input the original image pixels and neurons the encoded representation autoencoder.add(Dense(32, input_shape=(784, ), activation=&quot;relu&quot;)) # Add an output layer with as many neurons as the orginal image pixels autoencoder.add(Dense(784, activation=&quot;sigmoid&quot;)) # Compile your model with adadelta autoencoder.compile(optimizer=&#39;adadelta&#39;, loss=&#39;binary_crossentropy&#39;) # Summarize your model structure autoencoder.summary() . Model: &quot;sequential_1&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_1 (Dense) (None, 32) 25120 _________________________________________________________________ dense_2 (Dense) (None, 784) 25872 ================================================================= Total params: 50,992 Trainable params: 50,992 Non-trainable params: 0 _________________________________________________________________ . Great start! Your model is now ready. Let’s see what you can do with it! . 4.1.4 De-noising like an autoencoder . Okay, you have just built an autoencoder model. Let’s see how it handles a more challenging task. . First, you will build a model that encodes images, and you will check how different digits are represented with show_encodings() . You can change the number parameter of this function to check other digits in the console. . Then, you will apply your autoencoder to noisy images from MNIST , it should be able to clean the noisy artifacts. . X_test_noise is loaded in your workspace. The digits in this data look like this: . . Apply the power of the autoencoder! . # Build your encoder by using the first layer of your autoencoder encoder = Sequential() encoder.add(autoencoder.layers[0]) # Encode the noisy images and show the encodings for your favorite number [0-9] encodings = encoder.predict(X_test_noise) show_encodings(encodings, number = 1) . . # Predict on the noisy images with your autoencoder decoded_imgs = autoencoder.predict(X_test_noise) # Plot noisy vs decoded images compare_plot(X_test_noise, decoded_imgs) . Amazing! The noise is gone now! You could get a better reconstruction by using a convolutional autoencoder. . . 4.2 Intro to CNNs . . 4.2.1 Building a CNN model . Building a CNN model in Keras isn’t much more difficult than building any of the models you’ve already built throughout the course! You just need to make use of convolutional layers. . You’re going to build a shallow convolutional model that classifies the MNIST dataset of digits. The same one you de-noised with your autoencoder!. The images are 28×28 pixels and just have one channel. . Go ahead and build this small convolutional model! . # Import the Conv2D and Flatten layers and instantiate model from keras.layers import Conv2D,Flatten model = Sequential() # Add a convolutional layer of 32 filters of size 3x3 model.add(Conv2D(32, input_shape=(28, 28, 1), kernel_size=3, activation=&#39;relu&#39;)) # Add a convolutional layer of 16 filters of size 3x3 model.add(Conv2D(16, kernel_size=3, activation=&#39;relu&#39;)) # Flatten the previous layer output model.add(Flatten()) # Add as many outputs as classes with softmax activation model.add(Dense(10, activation = &#39;softmax&#39;)) . Well done! You can see that the key concepts are the same, you just have to use new layers! . 4.2.2 Looking at convolutions . Inspecting the activations of a convolutional layer is a cool thing. You have to do it at least once in your lifetime! . To do so, you will build a new model with the Keras Model object, which takes in a list of inputs and a list of outputs. The output you will provide to this new model is the first convolutional layer outputs when given an MNIST digit as input image. . The convolutional model you built in the previous exercise has already been trained for you. You can check it with model.summary() in the console. . Let’s look at a couple convolutional masks that were learned in the first convolutional layer of this model! . # Obtain a reference to the outputs of the first layer first_layer_output = model.layers[0].output # Build a model using the model&#39;s input and the first layer output first_layer_model = Model(inputs = model.layers[0].input, outputs = first_layer_output) # Use this model to predict on X_test activations = first_layer_model.predict(X_test) # Plot the activations of first digit of X_test for the 15th filter axs[0].matshow(activations[0,:,:,14], cmap = &#39;viridis&#39;) # Do the same but for the 18th filter now axs[1].matshow(activations[0,:,:,17], cmap = &#39;viridis&#39;) plt.show() . . Hurrah! Each neuron filter of the first layer learned a different convolution. The 15th filter (a.k.a convolutional mask) learned to detect horizontal traces in your digits. On the other hand, filter 18th seems to be checking for vertical traces. . 4.2.3 Preparing your input image . When using an already trained model like ResNet50 , we need to make sure that we fit the network the way it was originally trained. So if we want to use a trained model on our custom images, these images need to have the same dimensions as the one used in the original model. . The original ResNet50 model was trained with images of size 224×224 pixels and a number of preprocessing operations; like the subtraction of the mean pixel value in the training set for all training images. . You will go over these preprocessing steps as you prepare this dog’s (named Ivy) image into one that can be classified by ResNet50 . . # Import image and preprocess_input from keras.preprocessing import image from keras.applications.resnet50 import preprocess_input # Load the image with the right target size for your model img = image.load_img(img_path, target_size=(224, 224)) # Turn it into an array img_array = image.img_to_array(img) # Expand the dimensions of the image, this is so that it fits the expected model input format img_expanded = np.expand_dims(img_array, axis = 0) # Pre-process the img in the same way original images were img_ready = preprocess_input(img_expanded) . Alright! Ivy is now ready for ResNet50. Do you know this dog’s breed? Let’s see what this model thinks it is! . 4.2.4 Using a real world model . Okay, so Ivy’s picture is ready to be used by ResNet50 . It is stored in img_ready and now looks like this: . ResNet50 is a model trained on the Imagenet dataset that is able to distinguish between 1000 different objects. ResNet50 is a deep model with 50 layers, you can check it in 3D here . . ResNet50 and decode_predictions have both been imported from keras.applications.resnet50 for you. . It’s time to use this trained model to find out Ivy’s breed! . # Instantiate a ResNet50 model with &#39;imagenet&#39; weights model = ResNet50(weights=&#39;imagenet&#39;) # Predict with ResNet50 on your already processed img preds = model.predict(img_ready) # Decode the first 3 predictions print(&#39;Predicted:&#39;, decode_predictions(preds, top=3)[0]) . Predicted: [(‘n02088364’, ‘beagle’, 0.8280003), (‘n02089867’, ‘Walker_hound’, 0.12915272), (‘n02089973’, ‘English_foxhound’, 0.03711732)] . Amazing! Now you know Ivy is a Beagle and that deep learning models that have already been trained for you are easy to use! . . 4.3 Intro to LSTMs . . 4.3.1 Text prediction with LSTMs . During the following exercises you will build an LSTM model that is able to predict the next word using a small text dataset. This dataset consist of cleaned quotes from the The Lord of the Ring movies. You can find them in the text variable. . You will turn this text into sequences of length 4 and make use of the Keras Tokenizer to prepare the features and labels for your model! . The Keras Tokenizer is already imported for you to use. It assigns a unique number to each unique word, and stores the mappings in a dictionary. This is important since the model deals with numbers but we later will want to decode the output numbers back into words. . You’re working with this small chunk of The Lord of The Ring quotes: . It is not the strength of the body but the strength of the spirit. | It is useless to meet revenge with revenge it will heal nothing. | Even the smallest person can change the course of history. | All we have to decide is what to do with the time that is given us. | The burned hand teaches best. After that, advice about fire goes to the heart. | . text &#39;it is not the strength of the body but the strength of the spirit it is useless to meet revenge with revenge it will heal nothing even the smallest person can change the course of history all we have to decide is what to do with the time that is given us the burned hand teaches best after that advice about fire goes to the heart&#39; . # Split text into an array of words words = text.split() # Make sentences of 4 words each, moving one word at a time sentences = [] for i in range(4, len(words)): sentences.append(&#39; &#39;.join(words[i-4:i])) # Instantiate a Tokenizer, then fit it on the sentences tokenizer = Tokenizer() tokenizer.fit_on_texts(sentences) # Turn sentences into a sequence of numbers sequences = tokenizer.texts_to_sequences(sentences) print(&quot;Sentences: n {} n Sequences: n {}&quot;.format(sentences[:5],sequences[:5])) . Lines: [&#39;it is not the&#39;, &#39;is not the strength&#39;, &#39;not the strength of&#39;, &#39;the strength of the&#39;, &#39;strength of the body&#39;] Sequences: [[5, 2, 42, 1], [2, 42, 1, 6], [42, 1, 6, 4], [1, 6, 4, 1], [6, 4, 1, 10]] . Great! Your lines are now sequences of numbers, check that identical words are assigned the same number. . 4.3.2 Build your LSTM model . You’ve already prepared your sequences of text, with each of the sequences consisting of four words. It’s time to build your LSTM model! . Your model will be trained on the first three words of each sequence, predicting the 4th one. You are going to use an Embedding layer that will essentially learn to turn words into vectors. These vectors will then be passed to a simple LSTM layer. Our output is a Dense layer with as many neurons as words in the vocabulary and softmax activation. This is because we want to obtain the highest probable next word out of all possible words. . The size of the vocabulary of words (the unique number of words) is stored in vocab_size . . # Import the Embedding, LSTM and Dense layer from keras.layers import Embedding, LSTM, Dense model = Sequential() # Add an Embedding layer with the right parameters model.add(Embedding(input_dim=vocab_size, output_dim=8, input_length=3)) # Add a 32 unit LSTM layer model.add(LSTM(32)) # Add a hidden Dense layer of 32 units and an output layer of vocab_size with softmax model.add(Dense(32, activation=&#39;relu&#39;)) model.add(Dense(vocab_size, activation=&#39;softmax&#39;)) model.summary() . Model: &quot;sequential_1&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_1 (Embedding) (None, 3, 8) 352 _________________________________________________________________ lstm_1 (LSTM) (None, 32) 5248 _________________________________________________________________ dense_1 (Dense) (None, 32) 1056 _________________________________________________________________ dense_2 (Dense) (None, 44) 1452 ================================================================= Total params: 8,108 Trainable params: 8,108 Non-trainable params: 0 _________________________________________________________________ . That’s a nice looking model you’ve built! You’ll see that this model is powerful enough to learn text relationships. Specially because we aren’t using a lot of text in this tiny example. . 4.3.3 Decode your predictions . Your LSTM model has already been trained for you so that you don’t have to wait. It’s time to define a function that decodes its predictions. . Since you are predicting on a model that uses the softmax function, argmax() is used to obtain the position of the output layer with the highest probability, that is the index representing the most probable next word. . The tokenizer you previously created and fitted, is loaded for you. You will be making use of its internal index_word dictionary to turn the model’s next word prediction (which is an integer) into the actual written word it represents. . You’re very close to experimenting with your model! . def predict_text(test_text): if len(test_text.split())!=3: print(&#39;Text input should be 3 words!&#39;) return False # Turn the test_text into a sequence of numbers test_seq = tokenizer.texts_to_sequences([test_text]) test_seq = np.array(test_seq) # Get the model&#39;s next word prediction by passing in test_seq pred = model.predict(test_seq).argmax(axis = 1)[0] # Return the word associated to the predicted index return tokenizer.index_word[pred] . Great job! It’s finally time to try out your model to see how well it does! . 4.3.4 Test your model! . The function you just built, predict_text() , is ready to use. . Try out these strings on your LSTM model: . &#39;meet revenge with&#39; | &#39;the course of&#39; | &#39;strength of the&#39; | . Which sentence could be made with the word output from the sentences above? . Possible Answers . A worthless gnome is king . | *Revenge is your history and spirit * . | Take a sword and ride to Florida . | . predict_text(&#39;meet revenge with&#39;) &#39;revenge&#39; predict_text(&#39;the course of&#39;) &#39;history&#39; predict_text(&#39;strength of the&#39;) &#39;spirit&#39; . . 4.4 You’re done! . . . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/introduction-to-deep-learning-with-keras.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/introduction-to-deep-learning-with-keras.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post24": {
            "title": "Introduction to Databases in Python",
            "content": "Introduction to Databases in Python . This is the memo of the 3rd course (4 courses in all) of ‘Importing &amp; Cleaning Data with Python’ skill track. . You can find the original course HERE . . . 1. Basics of Relational Databases . . 1.1 Introduction to Databases . #### Relational model . Tables, Columns, Rows, and Relationships are part of the relational model. . . 1.2 Connecting to your database . Database types: . SQLite | PostgreSQL | MySQL | MS SQL | Oracle | etc. | . . #### Engines and connection strings . Alright, it’s time to create your first engine! An engine is just a common interface to a database, and the information it requires to connect to one is contained in a connection string, for example sqlite:///example.sqlite . Here, sqlite in sqlite:/// is the database driver, while example.sqlite is a SQLite file contained in the local directory. . You can learn a lot more about connection strings in the SQLAlchemy documentation . . Your job in this exercise is to create an engine that connects to a local SQLite file named census.sqlite . Then, print the names of the tables the engine contains using the .table_names() method. . # Import create_engine from sqlalchemy import create_engine # Create an engine that connects to the census.sqlite file: engine engine = create_engine(&#39;sqlite:///census.sqlite&#39;) # Print table names print(engine.table_names()) # [&#39;census&#39;, &#39;state_fact&#39;] . This database has two tables, as you can see: &#39;census&#39; and &#39;state_fact&#39; . You’ll be exploring both of these and more throughout this course! . #### Autoloading Tables from a database . SQLAlchemy can be used to automatically load tables from a database using something called reflection. Reflection is the process of reading the database and building the metadata based on that information. It’s the opposite of creating a Table by hand and is very useful for working with existing databases. . To perform reflection, you will first need to import and initialize a MetaData object. MetaData objects contain information about tables stored in a database. During reflection, the MetaData object will be populated with information about the reflected table automatically, so we only need to initialize it before reflecting by calling MetaData() . . You will also need to import the Table object from the SQLAlchemy package. Then, you use this Table object to read your table from the engine, autoload the columns, and populate the metadata. This can be done with a single call to Table() using the Table object in this manner is a lot like passing arguments to a function. For example, to autoload the columns with the engine, you have to specify the keyword arguments autoload=True and autoload_with=engine to Table() . Finally, to view information about the object you just created, you will use the repr() function. For any Python object, repr() returns a text representation of that object. For SQLAlchemy Table objects, it will return the information about that table contained in the metadata. . In this exercise, your job is to reflect the &quot;census&quot; table available on your engine into a variable called census . . # Import create_engine, MetaData, and Table from sqlalchemy import create_engine, MetaData, Table # Create engine: engine engine = create_engine(&#39;sqlite:///census.sqlite&#39;) # Create a metadata object: metadata metadata = MetaData() # Reflect census table from the engine: census census = Table(&#39;census&#39;, metadata, autoload=True, autoload_with=engine) # Print census table metadata print(repr(census)) . Table(&#39;census&#39;, MetaData(bind=None), Column(&#39;state&#39;, VARCHAR(length=30), table=&lt;census&gt;), Column(&#39;sex&#39;, VARCHAR(length=1), table=&lt;census&gt;), Column(&#39;age&#39;, INTEGER(), table=&lt;census&gt;), Column(&#39;pop2000&#39;, INTEGER(), table=&lt;census&gt;), Column(&#39;pop2008&#39;, INTEGER(), table=&lt;census&gt;), schema=None) . Reflecting a table allows you to work with it in Python. . #### Viewing Table details . Now you can begin to learn more about the columns and structure of your table. It is important to get an understanding of your database by examining the column names. This can be done by using the .columns attribute and accessing the .keys() method. For example, census.columns.keys() would return a list of column names of the census table. . Following this, we can use the metadata container to find out more details about the reflected table such as the columns and their types. For example, information about the table objects are stored in the metadata.tables dictionary, so you can get the metadata of your census table with metadata.tables[&#39;census&#39;] . This is similar to your use of the repr() function on the census table from the previous exercise. . from sqlalchemy import create_engine, MetaData, Table engine = create_engine(&#39;sqlite:///census.sqlite&#39;) metadata = MetaData() # Reflect the census table from the engine: census census = Table(&#39;census&#39;, metadata, autoload=True, autoload_with=engine) # Print the column names print(census.columns.keys()) # Print full metadata of census print(repr(metadata.tables[&#39;census&#39;])) . [&#39;state&#39;, &#39;sex&#39;, &#39;age&#39;, &#39;pop2000&#39;, &#39;pop2008&#39;] Table(&#39;census&#39;, MetaData(bind=None), Column(&#39;state&#39;, VARCHAR(length=30), table=&lt;census&gt;), Column(&#39;sex&#39;, VARCHAR(length=1), table=&lt;census&gt;), Column(&#39;age&#39;, INTEGER(), table=&lt;census&gt;), Column(&#39;pop2000&#39;, INTEGER(), table=&lt;census&gt;), Column(&#39;pop2008&#39;, INTEGER(), table=&lt;census&gt;), schema=None) . The census table, as you can see, has five columns. Knowing the names of these columns and their data types will make it easier for you to structure your queries. . . 1.3 Introduction to SQL . raw SQL vs SQLAlchemy . . #### Selecting data from a Table: raw SQL . As you have seen in the video, to access and manipulate the data in the database, we will first need to establish a connection to it by using the .connect() method on the engine. This is because the create_engine() function that you have used before returns an instance of an engine, but it does not actually open a connection until an action is called that would require a connection, such as a query. . Using what we just learned about SQL and applying the .execute() method on our connection, we can leverage a raw SQL query to query all the records in our census table. The object returned by the .execute() method is a ResultProxy . On this ResultProxy, we can then use the .fetchall() method to get our results – that is, the ResultSet . . In this exercise, you’ll use a traditional SQL query. Notice that when you execute a query using raw SQL, you will query the table in the database directly . In particular, no reflection step is needed. . from sqlalchemy import create_engine engine = create_engine(&#39;sqlite:///census.sqlite&#39;) # Create a connection on engine connection = engine.connect() # Build select statement for census table: stmt stmt = &#39;SELECT * FROM census&#39; # Execute the statement and fetch the results: results results = connection.execute(stmt).fetchall() connection.execute(stmt) # &lt;sqlalchemy.engine.result.ResultProxy at 0x7f19c77ec0b8&gt; # Print results print(results) . [(&#39;Illinois&#39;, &#39;M&#39;, 0, 89600, 95012), (&#39;Illinois&#39;, &#39;M&#39;, 1, 88445, 91829), (&#39;Illinois&#39;, &#39;M&#39;, 2, 88729, 89547), (&#39;Illinois&#39;, &#39;M&#39;, 3, 88868, 90037), (&#39;Illinois&#39;, &#39;M&#39;, 4, 91947, 91111), (&#39;Illinois&#39;, &#39;M&#39;, 5, 93894, 89802), (&#39;Illinois&#39;, &#39;M&#39;, 6, 93676, 88931), (&#39;Illinois&#39;, &#39;M&#39;, 7, 94818, 90940), (&#39;Illinois&#39;, &#39;M&#39;, 8, 95035, 86943), (&#39;Illinois&#39;, &#39;M&#39;, 9, 96436, 86055), (&#39;Illinois&#39;, &#39;M&#39;, 10, 97280, 86565), (&#39;Illinois&#39;, &#39;M&#39;, 11, 94029, 86606), (&#39;Illinois&#39;, &#39;M&#39;, 12, 92402, 89596), (&#39;Illinois&#39;, &#39;M&#39;, 13, 89926, 91661), (&#39;Illinois&#39;, &#39;M&#39;, 14, 90717, 91256), (&#39;Illinois&#39;, &#39;M&#39;, 15, 92178, 92729), (&#39;Illinois&#39;, &#39;M&#39;, 16, 90587, 93083), ... . Notice that the stmt converts into a SQL statement listing all the records for all the columns in the table.This output is quite unwieldy though, and fetching all the records in the table might take a long time, so in the next exercises, you will learn how to fetch only the first few records of a ResultProxy. . #### Selecting data from a Table with SQLAlchemy . It’s now time to build your first select statement using SQLAlchemy. SQLAlchemy provides a nice “Pythonic” way of interacting with databases. When you used raw SQL in the last exercise, you queried the database directly. When using SQLAlchemy, you will go through a Table object instead, and SQLAlchemy will take case of translating your query to an appropriate SQL statement for you. So rather than dealing with the differences between specific dialects of traditional SQL such as MySQL or PostgreSQL, you can leverage the Pythonic framework of SQLAlchemy to streamline your workflow and more efficiently query your data. For this reason, it is worth learning even if you may already be familiar with traditional SQL. . In this exercise, you’ll once again build a statement to query all records from the census table. This time, however, you’ll make use of the select() function of the sqlalchemy module. This function requires a list of tables or columns as the only required argument: for example, select([my_table]) . . You will also fetch only a few records of the ResultProxy by using .fetchmany() with a size argument specifying the number of records to fetch. . # Import select from sqlalchemy import select # Reflect census table via engine: census census = Table(&#39;census&#39;, metadata, autoload=True, autoload_with=engine) # Build select statement for census table: stmt stmt = select([census]) # Print the emitted statement to see the SQL string print(stmt) # SELECT census.state, census.sex, census.age, census.pop2000, census.pop2008 FROM census # Execute the statement on connection and fetch 10 records: result results = connection.execute(stmt).fetchmany(size=10) # Execute the statement and print the results print(results) . [(&#39;Illinois&#39;, &#39;M&#39;, 0, 89600, 95012), (&#39;Illinois&#39;, &#39;M&#39;, 1, 88445, 91829), (&#39;Illinois&#39;, &#39;M&#39;, 2, 88729, 89547), (&#39;Illinois&#39;, &#39;M&#39;, 3, 88868, 90037), (&#39;Illinois&#39;, &#39;M&#39;, 4, 91947, 91111), (&#39;Illinois&#39;, &#39;M&#39;, 5, 93894, 89802), (&#39;Illinois&#39;, &#39;M&#39;, 6, 93676, 88931), (&#39;Illinois&#39;, &#39;M&#39;, 7, 94818, 90940), (&#39;Illinois&#39;, &#39;M&#39;, 8, 95035, 86943), (&#39;Illinois&#39;, &#39;M&#39;, 9, 96436, 86055)] . #### Handling a ResultSet . Recall the differences between a ResultProxy and a ResultSet: . ResultProxy: The object returned by the .execute() method. It can be used in a variety of ways to get the data returned by the query. | ResultSet: The actual data asked for in the query when using a fetch method such as .fetchall() on a ResultProxy. | . This separation between the ResultSet and ResultProxy allows us to fetch as much or as little data as we desire. . Once we have a ResultSet, we can use Python to access all the data within it by column name and by list style indexes. For example, you can get the first row of the results by using results[0] . With that first row then assigned to a variable first_row , you can get data from the first column by either using first_row[0] or by column name such as first_row[&#39;column_name&#39;] . You’ll now practice exactly this using the ResultSet you obtained from the census table in the previous exercise. . # Get the first row of the results by using an index: first_row first_row = results[0] # Print the first row of the results print(first_row) # Print the first column of the first row by accessing it by its index print(first_row[0]) print(results[0][0]) # Print the &#39;state&#39; column of the first row by using its name print(first_row[&#39;state&#39;]) print(results[0][&#39;state&#39;]) . results[:3] [(&#39;Illinois&#39;, &#39;M&#39;, 0, 89600, 95012), (&#39;Illinois&#39;, &#39;M&#39;, 1, 88445, 91829), (&#39;Illinois&#39;, &#39;M&#39;, 2, 88729, 89547)] results[:3][0] (&#39;Illinois&#39;, &#39;M&#39;, 0, 89600, 95012) results[:3][0][3] 89600 . . 2. Applying Filtering, Ordering and Grouping to Queries . . 2.1 Filtering and targeting data . #### Connecting to a PostgreSQL database . In these exercises, you will be working with real databases hosted on the cloud via Amazon Web Services (AWS)! . Let’s begin by connecting to a PostgreSQL database. When connecting to a PostgreSQL database, many prefer to use the psycopg2 database driver as it supports practically all of PostgreSQL’s features efficiently and is the standard dialect for PostgreSQL in SQLAlchemy. . You might recall from Chapter 1 that we use the create_engine() function and a connection string to connect to a database. In general, connection strings have the form &quot;dialect+driver://username:password@host:port/database&quot; . There are three components to the connection string in this exercise: the dialect and driver ( &#39;postgresql+psycopg2://&#39; ), followed by the username and password ( &#39; username:password &#39; ), followed by the host and port ( &#39;@postgresql.csrrinzqubik.us-east-1.rds.amazonaws.com:1234/&#39; ), and finally, the database name ( &#39;census&#39; ). You will have to pass this string as an argument to create_engine() in order to connect to the database. . # Import create_engine function from sqlalchemy import create_engine # Create an engine to the census database engine = create_engine(&#39;postgresql+psycopg2://username:password@postgresql.csrrinzqubik.us-east-1.rds.amazonaws.com:1234/census&#39;) # Use the .table_names() method on the engine to print the table names print(engine.table_names()) # [&#39;census&#39;, &#39;state_fact&#39;, &#39;vrska&#39;, &#39;census1&#39;, &#39;data&#39;, &#39;data1&#39;, &#39;employees3&#39;, &#39;users&#39;, &#39;employees&#39;, &#39;employees_2&#39;] . #### Filter data selected from a Table – Simple . Having connected to the database, it’s now time to practice filtering your queries! . As mentioned in the video, a where() clause is used to filter the data that a statement returns. For example, to select all the records from the census table where the sex is Female (or &#39;F&#39; ) we would do the following: . select([census]).where(census.columns.sex == &#39;F&#39;) . In addition to == we can use basically any python comparison operator (such as &lt;= , != , etc) in the where() clause. . # Create a select query: stmt stmt = select([census]) # Add a where clause to filter the results to only those for New York : stmt_filtered stmt = stmt.where(census.columns.state == &#39;New York&#39;) # Execute the query to retrieve all the data returned: results results = connection.execute(stmt).fetchall() # Loop over the results and print the age, sex, and pop2000 for result in results: print(result.age, result.sex, result.pop2000) . 0 M 126237 1 M 124008 2 M 124725 ... 83 M 21687 84 M 18873 85 M 88366 0 F 120355 1 F 118219 2 F 119577 ... 84 F 37436 85 F 226378 . #### Filter data selected from a Table – Expressions . In addition to standard Python comparators, we can also use methods such as in_() to create more powerful where() clauses. You can see a full list of expressions in the SQLAlchemy Documentation . . Method in_() , when used on a column, allows us to include records where the value of a column is among a list of possible values. For example, where(census.columns.age.in_([20, 30, 40])) will return only records for people who are exactly 20, 30, or 40 years old. . In this exercise, you will continue working with the census table, and select the records for people from the three most densely populated states. . # Define a list of states for which we want results states = [&#39;New York&#39;, &#39;California&#39;, &#39;Texas&#39;] # Create a query for the census table: stmt stmt = select([census]) # Append a where clause to match all the states in_ the list states stmt = stmt.where(census.columns.state.in_(states)) # Loop over the ResultProxy and print the state and its population in 2000 for result in connection.execute(stmt): print(result.state, result.pop2000) . California 252494 California 247978 .. New York 122770 New York 123978 ... Texas 27961 Texas 171538 . Along with in_ , you can also use methods like and_ , any_ to create more powerful where() clauses. You might have noticed that we did not use any of the fetch methods to retrieve a ResultSet like in the previous exercises. Indeed, if you are only interested in manipulating one record at a time, you can iterate over the ResultProxy directly! . #### Filter data selected from a Table – Advanced . SQLAlchemy also allows users to use conjunctions such as and_() , or_() , and not_() to build more complex filtering. For example, we can get a set of records for people in New York who are 21 or 37 years old with the following code: . select([census]).where( and_(census.columns.state == &#39;New York&#39;, or_(census.columns.age == 21, census.columns.age == 37 ) ) ) . An equivalent SQL statement would be,for example, . SELECT * FROM census WHERE state = &#39;New York&#39; AND (age = 21 OR age = 37) . # Import and_ from sqlalchemy import and_ # Build a query for the census table: stmt stmt = select([census]) # Append a where clause to select only non-male records from California using and_ stmt = stmt.where( # The state of California with a non-male sex and_(census.columns.state == &#39;California&#39;, census.columns.sex != &#39;M&#39; ) ) # Loop over the ResultProxy printing the age and sex for result in connection.execute(stmt): print(result.age, result.sex) . 0 F 1 F 2 F ... 84 F 85 F . . 2.2 Overview of ordering . #### Ordering by a single column . To sort the result output by a field, we use the .order_by() method. By default, the .order_by() method sorts from lowest to highest on the supplied column. . # Build a query to select the state column: stmt stmt = select([census.columns.state]) # Order stmt by the state column stmt = stmt.order_by(census.columns.state) # Execute the query and store the results: results results = connection.execute(stmt).fetchall() # Print the first 10 results print(results[:10]) # [(&#39;Alabama&#39;,), (&#39;Alabama&#39;,), (&#39;Alabama&#39;,), (&#39;Alabama&#39;,), (&#39;Alabama&#39;,), (&#39;Alabama&#39;,), (&#39;Alabama&#39;,), (&#39;Alabama&#39;,), (&#39;Alabama&#39;,), (&#39;Alabama&#39;,)] . #### Ordering in descending order by a single column . You can also use .order_by() to sort from highest to lowest by wrapping a column in the desc() function. Although you haven’t seen this function in action, it generalizes what you have already learned. . Pass desc() (for “descending”) inside an .order_by() with the name of the column you want to sort by. For instance, stmt.order_by(desc(table.columns.column_name)) sorts column_name in descending order. . # Import desc from sqlalchemy import desc # Build a query to select the state column: stmt stmt = select([census.columns.state]) # Order stmt by state in descending order: rev_stmt rev_stmt = stmt.order_by(desc(census.columns.state)) # Execute the query and store the results: rev_results rev_results = connection.execute(rev_stmt).fetchall() # Print the first 10 rev_results print(rev_results[:10]) # [(&#39;Wyoming&#39;,), (&#39;Wyoming&#39;,), (&#39;Wyoming&#39;,), (&#39;Wyoming&#39;,), (&#39;Wyoming&#39;,), (&#39;Wyoming&#39;,), (&#39;Wyoming&#39;,), (&#39;Wyoming&#39;,), (&#39;Wyoming&#39;,), (&#39;Wyoming&#39;,)] . #### Ordering by multiple columns . We can pass multiple arguments to the .order_by() method to order by multiple columns. In fact, we can also sort in ascending or descending order for each individual column. . # Build a query to select state and age: stmt stmt = select([census.columns.state, census.columns.age]) # Append order by to ascend by state and descend by age stmt = stmt.order_by(census.columns.state, desc(census.columns.age)) # Execute the statement and store all the records: results results = connection.execute(stmt).fetchall() # Print the first 20 results print(results[:20]) . [(&#39;Alabama&#39;, 85), (&#39;Alabama&#39;, 85), (&#39;Alabama&#39;, 84), (&#39;Alabama&#39;, 84), (&#39;Alabama&#39;, 83), (&#39;Alabama&#39;, 83), (&#39;Alabama&#39;, 82), (&#39;Alabama&#39;, 82), (&#39;Alabama&#39;, 81), (&#39;Alabama&#39;, 81), (&#39;Alabama&#39;, 80), (&#39;Alabama&#39;, 80), (&#39;Alabama&#39;, 79), (&#39;Alabama&#39;, 79), (&#39;Alabama&#39;, 78), (&#39;Alabama&#39;, 78), (&#39;Alabama&#39;, 77), (&#39;Alabama&#39;, 77), (&#39;Alabama&#39;, 76), (&#39;Alabama&#39;, 76)] . . 2.3 Counting, summing and grouping data . #### Counting distinct data . SQLAlchemy’s func module provides access to built-in SQL functions that can make operations like counting and summing faster and more efficient. . We can use func.sum() to get a sum of the pop2008 column of census as shown below: . select([func.sum(census.columns.pop2008)]) . If instead you want to count the number of values in pop2008 , you could use func.count() like this: . select([func.count(census.columns.pop2008)]) . Furthermore, if you only want to count the distinct values of pop2008 , you can use the .distinct() method: . select([func.count(census.columns.pop2008.distinct())]) . In this exercise, you will practice using func.count() and .distinct() to get a count of the distinct number of states in census . . So far, you’ve seen .fetchall() , .fetchmany() , and .first() used on a ResultProxy to get the results. The ResultProxy also has a method called .scalar() for getting just the value of a query that returns only one row and column. . This can be very useful when you are querying for just a count or sum. . # Build a query to count the distinct states values: stmt stmt = select([func.count(census.columns.state.distinct())]) # Execute the query and store the scalar result: distinct_state_count distinct_state_count = connection.execute(stmt).scalar() # Print the distinct_state_count print(distinct_state_count) # 51 . connection.execute(stmt).fetchall() # [(51,)] . Notice the use of the .scalar() method: This is useful when you want to get just the value of a query that returns only one row and column, like in this case. . #### Count of records by state . Often, we want to get a count for each record with a particular value in another column. The .group_by() method helps answer this type of query. You can pass a column to the .group_by() method and use in an aggregate function like sum() or count() . Much like the .order_by() method, .group_by() can take multiple columns as arguments. . # Import func from sqlalchemy import func # Build a query to select the state and count of ages by state: stmt stmt = select([census.columns.state, func.count(census.columns.age)]) # Group stmt by state stmt = stmt.group_by(census.columns.state) # Execute the statement and store all the records: results results = connection.execute(stmt).fetchall() # Print results print(results) # Print the keys/column names of the results returned print(results[0].keys()) . [(&#39;Alabama&#39;, 172), (&#39;Alaska&#39;, 172), (&#39;Arizona&#39;, 172), (&#39;Arkansas&#39;, 172), (&#39;California&#39;, 172), ... (&#39;Wisconsin&#39;, 172), (&#39;Wyoming&#39;, 172)] [&#39;state&#39;, &#39;count_1&#39;] . Notice that the key for the count method just came out as count_1 . This can make it hard in complex queries to tell what column is being referred to: In the next exercise, you’ll practice assigning more descriptive labels when performing such calculations. . #### Determining the population sum by state . To avoid confusion with query result column names like count_1 , we can use the .label() method to provide a name for the resulting column. This gets appended to the function method we are using, and its argument is the name we want to use. . We can pair func.sum() with .group_by() to get a sum of the population by State and use the label() method to name the output. . We can also create the func.sum() expression before using it in the select statement. We do it the same way we would inside the select statement and store it in a variable. Then we use that variable in the select statement where the func.sum() would normally be. . # Import func from sqlalchemy import func # Build an expression to calculate the sum of pop2008 labeled as population pop2008_sum = func.sum(census.columns.pop2008).label(&#39;population&#39;) # Build a query to select the state and sum of pop2008: stmt stmt = select([census.columns.state, pop2008_sum]) # Group stmt by state stmt = stmt.group_by(census.columns.state) # Execute the statement and store all the records: results results = connection.execute(stmt).fetchall() # Print results print(results) # Print the keys/column names of the results returned print(results[0].keys()) . [(&#39;Alabama&#39;, 4649367), (&#39;Alaska&#39;, 664546), (&#39;Arizona&#39;, 6480767), (&#39;Arkansas&#39;, 2848432), ... (&#39;Wisconsin&#39;, 5625013), (&#39;Wyoming&#39;, 529490)] [&#39;state&#39;, &#39;population&#39;] . . 2.4 Use pandas and matplotlib to visualize our data . #### ResultsSets and pandas dataframes . # import pandas import pandas as pd # Create a DataFrame from the results: df df = pd.DataFrame(results) # Set column names df.columns = results[0].keys() # Print the Dataframe print(df) . state population 0 California 36609002 1 Texas 24214127 2 New York 19465159 3 Florida 18257662 4 Illinois 12867077 . If you enjoy using pandas for your data scientific needs, you’ll want to always feed ResultProxies into pandas DataFrames! . #### From SQLAlchemy results to a plot . # Import pyplot as plt from matplotlib from matplotlib import pyplot as plt # Create a DataFrame from the results: df df = pd.DataFrame(results) # Set Column names df.columns = results[0].keys() # Print the DataFrame print(df) # Plot the DataFrame df.plot.bar() plt.show() . state population 0 California 36609002 1 Texas 24214127 2 New York 19465159 3 Florida 18257662 4 Illinois 12867077 . . You’re ready to learn about more advanced SQLAlchemy Queries! . . 3. Advanced SQLAlchemy Queries . . 3.1 Calculating values in a query . #### Connecting to a MySQL database . # Import create_engine function from sqlalchemy import create_engine # Create an engine to the census database engine = create_engine(&#39;mysql+pymysql://username:password@courses.csrrinzqubik.us-east-1.rds.amazonaws.com:3306/census&#39;) # Print the table names print(engine.table_names()) # [&#39;census&#39;, &#39;state_fact&#39;] . #### Calculating a difference between two columns . Often, you’ll need to perform math operations as part of a query, such as if you wanted to calculate the change in population from 2000 to 2008. For math operations on numbers, the operators in SQLAlchemy work the same way as they do in Python. . You can use these operators to perform addition ( + ), subtraction ( - ), multiplication ( * ), division ( / ), and modulus ( % ) operations. Note: They behave differently when used with non-numeric column types. . Let’s now find the top 5 states by population growth between 2000 and 2008. . census.columns.keys() [&#39;state&#39;, &#39;sex&#39;, &#39;age&#39;, &#39;pop2000&#39;, &#39;pop2008&#39;] connection.execute(select([census])).fetchmany(3) [(&#39;Illinois&#39;, &#39;M&#39;, 0, 89600, 95012), (&#39;Illinois&#39;, &#39;M&#39;, 1, 88445, 91829), (&#39;Illinois&#39;, &#39;M&#39;, 2, 88729, 89547)] . # Build query to return state names by population difference from 2008 to 2000: stmt stmt = select([census.columns.state, (census.columns.pop2008-census.columns.pop2000).label(&#39;pop_change&#39;)]) # Append group by for the state: stmt_grouped stmt_grouped = stmt.group_by(census.columns.state) # Append order by for pop_change descendingly: stmt_ordered stmt_ordered = stmt_grouped.order_by(desc(&#39;pop_change&#39;)) # Return only 5 results: stmt_top5 stmt_top5 = stmt_ordered.limit(5) # Use connection to execute stmt_top5 and fetch all results results = connection.execute(stmt_top5).fetchall() # Print the state and population change for each record for result in results: print(&#39;{}:{}&#39;.format(result.state, result.pop_change)) . California:105705 Florida:100984 Texas:51901 New York:47098 Pennsylvania:42387 . #### Determining the overall percentage of women . It’s possible to combine functions and operators in a single select statement as well. These combinations can be exceptionally handy when we want to calculate percentages or averages, and we can also use the case() expression to operate on data that meets specific criteria while not affecting the query as a whole. The case() expression accepts a list of conditions to match and the column to return if the condition matches, followed by an else_ if none of the conditions match. We can wrap this entire expression in any function or math operation we like. . Often when performing integer division, we want to get a float back. While some databases will do this automatically, you can use the cast() function to convert an expression to a particular type. . # import case, cast and Float from sqlalchemy from sqlalchemy import case, cast, Float # Build an expression to calculate female population in 2000 female_pop2000 = func.sum( case([ (census.columns.sex == &#39;F&#39;, census.columns.pop2000) ], else_=0)) # Cast an expression to calculate total population in 2000 to Float total_pop2000 = cast(func.sum(census.columns.pop2000), Float) # Build a query to calculate the percentage of women in 2000: stmt stmt = select([female_pop2000 / total_pop2000 * 100]) # Execute the query and store the scalar result: percent_female percent_female = connection.execute(stmt).scalar() # Print the percentage print(percent_female) # 51.0946743229 . . 3.2 SQL relationships . #### Automatic joins with an established relationship . If you have two tables that already have an established relationship, you can automatically use that relationship by just adding the columns we want from each table to the select statement. Recall that Jason constructed the following query: . stmt = select([census.columns.pop2008, state_fact.columns.abbreviation]) . in order to join the census and state_fact tables and select the pop2008 column from the first and the abbreviation column from the second. In this case, the census and state_fact tables had a pre-defined relationship: the state column of the former corresponded to the name column of the latter. . In this exercise, you’ll use the same predefined relationship to select the pop2000 and abbreviation columns! . # Build a statement to join census and state_fact tables: stmt stmt = select([census.columns.pop2000, state_fact.columns.abbreviation]) # Execute the statement and get the first result: result result = connection.execute(stmt).first() # Loop over the keys in the result object and print the key and value for key in result.keys(): print(key, getattr(result, key)) pop2000 89600 abbreviation IL . result # (89600, &#39;IL&#39;) . #### Joins . If you aren’t selecting columns from both tables or the two tables don’t have a defined relationship, you can still use the .join() method on a table to join it with another table and get extra data related to our query. . The join() takes the table object you want to join in as the first argument and a condition that indicates how the tables are related to the second argument. . Finally, you use the .select_from() method on the select statement to wrap the join clause. . For example, the following code joins the census table to the state_fact table such that the state column of the census table corresponded to the name column of the state_fact table. . stmt = stmt.select_from( census.join( state_fact, census.columns.state == state_fact.columns.name) . connection.execute(select([state_fact])).keys() [&#39;id&#39;, &#39;name&#39;, &#39;abbreviation&#39;, &#39;country&#39;, &#39;type&#39;, &#39;sort&#39;, &#39;status&#39;, &#39;occupied&#39;, &#39;notes&#39;, &#39;fips_state&#39;, &#39;assoc_press&#39;, &#39;standard_federal_region&#39;, &#39;census_region&#39;, &#39;census_region_name&#39;, &#39;census_division&#39;, &#39;census_division_name&#39;, &#39;circuit_court&#39;] connection.execute(select([state_fact])).fetchmany(2) [(&#39;13&#39;, &#39;Illinois&#39;, &#39;IL&#39;, &#39;USA&#39;, &#39;state&#39;, &#39;10&#39;, &#39;current&#39;, &#39;occupied&#39;, &#39;&#39;, &#39;17&#39;, &#39;Ill.&#39;, &#39;V&#39;, &#39;2&#39;, &#39;Midwest&#39;, &#39;3&#39;, &#39;East North Central&#39;, &#39;7&#39;), (&#39;30&#39;, &#39;New Jersey&#39;, &#39;NJ&#39;, &#39;USA&#39;, &#39;state&#39;, &#39;10&#39;, &#39;current&#39;, &#39;occupied&#39;, &#39;&#39;, &#39;34&#39;, &#39;N.J.&#39;, &#39;II&#39;, &#39;1&#39;, &#39;Northeast&#39;, &#39;2&#39;, &#39;Mid-Atlantic&#39;, &#39;3&#39;)] . # Build a statement to select the census and state_fact tables: stmt stmt = select([census, state_fact]) # Add a select_from clause that wraps a join for the census and state_fact # tables where the census state column and state_fact name column match stmt_join = stmt.select_from( census.join(state_fact, census.columns.state == state_fact.columns.name)) # Execute the statement and get the first result: result result = connection.execute(stmt_join).first() # Loop over the keys in the result object and print the key and value for key in result.keys(): print(key, getattr(result, key)) . state Illinois sex M age 0 pop2000 89600 pop2008 95012 id 13 name Illinois abbreviation IL country USA type state sort 10 status current occupied occupied notes fips_state 17 assoc_press Ill. standard_federal_region V census_region 2 census_region_name Midwest census_division 3 census_division_name East North Central circuit_court 7 . #### More practice with joins . You can use the same select statement you built in the last exercise, however, let’s add a twist and only return a few columns and use the other table in a group_by() clause. . stmt = select([ census.columns.state, func.sum(census.columns.pop2008), state_fact.columns.census_division_name ]) connection.execute(stmt).fetchmany(3) # [(&#39;Texas&#39;, 15446707263, &#39;South Atlantic&#39;)] . stmt_joined = stmt.select_from( census.join(state_fact, census.columns.state == state_fact.columns.name) ) connection.execute(stmt_joined).fetchmany(3) # [(&#39;Texas&#39;, 302287703, &#39;West South Central&#39;)] . stmt_grouped = stmt_joined.group_by(state_fact.columns.name) connection.execute(stmt_grouped).fetchmany(3) # [(&#39;Alabama&#39;, 4649367, &#39;East South Central&#39;), (&#39;Alaska&#39;, 664546, &#39;Pacific&#39;), (&#39;Arizona&#39;, 6480767, &#39;Mountain&#39;)] . # Build a statement to select the state, sum of 2008 population and census # division name: stmt stmt = select([ census.columns.state, func.sum(census.columns.pop2008), state_fact.columns.census_division_name ]) # Append select_from to join the census and state_fact tables by the census state and state_fact name columns stmt_joined = stmt.select_from( census.join(state_fact, census.columns.state == state_fact.columns.name) ) # Append a group by for the state_fact name column stmt_grouped = stmt_joined.group_by(state_fact.columns.name) # Execute the statement and get the results: results results = connection.execute(stmt_grouped).fetchall() # Loop over the results object and print each record. for record in results: print(record) . (&#39;Alabama&#39;, 4649367, &#39;East South Central&#39;) (&#39;Alaska&#39;, 664546, &#39;Pacific&#39;) ... (&#39;Wisconsin&#39;, 5625013, &#39;East North Central&#39;) (&#39;Wyoming&#39;, 529490, &#39;Mountain&#39;) . The ability to join tables like this is what makes relational databases so powerful. . . 3.3 Working with hierarchical tables . #### Using alias to handle same table joined queries . Often, you’ll have tables that contain hierarchical data, such as employees and managers who are also employees. For this reason, you may wish to join a table to itself on different columns. The .alias() method, which creates a copy of a table, helps accomplish this task. Because it’s the same table, you only need a where clause to specify the join condition. . Here, you’ll use the .alias() method to build a query to join the employees table against itself to determine to whom everyone reports. . employees.columns.keys() # [&#39;id&#39;, &#39;name&#39;, &#39;job&#39;, &#39;mgr&#39;, &#39;hiredate&#39;, &#39;sal&#39;, &#39;comm&#39;, &#39;dept&#39;] connection.execute(select([employees.columns.name, employees.columns.mgr])).fetchmany(5) # [(&#39;JOHNSON&#39;, 6), (&#39;HARDING&#39;, 9), (&#39;TAFT&#39;, 2), (&#39;HOOVER&#39;, 2), (&#39;LINCOLN&#39;, 6)] . # Make an alias of the employees table: managers managers = employees.alias() # Build a query to select names of managers and their employees: stmt stmt = select( [managers.columns.name.label(&#39;manager&#39;), employees.columns.name.label(&#39;employee&#39;)] ) # Match managers id with employees mgr: stmt_matched stmt_matched = stmt.where(managers.columns.id == employees.columns.mgr) # Order the statement by the managers name: stmt_ordered stmt_ordered = stmt_matched.order_by(managers.columns.name) # Execute statement: results results = connection.execute(stmt_ordered).fetchall() # Print records for record in results: print(record) . (&#39;FILLMORE&#39;, &#39;GRANT&#39;) (&#39;FILLMORE&#39;, &#39;ADAMS&#39;) (&#39;FILLMORE&#39;, &#39;MONROE&#39;) (&#39;GARFIELD&#39;, &#39;JOHNSON&#39;) (&#39;GARFIELD&#39;, &#39;LINCOLN&#39;) (&#39;GARFIELD&#39;, &#39;POLK&#39;) (&#39;GARFIELD&#39;, &#39;WASHINGTON&#39;) (&#39;HARDING&#39;, &#39;TAFT&#39;) (&#39;HARDING&#39;, &#39;HOOVER&#39;) (&#39;JACKSON&#39;, &#39;HARDING&#39;) (&#39;JACKSON&#39;, &#39;GARFIELD&#39;) (&#39;JACKSON&#39;, &#39;FILLMORE&#39;) (&#39;JACKSON&#39;, &#39;ROOSEVELT&#39;) . #### Leveraging functions and group_bys with hierarchical data . It’s also common to want to roll up data which is in a hierarchical table. Rolling up data requires making sure you’re careful which alias you use to perform the group_bys and which table you use for the function. . Here, your job is to get a count of employees for each manager. . connection.execute(select([func.count(employees.columns.id)])).fetchmany(3) [(14,)] . # Make an alias of the employees table: managers managers = employees.alias() # Build a query to select names of managers and counts of their employees: stmt stmt = select([managers.columns.name, func.count(employees.columns.id)]) # Append a where clause that ensures the manager id and employee mgr are equal stmt_matched = stmt.where(managers.columns.id == employees.columns.mgr) # Group by Managers Name stmt_grouped = stmt_matched.group_by(managers.columns.name) # Execute statement: results results = connection.execute(stmt_grouped).fetchall() # print manager for record in results: print(record) . (&#39;FILLMORE&#39;, 3) (&#39;GARFIELD&#39;, 4) (&#39;HARDING&#39;, 2) (&#39;JACKSON&#39;, 4) . . 3.4 Dealing with large ResultSets . #### Working on blocks of records . Sometimes you may have the need to work on a large ResultProxy, and you may not have the memory to load all the results at once. . To work around that issue, you can get blocks of rows from the ResultProxy by using the .fetchmany() method inside a loop. With .fetchmany() , give it an argument of the number of records you want. When you reach an empty list, there are no more rows left to fetch, and you have processed all the results of the query. . Then you need to use the .close() method to close out the connection to the database. . results_proxy &lt;sqlalchemy.engine.result.ResultProxy at 0x7f4b7dc70160&gt; results_proxy.fetchmany(3) [(&#39;Illinois&#39;, &#39;M&#39;, 0, 89600, 95012), (&#39;Illinois&#39;, &#39;M&#39;, 1, 88445, 91829), (&#39;Illinois&#39;, &#39;M&#39;, 2, 88729, 89547)] results_proxy.fetchmany(3) [(&#39;Illinois&#39;, &#39;M&#39;, 3, 88868, 90037), (&#39;Illinois&#39;, &#39;M&#39;, 4, 91947, 91111), (&#39;Illinois&#39;, &#39;M&#39;, 5, 93894, 89802)] results_proxy.fetchmany(1)[0] (&#39;Illinois&#39;, &#39;M&#39;, 9, 96436, 86055) results_proxy.fetchmany(1)[0][&#39;state&#39;] &#39;Illinois&#39; state_count {} . # Start a while loop checking for more results while more_results: # Fetch the first 50 results from the ResultProxy: partial_results partial_results = results_proxy.fetchmany(50) # if empty list, set more_results to False if partial_results == []: more_results = False # Loop over the fetched records and increment the count for the state for row in partial_results: if row.state in state_count: state_count[row.state] += 1 else: state_count[row.state] = 1 # Close the ResultProxy, and thus the connection results_proxy.close() # Print the count by state print(state_count) . {&#39;Illinois&#39;: 172, &#39;New Jersey&#39;: 172, &#39;District of Columbia&#39;: 172, &#39;North Dakota&#39;: 75, &#39;Florida&#39;: 172, &#39;Maryland&#39;: 49, &#39;Idaho&#39;: 172, &#39;Massachusetts&#39;: 16} . As a data scientist, you’ll inevitably come across huge databases, and being able to work on them in blocks is a vital skill. . . 4. Creating and Manipulating your own Databases . . 4.1 Creating databases and tables . #### Creating tables with SQLAlchemy . Previously, you used the Table object to reflect a table from an existing database, but what if you wanted to create a new table? You’d still use the Table object; however, you’d need to replace the autoload and autoload_with parameters with Column objects. . The Column object takes a name, a SQLAlchemy type with an optional format, and optional keyword arguments for different constraints. . When defining the table, recall how in the video Jason passed in 255 as the maximum length of a String by using Column(&#39;name&#39;, String(255)) . Checking out the slides from the video may help: you can download them by clicking on ‘Slides’ next to the IPython Shell. . After defining the table, you can create the table in the database by using the .create_all() method on metadata and supplying the engine as the only parameter. Go for it! . metadata MetaData(bind=None) engine Engine(sqlite:///:memory:) . # Import Table, Column, String, Integer, Float, Boolean from sqlalchemy from sqlalchemy import Table, Column, String, Integer, Float, Boolean # Define a new table with a name, count, amount, and valid column: data data = Table(&#39;data&#39;, metadata, Column(&#39;name&#39;, String(255)), Column(&#39;count&#39;, Integer()), Column(&#39;amount&#39;, Float()), Column(&#39;valid&#39;, Boolean()) ) # Use the metadata to create the table metadata.create_all(engine) # Print table details print(repr(data)) . Table(&#39;data&#39;, MetaData(bind=None), Column(&#39;name&#39;, String(length=255), table=&lt;data&gt;), Column(&#39;count&#39;, Integer(), table=&lt;data&gt;), Column(&#39;amount&#39;, Float(), table=&lt;data&gt;), Column(&#39;valid&#39;, Boolean(), table=&lt;data&gt;), schema=None) . When creating a table, it’s important to carefully think about what data types each column should be. . #### Constraints and data defaults . You’re now going to practice creating a table with some constraints! Often, you’ll need to make sure that a column is unique, nullable, a positive value, or related to a column in another table. This is where constraints come in. . You can also set a default value for the column if no data is passed to it via the default keyword on the column. . # Import Table, Column, String, Integer, Float, Boolean from sqlalchemy from sqlalchemy import Table, Column, String, Integer, Float, Boolean # Define a new table with a name, count, amount, and valid column: data data = Table(&#39;data&#39;, metadata, Column(&#39;name&#39;, String(255), unique=True), Column(&#39;count&#39;, Integer(), default=1), Column(&#39;amount&#39;, Float()), Column(&#39;valid&#39;, Boolean(), default=False) ) # Use the metadata to create the table metadata.create_all(engine) # Print the table details print(repr(metadata.tables[&#39;data&#39;])) . Table(&#39;data&#39;, MetaData(bind=None), Column(&#39;name&#39;, String(length=255), table=&lt;data&gt;), Column(&#39;count&#39;, Integer(), table=&lt;data&gt;, default=ColumnDefault(1)), Column(&#39;amount&#39;, Float(), table=&lt;data&gt;), Column(&#39;valid&#39;, Boolean(), table=&lt;data&gt;, default=ColumnDefault(False)), schema=None) . . 4.2 Inserting data into a table . #### Inserting a single row . There are several ways to perform an insert with SQLAlchemy; however, we are going to focus on the one that follows the same pattern as the select statement. . It uses an insert statement where you specify the table as an argument, and supply the data you wish to insert into the value via the .values() method as keyword arguments . data Table(&#39;data&#39;, MetaData(bind=None), Column(&#39;name&#39;, String(length=255), table=&lt;data&gt;), Column(&#39;count&#39;, Integer(), table=&lt;data&gt;), Column(&#39;amount&#39;, Float(), table=&lt;data&gt;), Column(&#39;valid&#39;, Boolean(), table=&lt;data&gt;), schema=None) type(data) sqlalchemy.sql.schema.Table repr(data) &quot;Table(&#39;data&#39;, MetaData(bind=None), Column(&#39;name&#39;, String(length=255), table=&lt;data&gt;), Column(&#39;count&#39;, Integer(), table=&lt;data&gt;), Column(&#39;amount&#39;, Float(), table=&lt;data&gt;), Column(&#39;valid&#39;, Boolean(), table=&lt;data&gt;), schema=None)&quot; type(repr(data)) str . # Import insert and select from sqlalchemy from sqlalchemy import insert, select # Build an insert statement to insert a record into the data table: insert_stmt insert_stmt = insert(data).values(name=&#39;Anna&#39;, count=1, amount=1000.00, valid=True) # Execute the insert statement via the connection: results results = connection.execute(insert_stmt) # Print result rowcount print(results.rowcount) # 1 # Build a select statement to validate the insert: select_stmt select_stmt = select([data]).where(data.columns.name == &#39;Anna&#39;) # Print the result of executing the query. print(connection.execute(select_stmt).first()) # (&#39;Anna&#39;, 1, 1000.0, True) . #### Inserting multiple records at once . When inserting multiple records at once, you do not use the .values() method. Instead, you’ll want to first build a list of dictionaries that represents the data you want to insert, with keys being the names of the columns. . In the .execute() method, you can pair this list of dictionaries with an insert statement, which will insert all the records in your list of dictionaries. . # Build a list of dictionaries: values_list values_list = [ {&#39;name&#39;: &#39;Anna&#39;, &#39;count&#39;: 1, &#39;amount&#39;: 1000.00, &#39;valid&#39;: True}, {&#39;name&#39;: &#39;Taylor&#39;, &#39;count&#39;: 1, &#39;amount&#39;: 750.00, &#39;valid&#39;: False}] # Build an insert statement for the data table: stmt stmt = insert(data) # Execute stmt with the values_list: results results = connection.execute(stmt, values_list) # Print rowcount print(results.rowcount) # 2 . connection.execute(select([data])).fetchmany(3) # [(&#39;Anna&#39;, 1, 1000.0, True), (&#39;Taylor&#39;, 1, 750.0, False)] . #### Loading a CSV into a table . You’re now going to learn how to load the contents of a CSV file into a table. . One way to do that would be to read a CSV file line by line, create a dictionary from each line, and then use insert() , like you did in the previous exercise. . But there is a faster way using pandas . You can read a CSV file into a DataFrame using the read_csv() function (this function should be familiar to you, but you can run help(pd.read_csv) in the console to refresh your memory!). Then, you can call the .to_sql() method on the DataFrame to load it into a SQL table in a database. The columns of the DataFrame should match the columns of the SQL table. . .to_sql() has many parameters, but in this exercise we will use the following: . name is the name of the SQL table (as a string). | con is the connection to the database that you will use to upload the data. | if_exists specifies how to behave if the table already exists in the database; possible values are &quot;fail&quot; , &quot;replace&quot; , and &quot;append&quot; . | index ( True or False ) specifies whether to write the DataFrame’s index as a column. | . In this exercise, you will upload the data contained in the census.csv file into an existing table &quot;census&quot; . The connection to the database has already been created for you. . # import pandas import pandas as pd # read census.csv into a dataframe : census_df census_df = pd.read_csv(&quot;census.csv&quot;, header=None) # rename the columns of the census dataframe census_df.columns = [&#39;state&#39;, &#39;sex&#39;, &#39;age&#39;, &#39;pop2000&#39;, &#39;pop2008&#39;] census_df.head(3) . state sex age pop2000 pop2008 0 Illinois M 0 89600 95012 1 Illinois M 1 88445 91829 2 Illinois M 2 88729 89547 . connection &lt;sqlalchemy.engine.base.Connection at 0x7feca8d64e10&gt; . # append the data from census_df to the &quot;census&quot; table via connection census_df.to_sql(name=&#39;census&#39;, con=connection, if_exists=&#39;append&#39;, index=False) . connection.execute(select([func.count(census)])).fetchmany(3) [(8772,)] census_df.shape (8772, 5) . The pandas package provides us with an efficient way to load a DataFrames into a SQL table. If you would create and execute a statement to select all records from census, you would see that there are 8772 rows in the table. . . 4.3 Updating data in a database . #### Updating individual records . The update statement is very similar to an insert statement. For example, you can update all wages in the employees table as follows: . stmt = update(employees).values(wage=100.00) . The update statement also typically uses a where clause to help us determine what data to update. For example, to only update the record for the employee with ID 15, you would append the previous statement as follows: . stmt = stmt.where(employees.columns.id == 15) . repr(state_fact) # &quot;Table(&#39;state_fact&#39;, MetaData(bind=None), Column(&#39;id&#39;, VARCHAR(length=256), table=&lt;state_fact&gt;), Column(&#39;name&#39;, VARCHAR(length=256), table=&lt;state_fact&gt;), Column(&#39;abbreviation&#39;, VARCHAR(length=256), table=&lt;state_fact&gt;), Column(&#39;country&#39;, VARCHAR(length=256), table=&lt;state_fact&gt;), Column(&#39;type&#39;, VARCHAR(length=256), table=&lt;state_fact&gt;), Column(&#39;sort&#39;, VARCHAR(length=256), table=&lt;state_fact&gt;), Column(&#39;status&#39;, VARCHAR(length=256), table=&lt;state_fact&gt;), Column(&#39;occupied&#39;, VARCHAR(length=256), table=&lt;state_fact&gt;), Column(&#39;notes&#39;, VARCHAR(length=256), table=&lt;state_fact&gt;), Column(&#39;fips_state&#39;, VARCHAR(length=256), table=&lt;state_fact&gt;), Column(&#39;assoc_press&#39;, VARCHAR(length=256), table=&lt;state_fact&gt;), Column(&#39;standard_federal_region&#39;, VARCHAR(length=256), table=&lt;state_fact&gt;), Column(&#39;census_region&#39;, VARCHAR(length=256), table=&lt;state_fact&gt;), Column(&#39;census_region_name&#39;, VARCHAR(length=256), table=&lt;state_fact&gt;), Column(&#39;census_division&#39;, VARCHAR(length=256), table=&lt;state_fact&gt;), Column(&#39;census_division_name&#39;, VARCHAR(length=256), table=&lt;state_fact&gt;), Column(&#39;circuit_court&#39;, VARCHAR(length=256), table=&lt;state_fact&gt;), schema=None)&quot; . # Build a select statement: select_stmt select_stmt = select([state_fact]).where(state_fact.columns.name == &#39;New York&#39;) # Execute select_stmt and fetch the results results = connection.execute(select_stmt).fetchall() # Print the results of executing the select_stmt print(results) # [(&#39;32&#39;, &#39;New York&#39;, &#39;NY&#39;, &#39;USA&#39;, &#39;state&#39;, &#39;10&#39;, &#39;current&#39;, &#39;occupied&#39;, &#39;&#39;, &#39;0&#39;, &#39;N.Y.&#39;, &#39;II&#39;, &#39;1&#39;, &#39;Northeast&#39;, &#39;2&#39;, &#39;Mid-Atlantic&#39;, &#39;2&#39;)] # Print the FIPS code for the first row of the result print(results[0][&#39;fips_state&#39;]) # 0 . select_stmt = select([state_fact]).where(state_fact.columns.name == &#39;New York&#39;) results = connection.execute(select_stmt).fetchall() print(results) print(results[0][&#39;fips_state&#39;]) # Build a statement to update the fips_state to 36: update_stmt update_stmt = update(state_fact).values(fips_state = 36) # Append a where clause to limit it to records for New York state update_stmt = update_stmt.where(state_fact.columns.name == &#39;New York&#39;) # Execute the statement: update_results update_results = connection.execute(update_stmt) . # Execute select_stmt again and fetch the new results new_results = connection.execute(select_stmt).fetchall() # Print the new_results print(new_results) # [(&#39;32&#39;, &#39;New York&#39;, &#39;NY&#39;, &#39;USA&#39;, &#39;state&#39;, &#39;10&#39;, &#39;current&#39;, &#39;occupied&#39;, &#39;&#39;, &#39;36&#39;, &#39;N.Y.&#39;, &#39;II&#39;, &#39;1&#39;, &#39;Northeast&#39;, &#39;2&#39;, &#39;Mid-Atlantic&#39;, &#39;2&#39;)] # Print the FIPS code for the first row of the new_results print(new_results[0][&#39;fips_state&#39;]) # 36 . #### Updating multiple records . By using a where clause that selects more records, you can update multiple records at once. . Unlike inserting, updating multiple records works exactly the same way as updating a single record (as long as you are updating them with the same value). . # Build a statement to update the notes to &#39;The Wild West&#39;: stmt stmt = update(state_fact).values(notes=&#39;The Wild West&#39;) # Append a where clause to match the West census region records: stmt_west stmt_west = stmt.where(state_fact.columns.census_region_name == &#39;West&#39;) # Execute the statement: results results = connection.execute(stmt_west) # Print rowcount print(results.rowcount) # 13 . #### Correlated updates . You can also update records with data from a select statement. This is called a correlated update. It works by defining a select statement that returns the value you want to update the record with and assigning that select statement as the value in update . . state_fact.columns.keys() [&#39;id&#39;, &#39;name&#39;, ... &#39;fips_state&#39;, ...] connection.execute(select([state_fact])).fetchmany(3) # [(&#39;13&#39;, &#39;Illinois&#39;, &#39;IL&#39;, &#39;USA&#39;, &#39;state&#39;, &#39;10&#39;, &#39;current&#39;, &#39;occupied&#39;, &#39;&#39;, &#39;17&#39;, &#39;Ill.&#39;, &#39;V&#39;, &#39;2&#39;, &#39;Midwest&#39;, &#39;3&#39;, &#39;East North Central&#39;, &#39;7&#39;), (&#39;30&#39;, &#39;New Jersey&#39;, &#39;NJ&#39;, &#39;USA&#39;, &#39;state&#39;, &#39;10&#39;, &#39;current&#39;, &#39;occupied&#39;, &#39;&#39;, &#39;34&#39;, &#39;N.J.&#39;, &#39;II&#39;, &#39;1&#39;, &#39;Northeast&#39;, &#39;2&#39;, &#39;Mid-Atlantic&#39;, &#39;3&#39;), (&#39;34&#39;, &#39;North Dakota&#39;, &#39;ND&#39;, &#39;USA&#39;, &#39;state&#39;, &#39;10&#39;, &#39;current&#39;, &#39;occupied&#39;, &#39;&#39;, &#39;38&#39;, &#39;N.D.&#39;, &#39;VIII&#39;, &#39;2&#39;, &#39;Midwest&#39;, &#39;4&#39;, &#39;West North Central&#39;, &#39;8&#39;)] flat_census.columns.keys() # [&#39;state_name&#39;, &#39;fips_code&#39;] connection.execute(select([flat_census])).fetchmany(3) # [(None, &#39;17&#39;), (None, &#39;34&#39;), (None, &#39;38&#39;)] . # Build a statement to select name from state_fact: fips_stmt fips_stmt = select([state_fact.columns.name]) # Append a where clause to match the fips_state to flat_census fips_code: fips_stmt fips_stmt = fips_stmt.where( state_fact.columns.fips_state == flat_census.columns.fips_code) # Build an update statement to set the name to fips_stmt_where: update_stmt update_stmt = update(flat_census).values(state_name=fips_stmt) # Execute update_stmt: results results = connection.execute(update_stmt) # Print rowcount print(results.rowcount) # 51 . connection.execute(select([flat_census])).fetchmany(3) [(&#39;Illinois&#39;, &#39;17&#39;), (&#39;New Jersey&#39;, &#39;34&#39;), (&#39;North Dakota&#39;, &#39;38&#39;)] . . 4.4 Removing data from a database . #### Deleting all the records from a table . Often, you’ll need to empty a table of all of its records so you can reload the data. You can do this with a delete statement with just the table as an argument. For example, delete the table extra_employees by executing as follows: . delete_stmt = delete(extra_employees) result_proxy = connection.execute(delete_stmt) . Do be careful, though, as deleting cannot be undone! . # Import delete, select from sqlalchemy import delete, select # Build a statement to empty the census table: stmt delete_stmt = delete(census) # Execute the statement: results results = connection.execute(delete_stmt) # Print affected rowcount print(results.rowcount) # 8772 # Build a statement to select all records from the census table : select_stmt select_stmt = select([census]) # Print the results of executing the statement to verify there are no rows print(connection.execute(select_stmt).fetchall()) # [] . As you can see, there are no records left in the census table after executing the delete statement! . #### Deleting specific records . By using a where() clause, you can target the delete statement to remove only certain records. For example, delete all rows from the employees table that had id 3 with the following delete statement: . delete(employees).where(employees.columns.id == 3) . Here you’ll delete ALL rows which have &#39;M&#39; in the sex column and 36 in the age column. . We have included code at the start which computes the total number of these rows. It is important to make sure that this is the number of rows that you actually delete. . # Build a statement to count records using the sex column for Men (&#39;M&#39;) age 36: count_stmt count_stmt = select([func.count(census.columns.sex)]).where( and_(census.columns.sex == &#39;M&#39;, census.columns.age == 36) ) # Execute the select statement and use the scalar() fetch method to save the record count to_delete = connection.execute(count_stmt).scalar() # Build a statement to delete records from the census table: delete_stmt delete_stmt = delete(census) # Append a where clause to target Men (&#39;M&#39;) age 36: delete_stmt delete_stmt = delete_stmt.where( and_(census.columns.sex == &#39;M&#39;, census.columns.age == 36)) # Execute the statement: results results = connection.execute(delete_stmt) # Print affected rowcount and to_delete record count, make sure they match print(results.rowcount, to_delete) # 51 51 . You may frequently be required to remove specific records from a table, like in this case. . #### Deleting a table completely . You’re now going to practice dropping individual tables from a database with the .drop() method, as well as all tables in a database with the .drop_all() method! . As Spider-Man’s Uncle Ben said: With great power, comes great responsibility. Do be careful when deleting tables, as it’s not simple or fast to restore large databases! . Remember, you can check to see if a table exists on an engine with the .exists(engine) method. . engine # Engine(sqlite:///census.sqlite) repr(state_fact) # &quot;Table(&#39;state_fact&#39;, MetaData(bind=None), Column(&#39;id&#39;, VARCHAR(length=256), table=&lt;state_fact&gt;), ... Column(&#39;circuit_court&#39;, VARCHAR(length=256), table=&lt;state_fact&gt;), schema=None)&quot; state_fact.exists(engine) # True repr(metadata.tables) # &quot;immutabledict({&#39;census&#39;: Table(&#39;census&#39;, MetaData(bind=None), Column(&#39;state&#39;, VARCHAR(length=30), table=&lt;census&gt;), ... , Column(&#39;pop2008&#39;, INTEGER(), table=&lt;census&gt;), schema=None), &#39;state_fact&#39;: Table(&#39;state_fact&#39;, MetaData(bind=None), Column(&#39;id&#39;, VARCHAR(length=256), table=&lt;state_fact&gt;), Column(&#39;circuit_court&#39;, VARCHAR(length=256), ... , table=&lt;state_fact&gt;), schema=None)})&quot; . # Drop the state_fact table state_fact.drop(engine) # Check to see if state_fact exists print(state_fact.exists(engine)) # False # Drop all tables metadata.drop_all(engine) # Check to see if census exists print(census.exists(engine)) # False . . 5. Putting it all together . . 5.1 Census case study . #### Setup the engine and metadata . In this exercise, your job is to create an engine to the database that will be used in this chapter. Then, you need to initialize its metadata. . Recall how you did this in Chapter 1 by leveraging create_engine() and MetaData() . . # Import create_engine, MetaData from sqlalchemy import create_engine, MetaData # Define an engine to connect to chapter5.sqlite: engine engine = create_engine(&#39;sqlite:///chapter5.sqlite&#39;) # Initialize MetaData: metadata metadata = MetaData() . engine # Engine(sqlite:///chapter5.sqlite) type(engine) # sqlalchemy.engine.base.Engine metadata # MetaData(bind=None) type(metadata) # sqlalchemy.sql.schema.MetaData . #### Create the table to the database . Having setup the engine and initialized the metadata, you will now define the census table object and then create it in the database using the metadata and engine from the previous exercise. . To create it in the database, you will have to use the .create_all() method on the metadata with engine as the argument. . # Import Table, Column, String, and Integer from sqlalchemy import Table, Column, String, Integer # Build a census table: census census = Table(&#39;census&#39;, metadata, Column(&#39;state&#39;, String(30)), Column(&#39;sex&#39;, String(1)), Column(&#39;age&#39;, Integer()), Column(&#39;pop2000&#39;, Integer()), Column(&#39;pop2008&#39;, Integer())) # Create the table in the database metadata.create_all(engine) . When creating columns of type String() , it’s important to spend some time thinking about what their maximum lengths should be. . . 5.2 Populating the database . #### Reading the data from the CSV . Leverage the Python CSV module from the standard library and load the data into a list of dictionaries. . csv_reader &lt;_csv.reader at 0x7f3e282604a8&gt; . # Create an empty list: values_list values_list = [] # Iterate over the rows for row in csv_reader: # Create a dictionary with the values data = {&#39;state&#39;: row[0], &#39;sex&#39;: row[1], &#39;age&#39;:row[2], &#39;pop2000&#39;: row[3], &#39;pop2008&#39;: row[4]} # Append the dictionary to the values list values_list.append(data) values_list[:2] . [{&#39;age&#39;: &#39;0&#39;, &#39;pop2000&#39;: &#39;89600&#39;, &#39;pop2008&#39;: &#39;95012&#39;, &#39;sex&#39;: &#39;M&#39;, &#39;state&#39;: &#39;Illinois&#39;}, {&#39;age&#39;: &#39;1&#39;, &#39;pop2000&#39;: &#39;88445&#39;, &#39;pop2008&#39;: &#39;91829&#39;, &#39;sex&#39;: &#39;M&#39;, &#39;state&#39;: &#39;Illinois&#39;}] . #### Load data from a list into the Table . # Import insert from sqlalchemy import insert # Build insert statement: stmt stmt = insert(census) # Use values_list to insert data: results results = connection.execute(stmt, values_list) # Print rowcount print(results.rowcount) # 8772 from sqlalchemy import select connection.execute(select([census])).fetchmany(3) [(&#39;Illinois&#39;, &#39;M&#39;, 0, 89600, 95012), (&#39;Illinois&#39;, &#39;M&#39;, 1, 88445, 91829), (&#39;Illinois&#39;, &#39;M&#39;, 2, 88729, 89547)] . . 5.3 Example queries . #### Determine the average age by population . To calculate a weighted average, we first find the total sum of weights multiplied by the values we’re averaging, then divide by the sum of all the weights. . In this exercise, however, you will make use of func.sum() together with select to select the weighted average of a column from a table. You will still work with the census data, and you will compute the average of age weighted by state population in the year 2000, and then group this weighted average by sex. . # Import select and func from sqlalchemy import select, func # Select sex and average age weighted by 2000 population stmt = select([(func.sum(census.columns.pop2000 * census.columns.age) / func.sum(census.columns.pop2000)).label(&#39;average_age&#39;), census.columns.sex]) # Group by sex stmt = stmt.group_by(census.columns.sex) # Execute the query and fetch all the results results = connection.execute(stmt).fetchall() # Print the sex and average age column for each result for result in results: print(result.sex, result.average_age) # F 37 # M 34 results # [(37, &#39;F&#39;), (34, &#39;M&#39;)] . #### Determine the percentage of population by gender and state . In this exercise, you will write a query to determine the percentage of the population in 2000 that comprised of women. You will group this query by state. . # import case, cast and Float from sqlalchemy from sqlalchemy import case, cast, Float # Build a query to calculate the percentage of women in 2000: stmt stmt = select([census.columns.state, (func.sum( case([ (census.columns.sex == &#39;F&#39;, census.columns.pop2000) ], else_=0)) / cast(func.sum(census.columns.pop2000), Float) * 100).label(&#39;percent_female&#39;) ]) # Group By state stmt = stmt.group_by(census.columns.state) # Execute the query and store the results: results results = connection.execute(stmt).fetchall() # Print the percentage for result in results: print(result.state, result.percent_female) . Alabama 51.8324077702 Alaska 49.3014978935 ... Wisconsin 50.6148645265 Wyoming 49.9459554265 . #### Determine the difference by state from the 2000 and 2008 censuses . In this final exercise, you will write a query to calculate the states that changed the most in population. You will limit your query to display only the top 10 states. . # Build query to return state name and population difference from 2008 to 2000 stmt = select([census.columns.state, (census.columns.pop2008-census.columns.pop2000).label(&#39;pop_change&#39;) ]) # Group by State stmt = stmt.group_by(census.columns.state) # Order by Population Change stmt = stmt.order_by(desc(&#39;pop_change&#39;)) # Limit to top 10 stmt = stmt.limit(10) # Use connection to execute the statement and fetch all results results = connection.execute(stmt).fetchall() # Print the state and population change for each record for result in results: print(&#39;{}:{}&#39;.format(result.state, result.pop_change)) . California:105705 Florida:100984 Texas:51901 New York:47098 Pennsylvania:42387 Arizona:29509 Ohio:29392 Illinois:26221 Michigan:25126 North Carolina:24108 . The End. . Thank you for reading. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/introduction-to-databases-in-python.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/introduction-to-databases-in-python.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post25": {
            "title": "Intro to Python for Finance",
            "content": "Intro to Python for Finance . This is a memo. This course does not have a track yet. . **A introduction course for those who have no experience in Python. . You can find the original course HERE** . . . 1. Basic of Python . . Nothing special here. . . 2. Lists . . #### sort() &amp; max() . # list sort prices = [159.54, 37.13, 71.17] prices.sort() print(prices) # [37.13, 71.17, 159.54] # max value of a list prices = [159.54, 37.13, 71.17] price_max = max(prices) print(price_max) # 159.54 . #### append() &amp; extend() . # Append a name to the list names names.append(&#39;Amazon.com&#39;) print(names) # Extend list names more_elements = [&#39;DowDuPont&#39;, &#39;Alphabet Inc&#39;] names.append(more_elements) print(names) # [&#39;Apple Inc&#39;, &#39;Coca-Cola&#39;, &#39;Walmart&#39;, &#39;Amazon.com&#39;] # append a list to the list will produce a nested list. # [&#39;Apple Inc&#39;, &#39;Coca-Cola&#39;, &#39;Walmart&#39;, &#39;Amazon.com&#39;, [&#39;DowDuPont&#39;, &#39;Alphabet Inc&#39;]] # to get a flat list, use extend [&#39;Apple Inc&#39;, &#39;Coca-Cola&#39;, &#39;Walmart&#39;, &#39;Amazon.com&#39;, &#39;DowDuPont&#39;, &#39;Alphabet Inc&#39;] . #### index() . # get max price max_price = max(prices) # Identify index of max price max_index = prices.index(max_price) # Identify the name of the company with max price max_stock_name = names[max_index] print(&#39;The largest stock price is associated with &#39; + max_stock_name + &#39; and is $&#39; + str(max_price) + &#39;.&#39;) # The largest stock price is associated with Amazon.com and is $1705.54. . . 3. Numpy arrays . . #### step slicing . # Subset every third element print(prices_array) # [170.12 93.29 55.28 145.3 171.81 59.5 100.5 ] prices_subset_3 = prices_array[0:7:3] print(prices_subset_3) # [170.12 145.3 100.5 ] . #### 2D array . # Create a 2D array of prices and earnings stock_array = np.array([prices, earnings]) print(stock_array) # [[170.12 93.29 55.28 145.3 171.81 59.5 100.5 ] # [ 9.2 5.31 2.41 5.91 15.42 2.51 6.79]] # Print the shape of stock_array print(stock_array.shape) # (2, 7) # Print the size of stock_array print(stock_array.size) # 14 . #### np.transpose() . # Transpose stock_array stock_array_transposed = np.transpose(stock_array) print(stock_array_transposed) # [[170.12 9.2 ] [ 93.29 5.31] [ 55.28 2.41] [145.3 5.91] [171.81 15.42] [ 59.5 2.51] [100.5 6.79]] # Print the shape of stock_array print(stock_array_transposed.shape) # (7, 2) # Print the size of stock_array print(stock_array_transposed.size) # 14 . #### Subsetting 2D arrays . # original array stock_array_transposed [[170.12 9.2 ] [ 93.29 5.31] [ 55.28 2.41] [145.3 5.91] [171.81 15.42] [ 59.5 2.51] [100.5 6.79]] . # Subset all rows and column array[:, column_index] # Subset the first (0th) columns prices = stock_array_transposed[:, 0] print(prices) # [170.12 93.29 55.28 145.3 171.81 59.5 100.5 ] ########################################## # Subset all columns and row array[row_index, :] # Subset the first 3 rows (0,1,2) stock_array_transposed[0:3] array([[170.12, 9.2 ], [ 93.29, 5.31], [ 55.28, 2.41]]) ########################################## # Subset single value array[row_index][column_index] # Subset the 3rd row, 1st column stock_array_transposed[2][0] 55.28 . #### np.mean &amp; np.std() . # Calculate mean and standard deviation np.mean(array) np.std(array) . #### np.arange() . # Create and print company IDs company_ids = np.arange(1, 8, 1) print(company_ids) # [1 2 3 4 5 6 7] # Use array slicing to select specific company IDs company_ids_odd = np.arange(1, 8, 2) print(company_ids_odd) # [1 3 5 7] . #### boolean slice I: numeric . # Find the mean price_mean = np.mean(prices) # Create boolean array boolean_array = (prices &gt; price_mean) print(boolean_array) [ True False False True True False False] # Select prices that are greater than average above_avg = prices[boolean_array] print(above_avg) [170.12 145.3 171.81] . #### boolean slice II: string . sectors array([&#39;Information Technology&#39;, &#39;Health Care&#39;, &#39;Health Care&#39;, &#39;Information Technologies&#39;, &#39;Health Care&#39;], dtype=&#39;&lt;U24&#39;) names array([&#39;Apple Inc&#39;, &#39;Abbvie Inc&#39;, &#39;Abbott Laboratories&#39;, &#39;Accenture Technologies&#39;, &#39;Allergan Plc&#39;], dtype=&#39;&lt;U22&#39;) . # Create boolean array boolean_array = (sectors == &#39;Health Care&#39;) print(boolean_array) # [False True True False True] # Print only health care companies health_care = names[boolean_array] print(health_care) # [&#39;Abbvie Inc&#39; &#39;Abbott Laboratories&#39; &#39;Allergan Plc&#39;] . . Visualization . | #### First plot (line plot) . # Import matplotlib.pyplot with the alias plt import matplotlib.pyplot as plt # Plot the price of stock over time plt.plot(days, prices, color=&quot;red&quot;, linestyle=&quot;--&quot;) # Display the plot plt.show() . . #### Add label and title . import matplotlib.pyplot as plt # Plot price as a function of time plt.plot(days, prices, color=&quot;red&quot;, linestyle=&quot;--&quot;) # Add x and y labels plt.xlabel(&#39;Days&#39;) plt.ylabel(&#39;Prices, $&#39;) # Add plot title plt.title(&#39;Company Stock Prices Over Time&#39;) # Show plot plt.show() . . #### Multi graphs . # Plot two lines of varying colors plt.plot(days, prices1, color=&#39;red&#39;) plt.plot(days, prices2, color=&#39;green&#39;) # Add labels plt.xlabel(&#39;Days&#39;) plt.ylabel(&#39;Prices, $&#39;) plt.title(&#39;Stock Prices Over Time&#39;) plt.show() . . #### Scatter plot . # Import pyplot as plt import matplotlib.pyplot as plt # Plot price as a function of time plt.scatter(days, prices, color=&#39;green&#39;, s=0.1) # Show plot plt.show() . . #### histogram . # Plot histogram plt.hist(prices, bins=100) # Display plot plt.show() . . You can see that these prices are not normally distributed, they are skewed to the left! . #### Comparing two histograms . # Plot histogram of stocks_A plt.hist(stock_A, bins=100, alpha=0.4) # Plot histogram of stocks_B plt.hist(stock_B, bins=100, alpha=0.4) # Display plot plt.show() . . #### Add legend . # Plot stock_A and stock_B histograms plt.hist(stock_A, bins=100, alpha=0.4, label=&#39;Stock A&#39;) plt.hist(stock_B, bins=100, alpha=0.4, label=&#39;Stock B&#39;) # Add the legend plt.legend() # Display the plot plt.show() . . . 5. S&amp;P 100 Case Study . . Nothing special here. . The End. . Thank you for reading. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/intro-to-python-for-finance.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/intro-to-python-for-finance.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post26": {
            "title": "Intro to Financial Concepts using Python",
            "content": "Intro to Financial Concepts using Python . This is a memo. This course does not have a track yet. . You can find the original course HERE . . . . 1. The Time Value of Money . . 1.1 Fundamental Financial Concepts . . #### Growth and Rate of Return . Growth and Rate of Return are two concepts that are ubiquitous throughout the financial world. . Calculate the future value (cumulative return) of a $100 investment which grows at a rate of 6% per year for 30 years in a row and assign it to future_value . . # Calculate the future value of the investment and print it out future_value = 100 * (1 + 0.06) ** 30 print(&quot;Future Value of Investment: &quot; + str(round(future_value, 2))) # Future Value of Investment: 574.35 . #### Compound Interest . As you saw in the previous exercise, both time and the rate of return are very important variables when forecasting the future value of an investment. . Another important variable is the number of compounding periods, which can greatly affect compounded returns over time. . # Predefined variables initial_investment = 100 growth_periods = 30 growth_rate = 0.06 # Calculate the value for the investment compounded once per year compound_periods_1 = 1 investment_1 = initial_investment*(1 + growth_rate / compound_periods_1)**(compound_periods_1*growth_periods) print(&quot;Investment 1: &quot; + str(round(investment_1, 2))) # Calculate the value for the investment compounded quarterly compound_periods_2 = 4 investment_2 = initial_investment*(1 + growth_rate / compound_periods_2)**(compound_periods_2*growth_periods) print(&quot;Investment 2: &quot; + str(round(investment_2, 2))) # Calculate the value for the investment compounded monthly compound_periods_3 = 12 investment_3 = initial_investment*(1 + growth_rate / compound_periods_3)**(compound_periods_3*growth_periods) print(&quot;Investment 3: &quot; + str(round(investment_3, 2))) # Investment 1: 574.35 # Investment 2: 596.93 # Investment 3: 602.26 . #### Discount Factors and Depreciation . Unfortunately, not everything grows in value over time. . In fact, many assets depreciate , or lose value over time. To simulate this, you can simply assume a negative expected rate of return. . Calculate the future value of a $100 investment that depreciates in value by 5% per year for 10 years and assign it to future_value . . # Calculate the future value initial_investment = 100 growth_rate = -0.05 growth_periods = 10 future_value = initial_investment*(1 + growth_rate)**(growth_periods) print(&quot;Future value: &quot; + str(round(future_value, 2))) # Calculate the discount factor discount_factor = 1/((1 + growth_rate)**(growth_periods)) print(&quot;Discount factor: &quot; + str(round(discount_factor, 2))) # Derive the initial value of the investment initial_investment_again = future_value * discount_factor print(&quot;Initial value: &quot; + str(round(initial_investment_again, 2))) # Future value: 59.87 # Discount factor: 1.67 # Initial value: 100.0 . . 1.2 Present and Future Value . #### Present Value . Luckily for you, there is a module called numpy which contains many functions which will make your life much easier when working with financial values. . The .pv(rate, nper, pmt, fv) function, for example, allows you to calculate the present value of an investment as before with a few simple parameters: . rate: The rate of return of the investment | nper: The lifespan of the investment | pmt: The (fixed) payment at the beginning or end of each period (which is 0 in our example) | fv: The future value of the investment | . You can use this formula in many ways. For example, you can calculate the present value of future investments in today’s dollars. . Compute the present value of an investment which will yield $10,000 15 years from now at an inflation rate of 3% per year and assign it to investment_1 . . # Import numpy as np import numpy as np # Calculate investment_1 investment_1 = np.pv(rate=0.03, nper=15, pmt=0, fv=10000) # Note that the present value returned is negative, so we multiply the result by -1 print(&quot;Investment 1 is worth &quot; + str(round(-investment_1, 2)) + &quot; in today&#39;s dollars&quot;) # Calculate investment_2 investment_2 = np.pv(rate=0.05, nper=10, pmt=0, fv=10000) print(&quot;Investment 2 is worth &quot; + str(round(-investment_2, 2)) + &quot; in today&#39;s dollars&quot;) # Investment 1 is worth 6418.62 in today&#39;s dollars # Investment 2 is worth 6139.13 in today&#39;s dollars . Notice how a higher inflation rate leads to a lower present value. . #### Future Value . The numpy module also contains a similar function, .fv(rate, nper, pmt, pv) , which allows you to calculate the future value of an investment as before with a few simple parameters: . rate: The rate of return of the investment | nper: The lifespan of the investment | pmt: The (fixed) payment at the beginning or end of each period (which is 0 in our example) | pv: The present value of the investment | . It is important to note that in this function call, you must pass a negative value into the pv parameter if it represents a negative cash flow (cash going out). In other words, if you were to compute the future value of an investment, requiring an up-front cash payment, you would need to pass a negative value to the pv parameter in the .fv() function. . import numpy as np # Calculate investment_1 investment_1 = np.fv(rate=0.05, nper=15, pmt=0, pv=-10000) print(&quot;Investment 1 will yield a total of $&quot; + str(round(investment_1, 2)) + &quot; in 15 years&quot;) # Calculate investment_2 investment_2 = np.fv(rate=0.08, nper=15, pmt=0, pv=-10000) print(&quot;Investment 2 will yield a total of $&quot; + str(round(investment_2, 2)) + &quot; in 15 years&quot;) # Investment 1 will yield a total of $20789.28 in 15 years # Investment 2 will yield a total of $31721.69 in 15 years . Note how the growth rate dramatically affects the future value. . #### Adjusting Future Values for Inflation . You can now put together what you learned in the previous exercises by following a simple methodology: . First, forecast the future value of an investment given a rate of return | Second, discount the future value of the investment by a projected inflation rate | . The methodology above will use both the .fv() and .pv() functions to arrive at the projected value of a given investment in today’s dollars, adjusted for inflation. . import numpy as np # Calculate investment_1 investment_1 = np.fv(rate=0.08, nper=10, pmt=0, pv=-10000) print(&quot;Investment 1 will yield a total of $&quot; + str(round(investment_1, 2)) + &quot; in 10 years&quot;) # Calculate investment_2 investment_1_discounted = np.pv(rate=0.03, nper=10, pmt=0, fv=investment_1) print(&quot;After adjusting for inflation, investment 1 is worth $&quot; + str(round(-investment_1_discounted, 2)) + &quot; in today&#39;s dollars&quot;) # Investment 1 will yield a total of $21589.25 in 10 years # After adjusting for inflation, investment 1 is worth $16064.43 in today&#39;s dollars . You now know how to project the value of investments and adjust for inflation. . 1.3 Net Present Value and Cash Flows . #### Discounting Cash Flows . You can use numpy’s net present value function numpy.npv(rate, values) to calculate the net present value of a series of cash flows. . import numpy as np # Predefined array of cash flows cash_flows = np.array([100, 100, 100, 100, 100]) # Calculate investment_1 investment_1 = np.npv(rate=0.03, values=cash_flows) print(&quot;Investment 1&#39;s net present value is $&quot; + str(round(investment_1, 2)) + &quot; in today&#39;s dollars&quot;) # Calculate investment_2 investment_2 = np.npv(rate=0.05, values=cash_flows) print(&quot;Investment 2&#39;s net present value is $&quot; + str(round(investment_2, 2)) + &quot; in today&#39;s dollars&quot;) # Calculate investment_3 investment_3 = np.npv(rate=0.07, values=cash_flows) print(&quot;Investment 3&#39;s net present value is $&quot; + str(round(investment_3, 2)) + &quot; in today&#39;s dollars&quot;) # Investment 1&#39;s net present value is $471.71 in today&#39;s dollars # Investment 2&#39;s net present value is $454.6 in today&#39;s dollars # Investment 3&#39;s net present value is $438.72 in today&#39;s dollars . Notice how the higher discount rate leads to a lower NPV. . #### Initial Project Costs . The numpy.npv(rate, values) function is very powerful because it allows you to pass in both positive and negative values. . For this exercise, you will calculate the net present value of two potential projects with different cash flows: . | Year | Project 1 | Project 2 | | — | — | — | | 1 | -$250 (initial investment) | -$250 (initial investment) | | 2 | $100 cash flow | $300 cash flow | | 3 | $200 cash flow | -$250 (net investment) | | 4 | $300 cash flow | $300 cash flow | | 5 | $400 cash flow | $300 cash flow | . In this example, project 1 only requires an initial investment of $250, generating a slowly increasing series of cash flows over the next 4 years. . Project 2, on the other hand, requires an initial investment of $250 and an additional investment of $250 in year 3. However, project 2 continues to generate larger cash flows. . Assuming both projects don’t generate any more cash flows after the fifth year, which project would you decide to undertake? The best way to decide is by comparing the NPV of both projects. . import numpy as np # Create an array of cash flows for project 1 cash_flows_1 = np.array([-250, 100, 200, 300, 400]) # Create an array of cash flows for project 2 cash_flows_2 = np.array([-250, 300, -250, 300, 300]) # Calculate the net present value of project 1 investment_1 = np.npv(rate=0.03, values=cash_flows_1) print(&quot;The net present value of Investment 1 is worth $&quot; + str(round(investment_1, 2)) + &quot; in today&#39;s dollars&quot;) # Calculate the net present value of project 2 investment_2 = np.npv(rate=0.03, values=cash_flows_2) print(&quot;The net present value of Investment 2 is worth $&quot; + str(round(investment_2, 2)) + &quot; in today&#39;s dollars&quot;) # The net present value of Investment 1 is worth $665.54 in today&#39;s dollars # The net present value of Investment 2 is worth $346.7 in today&#39;s dollars . #### Diminishing Cash Flows . Remember how compounded returns grow rapidly over time? Well, it works in the reverse, too. Compounded discount factors over time will quickly shrink a number towards zero. . For example, $100 at a 3% annual discount for 1 year is still worth roughly $97.08: . . This means that the longer in the future your cash flows will be received (or paid), the close to 0 that number will be. . import numpy as np # Calculate investment_1 investment_1 = np.pv(rate=0.03, nper=30, pmt=0, fv=100) print(&quot;Investment 1 is worth $&quot; + str(round(-investment_1, 2)) + &quot; in today&#39;s dollars&quot;) # Calculate investment_2 investment_2 = np.pv(rate=0.03, nper=50, pmt=0, fv=100) print(&quot;Investment 2 is worth $&quot; + str(round(-investment_2, 2)) + &quot; in today&#39;s dollars&quot;) # Calculate investment_3 investment_3 = np.pv(rate=0.03, nper=100, pmt=0, fv=100) print(&quot;Investment 3 is worth $&quot; + str(round(-investment_3, 2)) + &quot; in today&#39;s dollars&quot;) # Investment 1 is worth $41.2 in today&#39;s dollars # Investment 2 is worth $22.81 in today&#39;s dollars # Investment 3 is worth $5.2 in today&#39;s dollars . The moral of the story? It’s generally better to have money now rather than later. . 2. Making Data-Driven Financial Decisions . . 2.1 A Tale of Two Project Proposals . . #### Project Proposals and Cash Flows Projections . Your project managers have projected the cash flows for each of the proposals. . Project 1 provides higher short term cash flows, but Project 2 becomes more profitable over time. . The cash flow projections for both projects are as follows: . | Year | Project 1 | Project 2 | | — | — | — | | 1 | -$1,000 (initial investment) | -$1,000 (initial investment) | | 2 | $200 (cash flow) | $150 (cash flow) | | 3 | $250 | $225 | | 4 | $300 | $300 | | 5 | $350 | $375 | | 6 | $400 | $425 | | 7 | $450 | $500 | | 8 | $500 | $575 | | 9 | $550 | $600 | | 10 | $600 | $625 | . Note: The projections are provided in thousands. For example, $1,000 = $1,000,000. We will use the smaller denominations to make everything easier to read. This is also commonly done in financial statements with thousands or even millions in order to represent millions or billions. . import numpy as np # Create a numpy array of cash flows for Project 1 cf_project_1 = np.array([-1000, 200, 250, 300, 350, 400, 450, 500, 550, 600]) # Create a numpy array of cash flows for Project 2 cf_project_2 = np.array([-1000, 150, 225, 300, 375, 425, 500, 575, 600, 625]) # Scale the original objects by 1000x cf_project1 = cf_project_1 * 1000 cf_project2 = cf_project_2 * 1000 . #### Internal Rate of Return . Now that you have the cash flow projections ready to go for each project, you want to compare the internal rate of return (IRR) of each project to help you decide which project would be most beneficial for your company in terms of yield (rate of return). . import numpy as np # Calculate the internal rate of return for Project 1 irr_project1 = np.irr(cf_project1) print(&quot;Project 1 IRR: &quot; + str(round(100*irr_project1, 2)) + &quot;%&quot;) # Calculate the internal rate of return for Project 2 irr_project2 = np.irr(cf_project2) print(&quot;Project 2 IRR: &quot; + str(round(100*irr_project2, 2)) + &quot;%&quot;) # Project 1 IRR: 28.92% # Project 2 IRR: 28.78% . #### Make a Decision Based on IRR . If you were making the decision solely based on internal rate of return, which project would you be more interested in (assuming the IRR is greater than your required rate of return)? . Assume your required rate of return is 10% for this example. . Project 1! Because higher internal rates of return are preferable! . . 2.2 The Weighted Average Cost of Capital . (WACC) . . #### Debt and Equity Financing . In the previous chapter, you were able to assume that your discount rate for the NPV calculation was solely based on a measure such as inflation. . However, in this chapter, you are the CEO of a new company that has outstanding debt and financing costs, which you will have to adjust for. . You will use the WACC as your discount rate in upcoming exercises. . For this exercise, assume you take out a $1,000,000 loan to finance the project, which will be your company’s only outstanding debt. This loan will represent 50% of your company’s total financing of $2,000,000. The remaining funding comes from the market value of equity. . # Set the market value of debt mval_debt = 1000000 # Set the market value of equity mval_equity = 1000000 # Compute the total market value of your company&#39;s financing mval_total = mval_debt + mval_equity # Compute the proportion of your company&#39;s financing via debt percent_debt = mval_debt / mval_total print(&quot;Debt Financing: &quot; + str(round(100*percent_debt, 2)) + &quot;%&quot;) # Compute the proportion of your company&#39;s financing via equity percent_equity = mval_equity / mval_total print(&quot;Equity Financing: &quot; + str(round(100*percent_equity, 2)) + &quot;%&quot;) # Debt Financing: 50.0% # Equity Financing: 50.0% . #### Calculating WACC . In addition to determining the proportion of both equity and debt financing, you will need to estimate the cost of financing via both debt and equity in order to estimate your WACC. . The cost of debt financing can be estimated as the amount you will have to pay on a new loan. This can be estimated by looking at the interest rates of loans of similar sizes to similar companies, or could be based on previous loans your company may already have been issued. . The cost of equity financing can be estimated as the return on equity of similar companies. Calculating the return on equity is a simple accounting exercise, but all you need to know is that essentially, investors will require a rate of return that is close to what could be earned by a similar investment. . # The proportion of debt vs equity financing is predefined percent_debt = 0.50 percent_equity = 0.50 # Set the cost of equity cost_equity = 0.18 # Set the cost of debt cost_debt = 0.12 # Set the corporate tax rate tax_rate = 0.35 # Calculate the WACC wacc = percent_equity * cost_equity + percent_debt * cost_debt * (1 - tax_rate) print(&quot;WACC: &quot; + str(round(100*wacc, 2)) + &quot;%&quot;) # WACC: 12.9% . #### Comparing Project NPV with IRR . Companies use their WACC as the discount rate when calculating the net present value of potential projects. . In the same way that you discounted values by inflation in the previous chapter to account for costs over time, companies adjust the cash flows of potential projects by their cost of financing (the WACC) to account for their investor’s required rate of return based on market conditions. . Now that you calculated the WACC, you can determine the net present value (NPV) of each project’s cash flows. The cash flows for projects 1 and 2 are available as cf_project1 and cf_project2 . . import numpy as np # Set your weighted average cost of capital equal to 12.9% wacc = 0.129 # Calculate the net present value for Project 1 npv_project1 = np.npv(rate=wacc, values=cf_project1) print(&quot;Project 1 NPV: &quot; + str(round(npv_project1, 2))) # Calculate the net present value for Project 2 npv_project2 = np.npv(rate=wacc, values=cf_project2) print(&quot;Project 2 NPV: &quot; + str(round(npv_project2, 2))) # Project 1 NPV: 856073.18 # Project 2 NPV: 904741.35 . If you were making the decision solely based on net present value, which project would you be more interested in? . Project 2! Higher net present value is a good thing. . #### Two Project With Different Lifespans . The board of the company has decided to go a different direction, involving slightly shorter term projects and lower initial investments. . Your project managers have come up with two new ideas, and projected the cash flows for each of the proposals. . Project 1 has a lifespan of 8 years, but Project 2 only has a lifespan of 7 years. Project 1 requires an initial investment of $700,000, but Project 2 only requires $400,000. . The cash flow projections for both projects are as follows: . | Year | Project 1 | Project 2 | | — | — | — | | 1 | -$700 (initial investment) | -$400 (initial investment) | | 2 | $100 (cash flow) | $50 (cash flow) | | 3 | $150 | $100 | | 4 | $200 | $150 | | 5 | $250 | $200 | | 6 | $300 | $250 | | 7 | $350 | $300 | | 8 | $400 | N / A | . import numpy as np # Create a numpy array of cash flows for Project 1 cf_project_1 = np.array([-700, 100, 150, 200, 250, 300, 350, 400]) # Create a numpy array of cash flows for Project 2 cf_project_2 = np.array([-400, 50, 100, 150, 200, 250, 300]) # Scale the original objects by 1000x cf_project1 = cf_project_1 * 1000 cf_project2 = cf_project_2 * 1000 . #### Calculating IRR and NPV With Different Project Lifespans . Now that you calculated the WACC, you can calculate and compare the IRRs and NPVs of each project. . While the IRR remains relatively comparable across projects, the NPV, on the other hand, will be much more difficult to compare given the additional year required for project 1. . Luckily, in the next exercise, we will introduce another method to compare the NPVs of the projects, but we will first need to compute the NPVs as before. . import numpy as np # Calculate the IRR for Project 1 irr_project1 = np.irr(cf_project1) print(&quot;Project 1 IRR: &quot; + str(round(100*irr_project1, 2)) + &quot;%&quot;) # Calculate the IRR for Project 2 irr_project2 = np.irr(cf_project2) print(&quot;Project 2 IRR: &quot; + str(round(100*irr_project2, 2)) + &quot;%&quot;) # Set the wacc equal to 12.9% wacc = 0.129 # Calculate the NPV for Project 1 npv_project1 = np.npv(rate=wacc, values=cf_project1) print(&quot;Project 1 NPV: &quot; + str(round(npv_project1, 2))) # Calculate the NPV for Project 2 npv_project2 = np.npv(rate=wacc, values=cf_project2) print(&quot;Project 2 NPV: &quot; + str(round(npv_project2, 2))) # Project 1 IRR: 22.94% # Project 2 IRR: 26.89% # Project 1 NPV: 302744.98 # Project 2 NPV: 231228.39 . The NPVs really aren’t comparable. . #### Using the Equivalent Annual Annuity Approach . Since the net present values of each project are not directly comparable given the different lifespans of each project, you will have to consider a different approach. . The equivalent annual annuity (EAA) approach allows us to compare two projects by essentially assuming that each project is an investment generating a flat interest rate each year (an annuity), and calculating the annual payment you would receive from each project, discounted to present value. . You can compute the EAA of each project using the .pmt(rate, nper, pv, fv) function in numpy . . import numpy as np # Calculate the EAA for Project 1 eaa_project1 = np.pmt(rate=wacc, nper=8, pv=-npv_project1, fv=0) print(&quot;Project 1 EAA: &quot; + str(round(eaa_project1, 2))) # Calculate the EAA for Project 2 eaa_project2 = np.pmt(rate=wacc, nper=7, pv=-npv_project2, fv=0) print(&quot;Project 2 EAA: &quot; + str(round(eaa_project2, 2))) # Project 1 EAA: 62872.2 # Project 2 EAA: 52120.61 . This is one of a few ways to deal with this problem. . #### Making a Data-Driven Decision on Projects of Different Lifespans . If you were making the decision solely based on the equivalent annual annuity analysis, which project would you be more interested in? . Project 1! . Higher EAA means higher annual returns. . 3. Simulating a Mortgage Loan . . 3.1 Mortgage Basics . . #### Taking Out a Mortgage Loan . You’re expecting a child soon, and its time to start looking for a home. . You’re currently living out of an apartment in New York City, but your blossoming career as a Data Scientist has allowed you to save up a sizable sum and purchase a home in neighboring Hoboken, New Jersey. . You have decided to purchase a beautiful brownstone home in the $800,000 range. While you do have a considerable amount of cash on hand, you don’t have enough to purchase the entire home outright, which means you will have to take the remaining balance out as a mortgage loan . From the sound of it, you’ll have to put about 20% down up-front to a mortgage loan of that size. . This up-front payment is known as a down payment . . import numpy as np # Set the value of the home you are looking to buy home_value = 800000 # What percentage are you paying up-front? down_payment_percent = 0.2 # Calculate the dollar value of the down payment down_payment = home_value * down_payment_percent print(&quot;Initial Down Payment: &quot; + str(down_payment)) # Calculate the value of the mortgage loan required after the down payment mortgage_loan = home_value - down_payment print(&quot;Mortgage Loan: &quot; + str(mortgage_loan)) # Initial Down Payment: 160000.0 # Mortgage Loan: 640000.0 . #### Calculating the Monthly Mortgage Payment . In order to make sure you can afford the home, you will have to calculate the monthly mortgage payment you will have to make on a loan that size. . Now, since you will be paying a monthly mortgage, you will have to convert each of the parameters into their monthly equivalents. Be careful when adjusting the interest rate, which is compounding! . In order to calculate the monthly mortgage payment, you will use the numpy function .pmt(rate, nper, pv) where: . rate = The periodic (monthly) interest rate | nper = The number of payment periods (months) in the lifespan of the mortgage loan | pv = The total value of the mortgage loan | . You have been given a 30-year mortgage loan quote for your desired amount at 3.75%. The value of the mortgage loan is available as mortgage_loan . . import numpy as np # Derive the equivalent monthly mortgage rate from the annual rate mortgage_rate_periodic = (1 + mortgage_rate) ** (1/12) - 1 # How many monthly payment periods will there be over 30 years? mortgage_payment_periods = 30 * 12 # Calculate the monthly mortgage payment (multiply by -1 to keep it positive) periodic_mortgage_payment = -1*np.pmt(mortgage_rate_periodic, mortgage_payment_periods, mortgage_loan) print(&quot;Monthly Mortgage Payment: &quot; + str(round(periodic_mortgage_payment, 2))) # Monthly Mortgage Payment: 2941.13 . . 3.2 Amortization, Interest and Principal . . #### Calculating Interest and Principal Payments . Due to the size of the mortgage loan, you begin the mortgage in the initial period by paying mostly interest and retaining very little principal , or equity that goes towards the ownership of your home. . This means that if you were to stop paying your mortgage and sell your home after only a few years, the bank would actually own most of the home because what you paid was mostly interest, and very little principal. . mortgage_loan 640000.0 periodic_mortgage_payment 2941.125363188976 mortgage_rate_periodic 0.003072541703255549 . # Calculate the amount of the first loan payment that will go towards interest initial_interest_payment = mortgage_loan * mortgage_rate_periodic print(&quot;Initial Interest Payment: &quot; + str(round(initial_interest_payment, 2))) # Calculate the amount of the first loan payment that will go towards principal initial_principal_payment = periodic_mortgage_payment - initial_interest_payment print(&quot;Initial Principal Payment: &quot; + str(round(initial_principal_payment, 2))) # Initial Interest Payment: 1966.43 # Initial Principal Payment: 974.7 . #### Simulating Periodic Payments (I) . You have all the tools you’ll need to simulate the mortgage payments over time. . Every time a mortgage payment is made, the following payment will have a slightly lower percentage, which is used to pay off interest. This means that more of the remainder will go towards the portion of the home that you own instead of the bank. This is important to determine how much you will gain from selling the home before paying off your mortgage, or to determine when your mortgage is underwater. But more on that later. . You will now write a simple program to calculate the interest and mortgage portions of each payment over time. . principal_remaining array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) . # Loop through each mortgage payment period for i in range(0, mortgage_payment_periods): # Handle the case for the first iteration if i == 0: previous_principal_remaining = mortgage_loan else: previous_principal_remaining = principal_remaining[i-1] # Calculate the interest and principal payments interest_payment = round(previous_principal_remaining*mortgage_rate_periodic, 2) principal_payment = round(periodic_mortgage_payment-interest_payment, 2) # Catch the case where all principal is paid off in the final period if previous_principal_remaining - principal_payment &lt; 0: principal_payment = previous_principal_remaining # Collect the principal remaining values in an array principal_remaining[i] = previous_principal_remaining - principal_payment # Print the payments for the first few periods print_payments(i, interest_payment, principal_payment, principal_remaining) . Period 0: Interest Paid: 1966.43 | Principal Paid: 974.7 | Remaining Balance: 639025.3 Period 1: Interest Paid: 1963.43 | Principal Paid: 977.7 | Remaining Balance: 638047.6000000001 Period 2: Interest Paid: 1960.43 | Principal Paid: 980.7 | Remaining Balance: 637066.9000000001 Period 3: Interest Paid: 1957.41 | Principal Paid: 983.72 | Remaining Balance: 636083.1800000002 Period 4: Interest Paid: 1954.39 | Principal Paid: 986.74 | Remaining Balance: 635096.4400000002 Period 5: Interest Paid: 1951.36 | Principal Paid: 989.77 | Remaining Balance: 634106.6700000002 . #### Simulating Periodic Payments (II) . You have decided to extend your program from the previous exercise to store the principal and interest payments made at each period, and to plot the results instead of simply printing them. . For this example, the plotting code is already done, so you just need to finish the logic inside the for loop and the initialization of the variables which will be updated at each iteration. . # Loop through each mortgage payment period for i in range(0, mortgage_payment_periods): # Handle the case for the first iteration if i == 0: previous_principal_remaining = mortgage_loan else: previous_principal_remaining = principal_remaining[i-1] # Calculate the interest based on the previous principal interest_payment = round(previous_principal_remaining*mortgage_rate_periodic, 2) principal_payment = round(periodic_mortgage_payment - interest_payment, 2) # Catch the case where all principal is paid off in the final period if previous_principal_remaining - principal_payment &lt; 0: principal_payment = previous_principal_remaining # Collect the historical values interest_paid[i] = interest_payment principal_paid[i] = principal_payment principal_remaining[i] = previous_principal_remaining - principal_payment # Plot the interest vs principal plt.plot(interest_paid, color=&quot;red&quot;) plt.plot(principal_paid, color=&quot;blue&quot;) plt.legend(handles=[interest_plot, principal_plot], loc=2) plt.show() . . . 3.3 Home Ownership, Home Prices and Recessions . . #### Cumulative Payments and Home Equity . You are faithfully paying your mortgage each month, but it’s difficult to tell how much of the house you actually own and how much interest you have paid in total over the years. . principal_paid , interest_paid , home_value and down_payment_percent from the previous exercise are available. . principal_paid array([ 974.7 , 977.7 , 980.7 , 983.72, 986.74, 989.77, 992.81, ... 2914.19, 2923.15, 2929.2 ]) interest_paid array([1966.43, 1963.43, 1960.43, 1957.41, 1954.39, 1951.36, 1948.32, ... 26.94, 17.98, 9. ]) home_value 800000 down_payment_percent 0.2 . import numpy as np # Calculate the cumulative home equity (principal) over time cumulative_home_equity = np.cumsum(principal_paid) # Calculate the cumulative interest paid over time cumulative_interest_paid = np.cumsum(interest_paid) # Calculate your percentage home equity over time cumulative_percent_owned = down_payment_percent + (cumulative_home_equity/home_value) # Plot the cumulative interest paid vs equity accumulated plt.plot(cumulative_interest_paid, color=&#39;red&#39;) plt.plot(cumulative_home_equity, color=&#39;blue&#39;) plt.legend(handles=[interest_plot, principal_plot], loc=2) plt.show() . print(cumulative_percent_owned) [0.20121838 0.2024405 0.20366638 0.20489603 0.20612945 0.20736666 ... 0.98178978 0.98541024 0.98904183 0.99268456 0.9963385 1. ] . . #### Rising Housing Prices . Home values have been rising steadily each year, and this is a rather large investment for you. . Calculate your home equity value over time given a steady growth rate of 0.25% per month. A repeated array of this growth rate (with a length equal to the number of mortgage payment periods) is already stored for you in an object called growth_array . . The home_value and cumulative_percent_owned variables from the previous exercise are available. . growth_array array([0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, ... 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025]) . import numpy as np # Calculate the cumulative growth over time cumulative_growth_forecast = np.cumprod(1 + growth_array) # Forecast the home value over time home_value_forecast = home_value * cumulative_growth_forecast # Forecast the home equity value owned over time cumulative_home_value_owned = home_value_forecast * cumulative_percent_owned # Plot the home value vs equity accumulated plt.plot(home_value_forecast, color=&#39;red&#39;) plt.plot(cumulative_home_value_owned, color=&#39;blue&#39;) plt.legend(handles=[homevalue_plot, homeequity_plot], loc=2) plt.show() . . Turned out to be a good investment. Now unfortunately, housing prices don’t always go up… . #### Falling Housing Prices and Underwater Mortgages . Unfortunately, you are also well aware that home prices don’t always rise. . An underwater mortgage is when the remaining amount you owe on your mortgage is actually higher than the value of the house itself. . In this exercise, you will calculate the worst case scenario where home prices drop steadily at the rate of 0.45% per month. To speed things up, the cumulative drop in home prices has already been forecasted and stored for you in a variable called cumulative_decline_forecast , which is an array of multiplicative discount factors compared to today’s price – no need to add 1 to the rate array. . principal_remaining array([639025.3 , 638047.6 , 637066.9 , 636083.18, 635096.44, 634106.67, ... 14568.18, 11671.81, 8766.54, 5852.35, 2929.2 , 0. ]) . import numpy as np import pandas as pd # Cumulative drop in home value over time as a ratio cumulative_decline_forecast = np.cumprod(1+decline_array) # Forecast the home value over time home_value_forecast = home_value * cumulative_decline_forecast # Find all periods where your mortgage is underwater underwater = home_value_forecast &lt; principal_remaining pd.value_counts(underwater) # Plot the home value vs principal remaining plt.plot(home_value_forecast, color=&#39;red&#39;) plt.plot(principal_remaining, color=&#39;blue&#39;) plt.legend(handles=[homevalue_plot, principal_plot], loc=2) plt.show() . . When the blue line is above the red line, you are ‘underwater’. Putting more money down and taking a smaller mortgage in the first place will help you avoid this situation. . 4. Budgeting Application . . 4.1 Budgeting Project Proposal . . #### Salary and Taxes . You just got a new job as an entry-level Data Scientist at a technology company in New York City with a decent starting salary of $85,000 per year. . Unfortunately, after state and local taxes, you can expect to be sending roughly 30% back to the government each year. . You will need to calculate your monthly take home pay after taxes in order to begin budgeting. . # Enter your annual salary salary = 85000 # Assume a tax rate of 30% tax_rate = 0.3 # Calculate your salary after taxes salary_after_taxes = salary * (1 - tax_rate) print(&quot;Salary after taxes: &quot; + str(round(salary_after_taxes, 2))) # Calculate your monthly salary after taxes monthly_takehome_salary = salary_after_taxes / 12 print(&quot;Monthly takehome salary: &quot; + str(round(monthly_takehome_salary, 2))) # Salary after taxes: 59500.0 # Monthly takehome salary: 4958.33 . #### Monthly Expenses and Savings . In order to make it in New York City, you have decided to split a two-bedroom apartment with a friend. You will have to budget for rent, food and entertainment, but it’s also a good idea to allocate an amount for unforeseen expenses each month. This unforeseen expenses budget could be used for anything ranging from new clothes or electronics to doctor appointments. . Set up your monthly budget as follows: . Rent: $1200 / month (Includes utilities) | Food: $30 / day (On average. Includes groceries and eating out.) | Entertainment: $200 / month (Movies, drinks, museums, parties…) | Unforeseen Expenses: 250 / month (Stay safe, and don’t drop your phone!) | . For this application, assume an average of 30 days per month. Whatever is left after your paying your monthly expenses will go into your savings account each month. . # Enter your monthly rent monthly_rent = 1200 # Enter your daily food budget daily_food_budget = 30 # Calculate your monthly food budget assuming 30 days per month monthly_food_budget = 30 * 30 # Set your monthly entertainment budget monthly_entertainment_budget = 200 # Allocate funds for unforeseen expenses, just in case monthly_unforeseen_expenses = 250 # Next, calculate your total monthly expenses monthly_expenses = monthly_rent + monthly_food_budget + monthly_entertainment_budget + monthly_unforeseen_expenses print(&quot;Monthly expenses: &quot; + str(round(monthly_expenses, 2))) # Finally, calculate your monthly take-home savings monthly_savings = monthly_takehome_salary - monthly_expenses print(&quot;Monthly savings: &quot; + str(round(monthly_savings, 2))) # Monthly expenses: 2550 # Monthly savings: 2408.33 . Expenses add up quickly, don’t they? . #### Forecast Salary Growth and Cost of Living . Due to both inflation and increased productivity from experience, you can expect your salary to grow at different rates depending on your job. Now, since you are working in a growing and in-demand career field as a Data Scientist, you can assume a steady growth in your annual salary every year based on performance. . You can assume an annual salary growth rate of 5%, which means if you start at $85,000 per year, you can expect to earn over $176,000 per year after 15 years. After taxes, assuming your tax rate hasn’t changed, that works out to roughly $125,000 per year, which is not unreasonable for a Data Scientist. In fact, you might even make it to that level in a few years! But just to be safe, you should be conservative with your projections. . For this application, assume all inflation and salary growth happens in smaller increments on a monthly basis instead of just one large increase at the end of each year. . import numpy as np # Create monthly forecasts up to 15 years from now forecast_months = 12*15 # Set your annual salary growth rate annual_salary_growth = 0.05 # Calculate your equivalent monthly salary growth rate monthly_salary_growth = (1 + annual_salary_growth) ** (1/12) - 1 # Forecast the cumulative growth of your salary cumulative_salary_growth_forecast = np.cumprod(np.repeat(1 + monthly_salary_growth, forecast_months)) # Calculate the actual salary forecast salary_forecast = monthly_takehome_salary * cumulative_salary_growth_forecast # Plot the forecasted salary plt.plot(salary_forecast, color=&#39;blue&#39;) plt.show() . . That’s becomes a mighty fine salary very quickly. . #### Forecast Growing Expenses Due to Inflation . You will also assume your monthly expenses will rise by an average of 2.5% per year due to inflation . This will lead to higher cost of living over time, paying for the same expenses each year but at a higher price. Luckily, your salary is growing faster than inflation, which means you should have more money going into savings each month. . import numpy as np # Set the annual inflation rate annual_inflation = 0.025 # Calculate the equivalent monthly inflation rate monthly_inflation = (1+annual_inflation)**(1/12) - 1 # Forecast cumulative inflation over time cumulative_inflation_forecast = np.cumprod(np.repeat(1 + monthly_inflation, forecast_months)) # Calculate your forecasted expenses expenses_forecast = monthly_expenses*cumulative_inflation_forecast # Plot the forecasted expenses plt.plot(expenses_forecast, color=&#39;red&#39;) plt.show() . . Even though you’re making more money, you’re spending more, too! . . 4.2 Net Worth, Saving, and Investing . #### Calculate Your Net Worth . Now that you have forecasted your savings and salary over time while taking career progression and inflation into account, you have constructed a time-series which you can use to calculate your cash flows, just like in Chapter 1. . For this example, all you need to do is subtract your forecasted monthly expenses from your forecasted monthly salary. The remaining cash flow will go straight into your savings account for each month. . You want to project your cumulative savings over time to see how effective your budgeting process will be given your projections. . salary_forecast and expenses_forecast from the previous exercises are available. . import numpy as np # Calculate your savings for each month savings_forecast = salary_forecast - expenses_forecast # Calculate your cumulative savings over time cumulative_savings = np.cumsum(savings_forecast) # Print the final cumulative savings after 15 years final_net_worth = cumulative_savings[-1] print(&quot;Your final net worth: &quot; + str(round(final_net_worth, 2))) # Plot the forecasted savings plt.plot(cumulative_savings, color=&#39;blue&#39;) plt.show() . . Not bad! But there’s a better way to accumulate wealth over time. . #### So You Want to Be a Millionaire? . Your projections show that you can accumulate over $700,000 in just 15 years by following a strict budget and growing your salary steadily over time. . But you want to be a millionaire in 15 years, retire young, sip margaritas and travel for the rest of your life. In order to do that, you’re going to need to invest. . Remember the .pmt() function from numpy ? You can use this function to calculate how much you need to save each month in order to accumulate your desired wealth over time. . You still have a lot to learn about the stock market, but your financial advisor told you that you can earn anywhere from 5-10% per year on your capital on average by investing in a low cost index fund. . You know that the stock market doesn’t always go up, but you will assume a modest 7% return per year, which has been the average annual return in the US stock market from 1950-2009. . import numpy as np # Set the annual investment return to 7% investment_rate_annual = 0.07 # Calculate the monthly investment return investment_rate_monthly = (1 + investment_rate_annual) ** (1/12) - 1 # Calculate your required monthly investment to amass $1M required_investment_monthly = np.pmt(rate=investment_rate_monthly, nper=forecast_months, pv=0, fv=-1000000) print(&quot;You will have to invest $&quot; + str(round(required_investment_monthly, 2)) + &quot; per month to amass $1M over 15 years&quot;) # You will have to invest $3214.35 per month to amass $1M over 15 years . $3000 per month?! Let’s start slow, and build it up over time. . #### Investing a Percentage of Your Income (I) . Unfortunately, you really can’t afford to save $3,000 per month in order to amass $1,000,000 after only 15 years. . But what you can do is start slowly, investing a small percentage of your take-home income each month, which should grow over time as your income grows as well. . In this exercise, you will lay the foundations to simulate this investing process over time. . The salary_forecast and expenses_forecast variables are available from the previous exercise. . The cash_flow_forecast is also available, and is an array of your forecasted salary minus your forecasted expenses. The monthly_investment_percentage variable is already set to 0.30. . import numpy as np # Calculate your monthly deposit into your investment account investment_deposit_forecast = cash_flow_forecast * monthly_investment_percentage # The rest goes into your savings account savings_forecast_new = cash_flow_forecast * (1 - monthly_investment_percentage) # Calculate your cumulative savings over time cumulative_savings_new = np.cumsum(savings_forecast_new) # Plot your forecasted monthly savings vs investments plt.plot(investment_deposit_forecast, color=&#39;red&#39;) plt.plot(savings_forecast_new, color=&#39;blue&#39;) plt.legend(handles=[investments_plot, savings_plot], loc=2) plt.show() . #### Investing a Percentage of Your Income (II) . To finish up your investment simulation, you will need to loop through each time period, calculate the growth of any investments you have already made, add your new monthly deposit, and calculate your net worth at each point in time. . Cumulative savings ( cumulative_savings_new ) from the previous exercise is available, and investment_portfolio and net_worth are pre-allocated empty numpy arrays of length equal to forecast_months . . import numpy as np # Loop through each forecast period for i in range(forecast_months): # Find the previous investment deposit amount if i == 0: previous_investment = 0 else: previous_investment = investment_portfolio[i-1] # Calculate the value of your previous investments, which have grown previous_investment_growth = previous_investment*(1 + investment_rate_monthly) # Add your new deposit to your investment portfolio investment_portfolio[i] = previous_investment_growth + investment_deposit_forecast[i] # Calculate your net worth at each point in time net_worth[i] = np.cumsum(cumulative_savings_new[i] + investment_portfolio[i]) # Plot your forecasted cumulative savings vs investments and net worth plot_investments(investment_portfolio, cumulative_savings_new, net_worth) . . . 4.3 The Power of Time and Compound Interest . . #### Investing Over Time . If you would like to accumulate $1,000,000 over 15 years, at 7% per year, you will have to invest $3214.35 per month: . np.pmt(rate=((1+0.07)**(1/12) - 1), nper=15*12, pv=0, fv=1000000) # -3214.351338524575 . But what if you were willing to wait an extra 15 years, for a total of 30 years? How much will you need to invest each month? . np.pmt(rate=((1+0.07)**(1/12) - 1), nper=30*12, pv=0, fv=1000000) # -855.1009225937204 . Compounded returns mean you only need to save $855.10 per month. . #### Inflation-Adjusted Net Worth . By saving 30% per year, your simulation shows that you can accumulate $896,962.66. Not quite a millionaire, but not bad at all! . For the sake of simplicity, let’s assume you were able to save $900,000 by following your budget. . But what if you retire 15 years from now? What is $900,000 going to be truly worth 15 years from now? . import numpy as np # Set your future net worth future_net_worth = 900000 # Set the annual inflation rate to 2.5% annual_inflation = 0.025 # Calculate the present value of your terminal wealth over 15 years inflation_adjusted_net_worth = np.pv(rate=annual_inflation, nper=15, pmt=0, fv=-1*future_net_worth) print(&quot;Your inflation-adjusted net worth: $&quot; + str(round(inflation_adjusted_net_worth, 2))) # Your inflation-adjusted net worth: $621419.0 . The End. . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/intro-to-financial-concepts-using-python.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/intro-to-financial-concepts-using-python.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post27": {
            "title": "Feature Engineering for NLP in Python",
            "content": "Feature Engineering for NLP in Python . This is the memo of the 13th course (23 courses in all) of ‘Machine Learning Scientist with Python’ skill track. . You can find the original course HERE . . ### Course Description . In this course, you will learn techniques that will allow you to extract useful information from text and process them into a format suitable for applying ML models. More specifically, you will learn about POS tagging, named entity recognition, readability scores, the n-gram and tf-idf models, and how to implement them using scikit-learn and spaCy. You will also learn to compute how similar two documents are to each other. In the process, you will predict the sentiment of movie reviews and build movie and Ted Talk recommenders. Following the course, you will be able to engineer critical features out of any text and solve some of the most challenging problems in data science! . ### . Basic features and readability scores | Text preprocessing, POS tagging and NER | N-Gram models | TF-IDF and similarity scores | . 1. Basic features and readability scores . . 1.1 Introduction to NLP feature engineering . . 1.1.1 Data format for ML algorithms . In this exercise, you have been given four dataframes df1 , df2 , df3 and df4 . The final column of each dataframe is the predictor variable and the rest of the columns are training features. . Using the console, determine which dataframe is in a suitable format to be trained by a classifier. . df3 feature 1 feature 2 feature 3 feature 4 feature 5 label 0 1 85 66 29 0 0 1 8 183 64 0 0 1 2 1 89 66 23 94 0 3 0 137 40 35 168 1 4 5 116 74 0 0 0 ... . 1.1.2 One-hot encoding . In the previous exercise, we encountered a dataframe df1 which contained categorical features and therefore, was unsuitable for applying ML algorithms to. . In this exercise, your task is to convert df1 into a format that is suitable for machine learning. . # Print the features of df1 print(df1.columns) # Perform one-hot encoding df1 = pd.get_dummies(df1, columns=[&#39;feature 5&#39;]) # Print the new features of df1 print(df1.columns) # Print first five rows of df1 print(df1.head()) . Index([&#39;feature 1&#39;, &#39;feature 2&#39;, &#39;feature 3&#39;, &#39;feature 4&#39;, &#39;feature 5&#39;, &#39;label&#39;], dtype=&#39;object&#39;) Index([&#39;feature 1&#39;, &#39;feature 2&#39;, &#39;feature 3&#39;, &#39;feature 4&#39;, &#39;label&#39;, &#39;feature 5_female&#39;, &#39;feature 5_male&#39;], dtype=&#39;object&#39;) feature 1 feature 2 feature 3 feature 4 label feature 5_female feature 5_male 0 29.0000 0 0 211.3375 1 1 0 1 0.9167 1 2 151.5500 1 0 1 2 2.0000 1 2 151.5500 0 1 0 3 30.0000 1 2 151.5500 0 0 1 4 25.0000 1 2 151.5500 0 1 0 . . 1.2 Basic feature extraction . . 1.2.1 Character count of Russian tweets . In this exercise, you have been given a dataframe tweets which contains some tweets associated with Russia’s Internet Research Agency and compiled by FiveThirtyEight. . Your task is to create a new feature ‘char_count’ in tweets which computes the number of characters for each tweet. Also, compute the average length of each tweet. The tweets are available in the content feature of tweets . . Be aware that this is real data from Twitter and as such there is always a risk that it may contain profanity or other offensive content (in this exercise, and any following exercises that also use real Twitter data). . tweets content 0 LIVE STREAM VIDEO=&gt; Donald Trump Rallies in Co... 1 Muslim Attacks NYPD Cops with Meat Cleaver. Me... 2 .@vfpatlas well that&#39;s a swella word there (di... ... . # Create a feature char_count tweets[&#39;char_count&#39;] = tweets[&#39;content&#39;].apply(len) # Print the average character count print(tweets[&#39;char_count&#39;].mean()) # 103.462 . Great job! Notice that the average character count of these tweets is approximately 104, which is much higher than the overall average tweet length of around 40 characters. Depending on what you’re working on, this may be something worth investigating into. For your information, there is research that indicates that fake news articles tend to have longer titles! Therefore, even extremely basic features such as character counts can prove to be very useful in certain applications. . 1.2.2 Word count of TED talks . ted is a dataframe that contains the transcripts of 500 TED talks. Your job is to compute a new feature word_count which contains the approximate number of words for each talk. Consequently, you also need to compute the average word count of the talks. The transcripts are available as the transcript feature in ted . . In order to complete this task, you will need to define a function count_words that takes in a string as an argument and returns the number of words in the string. You will then need to apply this function to the transcript feature of ted to create the new feature word_count and compute its mean. . # Function that returns number of words in a string def count_words(string): # Split the string into words words = string.split() # Return the number of words return len(words) # Create a new feature word_count ted[&#39;word_count&#39;] = ted[&#39;transcript&#39;].apply(count_words) # Print the average word count of the talks print(ted[&#39;word_count&#39;].mean()) # 1987.1 . Amazing work! You now know how to compute the number of words in a given piece of text. Also, notice that the average length of a talk is close to 2000 words. You can use the word_count feature to compute its correlation with other variables such as number of views, number of comments, etc. and derive extremely interesting insights about TED. . 1.2.3 Hashtags and mentions in Russian tweets . Let’s revisit the tweets dataframe containing the Russian tweets. In this exercise, you will compute the number of hashtags and mentions in each tweet by defining two functions count_hashtags() and count_mentions() respectively and applying them to the content feature of tweets . . In case you don’t recall, the tweets are contained in the content feature of tweets . . # Function that returns numner of hashtags in a string def count_hashtags(string): # Split the string into words words = string.split() # Create a list of words that are hashtags hashtags = [word for word in words if word.startswith(&#39;#&#39;)] # Return number of hashtags return(len(hashtags)) # Create a feature hashtag_count and display distribution tweets[&#39;hashtag_count&#39;] = tweets[&#39;content&#39;].apply(count_hashtags) tweets[&#39;hashtag_count&#39;].hist() plt.title(&#39;Hashtag count distribution&#39;) plt.show() . . # Function that returns number of mentions in a string def count_mentions(string): # Split the string into words words = string.split() # Create a list of words that are mentions mentions = [word for word in words if word.startswith(&#39;@&#39;)] # Return number of mentions return(len(mentions)) # Create a feature mention_count and display distribution tweets[&#39;mention_count&#39;] = tweets[&#39;content&#39;].apply(count_mentions) tweets[&#39;mention_count&#39;].hist() plt.title(&#39;Mention count distribution&#39;) plt.show() . . Excellent work! You now have a good grasp of how to compute various types of summary features. In the next lesson, we will learn about more advanced features that are capable of capturing more nuanced information beyond simple word and character counts. . . 1.3 Readability tests . . 1.3.1 Readability of ‘The Myth of Sisyphus’ . In this exercise, you will compute the Flesch reading ease score for Albert Camus’ famous essay The Myth of Sisyphus . We will then interpret the value of this score as explained in the video and try to determine the reading level of the essay. . The entire essay is in the form of a string and is available as sisyphus_essay . . # Import Textatistic from textatistic import Textatistic # Compute the readability scores readability_scores = Textatistic(sisyphus_essay).scores # Print the flesch reading ease score flesch = readability_scores[&#39;flesch_score&#39;] print(&quot;The Flesch Reading Ease is %.2f&quot; % (flesch)) # The Flesch Reading Ease is 81.67 . Excellent! You now know to compute the Flesch reading ease score for a given body of text. Notice that the score for this essay is approximately 81.67. This indicates that the essay is at the readability level of a 6th grade American student. . 1.3.2 Readability of various publications . In this exercise, you have been given excerpts of articles from four publications. Your task is to compute the readability of these excerpts using the Gunning fog index and consequently, determine the relative difficulty of reading these publications. . The excerpts are available as the following strings: . forbes – An excerpt from an article from Forbes magazine on the Chinese social credit score system. | harvard_law – An excerpt from a book review published in Harvard Law Review . | r_digest – An excerpt from a Reader’s Digest article on flight turbulence. | time_kids – An excerpt from an article on the ill effects of salt consumption published in TIME for Kids . | . # Import Textatistic from textatistic import Textatistic # List of excerpts excerpts = [forbes, harvard_law, r_digest, time_kids] # Loop through excerpts and compute gunning fog index gunning_fog_scores = [] for excerpt in excerpts: readability_scores = Textatistic(excerpt).scores gunning_fog = readability_scores[&#39;gunningfog_score&#39;] gunning_fog_scores.append(gunning_fog) # Print the gunning fog indices print(gunning_fog_scores) # [14.436002482929858, 20.735401069518716, 11.085587583148559, 5.926785009861934] . Great job! You are now adept at computing readability scores for various pieces of text. Notice that the Harvard Law Review excerpt has the highest Gunning fog index; indicating that it can be comprehended only by readers who have graduated college. On the other hand, the Time for Kids article, intended for children, has a much lower fog index and can be comprehended by 5th grade students. . 2. Text preprocessing, POS tagging and NER . . 2.1 Tokenization and Lemmatization . . 2.1.1 Tokenizing the Gettysburg Address . In this exercise, you will be tokenizing one of the most famous speeches of all time: the Gettysburg Address delivered by American President Abraham Lincoln during the American Civil War. . The entire speech is available as a string named gettysburg . . gettysburg &quot;Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. ... . import spacy # Load the en_core_web_sm model nlp = spacy.load(&#39;en_core_web_sm&#39;) # Create a Doc object doc = nlp(gettysburg) # Generate the tokens tokens = [token.text for token in doc] print(tokens) . [&#39;Four&#39;, &#39;score&#39;, &#39;and&#39;, &#39;seven&#39;, &#39;years&#39;, &#39;ago&#39;, &#39;our&#39;, &#39;fathers&#39;, &#39;brought&#39;, &#39;forth&#39;, &#39;on&#39;, &#39;this&#39;, &#39;continent&#39;, &#39;,&#39;, &#39;a&#39;, &#39;new&#39;, &#39;nation&#39;, ... . Excellent work! You now know how to tokenize a piece of text. In the next exercise, we will perform similar steps and conduct lemmatization. . 2.1.2 Lemmatizing the Gettysburg address . In this exercise, we will perform lemmatization on the same gettysburg address from before. . However, this time, we will also take a look at the speech, before and after lemmatization, and try to adjudge the kind of changes that take place to make the piece more machine friendly. . import spacy # Load the en_core_web_sm model nlp = spacy.load(&#39;en_core_web_sm&#39;) # Create a Doc object doc = nlp(gettysburg) # Generate lemmas lemmas = [token.lemma_ for token in doc] # Convert lemmas into a string print(&#39; &#39;.join(lemmas)) . four score and seven year ago -PRON- father bring forth on this continent , a new nation , conceive in liberty , and dedicate to the proposition that all man be create equal . . Excellent! You’re now proficient at performing lemmatization using spaCy. Observe the lemmatized version of the speech. It isn’t very readable to humans but it is in a much more convenient format for a machine to process. . . 2.2 Text cleaning . . 2.2.1 Cleaning a blog post . In this exercise, you have been given an excerpt from a blog post. Your task is to clean this text into a more machine friendly format. This will involve converting to lowercase, lemmatization and removing stopwords, punctuations and non-alphabetic characters. . The excerpt is available as a string blog and has been printed to the console. The list of stopwords are available as stopwords . . Twenty-first-century politics has witnessed an alarming rise of populism in the U.S. and Europe. The first warning signs came with the UK Brexit Referendum vote in 2016 swinging in the way of Leave. This was followed by a stupendous victory by billionaire Donald Trump to become the 45th President of the United States in November 2016. Since then, Europe has seen a steady rise in populist and far-right parties that have capitalized on Europe’s Immigration Crisis to raise nationalist and anti-Europe sentiments. Some instances include Alternative for Germany (AfD) winning 12.6% of all seats and entering the Bundestag, thus upsetting Germany’s political order for the first time since the Second World War, the success of the Five Star Movement in Italy and the surge in popularity of neo-nazism and neo-fascism in countries such as Hungary, Czech Republic, Poland and Austria. . # Load model and create Doc object nlp = spacy.load(&#39;en_core_web_sm&#39;) doc = nlp(blog) # Generate lemmatized tokens lemmas = [token.lemma_ for token in doc] # Remove stopwords and non-alphabetic tokens a_lemmas = [lemma for lemma in lemmas if lemma.isalpha() and lemma not in stopwords] # Print string after text cleaning print(&#39; &#39;.join(a_lemmas)) . century politic witness alarming rise populism europe warning sign come uk brexit referendum vote swinging way leave follow stupendous victory billionaire donald trump president united states november europe steady rise populist far right party capitalize europe immigration crisis raise nationalist anti europe sentiment instance include alternative germany afd win seat enter bundestag upset germany political order time second world war success star movement italy surge popularity neo nazism neo fascism country hungary czech republic poland austria . Great job! Take a look at the cleaned text; it is lowercased and devoid of numbers, punctuations and commonly used stopwords. Also, note that the word U.S. was present in the original text. Since it had periods in between, our text cleaning process completely removed it. This may not be ideal behavior. It is always advisable to use your custom functions in place of isalpha() for more nuanced cases. . 2.2.2 Cleaning TED talks in a dataframe . In this exercise, we will revisit the TED Talks from the first chapter. You have been a given a dataframe ted consisting of 5 TED Talks. Your task is to clean these talks using techniques discussed earlier by writing a function preprocess and applying it to the transcript feature of the dataframe. . The stopwords list is available as stopwords . . # Function to preprocess text def preprocess(text): # Create Doc object doc = nlp(text, disable=[&#39;ner&#39;, &#39;parser&#39;]) # Generate lemmas lemmas = [token.lemma_ for token in doc] # Remove stopwords and non-alphabetic characters a_lemmas = [lemma for lemma in lemmas if lemma.isalpha() and lemma not in stopwords] return &#39; &#39;.join(a_lemmas) # Apply preprocess to ted[&#39;transcript&#39;] ted[&#39;transcript&#39;] = ted[&#39;transcript&#39;].apply(preprocess) print(ted[&#39;transcript&#39;]) . 0 talk new lecture ted illusion create ted try r... 1 representation brain brain break left half log... 2 great honor today share digital universe creat... ... . Excellent job! You have preprocessed all the TED talk transcripts contained in ted and it is now in a good shape to perform operations such as vectorization (as we will soon see how). You now have a good understanding of how text preprocessing works and why it is important. In the next lessons, we will move on to generating word level features for our texts. . . 2.3 Part-of-speech(POS) tagging . . 2.3.1 POS tagging in Lord of the Flies . In this exercise, you will perform part-of-speech tagging on a famous passage from one of the most well-known novels of all time, Lord of the Flies , authored by William Golding. . The passage is available as lotf and has already been printed to the console. . He found himself understanding the wearisomeness of this life, where every path was an improvisation and a considerable part of one’s waking life was spent watching one’s feet. . # Load the en_core_web_sm model nlp = spacy.load(&#39;en_core_web_sm&#39;) # Create a Doc object doc = nlp(lotf) # Generate tokens and pos tags pos = [(token.text, token.pos_) for token in doc] print(pos) . [(&#39;He&#39;, &#39;PRON&#39;), (&#39;found&#39;, &#39;VERB&#39;), (&#39;himself&#39;, &#39;PRON&#39;), (&#39;understanding&#39;, &#39;VERB&#39;), (&#39;the&#39;, &#39;DET&#39;), (&#39;wearisomeness&#39;, &#39;NOUN&#39;), (&#39;of&#39;, &#39;ADP&#39;), (&#39;this&#39;, &#39;DET&#39;), (&#39;life&#39;, &#39;NOUN&#39;), (&#39;,&#39;, &#39;PUNCT&#39;), (&#39;where&#39;, &#39;ADV&#39;), (&#39;every&#39;, &#39;DET&#39;), (&#39;path&#39;, &#39;NOUN&#39;), (&#39;was&#39;, &#39;VERB&#39;), (&#39;an&#39;, &#39;DET&#39;), (&#39;improvisation&#39;, &#39;NOUN&#39;), (&#39;and&#39;, &#39;CCONJ&#39;), (&#39;a&#39;, &#39;DET&#39;), (&#39;considerable&#39;, &#39;ADJ&#39;), (&#39;part&#39;, &#39;NOUN&#39;), (&#39;of&#39;, &#39;ADP&#39;), (&#39;one&#39;, &#39;NUM&#39;), (&#39;’s&#39;, &#39;PART&#39;), (&#39;waking&#39;, &#39;NOUN&#39;), (&#39;life&#39;, &#39;NOUN&#39;), (&#39;was&#39;, &#39;VERB&#39;), (&#39;spent&#39;, &#39;VERB&#39;), (&#39;watching&#39;, &#39;VERB&#39;), (&#39;one&#39;, &#39;PRON&#39;), (&#39;’s&#39;, &#39;PART&#39;), (&#39;feet&#39;, &#39;NOUN&#39;), (&#39;.&#39;, &#39;PUNCT&#39;)] . Good job! Examine the various POS tags attached to each token and evaluate if they make intuitive sense to you. You will notice that they are indeed labelled correctly according to the standard rules of English grammar. . 2.3.2 Counting nouns in a piece of text . In this exercise, we will write two functions, nouns() and proper_nouns() that will count the number of other nouns and proper nouns in a piece of text respectively. . These functions will take in a piece of text and generate a list containing the POS tags for each word. It will then return the number of proper nouns/other nouns that the text contains. We will use these functions in the next exercise to generate interesting insights about fake news. . The en_core_web_sm model has already been loaded as nlp in this exercise. . nlp = spacy.load(&#39;en_core_web_sm&#39;) # Returns number of proper nouns def proper_nouns(text, model=nlp): # Create doc object doc = model(text) # Generate list of POS tags pos = [token.pos_ for token in doc] # Return number of proper nouns return pos.count(&#39;PROPN&#39;) print(proper_nouns(&quot;Abdul, Bill and Cathy went to the market to buy apples.&quot;, nlp)) # 3 . nlp = spacy.load(&#39;en_core_web_sm&#39;) # Returns number of other nouns def nouns(text, model=nlp): # Create doc object doc = model(text) # Generate list of POS tags pos = [token.pos_ for token in doc] # Return number of other nouns return pos.count(&#39;NOUN&#39;) print(nouns(&quot;Abdul, Bill and Cathy went to the market to buy apples.&quot;, nlp)) # 2 . Great job! You now know how to write functions that compute the number of instances of a particulat POS tag in a given piece of text. In the next exercise, we will use these functions to generate features from text in a dataframe. . 2.3.3 Noun usage in fake news . In this exercise, you have been given a dataframe headlines that contains news headlines that are either fake or real. Your task is to generate two new features num_propn and num_noun that represent the number of proper nouns and other nouns contained in the title feature of headlines . . Next, we will compute the mean number of proper nouns and other nouns used in fake and real news headlines and compare the values. If there is a remarkable difference, then there is a good chance that using the num_propn and num_noun features in fake news detectors will improve its performance. . To accomplish this task, the functions proper_nouns and nouns that you had built in the previous exercise have already been made available to you. . headlines Unnamed: 0 title label 0 0 You Can Smell Hillary’s Fear FAKE 1 1 Watch The Exact Moment Paul Ryan Committed Pol... FAKE 2 2 Kerry to go to Paris in gesture of sympathy REAL 3 3 Bernie supporters on Twitter erupt in anger ag... FAKE 4 4 The Battle of New York: Why This Primary Matters REAL . headlines[&#39;num_propn&#39;] = headlines[&#39;title&#39;].apply(proper_nouns) headlines[&#39;num_noun&#39;] = headlines[&#39;title&#39;].apply(nouns) # Compute mean of proper nouns real_propn = headlines[headlines[&#39;label&#39;] == &#39;REAL&#39;][&#39;num_propn&#39;].mean() fake_propn = headlines[headlines[&#39;label&#39;] == &#39;FAKE&#39;][&#39;num_propn&#39;].mean() # Compute mean of other nouns real_noun = headlines[headlines[&#39;label&#39;] == &#39;REAL&#39;][&#39;num_noun&#39;].mean() fake_noun = headlines[headlines[&#39;label&#39;] == &#39;FAKE&#39;][&#39;num_noun&#39;].mean() # Print results print(&quot;Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively&quot;%(real_propn, fake_propn)) print(&quot;Mean no. of other nouns in real and fake headlines are %.2f and %.2f respectively&quot;%(real_noun, fake_noun)) . Mean no. of proper nouns in real and fake headlines are 2.46 and 4.86 respectively Mean no. of other nouns in real and fake headlines are 2.30 and 1.44 respectively . Excellent work! You now know to construct features using POS tags information. Notice how the mean number of proper nouns is considerably higher for fake news than it is for real news. The opposite seems to be true in the case of other nouns. This fact can be put to great use in desgning fake news detectors. . . 2.4 Named entity recognition(NER) . . 2.4.1 Named entities in a sentence . In this exercise, we will identify and classify the labels of various named entities in a body of text using one of spaCy’s statistical models. We will also verify the veracity of these labels. . # Load the required model nlp = spacy.load(&#39;en_core_web_sm&#39;) # Create a Doc instance text = &#39;Sundar Pichai is the CEO of Google. Its headquarters is in Mountain View.&#39; doc = nlp(text) # Print all named entities and their labels for ent in doc.ents: print(ent.text, ent.label_) . Sundar Pichai ORG Google ORG Mountain View GPE . Good job! Notice how the model correctly predicted the labels of Google and Mountain View but mislabeled Sundar Pichai as an organization. As discussed in the video, the predictions of the model depend strongly on the data it is trained on. It is possible to train spaCy models on your custom data. You will learn to do this in more advanced NLP courses. . 2.4.2 Identifying people mentioned in a news article . In this exercise, you have been given an excerpt from a news article published in TechCrunch . Your task is to write a function find_people that identifies the names of people that have been mentioned in a particular piece of text. You will then use find_people to identify the people of interest in the article. . The article is available as the string tc and has been printed to the console. The required spacy model has also been already loaded as nlp . . def find_persons(text): # Create Doc object doc = nlp(text) # Identify the persons persons = [ent.text for ent in doc.ents if ent.label_ == &#39;PERSON&#39;] # Return persons return persons print(find_persons(tc)) # [&#39;Sheryl Sandberg&#39;, &#39;Mark Zuckerberg&#39;] . Excellent work! The article was related to Facebook and our function correctly identified both the people mentioned. You can now see how NER could be used in a variety of applications. Publishers may use a technique like this to classify news articles by the people mentioned in them. A question answering system could also use something like this to answer questions such as ‘Who are the people mentioned in this passage?’. With this, we come to an end of this chapter. In the next, we will learn how to conduct vectorization on documents. . 3. N-Gram models . . 3.1 Building a bag of words model . . 3.1.1 Word vectors with a given vocabulary . You have been given a corpus of documents and you have computed the vocabulary of the corpus to be the following: V : a, an, and, but, can, come, evening, forever, go, i, men, may, on, the, women . Which of the following corresponds to the bag of words vector for the document “men may come and men may go but i go on forever”? . (0, 0, 1, 1, 0, 1, 0, 1, 2, 1, 2, 2, 1, 0, 0) . Good job! That is, indeed, the correct answer. Each value in the vector corresponds to the frequency of the corresponding word in the vocabulary. . 3.1.2 BoW model for movie taglines . In this exercise, you have been provided with a corpus of more than 7000 movie tag lines. Your job is to generate the bag of words representation bow_matrix for these taglines. For this exercise, we will ignore the text preprocessing step and generate bow_matrix directly. . We will also investigate the shape of the resultant bow_matrix . The first five taglines in corpus have been printed to the console for you to examine. . corpus.shape (7033,) . # Import CountVectorizer from sklearn.feature_extraction.text import CountVectorizer # Create CountVectorizer object vectorizer = CountVectorizer() # Generate matrix of word vectors bow_matrix = vectorizer.fit_transform(corpus) # Print the shape of bow_matrix print(bow_matrix.shape) # (7033, 6614) . Excellent! You now know how to generate a bag of words representation for a given corpus of documents. Notice that the word vectors created have more than 6600 dimensions. However, most of these dimensions have a value of zero since most words do not occur in a particular tagline. . 3.1.3 Analyzing dimensionality and preprocessing . In this exercise, you have been provided with a lem_corpus which contains the pre-processed versions of the movie taglines from the previous exercise. In other words, the taglines have been lowercased and lemmatized, and stopwords have been removed. . Your job is to generate the bag of words representation bow_lem_matrix for these lemmatized taglines and compare its shape with that of bow_matrix obtained in the previous exercise. The first five lemmatized taglines in lem_corpus have been printed to the console for you to examine. . # Import CountVectorizer from sklearn.feature_extraction.text import CountVectorizer # Create CountVectorizer object vectorizer = CountVectorizer() # Generate matrix of word vectors bow_lem_matrix = vectorizer.fit_transform(lem_corpus) # Print the shape of bow_lem_matrix print(bow_lem_matrix.shape) # (6959, 5223) . Good job! Notice how the number of features have reduced significantly from around 6600 to around 5223 for pre-processed movie taglines. The reduced number of dimensions on account of text preprocessing usually leads to better performance when conducting machine learning and it is a good idea to consider it. However, as mentioned in a previous lesson, the final decision always depends on the nature of the application. . 3.1.4 Mapping feature indices with feature names . In the lesson video, we had seen that CountVectorizer doesn’t necessarily index the vocabulary in alphabetical order. In this exercise, we will learn to map each feature index to its corresponding feature name from the vocabulary. . We will use the same three sentences on lions from the video. The sentences are available in a list named corpus and has already been printed to the console. . [&#39;The lion is the king of the jungle&#39;, &#39;Lions have lifespans of a decade&#39;, &#39;The lion is an endangered species&#39;] . # Create CountVectorizer object vectorizer = CountVectorizer() # Generate matrix of word vectors bow_matrix = vectorizer.fit_transform(corpus) # Convert bow_matrix into a DataFrame bow_df = pd.DataFrame(bow_matrix.toarray()) # Map the column names to vocabulary bow_df.columns = vectorizer.get_feature_names() # Print bow_df print(bow_df) . an decade endangered have is ... lion lions of species the 0 0 0 0 0 1 ... 1 0 1 0 3 1 0 1 0 1 0 ... 0 1 1 0 0 2 1 0 1 0 1 ... 1 0 0 1 1 [3 rows x 13 columns] . Great job! Observe that the column names refer to the token whose frequency is being recorded. Therefore, since the first column name is an , the first feature represents the number of times the word ‘an’ occurs in a particular sentence. get_feature_names() essentially gives us a list which represents the mapping of the feature indices to the feature name in the vocabulary. . . 3.2 Building a BoW Naive Bayes classifier . . 3.2.1 BoW vectors for movie reviews . In this exercise, you have been given two pandas Series, X_train and X_test , which consist of movie reviews. They represent the training and the test review data respectively. Your task is to preprocess the reviews and generate BoW vectors for these two sets using CountVectorizer . . Once we have generated the BoW vector matrices X_train_bow and X_test_bow , we will be in a very good position to apply a machine learning model to it and conduct sentiment analysis. . # Import CountVectorizer from sklearn.feature_extraction.text import CountVectorizer # Create a CountVectorizer object vectorizer = CountVectorizer(lowercase=True, stop_words=&#39;english&#39;) # Fit and transform X_train X_train_bow = vectorizer.fit_transform(X_train) # Transform X_test X_test_bow = vectorizer.transform(X_test) # Print shape of X_train_bow and X_test_bow print(X_train_bow.shape) print(X_test_bow.shape) # (750, 8158) # (250, 8158) . Great job! You now have a good idea of preprocessing text and transforming them into their bag-of-words representation using CountVectorizer . In this exercise, you have set the lowercase argument to True . However, note that this is the default value of lowercase and passing it explicitly is not necessary. Also, note that both X_train_bow and X_test_bow have 8158 features. There were words present in X_test that were not in X_train . CountVectorizer chose to ignore them in order to ensure that the dimensions of both sets remain the same. . 3.2.2 Predicting the sentiment of a movie review . In the previous exercise, you generated the bag-of-words representations for the training and test movie review data. In this exercise, we will use this model to train a Naive Bayes classifier that can detect the sentiment of a movie review and compute its accuracy. Note that since this is a binary classification problem, the model is only capable of classifying a review as either positive (1) or negative (0). It is incapable of detecting neutral reviews. . In case you don’t recall, the training and test BoW vectors are available as X_train_bow and X_test_bow respectively. The corresponding labels are available as y_train and y_test respectively. Also, for you reference, the original movie review dataset is available as df . . # Create a MultinomialNB object clf = MultinomialNB() # Fit the classifier clf.fit(X_train_bow, y_train) # Measure the accuracy accuracy = clf.score(X_test_bow, y_test) print(&quot;The accuracy of the classifier on the test set is %.3f&quot; % accuracy) # Predict the sentiment of a negative review review = &quot;The movie was terrible. The music was underwhelming and the acting mediocre.&quot; prediction = clf.predict(vectorizer.transform([review]))[0] print(&quot;The sentiment predicted by the classifier is %i&quot; % (prediction)) . Excellent work! You have successfully performed basic sentiment analysis. Note that the accuracy of the classifier is 73.2%. Considering the fact that it was trained on only 750 reviews, this is reasonably good performance. The classifier also correctly predicts the sentiment of a mini negative review which we passed into it. . . 3.3 Building n-gram models . . 3.3.1 n-gram models for movie tag lines . In this exercise, we have been provided with a corpus of more than 9000 movie tag lines. Our job is to generate n-gram models up to n equal to 1, n equal to 2 and n equal to 3 for this data and discover the number of features for each model. . We will then compare the number of features generated for each model. . # Generate n-grams upto n=1 vectorizer_ng1 = CountVectorizer(ngram_range=(1,1)) ng1 = vectorizer_ng1.fit_transform(corpus) # Generate n-grams upto n=2 vectorizer_ng2 = CountVectorizer(ngram_range=(1,2)) ng2 = vectorizer_ng2.fit_transform(corpus) # Generate n-grams upto n=3 vectorizer_ng3 = CountVectorizer(ngram_range=(1, 3)) ng3 = vectorizer_ng3.fit_transform(corpus) # Print the number of features for each model print(&quot;ng1, ng2 and ng3 have %i, %i and %i features respectively&quot; % (ng1.shape[1], ng2.shape[1], ng3.shape[1])) # ng1, ng2 and ng3 have 6614, 37100 and 76881 features respectively . Good job! You now know how to generate n-gram models containing higher order n-grams. Notice that ng2 has over 37,000 features whereas ng3 has over 76,000 features. This is much greater than the 6,000 dimensions obtained for ng1 . As the n-gram range increases, so does the number of features, leading to increased computational costs and a problem known as the curse of dimensionality. . 3.3.2 Higher order n-grams for sentiment analysis . Similar to a previous exercise, we are going to build a classifier that can detect if the review of a particular movie is positive or negative. However, this time, we will use n-grams up to n=2 for the task. . The n-gram training reviews are available as X_train_ng . The corresponding test reviews are available as X_test_ng . Finally, use y_train and y_test to access the training and test sentiment classes respectively. . # Define an instance of MultinomialNB clf_ng = MultinomialNB() # Fit the classifier clf_ng.fit(X_train_ng, y_train) # Measure the accuracy accuracy = clf_ng.score(X_test_ng, y_test) print(&quot;The accuracy of the classifier on the test set is %.3f&quot; % accuracy) # Predict the sentiment of a negative review review = &quot;The movie was not good. The plot had several holes and the acting lacked panache.&quot; prediction = clf_ng.predict(ng_vectorizer.transform([review]))[0] print(&quot;The sentiment predicted by the classifier is %i&quot; % (prediction)) . The accuracy of the classifier on the test set is 0.758 The sentiment predicted by the classifier is 0 . Excellent job! You’re now adept at performing sentiment analysis using text. Notice how this classifier performs slightly better than the BoW version. Also, it succeeds at correctly identifying the sentiment of the mini-review as negative. In the next chapter, we will learn more complex methods of vectorizing textual data. . 3.3.3 Comparing performance of n-gram models . You now know how to conduct sentiment analysis by converting text into various n-gram representations and feeding them to a classifier. In this exercise, we will conduct sentiment analysis for the same movie reviews from before using two n-gram models: unigrams and n-grams upto n equal to 3. . We will then compare the performance using three criteria: accuracy of the model on the test set, time taken to execute the program and the number of features created when generating the n-gram representation. . start_time = time.time() # Splitting the data into training and test sets train_X, test_X, train_y, test_y = train_test_split(df[&#39;review&#39;], df[&#39;sentiment&#39;], test_size=0.5, random_state=42, stratify=df[&#39;sentiment&#39;]) # Generating ngrams vectorizer = CountVectorizer(ngram_range=(1,1)) train_X = vectorizer.fit_transform(train_X) test_X = vectorizer.transform(test_X) # Fit classifier clf = MultinomialNB() clf.fit(train_X, train_y) # Print accuracy, time and number of dimensions print(&quot;The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.&quot; % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1])) # The program took 0.196 seconds to complete. The accuracy on the test set is 0.75. The ngram representation had 12347 features. . vectorizer = CountVectorizer(ngram_range=(1,3)) # The program took 2.933 seconds to complete. The accuracy on the test set is 0.77. The ngram representation had 178240 features. . Amazing work! The program took around 0.2 seconds in the case of the unigram model and more than 10 times longer for the higher order n-gram model. The unigram model had over 12,000 features whereas the n-gram model for upto n=3 had over 178,000! Despite taking higher computation time and generating more features, the classifier only performs marginally better in the latter case, producing an accuracy of 77% in comparison to the 75% for the unigram model. . 4. TF-IDF and similarity scores . . 4.1 Building tf-idf document vectors . . 4.1.1 tf-idf weight of commonly occurring words . The word bottle occurs 5 times in a particular document D and also occurs in every document of the corpus. What is the tf-idf weight of bottle in D ? . 0 . Correct! In fact, the tf-idf weight for bottle in every document will be 0. This is because the inverse document frequency is constant across documents in a corpus and since bottle occurs in every document, its value is log(1), which is 0. . 4.1.2 tf-idf vectors for TED talks . In this exercise, you have been given a corpus ted which contains the transcripts of 500 TED Talks. Your task is to generate the tf-idf vectors for these talks. . In a later lesson, we will use these vectors to generate recommendations of similar talks based on the transcript. . # Import TfidfVectorizer from sklearn.feature_extraction.text import TfidfVectorizer # Create TfidfVectorizer object vectorizer = TfidfVectorizer() # Generate matrix of word vectors tfidf_matrix = vectorizer.fit_transform(ted) # Print the shape of tfidf_matrix print(tfidf_matrix.shape) # (500, 29158) . Good job! You now know how to generate tf-idf vectors for a given corpus of text. You can use these vectors to perform predictive modeling just like we did with CountVectorizer . In the next few lessons, we will see another extremely useful application of the vectorized form of documents: generating recommendations. . . 4.2 Cosine similarity . . 4.2.1 Computing dot product . In this exercise, we will learn to compute the dot product between two vectors, A = (1, 3) and B = (-2, 2), using the numpy library. More specifically, we will use the np.dot() function to compute the dot product of two numpy arrays. . # Initialize numpy vectors A = np.array([1,3]) B = np.array([-2,2]) # Compute dot product dot_prod = np.dot(A, B) # Print dot product print(dot_prod) # 4 . Good job! The dot product of the two vectors is 1 * -2 + 3 * 2 = 4, which is indeed the output produced. We will not be using np.dot() too much in this course but it can prove to be a helpful function while computing dot products between two standalone vectors. . 4.2.2 Cosine similarity matrix of a corpus . In this exercise, you have been given a corpus , which is a list containing five sentences. The corpus is printed in the console. You have to compute the cosine similarity matrix which contains the pairwise cosine similarity score for every pair of sentences (vectorized using tf-idf). . Remember, the value corresponding to the ith row and jth column of a similarity matrix denotes the similarity score for the ith and jth vector. . corpus [&#39;The sun is the largest celestial body in the solar system&#39;, &#39;The solar system consists of the sun and eight revolving planets&#39;, &#39;Ra was the Egyptian Sun God&#39;, &#39;The Pyramids were the pinnacle of Egyptian architecture&#39;, &#39;The quick brown fox jumps over the lazy dog&#39;] . # Initialize an instance of tf-idf Vectorizer tfidf_vectorizer = TfidfVectorizer() # Generate the tf-idf vectors for the corpus tfidf_matrix = tfidf_vectorizer.fit_transform(corpus) # Compute and print the cosine similarity matrix cosine_sim = cosine_similarity(tfidf_matrix,tfidf_matrix) print(cosine_sim) . [[1. 0.36413198 0.18314713 0.18435251 0.16336438] [0.36413198 1. 0.15054075 0.21704584 0.11203887] [0.18314713 0.15054075 1. 0.21318602 0.07763512] [0.18435251 0.21704584 0.21318602 1. 0.12960089] [0.16336438 0.11203887 0.07763512 0.12960089 1. ]] . Great work! As you will see in a subsequent lesson, computing the cosine similarity matrix lies at the heart of many practical systems such as recommenders. From our similarity matrix, we see that the first and the second sentence are the most similar. Also the fifth sentence has, on average, the lowest pairwise cosine scores. This is intuitive as it contains entities that are not present in the other sentences. . . 4.3 Building a plot line based recommender . . 4.3.1 Comparing linear_kernel and cosine_similarity . In this exercise, you have been given tfidf_matrix which contains the tf-idf vectors of a thousand documents. Your task is to generate the cosine similarity matrix for these vectors first using cosine_similarity and then, using linear_kernel . . We will then compare the computation times for both functions. . # Record start time start = time.time() # Compute cosine similarity matrix cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix) # Print cosine similarity matrix print(cosine_sim) # Print time taken print(&quot;Time taken: %s seconds&quot; %(time.time() - start)) . [[1. 0. 0. ... 0. 0. 0. ] [0. 1. 0. ... 0. 0. 0. ] [0. 0. 1. ... 0. 0.01418221 0. ] ... [0. 0. 0. ... 1. 0.01589009 0. ] [0. 0. 0.01418221 ... 0.01589009 1. 0. ] [0. 0. 0. ... 0. 0. 1. ]] Time taken: 0.33341264724731445 seconds . # Compute cosine similarity matrix cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix) . Good job! Notice how both linear_kernel and cosine_similarity produced the same result. However, linear_kernel took a smaller amount of time to execute. When you’re working with a very large amount of data and your vectors are in the tf-idf representation, it is good practice to default to linear_kernel to improve performance. (NOTE: In case, you see linear_kernel taking more time, it’s because the dataset we’re dealing with is extremely small and Python’s time module is incapable of capture such minute time differences accurately) . 4.3.2 Plot recommendation engine . In this exercise, we will build a recommendation engine that suggests movies based on similarity of plot lines. You have been given a get_recommendations() function that takes in the title of a movie, a similarity matrix and an indices series as its arguments and outputs a list of most similar movies. indices has already been provided to you. . You have also been given a movie_plots Series that contains the plot lines of several movies. Your task is to generate a cosine similarity matrix for the tf-idf vectors of these plots. . Consequently, we will check the potency of our engine by generating recommendations for one of my favorite movies, The Dark Knight Rises. . # Initialize the TfidfVectorizer tfidf = TfidfVectorizer(stop_words=&#39;english&#39;) # Construct the TF-IDF matrix tfidf_matrix = tfidf.fit_transform(movie_plots) # Generate the cosine similarity matrix cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix) # Generate recommendations print(get_recommendations(&#39;The Dark Knight Rises&#39;, cosine_sim, indices)) . 1 Batman Forever 2 Batman 3 Batman Returns 8 Batman: Under the Red Hood 9 Batman: Year One 10 Batman: The Dark Knight Returns, Part 1 11 Batman: The Dark Knight Returns, Part 2 5 Batman: Mask of the Phantasm 7 Batman Begins 4 Batman &amp; Robin Name: title, dtype: object . Congratulations! You’ve just built your very first recommendation system. Notice how the recommender correctly identifies &#39;The Dark Knight Rises&#39; as a Batman movie and recommends other Batman movies as a result. This sytem is, of course, very primitive and there are a host of ways in which it could be improved. One method would be to look at the cast, crew and genre in addition to the plot to generate recommendations. We will not be covering this in this course but you have all the tools necessary to accomplish this. Do give it a try! . 4.3.3 The recommender function . In this exercise, we will build a recommender function get_recommendations() , as discussed in the lesson and the previous exercise. As we know, it takes in a title, a cosine similarity matrix, and a movie title and index mapping as arguments and outputs a list of 10 titles most similar to the original title (excluding the title itself). . You have been given a dataset metadata that consists of the movie titles and overviews. The head of this dataset has been printed to console. . title tagline 938 Cinema Paradiso A celebration of youth, friendship, and the ev... 630 Spy Hard All the action. All the women. Half the intell... 682 Stonewall The fight for the right to love 514 Killer You only hurt the one you love. 365 Jason&#39;s Lyric Love is courage. . # Generate mapping between titles and index indices = pd.Series(metadata.index, index=metadata[&#39;title&#39;]).drop_duplicates() def get_recommendations(title, cosine_sim, indices): # Get index of movie that matches title idx = indices[title] # Sort the movies based on the similarity scores sim_scores = list(enumerate(cosine_sim[idx])) sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) # Get the scores for 10 most similar movies sim_scores = sim_scores[1:11] # Get the movie indices movie_indices = [i[0] for i in sim_scores] # Return the top 10 most similar movies return metadata[&#39;title&#39;].iloc[movie_indices] . Good job! With this recommender function in our toolkit, we are now in a very good place to build the rest of the components of our recommendation engine. . 4.3.4 TED talk recommender . In this exercise, we will build a recommendation system that suggests TED Talks based on their transcripts. You have been given a get_recommendations() function that takes in the title of a talk, a similarity matrix and an indices series as its arguments, and outputs a list of most similar talks. indices has already been provided to you. . You have also been given a transcripts series that contains the transcripts of around 500 TED talks. Your task is to generate a cosine similarity matrix for the tf-idf vectors of the talk transcripts. . Consequently, we will generate recommendations for a talk titled ‘5 ways to kill your dreams’ by Brazilian entrepreneur Bel Pesce. . transcripts 0 I&#39;ve noticed something interesting about socie... 1 Hetain Patel: (In Chinese)Yuyu Rau: Hi, I&#39;m He... 2 (Music)Sophie Hawley-Weld: OK, you don&#39;t have ... . # Initialize the TfidfVectorizer tfidf = TfidfVectorizer(stop_words=&#39;english&#39;) # Construct the TF-IDF matrix tfidf_matrix = tfidf.fit_transform(transcripts) # Generate the cosine similarity matrix cosine_sim = linear_kernel(tfidf_matrix,tfidf_matrix) # Generate recommendations print(get_recommendations(&#39;5 ways to kill your dreams&#39;, cosine_sim, indices)) . 453 Success is a continuous journey 157 Why we do what we do 494 How to find work you love 149 My journey into movies that matter 447 One Laptop per Child 230 How to get your ideas to spread 497 Plug into your hard-wired happiness 495 Why you will fail to have a great career 179 Be suspicious of simple stories 53 To upgrade is human Name: title, dtype: object . Excellent work! You have successfully built a TED talk recommender. This recommender works surprisingly well despite being trained only on a small subset of TED talks. In fact, three of the talks recommended by our system is also recommended by the official TED website as talks to watch next after &#39;5 ways to kill your dreams&#39; ! . . 4.4 Beyond n-grams: word embeddings . . 4.4.1 Generating word vectors . In this exercise, we will generate the pairwise similarity scores of all the words in a sentence. The sentence is available as sent and has been printed to the console for your convenience. . sent &#39;I like apples and oranges&#39; . # Create the doc object doc = nlp(sent) # Compute pairwise similarity scores for token1 in doc: for token2 in doc: print(token1.text, token2.text, token1.similarity(token2)) . I I 1.0 I like 0.023032807 I apples 0.10175116 I and 0.047492094 I oranges 0.10894456 like I 0.023032807 like like 1.0 like apples 0.015370452 like and 0.189293 like oranges 0.021943133 apples I 0.10175116 apples like 0.015370452 apples apples 1.0 apples and -0.17736834 apples oranges 0.6315578 and I 0.047492094 and like 0.189293 and apples -0.17736834 and and 1.0 and oranges 0.018627528 oranges I 0.10894456 oranges like 0.021943133 oranges apples 0.6315578 oranges and 0.018627528 oranges oranges 1.0 . Good job! Notice how the words &#39;apples&#39; and &#39;oranges&#39; have the highest pairwaise similarity score. This is expected as they are both fruits and are more related to each other than any other pair of words. . 4.4.2 Computing similarity of Pink Floyd songs . In this final exercise, you have been given lyrics of three songs by the British band Pink Floyd, namely ‘High Hopes’, ‘Hey You’ and ‘Mother’. The lyrics to these songs are available as hopes , hey and mother respectively. . Your task is to compute the pairwise similarity between mother and hopes , and mother and hey . . mother &quot; nMother do you think they&#39;ll drop the bomb? nMother do you think they&#39;ll like this song? nMother do you think they&#39;ll try to ... . # Create Doc objects mother_doc = nlp(mother) hopes_doc = nlp(hopes) hey_doc = nlp(hey) # Print similarity between mother and hopes print(mother_doc.similarity(hopes_doc)) # 0.6006234924640204 # Print similarity between mother and hey print(mother_doc.similarity(hey_doc)) # 0.9135920924498578 . Excellent work! Notice that ‘Mother’ and ‘Hey You’ have a similarity score of 0.9 whereas ‘Mother’ and ‘High Hopes’ has a score of only 0.6. This is probably because ‘Mother’ and ‘Hey You’ were both songs from the same album ‘The Wall’ and were penned by Roger Waters. On the other hand, ‘High Hopes’ was a part of the album ‘Division Bell’ with lyrics by David Gilmour and his wife, Penny Samson. Treat yourself by listening to these songs. They’re some of the best! . . 4.5 Final thoughts . . . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/feature-engineering-for-nlp-in-python.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/feature-engineering-for-nlp-in-python.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post28": {
            "title": "Feature Engineering for Machine Learning in Python",
            "content": "Feature Engineering for Machine Learning in Python . This is the memo of the 10th course (23 courses in all) of ‘Machine Learning Scientist with Python’ skill track. . You can find the original course HERE . . ### Course Description . Every day you read about the amazing breakthroughs in how the newest applications of machine learning are changing the world. Often this reporting glosses over the fact that a huge amount of data munging and feature engineering must be done before any of these fancy models can be used. In this course, you will learn how to do just that. You will work with Stack Overflow Developers survey, and historic US presidential inauguration addresses, to understand how best to preprocess and engineer features from categorical, continuous, and unstructured data. This course will give you hands-on experience on how to prepare any data for your own machine learning models. . ### . Creating Features | Dealing with Messy Data | Conforming to Statistical Assumptions | Dealing with Text Data | 1. Creating Features . . 1.1 Why generate features? . . 1.1.1 Getting to know your data . Pandas is one the most popular packages used to work with tabular data in Python. It is generally imported using the alias pd and can be used to load a CSV (or other delimited files) using read_csv() . . You will be working with a modified subset of the Stackoverflow survey response data in the first three chapters of this course. This data set records the details, and preferences of thousands of users of the StackOverflow website. . # Import pandas import pandas as pd # Import so_survey_csv into so_survey_df so_survey_df = pd.read_csv(so_survey_csv) # Print the first five rows of the DataFrame print(so_survey_df.head()) # Print the data type of each column print(so_survey_df.dtypes) . SurveyDate FormalEducation ConvertedSalary Hobby Country ... VersionControl Age Years Experience Gender RawSalary 0 2/28/18 20:20 Bachelor&#39;s degree (BA. BS. B.Eng.. etc.) NaN Yes South Africa ... Git 21 13 Male NaN 1 6/28/18 13:26 Bachelor&#39;s degree (BA. BS. B.Eng.. etc.) 70841.0 Yes Sweeden ... Git;Subversion 38 9 Male 70,841.00 2 6/6/18 3:37 Bachelor&#39;s degree (BA. BS. B.Eng.. etc.) NaN No Sweeden ... Git 45 11 NaN NaN 3 5/9/18 1:06 Some college/university study without earning ... 21426.0 Yes Sweeden ... Zip file back-ups 46 12 Male 21,426.00 4 4/12/18 22:41 Bachelor&#39;s degree (BA. BS. B.Eng.. etc.) 41671.0 Yes UK ... Git 39 7 Male £41,671.00 [5 rows x 11 columns] SurveyDate object FormalEducation object ConvertedSalary float64 Hobby object Country object StackOverflowJobsRecommend float64 VersionControl object Age int64 Years Experience int64 Gender object RawSalary object dtype: object . 1.1.2 Selecting specific data types . Often a data set will contain columns with several different data types (like the one you are working with). The majority of machine learning models require you to have a consistent data type across features. Similarly, most feature engineering techniques are applicable to only one type of data at a time. For these reasons among others, you will often want to be able to access just the columns of certain types when working with a DataFrame. . # Create subset of only the numeric columns so_numeric_df = so_survey_df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]) # Print the column names contained in so_survey_df_num print(so_numeric_df.columns) # Index([&#39;ConvertedSalary&#39;, &#39;StackOverflowJobsRecommend&#39;, &#39;Age&#39;, &#39;Years Experience&#39;], dtype=&#39;object&#39;) . . 1.2 Dealing with categorical features . . 1.2.1 One-hot encoding and dummy variables . To use categorical variables in a machine learning model, you first need to represent them in a quantitative way. The two most common approaches are to one-hot encode the variables using or to use dummy variables. In this exercise, you will create both types of encoding, and compare the created column sets. We will continue using the same DataFrame from previous lesson loaded as so_survey_df and focusing on its Country column. . # Convert the Country column to a one hot encoded Data Frame one_hot_encoded = pd.get_dummies(so_survey_df, columns=[&#39;Country&#39;], prefix=&#39;OH&#39;) # Print the columns names print(one_hot_encoded.columns) . Index([&#39;SurveyDate&#39;, &#39;FormalEducation&#39;, &#39;ConvertedSalary&#39;, &#39;Hobby&#39;, &#39;StackOverflowJobsRecommend&#39;, &#39;VersionControl&#39;, &#39;Age&#39;, &#39;Years Experience&#39;, &#39;Gender&#39;, &#39;RawSalary&#39;, &#39;OH_France&#39;, &#39;OH_India&#39;, &#39;OH_Ireland&#39;, &#39;OH_Russia&#39;, &#39;OH_South Africa&#39;, &#39;OH_Spain&#39;, &#39;OH_Sweeden&#39;, &#39;OH_UK&#39;, &#39;OH_USA&#39;, &#39;OH_Ukraine&#39;], dtype=&#39;object&#39;) . # Create dummy variables for the Country column dummy = pd.get_dummies(so_survey_df, columns=[&#39;Country&#39;], drop_first=True, prefix=&#39;DM&#39;) # Print the columns names print(dummy.columns) . Index([&#39;SurveyDate&#39;, &#39;FormalEducation&#39;, &#39;ConvertedSalary&#39;, &#39;Hobby&#39;, &#39;StackOverflowJobsRecommend&#39;, &#39;VersionControl&#39;, &#39;Age&#39;, &#39;Years Experience&#39;, &#39;Gender&#39;, &#39;RawSalary&#39;, &#39;DM_India&#39;, &#39;DM_Ireland&#39;, &#39;DM_Russia&#39;, &#39;DM_South Africa&#39;, &#39;DM_Spain&#39;, &#39;DM_Sweeden&#39;, &#39;DM_UK&#39;, &#39;DM_USA&#39;, &#39;DM_Ukraine&#39;], dtype=&#39;object&#39;) . Did you notice that the column for France was missing when you created dummy variables? Now you can choose to use one-hot encoding or dummy variables where appropriate. . 1.2.2 Dealing with uncommon categories . Some features can have many different categories but a very uneven distribution of their occurrences. Take for example Data Science’s favorite languages to code in, some common choices are Python, R, and Julia, but there can be individuals with bespoke choices, like FORTRAN, C etc. In these cases, you may not want to create a feature for each value, but only the more common occurrences. . countries.value_counts() South Africa 166 USA 164 Spain 134 Sweeden 119 France 115 Russia 97 India 95 UK 95 Ukraine 9 Ireland 5 Name: Country, dtype: int64 . # Create a series out of the Country column countries = so_survey_df[&#39;Country&#39;] # Get the counts of each category country_counts = countries.value_counts() # Create a mask for only categories that occur less than 10 times mask = countries.isin(country_counts[country_counts &lt; 10].index) # Label all other categories as Other countries[mask] = &#39;Other&#39; # Print the updated category counts print(countries.value_counts()) . South Africa 166 USA 164 Spain 134 Sweeden 119 France 115 Russia 97 India 95 UK 95 Other 14 Name: Country, dtype: int64 . Good work, now you can work with large data sets while grouping low frequency categories. . . 1.3 Numeric variables . . 1.3.1 Binarizing columns . While numeric values can often be used without any feature engineering, there will be cases when some form of manipulation can be useful. For example on some occasions, you might not care about the magnitude of a value but only care about its direction, or if it exists at all. In these situations, you will want to binarize a column. In the so_survey_df data, you have a large number of survey respondents that are working voluntarily (without pay). You will create a new column titled Paid_Job indicating whether each person is paid (their salary is greater than zero). . # Create the Paid_Job column filled with zeros so_survey_df[&#39;Paid_Job&#39;] = 0 # Replace all the Paid_Job values where ConvertedSalary is &gt; 0 so_survey_df.loc[so_survey_df[&#39;ConvertedSalary&#39;]&gt;0, &#39;Paid_Job&#39;] = 1 # Print the first five rows of the columns print(so_survey_df[[&#39;Paid_Job&#39;, &#39;ConvertedSalary&#39;]].head()) . Paid_Job ConvertedSalary 0 0 0.0 1 1 70841.0 2 0 0.0 3 1 21426.0 4 1 41671.0 . Good work, binarizing columns can also be useful for your target variables. . 1.3.2 Binning values . For many continuous values you will care less about the exact value of a numeric column, but instead care about the bucket it falls into. This can be useful when plotting values, or simplifying your machine learning models. It is mostly used on continuous variables where accuracy is not the biggest concern e.g. age, height, wages. . Bins are created using pd.cut(df[&#39;column_name&#39;], bins) where bins can be an integer specifying the number of evenly spaced bins, or a list of bin boundaries. . # Bin the continuous variable ConvertedSalary into 5 bins so_survey_df[&#39;equal_binned&#39;] = pd.cut(so_survey_df[&#39;ConvertedSalary&#39;], bins=5) # Print the first 5 rows of the equal_binned column print(so_survey_df[[&#39;equal_binned&#39;, &#39;ConvertedSalary&#39;]].head()) . equal_binned ConvertedSalary 0 (-2000.0, 400000.0] 0.0 1 (-2000.0, 400000.0] 70841.0 2 (-2000.0, 400000.0] 0.0 3 (-2000.0, 400000.0] 21426.0 4 (-2000.0, 400000.0] 41671.0 . # Import numpy import numpy as np # Specify the boundaries of the bins bins = [-np.inf, 10000, 50000, 100000, 150000, np.inf] # Bin labels labels = [&#39;Very low&#39;, &#39;Low&#39;, &#39;Medium&#39;, &#39;High&#39;, &#39;Very high&#39;] # Bin the continuous variable ConvertedSalary using these boundaries so_survey_df[&#39;boundary_binned&#39;] = pd.cut(so_survey_df[&#39;ConvertedSalary&#39;], bins=bins, labels=labels) # Print the first 5 rows of the boundary_binned column print(so_survey_df[[&#39;boundary_binned&#39;, &#39;ConvertedSalary&#39;]].head()) . boundary_binned ConvertedSalary 0 Very low 0.0 1 Medium 70841.0 2 Very low 0.0 3 Low 21426.0 4 Low 41671.0 . Now you can bin columns with equal spacing and predefined boundaries. . 2. Dealing with Messy Data . . 2.1 Why do missing values exist? . . 2.1.1 How sparse is my data? . Most data sets contain missing values, often represented as NaN (Not a Number). If you are working with Pandas you can easily check how many missing values exist in each column. . Let’s find out how many of the developers taking the survey chose to enter their age (found in the Age column of so_survey_df ) and their gender ( Gender column of so_survey_df ). . # Subset the DataFrame sub_df = so_survey_df[[&#39;Age&#39;,&#39;Gender&#39;]] # Print the number of non-missing values print(sub_df.notnull().sum()) Age 999 Gender 693 dtype: int64 . 2.1.2 Finding the missing values . While having a summary of how much of your data is missing can be useful, often you will need to find the exact locations of these missing values. Using the same subset of the StackOverflow data from the last exercise ( sub_df ), you will show how a value can be flagged as missing. . # Print the locations of the missing values print(sub_df.head(10).isnull()) Age Gender 0 False False 1 False False 2 False True 3 False False 4 False False 5 False False 6 False False 7 False False 8 False False 9 False True # Print the locations of the non-missing values print(sub_df.head(10).notnull()) Age Gender 0 True True 1 True True 2 True False 3 True True 4 True True 5 True True 6 True True 7 True True 8 True True 9 True False . . 2.2 Dealing with missing values (I) . . 2.2.1 Listwise deletion . The simplest way to deal with missing values in your dataset when they are occurring entirely at random is to remove those rows, also called ‘listwise deletion’. . Depending on the use case, you will sometimes want to remove all missing values in your data while other times you may want to only remove a particular column if too many values are missing in that column. . # Print the number of rows and columns print(so_survey_df.shape) # (999, 11) # Create a new DataFrame dropping all incomplete rows no_missing_values_rows = so_survey_df.dropna() # Print the shape of the new DataFrame print(no_missing_values_rows.shape) # (264, 11) # Create a new DataFrame dropping all columns with incomplete rows no_missing_values_cols = so_survey_df.dropna(how=&#39;any&#39;, axis=1) # Print the shape of the new DataFrame print(no_missing_values_cols.shape) # (999, 7) # Drop all rows where Gender is missing no_gender = so_survey_df.dropna(subset=[&#39;Gender&#39;]) # Print the shape of the new DataFrame print(no_gender.shape) # (693, 11) . Correct, as you can see dropping all rows that contain any missing values may greatly reduce the size of your dataset. So you need to think carefully and consider several trade-offs when deleting missing values. . 2.2.2 Replacing missing values with constants . While removing missing data entirely maybe a correct approach in many situations, this may result in a lot of information being omitted from your models. . You may find categorical columns where the missing value is a valid piece of information in itself, such as someone refusing to answer a question in a survey. In these cases, you can fill all missing values with a new category entirely, for example ‘No response given’. . # Print the count of occurrences print(so_survey_df[&#39;Gender&#39;].value_counts()) . Male 632 Female 53 Transgender 2 Female;Male 2 Female;Transgender 1 Male;Non-binary. genderqueer. or gender non-conforming 1 Female;Male;Transgender;Non-binary. genderqueer. or gender non-conforming 1 Non-binary. genderqueer. or gender non-conforming 1 Name: Gender, dtype: int64 . # Replace missing values so_survey_df[&#39;Gender&#39;].fillna(&#39;Not Given&#39;, inplace=True) # Print the count of each value print(so_survey_df[&#39;Gender&#39;].value_counts()) . Male 632 Not Given 306 Female 53 Transgender 2 Female;Male 2 Female;Transgender 1 Male;Non-binary. genderqueer. or gender non-conforming 1 Female;Male;Transgender;Non-binary. genderqueer. or gender non-conforming 1 Non-binary. genderqueer. or gender non-conforming 1 Name: Gender, dtype: int64 . . 2.3 Dealing with missing values (II) . . 2.3.1 Filling continuous missing values . In the last lesson, you dealt with different methods of removing data missing values and filling in missing values with a fixed string. These approaches are valid in many cases, particularly when dealing with categorical columns but have limited use when working with continuous values. In these cases, it may be most valid to fill the missing values in the column with a value calculated from the entries present in the column. . # Fill missing values with the mean so_survey_df[&#39;StackOverflowJobsRecommend&#39;].fillna(so_survey_df[&#39;StackOverflowJobsRecommend&#39;].mean(), inplace=True) # Round the StackOverflowJobsRecommend values so_survey_df[&#39;StackOverflowJobsRecommend&#39;] = round(so_survey_df[&#39;StackOverflowJobsRecommend&#39;]) # Print the top 5 rows print(so_survey_df[&#39;StackOverflowJobsRecommend&#39;].head()) 0 7.0 1 7.0 2 8.0 3 7.0 4 8.0 Name: StackOverflowJobsRecommend, dtype: float64 . Nicely done, remember you should only round your values if you are certain it is applicable. . 2.3.2 Imputing values in predictive models . When working with predictive models you will often have a separate train and test DataFrames. In these cases you want to ensure no information from your test set leaks into your train set. When filling missing values in data to be used in these situations how should approach the two data sets? . Apply the measures of central tendency (mean/median etc.) calculated on the train set to both the train and test sets. . Values calculated on the train test should be applied to both DataFrames. . . 2.4 Dealing with other data issues . . 2.4.1 Dealing with stray characters (I) . In this exercise, you will work with the RawSalary column of so_survey_df which contains the wages of the respondents along with the currency symbols and commas, such as $42,000 . When importing data from Microsoft Excel, more often that not you will come across data in this form. . so_survey_df[&#39;RawSalary&#39;] 0 NaN 1 70,841.00 2 NaN 3 21,426.00 4 £41,671.00 . # Remove the commas in the column so_survey_df[&#39;RawSalary&#39;] = so_survey_df[&#39;RawSalary&#39;].str.replace(&#39;,&#39;, &#39;&#39;) # Remove the dollar signs in the column so_survey_df[&#39;RawSalary&#39;] = so_survey_df[&#39;RawSalary&#39;].str.replace(&#39;$&#39;,&#39;&#39;) . 2.4.2 Dealing with stray characters (II) . In the last exercise, you could tell quickly based off of the df.head() call which characters were causing an issue. In many cases this will not be so apparent. There will often be values deep within a column that are preventing you from casting a column as a numeric type so that it can be used in a model or further feature engineering. . One approach to finding these values is to force the column to the data type desired using pd.to_numeric() , coercing any values causing issues to NaN, Then filtering the DataFrame by just the rows containing the NaN values. . Try to cast the RawSalary column as a float and it will fail as an additional character can now be found in it. Find the character and remove it so the column can be cast as a float. . # Attempt to convert the column to numeric values numeric_vals = pd.to_numeric(so_survey_df[&#39;RawSalary&#39;], errors=&#39;coerce&#39;) # Find the indexes of missing values idx = numeric_vals.isna() # Print the relevant rows print(so_survey_df[&#39;RawSalary&#39;][idx]) 0 NaN 2 NaN 4 £41671.00 6 NaN 8 NaN ... . # Replace the offending characters so_survey_df[&#39;RawSalary&#39;] = so_survey_df[&#39;RawSalary&#39;].str.replace(&#39;£&#39;,&#39;&#39;) # Convert the column to float so_survey_df[&#39;RawSalary&#39;] = so_survey_df[&#39;RawSalary&#39;].astype(&#39;float&#39;) # Print the column print(so_survey_df[&#39;RawSalary&#39;]) . Remember that even after removing all the relevant characters, you still need to change the type of the column to numeric if you want to plot these continuous values. . 2.4.3 Method chaining . When applying multiple operations on the same column (like in the previous exercises), you made the changes in several steps, assigning the results back in each step. However, when applying multiple successive operations on the same column, you can “chain” these operations together for clarity and ease of management. This can be achieved by calling multiple methods sequentially: . # Method chaining df[&#39;column&#39;] = df[&#39;column&#39;].method1().method2().method3() # Same as df[&#39;column&#39;] = df[&#39;column&#39;].method1() df[&#39;column&#39;] = df[&#39;column&#39;].method2() df[&#39;column&#39;] = df[&#39;column&#39;].method3() . In this exercise you will repeat the steps you performed in the last two exercises, but do so using method chaining. . # Use method chaining so_survey_df[&#39;RawSalary&#39;] = so_survey_df[&#39;RawSalary&#39;] .str.replace(&#39;,&#39;,&#39;&#39;) .str.replace(&#39;$&#39;,&#39;&#39;) .str.replace(&#39;£&#39;,&#39;&#39;) .astype(&#39;float&#39;) # Print the RawSalary column print(so_survey_df[&#39;RawSalary&#39;]) . Custom functions can be also used when method chaining using the .apply() method. . 3. Conforming to Statistical Assumptions . . 3.1 Data distributions . . 3.1.1 What does your data look like? (I) . Up until now you have focused on creating new features and dealing with issues in your data. Feature engineering can also be used to make the most out of the data that you already have and use it more effectively when creating machine learning models. . Many algorithms may assume that your data is normally distributed, or at least that all your columns are on the same scale. This will often not be the case, e.g. one feature may be measured in thousands of dollars while another would be number of years. In this exercise, you will create plots to examine the distributions of some numeric columns in the so_survey_df DataFrame, stored in so_numeric_df . . # Create a histogram so_numeric_df.hist() plt.show() . . # Create a boxplot of two columns so_numeric_df[[&#39;Age&#39;, &#39;Years Experience&#39;]].boxplot() plt.show() . . # Create a boxplot of ConvertedSalary so_numeric_df[[&#39;ConvertedSalary&#39;]].boxplot() plt.show() . . 3.1.2 What does your data look like? (II) . In the previous exercise you looked at the distribution of individual columns. While this is a good start, a more detailed view of how different features interact with each other may be useful as this can impact your decision on what to transform and how. . # Import packages from matplotlib import pyplot as plt import seaborn as sns # Plot pairwise relationships sns.pairplot(so_numeric_df) # Show plot plt.show() . . # Print summary statistics print(so_numeric_df.describe()) . ConvertedSalary Age Years Experience count 9.990000e+02 999.000000 999.000000 mean 6.161746e+04 36.003003 9.961962 std 1.760924e+05 13.255127 4.878129 min 0.000000e+00 18.000000 0.000000 25% 0.000000e+00 25.000000 7.000000 50% 2.712000e+04 35.000000 10.000000 75% 7.000000e+04 45.000000 13.000000 max 2.000000e+06 83.000000 27.000000 . Good work, understanding these summary statistics of a column can be very valuable when deciding what transformations are necessary. . 3.1.3 When don’t you have to transform your data? . While making sure that all of your data is on the same scale is advisable for most analyses, for which of the following machine learning models is normalizing data not always necessary? . Decision Trees . As decision trees split along a singular point, they do not require all the columns to be on the same scale. . . 3.2 Scaling and transformations . . 3.2.1 Normalization . As discussed in the video, in normalization you linearly scale the entire column between 0 and 1, with 0 corresponding with the lowest value in the column, and 1 with the largest. . When using scikit-learn (the most commonly used machine learning library in Python) you can use a MinMaxScaler to apply normalization. (It is called this as it scales your values between a minimum and maximum value.) . # Import MinMaxScaler from sklearn.preprocessing import MinMaxScaler # Instantiate MinMaxScaler MM_scaler = MinMaxScaler() # Fit MM_scaler to the data MM_scaler.fit(so_numeric_df[[&#39;Age&#39;]]) # Transform the data using the fitted scaler so_numeric_df[&#39;Age_MM&#39;] = MM_scaler.transform(so_numeric_df[[&#39;Age&#39;]]) # Compare the origional and transformed column print(so_numeric_df[[&#39;Age_MM&#39;, &#39;Age&#39;]].head()) . Age_MM Age 0 0.046154 21 1 0.307692 38 2 0.415385 45 3 0.430769 46 4 0.323077 39 . Did you notice that all values have been scaled between 0 and 1? . 3.2.2 Standardization . While normalization can be useful for scaling a column between two data points, it is hard to compare two scaled columns if even one of them is overly affected by outliers. One commonly used solution to this is called standardization, where instead of having a strict upper and lower bound, you center the data around its mean, and calculate the number of standard deviations away from mean each data point is. . # Import StandardScaler from sklearn.preprocessing import StandardScaler # Instantiate StandardScaler SS_scaler = StandardScaler() # Fit SS_scaler to the data SS_scaler.fit(so_numeric_df[[&#39;Age&#39;]]) # Transform the data using the fitted scaler so_numeric_df[&#39;Age_SS&#39;] = SS_scaler.transform(so_numeric_df[[&#39;Age&#39;]]) # Compare the origional and transformed column print(so_numeric_df[[&#39;Age_SS&#39;, &#39;Age&#39;]].head()) . Age_SS Age 0 -1.132431 21 1 0.150734 38 2 0.679096 45 3 0.754576 46 4 0.226214 39 . you can see that the values have been scaled linearly, but not between set values. . 3.2.3 Log transformation . In the previous exercises you scaled the data linearly, which will not affect the data’s shape. This works great if your data is normally distributed (or closely normally distributed), an assumption that a lot of machine learning models make. Sometimes you will work with data that closely conforms to normality, e.g the height or weight of a population. On the other hand, many variables in the real world do not follow this pattern e.g, wages or age of a population. In this exercise you will use a log transform on the ConvertedSalary column in the so_numeric_df DataFrame as it has a large amount of its data centered around the lower values, but contains very high values also. These distributions are said to have a long right tail. . # Import PowerTransformer from sklearn.preprocessing import PowerTransformer # Instantiate PowerTransformer pow_trans = PowerTransformer() # Train the transform on the data pow_trans.fit(so_numeric_df[[&#39;ConvertedSalary&#39;]]) # Apply the power transform to the data so_numeric_df[&#39;ConvertedSalary_LG&#39;] = pow_trans.transform(so_numeric_df[[&#39;ConvertedSalary&#39;]]) # Plot the data before and after the transformation so_numeric_df[[&#39;ConvertedSalary&#39;, &#39;ConvertedSalary_LG&#39;]].hist() plt.show() . so_numeric_df.head() ConvertedSalary Age Years Experience ConvertedSalary_LG 0 NaN 21 13 NaN 1 70841.0 38 9 0.312939 2 NaN 45 11 NaN 3 21426.0 46 12 -0.652182 4 41671.0 39 7 -0.135589 . . Did you notice the change in the shape of the distribution? ConvertedSalary_LG column looks much more normal than the original ConvertedSalary column. . 3.2.4 When can you use normalization? . When could you use normalization ( MinMaxScaler ) when working with a dataset? . When you know the the data has a strict upper and lower bound. . Normalization scales all points linearly between the upper and lower bound. . . 3.3 Removing outliers . . 3.3.1 Percentage based outlier removal . One way to ensure a small portion of data is not having an overly adverse effect is by removing a certain percentage of the largest and/or smallest values in the column. This can be achieved by finding the relevant quantile and trimming the data using it with a mask. This approach is particularly useful if you are concerned that the highest values in your dataset should be avoided. When using this approach, you must remember that even if there are no outliers, this will still remove the same top N percentage from the dataset. . # Find the 95th quantile quantile = so_numeric_df[&#39;ConvertedSalary&#39;].quantile(0.95) # Trim the outliers trimmed_df = so_numeric_df[so_numeric_df[&#39;ConvertedSalary&#39;] &lt; quantile] # The original histogram so_numeric_df[[&#39;ConvertedSalary&#39;]].hist() plt.show() plt.clf() # The trimmed histogram trimmed_df[[&#39;ConvertedSalary&#39;]].hist() plt.show() . . In the next exercise, you will work with a more statistically sound approach in removing outliers. . 3.3.2 Statistical outlier removal . While removing the top N% of your data is useful for ensuring that very spurious points are removed, it does have the disadvantage of always removing the same proportion of points, even if the data is correct. A commonly used alternative approach is to remove data that sits further than three standard deviations from the mean. You can implement this by first calculating the mean and standard deviation of the relevant column to find upper and lower bounds, and applying these bounds as a mask to the DataFrame. This method ensures that only data that is genuinely different from the rest is removed, and will remove fewer points if the data is close together. . # Find the mean and standard dev std = so_numeric_df[&#39;ConvertedSalary&#39;].std() mean = so_numeric_df[&#39;ConvertedSalary&#39;].mean() # Calculate the cutoff cut_off = std * 3 lower, upper = mean - cut_off, mean + cut_off # Trim the outliers trimmed_df = so_numeric_df[(so_numeric_df[&#39;ConvertedSalary&#39;] &lt; upper) &amp; (so_numeric_df[&#39;ConvertedSalary&#39;] &gt; lower)] # The trimmed box plot trimmed_df[[&#39;ConvertedSalary&#39;]].boxplot() plt.show() . . Did you notice the scale change on the y-axis? . . 3.4 Scaling and transforming new data . . 3.4.1 Train and testing transformations (I) . So far you have created scalers based on a column, and then applied the scaler to the same data that it was trained on. When creating machine learning models you will generally build your models on historic data (train set) and apply your model to new unseen data (test set). In these cases you will need to ensure that the same scaling is being applied to both the training and test data. . To do this in practice you train the scaler on the train set, and keep the trained scaler to apply it to the test set. You should never retrain a scaler on the test set. . For this exercise and the next, we split the so_numeric_df DataFrame into train ( so_train_numeric ) and test ( so_test_numeric ) sets. . # Import StandardScaler from sklearn.preprocessing import StandardScaler # Apply a standard scaler to the data SS_scaler = StandardScaler() # Fit the standard scaler to the data SS_scaler.fit(so_train_numeric[[&#39;Age&#39;]]) # Transform the test data using the fitted scaler so_test_numeric[&#39;Age_ss&#39;] = SS_scaler.transform(so_test_numeric[[&#39;Age&#39;]]) print(so_test_numeric[[&#39;Age&#39;, &#39;Age_ss&#39;]].head()) . Age Age_ss 700 35 -0.069265 701 18 -1.343218 702 47 0.829997 703 57 1.579381 704 41 0.380366 . Data leakage is one of the most common mistakes data scientists tend to make, and I hope that you won’t! . 3.4.2 Train and testing transformations (II) . Similar to applying the same scaler to both your training and test sets, if you have removed outliers from the train set, you probably want to do the same on the test set as well. Once again you should ensure that you use the thresholds calculated only from the train set to remove outliers from the test set. . Similar to the last exercise, we split the so_numeric_df DataFrame into train ( so_train_numeric ) and test ( so_test_numeric ) sets. . train_std = so_train_numeric[&#39;ConvertedSalary&#39;].std() train_mean = so_train_numeric[&#39;ConvertedSalary&#39;].mean() cut_off = train_std * 3 train_lower, train_upper = train_mean - cut_off, train_mean + cut_off # Trim the test DataFrame trimmed_df = so_test_numeric[(so_test_numeric[&#39;ConvertedSalary&#39;] &lt; train_upper) &amp; (so_test_numeric[&#39;ConvertedSalary&#39;] &gt; train_lower)] . Very well done. In the next chapter, you will deal with unstructured (text) data. . 4. Dealing with Text Data . . 4.1 Encoding text . . 4.1.1 Cleaning up your text . Unstructured text data cannot be directly used in most analyses. Multiple steps need to be taken to go from a long free form string to a set of numeric columns in the right format that can be ingested by a machine learning model. The first step of this process is to standardize the data and eliminate any characters that could cause problems later on in your analytic pipeline. . In this chapter you will be working with a new dataset containing the inaugural speeches of the presidents of the United States loaded as speech_df , with the speeches stored in the text column. . # Print the first 5 rows of the text column print(speech_df.text.head()) . 0 Fellow-Citizens of the Senate and of the House... 1 Fellow Citizens: I AM again called upon by th... 2 WHEN it was first perceived, in early times, t... 3 Friends and Fellow-Citizens: CALLED upon to u... 4 PROCEEDING, fellow-citizens, to that qualifica... Name: text, dtype: object . # Replace all non letter characters with a whitespace speech_df[&#39;text_clean&#39;] = speech_df[&#39;text&#39;].str.replace(&#39;[^a-zA-Z]&#39;, &#39; &#39;) # Change to lower case speech_df[&#39;text_clean&#39;] = speech_df[&#39;text_clean&#39;].str.lower() # Print the first 5 rows of the text_clean column print(speech_df[&#39;text_clean&#39;].head()) . 0 fellow citizens of the senate and of the house... 1 fellow citizens i am again called upon by th... 2 when it was first perceived in early times t... 3 friends and fellow citizens called upon to u... 4 proceeding fellow citizens to that qualifica... Name: text_clean, dtype: object . Great, now your text strings have been standardized and cleaned up. You can now use this new column ( text_clean ) to extract information about the speeches. . 4.1.2 High level text features . Once the text has been cleaned and standardized you can begin creating features from the data. The most fundamental information you can calculate about free form text is its size, such as its length and number of words. In this exercise (and the rest of this chapter), you will focus on the cleaned/transformed text column ( text_clean ) you created in the last exercise. . # Find the length of each text speech_df[&#39;char_cnt&#39;] = speech_df[&#39;text_clean&#39;].str.len() # Count the number of words in each text speech_df[&#39;word_cnt&#39;] = speech_df[&#39;text_clean&#39;].str.split().str.len() # Find the average length of word speech_df[&#39;avg_word_length&#39;] = speech_df[&#39;char_cnt&#39;] / speech_df[&#39;word_cnt&#39;] # Print the first 5 rows of these columns print(speech_df[[&#39;text_clean&#39;, &#39;char_cnt&#39;, &#39;word_cnt&#39;, &#39;avg_word_length&#39;]]) . text_clean char_cnt word_cnt avg_word_length 0 fellow citizens of the senate and of the house... 8616 1432 6.016760 1 fellow citizens i am again called upon by th... 787 135 5.829630 2 when it was first perceived in early times t... 13871 2323 5.971158 . These features may appear basic but can be quite useful in ML models. . . 4.2 Word counts . . 4.2.1 Counting words (I) . Once high level information has been recorded you can begin creating features based on the actual content of each text. One way to do this is to approach it in a similar way to how you worked with categorical variables in the earlier lessons. . For each unique word in the dataset a column is created. | For each entry, the number of times this word occurs is counted and the count value is entered into the respective column. | . These “count” columns can then be used to train machine learning models. . # Import CountVectorizer from sklearn.feature_extraction.text import CountVectorizer # Instantiate CountVectorizer cv = CountVectorizer() # Fit the vectorizer cv.fit(speech_df[&#39;text_clean&#39;]) # Print feature names print(cv.get_feature_names()) . [&#39;abandon&#39;, &#39;abandoned&#39;, &#39;abandonment&#39;, &#39;abate&#39;, &#39;abdicated&#39;, &#39;abeyance&#39;, &#39;abhorring&#39;, &#39;abide&#39;, &#39;abiding&#39;, &#39;abilities&#39;, &#39;ability&#39;, &#39;abject&#39;, &#39;able&#39;, ...] . 4.2.2 Counting words (II) . Once the vectorizer has been fit to the data, it can be used to transform the text to an array representing the word counts. This array will have a row per block of text and a column for each of the features generated by the vectorizer that you observed in the last exercise. . The vectorizer to you fit in the last exercise ( cv ) is available in your workspace. . # Apply the vectorizer cv_transformed = cv.transform(speech_df[&#39;text_clean&#39;]) # Print the full array cv_array = cv_transformed.toarray() print(cv_array) print(cv_array.shape) # (58, 9043) . [[0 0 0 ... 0 0 0] [0 0 0 ... 0 0 0] [0 1 0 ... 0 0 0] ... [0 1 0 ... 0 0 0] [0 0 0 ... 0 0 0] [0 0 0 ... 0 0 0]] . The speeches have 9043 unique words, which is a lot! In the next exercise, you will see how to create a limited set of features. . 4.2.3 Limiting your features . As you have seen, using the CountVectorizer with its default settings creates a feature for every single word in your corpus. This can create far too many features, often including ones that will provide very little analytical value. . For this purpose CountVectorizer has parameters that you can set to reduce the number of features: . min_df Use only words that occur in more than this percentage of documents. This can be used to remove outlier words that will not generalize across texts. | max_df Use only words that occur in less than this percentage of documents. This is useful to eliminate very common words that occur in every corpus without adding value such as “and” or “the”. | . # Import CountVectorizer from sklearn.feature_extraction.text import CountVectorizer # Specify arguements to limit the number of features generated cv = CountVectorizer(min_df=0.2, max_df=0.8) # Fit, transform, and convert into array cv_transformed = cv.fit_transform(speech_df[&#39;text_clean&#39;]) cv_array = cv_transformed.toarray() # Print the array shape print(cv_array.shape) # (58, 818) . 4.2.4 Text to DataFrame . Now that you have generated these count based features in an array you will need to reformat them so that they can be combined with the rest of the dataset. This can be achieved by converting the array into a pandas DataFrame, with the feature names you found earlier as the column names, and then concatenate it with the original DataFrame. . The numpy array ( cv_array ) and the vectorizer ( cv ) you fit in the last exercise are available in your workspace. . # Create a DataFrame with these features cv_df = pd.DataFrame(cv_array, columns=cv.get_feature_names()).add_prefix(&#39;Counts_&#39;) # Add the new columns to the original DataFrame speech_df_new = pd.concat([speech_df, cv_df], axis=1, sort=False) print(speech_df_new.head()) . Name Inaugural Address Date text text_clean ... Counts_years 0 George Washington First Inaugural Address Thursday, April 30, 1789 Fellow-Citizens of the Senate and of the House... fellow citizens of the senate and of the house... ... 1 1 George Washington Second Inaugural Address Monday, March 4, 1793 Fellow Citizens: I AM again called upon by th... fellow citizens i am again called upon by th... ... 0 2 John Adams Inaugural Address Saturday, March 4, 1797 WHEN it was first perceived, in early times, t... when it was first perceived in early times t... ... 3 3 Thomas Jefferson First Inaugural Address Wednesday, March 4, 1801 Friends and Fellow-Citizens: CALLED upon to u... friends and fellow citizens called upon to u... ... 0 4 Thomas Jefferson Second Inaugural Address Monday, March 4, 1805 PROCEEDING, fellow-citizens, to that qualifica... proceeding fellow citizens to that qualifica... ... 2 Counts_yet Counts_you Counts_young Counts_your 0 0 5 0 9 1 0 0 0 1 2 0 0 0 1 3 2 7 0 7 4 2 4 0 4 [5 rows x 826 columns] . With the new features combined with the orginial DataFrame they can be now used for ML models or analysis. . . 4.3 Term frequency-inverse document frequency . . 4.3.1 Tf-idf . While counts of occurrences of words can be useful to build models, words that occur many times may skew the results undesirably. To limit these common words from overpowering your model a form of normalization can be used. In this lesson you will be using Term frequency-inverse document frequency (Tf-idf) as was discussed in the video. Tf-idf has the effect of reducing the value of common words, while increasing the weight of words that do not occur in many documents. . # Import TfidfVectorizer from sklearn.feature_extraction.text import TfidfVectorizer # Instantiate TfidfVectorizer tv = TfidfVectorizer(max_features=100, stop_words=&#39;english&#39;) # Fit the vectroizer and transform the data tv_transformed = tv.fit_transform(speech_df[&#39;text_clean&#39;]) # Create a DataFrame with these features tv_df = pd.DataFrame(tv_transformed.toarray(), columns=tv.get_feature_names()).add_prefix(&#39;TFIDF_&#39;) print(tv_df.head()) . TFIDF_action TFIDF_administration TFIDF_america TFIDF_american TFIDF_americans ... TFIDF_war TFIDF_way TFIDF_work TFIDF_world TFIDF_years 0 0.000000 0.133415 0.000000 0.105388 0.0 ... 0.000000 0.060755 0.000000 0.045929 0.052694 1 0.000000 0.261016 0.266097 0.000000 0.0 ... 0.000000 0.000000 0.000000 0.000000 0.000000 2 0.000000 0.092436 0.157058 0.073018 0.0 ... 0.024339 0.000000 0.000000 0.063643 0.073018 3 0.000000 0.092693 0.000000 0.000000 0.0 ... 0.036610 0.000000 0.039277 0.095729 0.000000 4 0.041334 0.039761 0.000000 0.031408 0.0 ... 0.094225 0.000000 0.000000 0.054752 0.062817 [5 rows x 100 columns] . Did you notice that counting the word occurences and calculating the Tf-idf weights are very similar? This is one of the reasons scikit-learn is very popular, a consistent API. . 4.3.2 Inspecting Tf-idf values . After creating Tf-idf features you will often want to understand what are the most highest scored words for each corpus. This can be achieved by isolating the row you want to examine and then sorting the the scores from high to low. . The DataFrame from the last exercise ( tv_df ) is available in your workspace. . # Isolate the row to be examined sample_row = tv_df.iloc[0] # Print the top 5 words of the sorted output print(sample_row.sort_values(ascending=False).head()) TFIDF_government 0.367430 TFIDF_public 0.333237 TFIDF_present 0.315182 TFIDF_duty 0.238637 TFIDF_citizens 0.229644 Name: 0, dtype: float64 . 4.3.3 Transforming unseen data . When creating vectors from text, any transformations that you perform before training a machine learning model, you also need to apply on the new unseen (test) data. To achieve this follow the same approach from the last chapter: fit the vectorizer only on the training data, and apply it to the test data. . For this exercise the speech_df DataFrame has been split in two: . train_speech_df The training set consisting of the first 45 speeches. | test_speech_df The test set consisting of the remaining speeches. | . # Instantiate TfidfVectorizer tv = TfidfVectorizer(max_features=100, stop_words=&#39;english&#39;) # Fit the vectroizer and transform the data tv_transformed = tv.fit_transform(train_speech_df[&#39;text_clean&#39;]) # Transform test data test_tv_transformed = tv.transform(test_speech_df[&#39;text_clean&#39;]) # Create new features for the test set test_tv_df = pd.DataFrame(test_tv_transformed.toarray(), columns=tv.get_feature_names()).add_prefix(&#39;TFIDF_&#39;) print(test_tv_df.head()) . TFIDF_action TFIDF_administration TFIDF_america TFIDF_american TFIDF_authority ... TFIDF_war TFIDF_way TFIDF_work TFIDF_world TFIDF_years 0 0.000000 0.029540 0.233954 0.082703 0.000000 ... 0.079050 0.033313 0.000000 0.299983 0.134749 1 0.000000 0.000000 0.547457 0.036862 0.000000 ... 0.052851 0.066817 0.078999 0.277701 0.126126 2 0.000000 0.000000 0.126987 0.134669 0.000000 ... 0.042907 0.054245 0.096203 0.225452 0.043884 3 0.037094 0.067428 0.267012 0.031463 0.039990 ... 0.030073 0.038020 0.235998 0.237026 0.061516 4 0.000000 0.000000 0.221561 0.156644 0.028442 ... 0.021389 0.081124 0.119894 0.299701 0.153133 [5 rows x 100 columns] . The vectorizer should only be fit on the train set, never on your test set. . . 4.4 N-grams . . 4.4.1 Using longer n-grams . So far you have created features based on individual words in each of the texts. This can be quite powerful when used in a machine learning model but you may be concerned that by looking at words individually a lot of the context is being ignored. To deal with this when creating models you can use n-grams which are sequence of n words grouped together. For example: . bigrams: Sequences of two consecutive words | trigrams: Sequences of three consecutive words | . These can be automatically created in your dataset by specifying the ngram_range argument as a tuple (n1, n2) where all n-grams in the n1 to n2 range are included. . # Import CountVectorizer from sklearn.feature_extraction.text import CountVectorizer # Instantiate a trigram vectorizer cv_trigram_vec = CountVectorizer(max_features=100, stop_words=&#39;english&#39;, ngram_range=(3,3)) # Fit and apply trigram vectorizer cv_trigram = cv_trigram_vec.fit_transform(speech_df[&#39;text_clean&#39;]) # Print the trigram features print(cv_trigram_vec.get_feature_names()) . [&#39;ability preserve protect&#39;, &#39;agriculture commerce manufactures&#39;, &#39;america ideal freedom&#39;, &#39;amity mutual concession&#39;, &#39;anchor peace home&#39;, &#39;ask bow heads&#39;, ...] . Here you can see that by taking sequential word pairings, some context is preserved. . 4.4.2 Finding the most common words . Its always advisable once you have created your features to inspect them to ensure that they are as you would expect. This will allow you to catch errors early, and perhaps influence what further feature engineering you will need to do. . The vectorizer ( cv ) you fit in the last exercise and the sparse array consisting of word counts ( cv_trigram ) is available in your workspace. . # Create a DataFrame of the features cv_tri_df = pd.DataFrame(cv_trigram.toarray(), columns=cv_trigram_vec.get_feature_names()).add_prefix(&#39;Counts_&#39;) # Print the top 5 words in the sorted output print(cv_tri_df.sum().sort_values(ascending=False).head()) . Counts_constitution united states 20 Counts_people united states 13 Counts_preserve protect defend 10 Counts_mr chief justice 10 Counts_president united states 8 dtype: int64 . . 4.5 Wrap-up . . . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/feature-engineering-for-machine-learning-in-python.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/feature-engineering-for-machine-learning-in-python.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post29": {
            "title": "Extreme Gradient Boosting with XGBoost",
            "content": "Extreme Gradient Boosting with XGBoost . This is the memo of the 5th course (23 courses in all) of ‘Machine Learning Scientist with Python’ skill track. . You can find the original course HERE . . ##### PREREQUISITES . Supervised Learning with scikit-learn | Machine Learning with the Experts: School Budgets | . ### Course Description . Do you know the basics of supervised learning and want to use state-of-the-art models on real-world datasets? Gradient boosting is currently one of the most popular techniques for efficient modeling of tabular datasets of all sizes. XGboost is a very fast, scalable implementation of gradient boosting, with models using XGBoost regularly winning online data science competitions and being used at scale across different industries. In this course, you’ll learn how to use this powerful library alongside pandas and scikit-learn to build and tune supervised learning models. You’ll work with real-world datasets to solve classification and regression problems. . ### . Classification with XGBoost | Regression with XGBoost | Fine-tuning your XGBoost model | Using XGBoost in pipelines | . 1. Classification with XGBoost . . This chapter will introduce you to the fundamental idea behind XGBoost—boosted learners. Once you understand how XGBoost works, you’ll apply it to solve a common classification problem found in industry: predicting whether a customer will stop being a customer at some point in the future. . ### 1.1 Reminder of supervised learning . . 1.1.1 Which of these is a classification problem? . Given below are 4 potential machine learning problems you might encounter in the wild. Pick the one that is a classification problem. . Given past performance of stocks and various other financial data, predicting the exact price of a given stock (Google) tomorrow. | Given a large dataset of user behaviors on a website, generating an informative segmentation of the users based on their behaviors. | Predicting whether a given user will click on an ad given the ad content and metadata associated with the user. | Given a user’s past behavior on a video platform, presenting him/her with a series of recommended videos to watch next. | . 1.1.2 Which of these is a binary classification problem? . A classification problem involves predicting the category a given data point belongs to out of a finite set of possible categories. Depending on how many possible categories there are to predict, a classification problem can be either binary or multi-class. Let’s do another quick refresher here. Your job is to pick the binary classification problem out of the following list of supervised learning problems. . Predicting whether a given image contains a cat. | Predicting the emotional valence of a sentence (Valence can be positive, negative, or neutral). | Recommending the most tax-efficient strategy for tax filing in an automated accounting system. | Given a list of symptoms, generating a rank-ordered list of most likely diseases. | . . 1.2 Introducing XGBoost . . 1.2.1 XGBoost: Fit/Predict . It’s time to create your first XGBoost model! As Sergey showed you in the video, you can use the scikit-learn .fit() / .predict() paradigm that you are already familiar to build your XGBoost models, as the xgboost library has a scikit-learn compatible API! . Here, you’ll be working with churn data. This dataset contains imaginary data from a ride-sharing app with user behaviors over their first month of app usage in a set of imaginary cities as well as whether they used the service 5 months after sign-up. It has been pre-loaded for you into a DataFrame called churn_data – explore it in the Shell! . Your goal is to use the first month’s worth of data to predict whether the app’s users will remain users of the service at the 5 month mark. This is a typical setup for a churn prediction problem. To do this, you’ll split the data into training and test sets, fit a small xgboost model on the training set, and evaluate its performance on the test set by computing its accuracy. . pandas and numpy have been imported as pd and np , and train_test_split has been imported from sklearn.model_selection . Additionally, the arrays for the features and the target have been created as X and y . . churn_data.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 50000 entries, 0 to 49999 Data columns (total 13 columns): avg_dist 50000 non-null float64 avg_rating_by_driver 49799 non-null float64 avg_rating_of_driver 41878 non-null float64 avg_inc_price 50000 non-null float64 inc_pct 50000 non-null float64 weekday_pct 50000 non-null float64 fancy_car_user 50000 non-null bool city_Carthag 50000 non-null int64 city_Harko 50000 non-null int64 phone_iPhone 50000 non-null int64 first_month_cat_more_1_trip 50000 non-null int64 first_month_cat_no_trips 50000 non-null int64 month_5_still_here 50000 non-null int64 dtypes: bool(1), float64(6), int64(6) memory usage: 4.6 MB . churn_data.head(2) avg_dist avg_rating_by_driver avg_rating_of_driver avg_inc_price inc_pct ... city_Harko phone_iPhone first_month_cat_more_1_trip first_month_cat_no_trips month_5_still_here 0 3.67 5.0 4.7 1.1 15.4 ... 1 1 1 0 1 1 8.26 5.0 5.0 1.0 0.0 ... 0 0 0 1 0 [2 rows x 13 columns] . # Import xgboost import xgboost as xgb # Create arrays for the features and the target: X, y X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1] # Create the training and test sets X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123) # Instantiate the XGBClassifier: xg_cl xg_cl = xgb.XGBClassifier(objective=&#39;binary:logistic&#39;, n_estimators=10, seed=123) # Fit the classifier to the training set xg_cl.fit(X_train, y_train) # Predict the labels of the test set: preds preds = xg_cl.predict(X_test) # Compute the accuracy: accuracy accuracy = float(np.sum(preds==y_test))/y_test.shape[0] print(&quot;accuracy: %f&quot; % (accuracy)) # accuracy: 0.743300 . Your model has an accuracy of around 74%. In Chapter 3, you’ll learn about ways to fine tune your XGBoost models. For now, let’s refresh our memories on how decision trees work. . . 1.3 What is a decision tree? . . 1.3.1 Decision trees . Your task in this exercise is to make a simple decision tree using scikit-learn’s DecisionTreeClassifier on the breast cancer dataset that comes pre-loaded with scikit-learn. . This dataset contains numeric measurements of various dimensions of individual tumors (such as perimeter and texture) from breast biopsies and a single outcome value (the tumor is either malignant, or benign). . We’ve preloaded the dataset of samples (measurements) into X and the target values per tumor into y . Now, you have to split the complete dataset into training and testing sets, and then train a DecisionTreeClassifier . You’ll specify a parameter called max_depth . Many other parameters can be modified within this model, and you can check all of them out here . . # Import the necessary modules from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier # Create the training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123) # Instantiate the classifier: dt_clf_4 dt_clf_4 = DecisionTreeClassifier(max_depth=4) # Fit the classifier to the training set dt_clf_4.fit(X_train, y_train) # Predict the labels of the test set: y_pred_4 y_pred_4 = dt_clf_4.predict(X_test) # Compute the accuracy of the predictions: accuracy accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0] print(&quot;accuracy:&quot;, accuracy) # accuracy: 0.9649122807017544 . It’s now time to learn about what gives XGBoost its state-of-the-art performance: Boosting. . . 1.4 What is Boosting? . . https://xgboost.readthedocs.io/en/latest/tutorials/model.html . 1.4.1 Measuring accuracy . You’ll now practice using XGBoost’s learning API through its baked in cross-validation capabilities. As Sergey discussed in the previous video, XGBoost gets its lauded performance and efficiency gains by utilizing its own optimized data structure for datasets called a DMatrix . . In the previous exercise, the input datasets were converted into DMatrix data on the fly, but when you use the xgboost cv object, you have to first explicitly convert your data into a DMatrix . So, that’s what you will do here before running cross-validation on churn_data . . # Create the DMatrix: churn_dmatrix churn_dmatrix = xgb.DMatrix(data=X, label=y) # Create the parameter dictionary: params params = {&quot;objective&quot;:&quot;reg:logistic&quot;, &quot;max_depth&quot;:3} # Perform cross-validation: cv_results cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=3, num_boost_round=5, metrics=&quot;error&quot;, as_pandas=True, seed=123) # Print cv_results print(cv_results) # Print the accuracy print(((1-cv_results[&quot;test-error-mean&quot;]).iloc[-1])) . train-error-mean train-error-std test-error-mean test-error-std 0 0.28232 0.002366 0.28378 0.001932 1 0.26951 0.001855 0.27190 0.001932 2 0.25605 0.003213 0.25798 0.003963 3 0.25090 0.001845 0.25434 0.003827 4 0.24654 0.001981 0.24852 0.000934 0.75148 . cv_results stores the training and test mean and standard deviation of the error per boosting round (tree built) as a DataFrame. From cv_results , the final round &#39;test-error-mean&#39; is extracted and converted into an accuracy, where accuracy is 1-error . The final accuracy of around 75% is an improvement from earlier! . 1.4.2 Measuring AUC . Now that you’ve used cross-validation to compute average out-of-sample accuracy (after converting from an error), it’s very easy to compute any other metric you might be interested in. All you have to do is pass it (or a list of metrics) in as an argument to the metrics parameter of xgb.cv() . . Your job in this exercise is to compute another common metric used in binary classification – the area under the curve ( &quot;auc&quot; ). As before, churn_data is available in your workspace, along with the DMatrix churn_dmatrix and parameter dictionary params . . # Perform cross_validation: cv_results cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=3, num_boost_round=5, metrics=&quot;auc&quot;, as_pandas=True, seed=123) # Print cv_results print(cv_results) # Print the AUC print((cv_results[&quot;test-auc-mean&quot;]).iloc[-1]) . train-auc-mean train-auc-std test-auc-mean test-auc-std 0 0.768893 0.001544 0.767863 0.002820 1 0.790864 0.006758 0.789157 0.006846 2 0.815872 0.003900 0.814476 0.005997 3 0.822959 0.002018 0.821682 0.003912 4 0.827528 0.000769 0.826191 0.001937 0.826191 . An AUC of 0.84 is quite strong. As you have seen, XGBoost’s learning API makes it very easy to compute any metric you may be interested in. In Chapter 3, you’ll learn about techniques to fine-tune your XGBoost models to improve their performance even further. For now, it’s time to learn a little about exactly when to use XGBoost. . . 1.5 When should I use XGBoost? . . 1.5.1 Using XGBoost . XGBoost is a powerful library that scales very well to many samples and works for a variety of supervised learning problems. That said, as Sergey described in the video, you shouldn’t always pick it as your default machine learning library when starting a new project, since there are some situations in which it is not the best option. In this exercise, your job is to consider the below examples and select the one which would be the best use of XGBoost. . Visualizing the similarity between stocks by comparing the time series of their historical prices relative to each other. | Predicting whether a person will develop cancer using genetic data with millions of genes, 23 examples of genomes of people that didn’t develop cancer, 3 genomes of people who wound up getting cancer. | Clustering documents into topics based on the terms used in them. | Predicting the likelihood that a given user will click an ad from a very large clickstream log with millions of users and their web interactions. | . 2. Regression with XGBoost . . After a brief review of supervised regression, you’ll apply XGBoost to the regression task of predicting house prices in Ames, Iowa. You’ll learn about the two kinds of base learners that XGboost can use as its weak learners, and review how to evaluate the quality of your regression models. . 2.1 Regression review . 2.1.1 Which of these is a regression problem? . Here are 4 potential machine learning problems you might encounter in the wild. Pick the one that is a clear example of a regression problem. . Recommending a restaurant to a user given their past history of restaurant visits and reviews for a dining aggregator app. | Predicting which of several thousand diseases a given person is most likely to have given their symptoms. | Tagging an email as spam/not spam based on its content and metadata (sender, time sent, etc.). | Predicting the expected payout of an auto insurance claim given claim properties (car, accident type, driver prior history, etc.). | . . 2.2 Objective (loss) functions and base learners . . 2.2.1 Decision trees as base learners . It’s now time to build an XGBoost model to predict house prices – not in Boston, Massachusetts, as you saw in the video, but in Ames, Iowa! This dataset of housing prices has been pre-loaded into a DataFrame called df . If you explore it in the Shell, you’ll see that there are a variety of features about the house and its location in the city. . In this exercise, your goal is to use trees as base learners. By default, XGBoost uses trees as base learners, so you don’t have to specify that you want to use trees here with booster=&quot;gbtree&quot; . . xgboost has been imported as xgb and the arrays for the features and the target are available in X and y , respectively. . df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1460 entries, 0 to 1459 Data columns (total 57 columns): MSSubClass 1460 non-null int64 LotFrontage 1460 non-null float64 LotArea 1460 non-null int64 OverallQual 1460 non-null int64 OverallCond 1460 non-null int64 YearBuilt 1460 non-null int64 Remodeled 1460 non-null int64 GrLivArea 1460 non-null int64 BsmtFullBath 1460 non-null int64 BsmtHalfBath 1460 non-null int64 FullBath 1460 non-null int64 HalfBath 1460 non-null int64 BedroomAbvGr 1460 non-null int64 Fireplaces 1460 non-null int64 GarageArea 1460 non-null int64 MSZoning_FV 1460 non-null int64 MSZoning_RH 1460 non-null int64 MSZoning_RL 1460 non-null int64 MSZoning_RM 1460 non-null int64 Neighborhood_Blueste 1460 non-null int64 Neighborhood_BrDale 1460 non-null int64 Neighborhood_BrkSide 1460 non-null int64 Neighborhood_ClearCr 1460 non-null int64 Neighborhood_CollgCr 1460 non-null int64 Neighborhood_Crawfor 1460 non-null int64 Neighborhood_Edwards 1460 non-null int64 Neighborhood_Gilbert 1460 non-null int64 Neighborhood_IDOTRR 1460 non-null int64 Neighborhood_MeadowV 1460 non-null int64 Neighborhood_Mitchel 1460 non-null int64 Neighborhood_NAmes 1460 non-null int64 Neighborhood_NPkVill 1460 non-null int64 Neighborhood_NWAmes 1460 non-null int64 Neighborhood_NoRidge 1460 non-null int64 Neighborhood_NridgHt 1460 non-null int64 Neighborhood_OldTown 1460 non-null int64 Neighborhood_SWISU 1460 non-null int64 Neighborhood_Sawyer 1460 non-null int64 Neighborhood_SawyerW 1460 non-null int64 Neighborhood_Somerst 1460 non-null int64 Neighborhood_StoneBr 1460 non-null int64 Neighborhood_Timber 1460 non-null int64 Neighborhood_Veenker 1460 non-null int64 BldgType_2fmCon 1460 non-null int64 BldgType_Duplex 1460 non-null int64 BldgType_Twnhs 1460 non-null int64 BldgType_TwnhsE 1460 non-null int64 HouseStyle_1.5Unf 1460 non-null int64 HouseStyle_1Story 1460 non-null int64 HouseStyle_2.5Fin 1460 non-null int64 HouseStyle_2.5Unf 1460 non-null int64 HouseStyle_2Story 1460 non-null int64 HouseStyle_SFoyer 1460 non-null int64 HouseStyle_SLvl 1460 non-null int64 PavedDrive_P 1460 non-null int64 PavedDrive_Y 1460 non-null int64 SalePrice 1460 non-null int64 dtypes: float64(1), int64(56) memory usage: 650.2 KB . # Create the training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123) # Instantiate the XGBRegressor: xg_reg xg_reg = xgb.XGBRegressor(objective=&quot;reg:linear&quot;, n_estimators=10, seed=123) # Fit the regressor to the training set xg_reg.fit(X_train, y_train) # Predict the labels of the test set: preds preds = xg_reg.predict(X_test) # Compute the rmse: rmse rmse = np.sqrt(mean_squared_error(y_test, preds)) print(&quot;RMSE: %f&quot; % (rmse)) # RMSE: 78847.401758 . Next, you’ll train an XGBoost model using linear base learners and XGBoost’s learning API. Will it perform better or worse? . 2.2.2 Linear base learners . Now that you’ve used trees as base models in XGBoost, let’s use the other kind of base model that can be used with XGBoost – a linear learner. This model, although not as commonly used in XGBoost, allows you to create a regularized linear regression using XGBoost’s powerful learning API. However, because it’s uncommon, you have to use XGBoost’s own non-scikit-learn compatible functions to build the model, such as xgb.train() . . In order to do this you must create the parameter dictionary that describes the kind of booster you want to use (similarly to how you created the dictionary in Chapter 1 when you used xgb.cv() ). The key-value pair that defines the booster type (base model) you need is &quot;booster&quot;:&quot;gblinear&quot; . . Once you’ve created the model, you can use the .train() and .predict() methods of the model just like you’ve done in the past. . Here, the data has already been split into training and testing sets, so you can dive right into creating the DMatrix objects required by the XGBoost learning API. . # Convert the training and testing sets into DMatrixes: DM_train, DM_test DM_train = xgb.DMatrix(data=X_train, label=y_train) DM_test = xgb.DMatrix(data=X_test, label=y_test) # Create the parameter dictionary: params params = {&quot;booster&quot;:&quot;gblinear&quot;, &quot;objective&quot;:&quot;reg:linear&quot;} # Train the model: xg_reg xg_reg = xgb.train(params = params, dtrain=DM_train, num_boost_round=5) # Predict the labels of the test set: preds preds = xg_reg.predict(DM_test) # Compute and print the RMSE rmse = np.sqrt(mean_squared_error(y_test,preds)) print(&quot;RMSE: %f&quot; % (rmse)) # RMSE: 44159.721661 . It looks like linear base learners performed better! . 2.2.3 Evaluating model quality . It’s now time to begin evaluating model quality. . Here, you will compare the RMSE and MAE of a cross-validated XGBoost model on the Ames housing data. As in previous exercises, all necessary modules have been pre-loaded and the data is available in the DataFrame df . . # Create the DMatrix: housing_dmatrix housing_dmatrix = xgb.DMatrix(data=X, label=y) # Create the parameter dictionary: params params = {&quot;objective&quot;:&quot;reg:linear&quot;, &quot;max_depth&quot;:4} # Perform cross-validation: cv_results cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=&#39;rmse&#39;, as_pandas=True, seed=123) # Print cv_results print(cv_results) # Extract and print final boosting round metric print((cv_results[&quot;test-rmse-mean&quot;]).tail(1)) . train-rmse-mean train-rmse-std test-rmse-mean test-rmse-std 0 141767.535156 429.449158 142980.433594 1193.789595 1 102832.542969 322.468977 104891.392578 1223.157953 2 75872.617188 266.473250 79478.937500 1601.344539 3 57245.651368 273.626997 62411.924804 2220.148314 4 44401.295899 316.422824 51348.281250 2963.379118 4 51348.28125 Name: test-rmse-mean, dtype: float64 . # Create the DMatrix: housing_dmatrix housing_dmatrix = xgb.DMatrix(data=X, label=y) # Create the parameter dictionary: params params = {&quot;objective&quot;:&quot;reg:linear&quot;, &quot;max_depth&quot;:4} # Perform cross-validation: cv_results cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=&#39;mae&#39;, as_pandas=True, seed=123) # Print cv_results print(cv_results) # Extract and print final boosting round metric print((cv_results[&quot;test-mae-mean&quot;]).tail(1)) . train-mae-mean train-mae-std test-mae-mean test-mae-std 0 127343.570313 668.341212 127633.986328 2403.992416 1 89770.060547 456.948723 90122.496093 2107.910017 2 63580.789063 263.407042 64278.561524 1887.563581 3 45633.140625 151.885298 46819.169922 1459.812547 4 33587.090821 87.001007 35670.651367 1140.608182 4 35670.651367 Name: test-mae-mean, dtype: float64 . . 2.3 Regularization and base learners in XGBoost . . 2.3.1 Using regularization in XGBoost . Having seen an example of l1 regularization in the video, you’ll now vary the l2 regularization penalty – also known as &quot;lambda&quot; – and see its effect on overall model performance on the Ames housing dataset. . # Create the DMatrix: housing_dmatrix housing_dmatrix = xgb.DMatrix(data=X, label=y) reg_params = [1, 10, 100] # Create the initial parameter dictionary for varying l2 strength: params params = {&quot;objective&quot;:&quot;reg:squarederror&quot;,&quot;max_depth&quot;:3} # Create an empty list for storing rmses as a function of l2 complexity rmses_l2 = [] # Iterate over reg_params for reg in reg_params: # Update l2 strength params[&quot;lambda&quot;] = reg # Pass this updated param dictionary into cv cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, num_boost_round=5, metrics=&quot;rmse&quot;, as_pandas=True, seed=123) # Append best rmse (final round) to rmses_l2 rmses_l2.append(cv_results_rmse[&quot;test-rmse-mean&quot;].tail(1).values[0]) # Look at best rmse per l2 param print(&quot;Best rmse as a function of l2:&quot;) print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=[&quot;l2&quot;, &quot;rmse&quot;])) . Best rmse as a function of l2: l2 rmse 0 1 52275.357421 1 10 57746.064453 2 100 76624.628907 . It looks like as as the value of &#39;lambda&#39; increases, so does the RMSE. . 2.3.2 Visualizing individual XGBoost trees . Now that you’ve used XGBoost to both build and evaluate regression as well as classification models, you should get a handle on how to visually explore your models. Here, you will visualize individual trees from the fully boosted model that XGBoost creates using the entire housing dataset. . XGBoost has a plot_tree() function that makes this type of visualization easy. Once you train a model using the XGBoost learning API, you can pass it to the plot_tree() function along with the number of trees you want to plot using the num_trees argument. . # Create the DMatrix: housing_dmatrix housing_dmatrix = xgb.DMatrix(data=X, label=y) # Create the parameter dictionary: params params = {&quot;objective&quot;:&quot;reg:linear&quot;, &quot;max_depth&quot;:2} # Train the model: xg_reg xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10) # Plot the first tree xgb.plot_tree(xg_reg, num_trees=0) plt.show() # Plot the fifth tree xgb.plot_tree(xg_reg, num_trees=4) plt.show() # Plot the last tree sideways xgb.plot_tree(xg_reg, num_trees=9, rankdir=&quot;LR&quot;) plt.show() . . Have a look at each of the plots. They provide insight into how the model arrived at its final decisions and what splits it made to arrive at those decisions. This allows us to identify which features are the most important in determining house price. In the next exercise, you’ll learn another way of visualizing feature importances. . 2.3.3 Visualizing feature importances: What features are most important in my dataset . Another way to visualize your XGBoost models is to examine the importance of each feature column in the original dataset within the model. . One simple way of doing this involves counting the number of times each feature is split on across all boosting rounds (trees) in the model, and then visualizing the result as a bar graph, with the features ordered according to how many times they appear. XGBoost has a plot_importance() function that allows you to do exactly this, and you’ll get a chance to use it in this exercise! . # Create the DMatrix: housing_dmatrix housing_dmatrix = xgb.DMatrix(data=X, label=y) # Create the parameter dictionary: params params = {&quot;objective&quot;:&quot;reg:squarederror&quot;, &quot;max_depth&quot;:4} # Train the model: xg_reg xg_reg = xgb.train(dtrain=housing_dmatrix, params=params, num_boost_round=10) # Plot the feature importances xgb.plot_importance(xg_reg) plt.show() . . It looks like GrLivArea is the most important feature. . 3. Fine-tuning your XGBoost model . . This chapter will teach you how to make your XGBoost models as performant as possible. You’ll learn about the variety of parameters that can be adjusted to alter the behavior of XGBoost and how to tune them efficiently so that you can supercharge the performance of your models. . 3.1 Why tune your model? . 3.1.1 When is tuning your model a bad idea? . Now that you’ve seen the effect that tuning has on the overall performance of your XGBoost model, let’s turn the question on its head and see if you can figure out when tuning your model might not be the best idea. Given that model tuning can be time-intensive and complicated, which of the following scenarios would NOT call for careful tuning of your model ? . You have lots of examples from some dataset and very many features at your disposal. | You are very short on time before you must push an initial model to production and have little data to train your model on. | You have access to a multi-core (64 cores) server with lots of memory (200GB RAM) and no time constraints. | You must squeeze out every last bit of performance out of your xgboost model. | . 3.1.2 Tuning the number of boosting rounds . Let’s start with parameter tuning by seeing how the number of boosting rounds (number of trees you build) impacts the out-of-sample performance of your XGBoost model. You’ll use xgb.cv() inside a for loop and build one model per num_boost_round parameter. . Here, you’ll continue working with the Ames housing dataset. The features are available in the array X , and the target vector is contained in y . . # Create the DMatrix: housing_dmatrix housing_dmatrix = xgb.DMatrix(data=X, label=y) # Create the parameter dictionary for each tree: params params = {&quot;objective&quot;:&quot;reg:squarederror&quot;, &quot;max_depth&quot;:3} # Create list of number of boosting rounds num_rounds = [5, 10, 15] # Empty list to store final round rmse per XGBoost model final_rmse_per_round = [] # Iterate over num_rounds and build one model per num_boost_round parameter for curr_num_rounds in num_rounds: # Perform cross-validation: cv_results cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=&quot;rmse&quot;, as_pandas=True, seed=123) # Append final round RMSE final_rmse_per_round.append(cv_results[&quot;test-rmse-mean&quot;].tail().values[-1]) # Print the resultant DataFrame num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round)) print(pd.DataFrame(num_rounds_rmses,columns=[&quot;num_boosting_rounds&quot;,&quot;rmse&quot;])) . num_boosting_rounds rmse 0 5 50903.300781 1 10 34774.194011 2 15 32895.097005 . As you can see, increasing the number of boosting rounds decreases the RMSE. . 3.1.3 Automated boosting round selection using early_stopping . Now, instead of attempting to cherry pick the best possible number of boosting rounds, you can very easily have XGBoost automatically select the number of boosting rounds for you within xgb.cv() . This is done using a technique called early stopping . . Early stopping works by testing the XGBoost model after every boosting round against a hold-out dataset and stopping the creation of additional boosting rounds (thereby finishing training of the model early) if the hold-out metric ( &quot;rmse&quot; in our case) does not improve for a given number of rounds. Here you will use the early_stopping_rounds parameter in xgb.cv() with a large possible number of boosting rounds (50). Bear in mind that if the holdout metric continuously improves up through when num_boosting_rounds is reached, then early stopping does not occur. . Here, the DMatrix and parameter dictionary have been created for you. Your task is to use cross-validation with early stopping. Go for it! . # Create your housing DMatrix: housing_dmatrix housing_dmatrix = xgb.DMatrix(data=X, label=y) # Create the parameter dictionary for each tree: params params = {&quot;objective&quot;:&quot;reg:squarederror&quot;, &quot;max_depth&quot;:4} # Perform cross-validation with early stopping: cv_results cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, num_boost_round=50, early_stopping_rounds=10, metrics=&#39;rmse&#39;, as_pandas=True, seed=123) # Print cv_results print(cv_results) . train-rmse-mean train-rmse-std test-rmse-mean test-rmse-std 0 141871.630208 403.632409 142640.651042 705.571916 1 103057.031250 73.772931 104907.666667 111.114933 2 75975.963541 253.734987 79262.059895 563.766991 3 57420.529948 521.653556 61620.135417 1087.690754 4 44552.955729 544.169200 50437.561198 1846.448222 ... 45 11356.552734 565.368794 30758.543620 1947.456345 46 11193.556966 552.298481 30729.972005 1985.699316 47 11071.315430 604.089695 30732.664062 1966.998275 48 10950.778646 574.862348 30712.240885 1957.751118 49 10824.865560 576.666458 30720.854818 1950.511520 . . 3.2 Overview of XGBoost’s hyperparameters . . Linear based models are rarely used! . 3.2.1 Tuning eta . It’s time to practice tuning other XGBoost hyperparameters in earnest and observing their effect on model performance! You’ll begin by tuning the &quot;eta&quot; , also known as the learning rate. . The learning rate in XGBoost is a parameter that can range between 0 and 1 , with higher values of &quot;eta&quot; penalizing feature weights more strongly, causing much stronger regularization. . # Create your housing DMatrix: housing_dmatrix housing_dmatrix = xgb.DMatrix(data=X, label=y) # Create the parameter dictionary for each tree (boosting round) params = {&quot;objective&quot;:&quot;reg:squarederror&quot;, &quot;max_depth&quot;:3} # Create list of eta values and empty list to store final round rmse per xgboost model eta_vals = [0.001, 0.01, 0.1] best_rmse = [] # Systematically vary the eta for curr_val in eta_vals: params[&quot;eta&quot;] = curr_val # Perform cross-validation: cv_results cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=5, metrics=&quot;rmse&quot;, as_pandas=True, seed=123) # Append the final round rmse to best_rmse best_rmse.append(cv_results[&quot;test-rmse-mean&quot;].tail().values[-1]) # Print the resultant DataFrame print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[&quot;eta&quot;,&quot;best_rmse&quot;])) &#39;&#39;&#39; eta best_rmse 0 0.001 196653.989583 1 0.010 188532.578125 2 0.100 122784.299479 &#39;&#39;&#39; . 3.2.2 Tuning max_depth . In this exercise, your job is to tune max_depth , which is the parameter that dictates the maximum depth that each tree in a boosting round can grow to. Smaller values will lead to shallower trees, and larger values to deeper trees. . # Create your housing DMatrix housing_dmatrix = xgb.DMatrix(data=X,label=y) # Create the parameter dictionary params = {&quot;objective&quot;:&quot;reg:squarederror&quot;} # Create list of max_depth values max_depths = [2,5,10,20] best_rmse = [] # Systematically vary the max_depth for curr_val in max_depths: params[&quot;max_depths&quot;] = curr_val # Perform cross-validation cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, early_stopping_rounds=5, num_boost_round=10, metrics=&#39;rmse&#39;, seed=123, as_pandas=True) # Append the final round rmse to best_rmse best_rmse.append(cv_results[&quot;test-rmse-mean&quot;].tail().values[-1]) # Print the resultant DataFrame print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[&quot;max_depth&quot;,&quot;best_rmse&quot;])) &#39;&#39;&#39; max_depth best_rmse 0 2 35922.521485 1 5 35922.521485 2 10 35922.521485 3 20 35922.521485 &#39;&#39;&#39; . 3.2.3 Tuning colsample_bytree . Now, it’s time to tune &quot;colsample_bytree&quot; . You’ve already seen this if you’ve ever worked with scikit-learn’s RandomForestClassifier or RandomForestRegressor , where it just was called max_features . In both xgboost and sklearn , this parameter (although named differently) simply specifies the fraction of features to choose from at every split in a given tree. In xgboost , colsample_bytree must be specified as a float between 0 and 1. . # Create your housing DMatrix housing_dmatrix = xgb.DMatrix(data=X,label=y) # Create the parameter dictionary params={&quot;objective&quot;:&quot;reg:squarederror&quot;,&quot;max_depth&quot;:3} # Create list of hyperparameter values: colsample_bytree_vals colsample_bytree_vals = [0.1,0.5,0.8,1] best_rmse = [] # Systematically vary the hyperparameter value for curr_val in colsample_bytree_vals: params[&#39;colsample_bytree&#39;] = curr_val # Perform cross-validation cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, num_boost_round=10, early_stopping_rounds=5, metrics=&quot;rmse&quot;, as_pandas=True, seed=123) # Append the final round rmse to best_rmse best_rmse.append(cv_results[&quot;test-rmse-mean&quot;].tail().values[-1]) # Print the resultant DataFrame print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[&quot;colsample_bytree&quot;,&quot;best_rmse&quot;])) &#39;&#39;&#39; colsample_bytree best_rmse 0 0.1 48193.453125 1 0.5 36013.542968 2 0.8 35932.962891 3 1.0 35836.042969 &#39;&#39;&#39; . There are several other individual parameters that you can tune, such as &quot;subsample&quot; , which dictates the fraction of the training data that is used during any given boosting round. Next up: Grid Search and Random Search to tune XGBoost hyperparameters more efficiently! . . 3.3 Review of grid search and random search . . 3.3.1 Grid search with XGBoost . Now that you’ve learned how to tune parameters individually with XGBoost, let’s take your parameter tuning to the next level by using scikit-learn’s GridSearch and RandomizedSearch capabilities with internal cross-validation using the GridSearchCV and RandomizedSearchCV functions. You will use these to find the best model exhaustively from a collection of possible parameter values across multiple parameters simultaneously. Let’s get to work, starting with GridSearchCV ! . # Create your housing DMatrix: housing_dmatrix housing_dmatrix = xgb.DMatrix(data=X, label=y) # Create the parameter grid: gbm_param_grid gbm_param_grid = { &#39;colsample_bytree&#39;: [0.3, 0.7], &#39;n_estimators&#39;: [50], &#39;max_depth&#39;: [2, 5] } # Instantiate the regressor: gbm gbm = xgb.XGBRegressor() # Perform grid search: grid_mse grid_mse = GridSearchCV(param_grid=gbm_param_grid, estimator=gbm, scoring=&#39;neg_mean_squared_error&#39;, cv=4, verbose=1) # Fit grid_mse to the data grid_mse.fit(X, y) # Print the best parameters and lowest RMSE print(&quot;Best parameters found: &quot;, grid_mse.best_params_) print(&quot;Lowest RMSE found: &quot;, np.sqrt(np.abs(grid_mse.best_score_))) &#39;&#39;&#39; Best parameters found: {&#39;colsample_bytree&#39;: 0.7, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 50} Lowest RMSE found: 29916.562522854438 &#39;&#39;&#39; . 3.3.2 Random search with XGBoost . Often, GridSearchCV can be really time consuming, so in practice, you may want to use RandomizedSearchCV instead, as you will do in this exercise. The good news is you only have to make a few modifications to your GridSearchCV code to do RandomizedSearchCV . The key difference is you have to specify a param_distributions parameter instead of a param_grid parameter. . # Create the parameter grid: gbm_param_grid gbm_param_grid = { &#39;n_estimators&#39;: [25], &#39;max_depth&#39;: range(2, 12) } # Instantiate the regressor: gbm gbm = xgb.XGBRegressor(n_estimators=10) # Perform random search: grid_mse randomized_mse = RandomizedSearchCV(param_distributions=gbm_param_grid, estimator=gbm, scoring=&#39;neg_mean_squared_error&#39;, n_iter=5, cv=4, verbose=1) # Fit randomized_mse to the data randomized_mse.fit(X, y) # Print the best parameters and lowest RMSE print(&quot;Best parameters found: &quot;, randomized_mse.best_params_) print(&quot;Lowest RMSE found: &quot;, np.sqrt(np.abs(randomized_mse.best_score_))) &#39;&#39;&#39; Fitting 4 folds for each of 5 candidates, totalling 20 fits Best parameters found: {&#39;n_estimators&#39;: 25, &#39;max_depth&#39;: 6} Lowest RMSE found: 36909.98213965752 &#39;&#39;&#39; . . 3.4 Limits of grid search and random search . The search space size can be massive for Grid Search in certain cases, whereas for Random Search the number of hyperparameters has a significant effect on how long it takes to run. . 4. Using XGBoost in pipelines . . Take your XGBoost skills to the next level by incorporating your models into two end-to-end machine learning pipelines. You’ll learn how to tune the most important XGBoost hyperparameters efficiently within a pipeline, and get an introduction to some more advanced preprocessing techniques. . 4.1 Review of pipelines using sklearn . . 4.1.1 Exploratory data analysis . Before diving into the nitty gritty of pipelines and preprocessing, let’s do some exploratory analysis of the original, unprocessed Ames housing dataset . When you worked with this data in previous chapters, we preprocessed it for you so you could focus on the core XGBoost concepts. In this chapter, you’ll do the preprocessing yourself! . A smaller version of this original, unprocessed dataset has been pre-loaded into a pandas DataFrame called df . Your task is to explore df in the Shell and pick the option that is incorrect . The larger purpose of this exercise is to understand the kinds of transformations you will need to perform in order to be able to use XGBoost. . df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1460 entries, 0 to 1459 Data columns (total 21 columns): MSSubClass 1460 non-null int64 LotFrontage 1201 non-null float64 LotArea 1460 non-null int64 OverallQual 1460 non-null int64 OverallCond 1460 non-null int64 YearBuilt 1460 non-null int64 Remodeled 1460 non-null int64 GrLivArea 1460 non-null int64 BsmtFullBath 1460 non-null int64 BsmtHalfBath 1460 non-null int64 FullBath 1460 non-null int64 HalfBath 1460 non-null int64 BedroomAbvGr 1460 non-null int64 Fireplaces 1460 non-null int64 GarageArea 1460 non-null int64 MSZoning 1460 non-null object PavedDrive 1460 non-null object Neighborhood 1460 non-null object BldgType 1460 non-null object HouseStyle 1460 non-null object SalePrice 1460 non-null int64 dtypes: float64(1), int64(15), object(5) memory usage: 239.6+ KB . 4.1.2 Encoding categorical columns I: LabelEncoder . Now that you’ve seen what will need to be done to get the housing data ready for XGBoost, let’s go through the process step-by-step. . First, you will need to fill in missing values – as you saw previously, the column LotFrontage has many missing values. Then, you will need to encode any categorical columns in the dataset using one-hot encoding so that they are encoded numerically. You can watch this video from Supervised Learning with scikit-learn for a refresher on the idea. . The data has five categorical columns: MSZoning , PavedDrive , Neighborhood , BldgType , and HouseStyle . Scikit-learn has a LabelEncoder function that converts the values in each categorical column into integers. You’ll practice using this here. . # Import LabelEncoder from sklearn.preprocessing import LabelEncoder # Fill missing values with 0 df.LotFrontage = df.LotFrontage.fillna(0) # Create a boolean mask for categorical columns categorical_mask = (df.dtypes == object) # Get list of categorical column names categorical_columns = df.columns[categorical_mask].tolist() # Print the head of the categorical columns print(df[categorical_columns].head()) # Create LabelEncoder object: le le = LabelEncoder() # Apply LabelEncoder to categorical columns df[categorical_columns] = df[categorical_columns].apply(lambda x: le.fit_transform(x)) # Print the head of the LabelEncoded categorical columns print(df[categorical_columns].head()) . MSZoning PavedDrive Neighborhood BldgType HouseStyle 0 RL Y CollgCr 1Fam 2Story 1 RL Y Veenker 1Fam 1Story 2 RL Y CollgCr 1Fam 2Story 3 RL Y Crawfor 1Fam 2Story 4 RL Y NoRidge 1Fam 2Story MSZoning PavedDrive Neighborhood BldgType HouseStyle 0 3 2 5 0 5 1 3 2 24 0 2 2 3 2 5 0 5 3 3 2 6 0 5 4 3 2 15 0 5 . 4.1.3 Encoding categorical columns II: OneHotEncoder . Okay – so you have your categorical columns encoded numerically. Can you now move onto using pipelines and XGBoost? Not yet! In the categorical columns of this dataset, there is no natural ordering between the entries. As an example: Using LabelEncoder , the CollgCr Neighborhood was encoded as 5 , while the Veenker Neighborhood was encoded as 24 , and Crawfor as 6 . Is Veenker “greater” than Crawfor and CollgCr ? No – and allowing the model to assume this natural ordering may result in poor performance. . As a result, there is another step needed: You have to apply a one-hot encoding to create binary, or “dummy” variables. You can do this using scikit-learn’s OneHotEncoder . . # Import OneHotEncoder from sklearn.preprocessing import OneHotEncoder # Create OneHotEncoder: ohe ohe = OneHotEncoder(categorical_features=categorical_mask, sparse=False) # Apply OneHotEncoder to categorical columns - output is no longer a dataframe: df_encoded df_encoded = ohe.fit_transform(df) # Print the shape of the original DataFrame print(df.shape) # (1460, 21) # Print the shape of the transformed array print(df_encoded.shape) # (1460, 62) . 4.1.4 Encoding categorical columns III: DictVectorizer . Alright, one final trick before you dive into pipelines. The two step process you just went through – LabelEncoder followed by OneHotEncoder – can be simplified by using a DictVectorizer . . Using a DictVectorizer on a DataFrame that has been converted to a dictionary allows you to get label encoding as well as one-hot encoding in one go. . Your task is to work through this strategy in this exercise! . # Import DictVectorizer from sklearn.feature_extraction import DictVectorizer # Convert df into a dictionary: df_dict df_dict = df.to_dict(orient=&#39;records&#39;) # Create the DictVectorizer object: dv dv = DictVectorizer(sparse=False) # Apply dv on df: df_encoded df_encoded = dv.fit_transform(df_dict) # Print the resulting first five rows print(df_encoded[:5,:]) # Print the vocabulary print(dv.vocabulary_) &#39;&#39;&#39; {&#39;MSSubClass&#39;: 23, &#39;LotFrontage&#39;: 22, &#39;LotArea&#39;: 21, &#39;OverallQual&#39;: 55, &#39;OverallCond&#39;: 54, &#39;YearBuilt&#39;: 61, &#39;Remodeled&#39;: 59, &#39;GrLivArea&#39;: 11, &#39;BsmtFullBath&#39;: 6, &#39;BsmtHalfBath&#39;: 7, ..., &#39;Neighborhood=BrDale&#39;: 31, &#39;Neighborhood=SWISU&#39;: 47, &#39;MSZoning=RH&#39;: 26, &#39;Neighborhood=Blueste&#39;: 30} &#39;&#39;&#39; . type(df_dict) list df_dict [{&#39;BedroomAbvGr&#39;: 3, &#39;BldgType&#39;: &#39;1Fam&#39;, &#39;BsmtFullBath&#39;: 1, &#39;BsmtHalfBath&#39;: 0, &#39;Fireplaces&#39;: 0, &#39;FullBath&#39;: 2, &#39;GarageArea&#39;: 548, &#39;GrLivArea&#39;: 1710, &#39;HalfBath&#39;: 1, &#39;HouseStyle&#39;: &#39;2Story&#39;, &#39;LotArea&#39;: 8450, &#39;LotFrontage&#39;: 65.0, &#39;MSSubClass&#39;: 60, &#39;MSZoning&#39;: &#39;RL&#39;, &#39;Neighborhood&#39;: &#39;CollgCr&#39;, &#39;OverallCond&#39;: 5, &#39;OverallQual&#39;: 7, &#39;PavedDrive&#39;: &#39;Y&#39;, &#39;Remodeled&#39;: 0, &#39;SalePrice&#39;: 208500, &#39;YearBuilt&#39;: 2003}, ...... ] . Besides simplifying the process into one step, DictVectorizer has useful attributes such as vocabulary_ which maps the names of the features to their indices. . 4.1.5 Preprocessing within a pipeline . Now that you’ve seen what steps need to be taken individually to properly process the Ames housing data, let’s use the much cleaner and more succinct DictVectorizer approach and put it alongside an XGBoostRegressor inside of a scikit-learn pipeline. . # Import necessary modules from sklearn.feature_extraction import DictVectorizer from sklearn.pipeline import Pipeline # Fill LotFrontage missing values with 0 X.LotFrontage = X.LotFrontage.fillna(0) # Setup the pipeline steps: steps steps = [(&quot;ohe_onestep&quot;, DictVectorizer(sparse=False)), (&quot;xgb_model&quot;, xgb.XGBRegressor(objective=&quot;reg:squarederror&quot;))] # Create the pipeline: xgb_pipeline xgb_pipeline = Pipeline(steps) # Fit the pipeline xgb_pipeline.fit(X.to_dict(&#39;records&#39;), y) . . 4.2 Incorporating XGBoost into pipelines . 4.2.1 Cross-validating your XGBoost model . In this exercise, you’ll go one step further by using the pipeline you’ve created to preprocess and cross-validate your model. . # Import necessary modules from sklearn.feature_extraction import DictVectorizer from sklearn.pipeline import Pipeline from sklearn.model_selection import cross_val_score # Fill LotFrontage missing values with 0 X.LotFrontage = X.LotFrontage.fillna(0) # Setup the pipeline steps: steps steps = [(&quot;ohe_onestep&quot;, DictVectorizer(sparse=False)), (&quot;xgb_model&quot;, xgb.XGBRegressor(max_depth=2, objective=&quot;reg:squarederror&quot;))] # Create the pipeline: xgb_pipeline xgb_pipeline = Pipeline(steps) # Cross-validate the model cross_val_scores = cross_val_score(xgb_pipeline, X.to_dict(orient=&#39;records&#39;), y, scoring=&#39;neg_mean_squared_error&#39;) # Print the 10-fold RMSE print(&quot;10-fold RMSE: &quot;, np.mean(np.sqrt(np.abs(cross_val_scores)))) # 10-fold RMSE: 31233.18564354353 . 4.2.2 Kidney disease case study I: Categorical Imputer . You’ll now continue your exploration of using pipelines with a dataset that requires significantly more wrangling. The chronic kidney disease dataset contains both categorical and numeric features, but contains lots of missing values. The goal here is to predict who has chronic kidney disease given various blood indicators as features. . As Sergey mentioned in the video, you’ll be introduced to a new library, sklearn_pandas , that allows you to chain many more processing steps inside of a pipeline than are currently supported in scikit-learn. Specifically, you’ll be able to impute missing categorical values directly using the Categorical_Imputer() class in sklearn_pandas , and the DataFrameMapper() class to apply any arbitrary sklearn-compatible transformer on DataFrame columns, where the resulting output can be either a NumPy array or DataFrame. . We’ve also created a transformer called a Dictifier that encapsulates converting a DataFrame using .to_dict(&quot;records&quot;) without you having to do it explicitly (and so that it works in a pipeline). Finally, we’ve also provided the list of feature names in kidney_feature_names , the target name in kidney_target_name , the features in X , and the target in y . . In this exercise, your task is to apply the CategoricalImputer to impute all of the categorical columns in the dataset. You can refer to how the numeric imputation mapper was created as a template. Notice the keyword arguments input_df=True and df_out=True ? This is so that you can work with DataFrames instead of arrays. By default, the transformers are passed a numpy array of the selected columns as input, and as a result, the output of the DataFrame mapper is also an array. Scikit-learn transformers have historically been designed to work with numpy arrays, not pandas DataFrames, even though their basic indexing interfaces are similar. . # Import necessary modules from sklearn_pandas import DataFrameMapper from sklearn_pandas import CategoricalImputer # Check number of nulls in each feature column nulls_per_column = X.isnull().sum() print(nulls_per_column) # Create a boolean mask for categorical columns categorical_feature_mask = X.dtypes == object # Get list of categorical column names categorical_columns = X.columns[categorical_feature_mask].tolist() # Get list of non-categorical column names non_categorical_columns = X.columns[~categorical_feature_mask].tolist() # Apply numeric imputer numeric_imputation_mapper = DataFrameMapper( [([numeric_feature], Imputer(strategy=&quot;median&quot;)) for numeric_feature in non_categorical_columns], input_df=True, df_out=True ) # Apply categorical imputer categorical_imputation_mapper = DataFrameMapper( [(category_feature, CategoricalImputer()) for category_feature in categorical_columns], input_df=True, df_out=True ) . print(nulls_per_column) age 9 bp 12 sg 47 al 46 su 49 bgr 44 bu 19 sc 17 sod 87 pot 88 hemo 52 pcv 71 wc 106 rc 131 rbc 152 pc 65 pcc 4 ba 4 htn 2 dm 2 cad 2 appet 1 pe 1 ane 1 dtype: int64 . 4.2.3 Kidney disease case study II: Feature Union . Having separately imputed numeric as well as categorical columns, your task is now to use scikit-learn’s FeatureUnion to concatenate their results, which are contained in two separate transformer objects – numeric_imputation_mapper , and categorical_imputation_mapper , respectively. . You may have already encountered FeatureUnion in Machine Learning with the Experts: School Budgets . Just like with pipelines, you have to pass it a list of (string, transformer) tuples, where the first half of each tuple is the name of the transformer. . # Import FeatureUnion from sklearn.pipeline import FeatureUnion # Combine the numeric and categorical transformations numeric_categorical_union = FeatureUnion([ (&quot;num_mapper&quot;, numeric_imputation_mapper), (&quot;cat_mapper&quot;, categorical_imputation_mapper) ]) . 4.2.4 Kidney disease case study III: Full pipeline . It’s time to piece together all of the transforms along with an XGBClassifier to build the full pipeline! . Besides the numeric_categorical_union that you created in the previous exercise, there are two other transforms needed: the Dictifier() transform which we created for you, and the DictVectorizer() . . After creating the pipeline, your task is to cross-validate it to see how well it performs. . # Create full pipeline pipeline = Pipeline([ (&quot;featureunion&quot;, numeric_categorical_union), (&quot;dictifier&quot;, Dictifier()), (&quot;vectorizer&quot;, DictVectorizer(sort=False)), (&quot;clf&quot;, xgb.XGBClassifier(max_depth=3)) ]) # Perform cross-validation cross_val_scores = cross_val_score(pipeline, kidney_data, y, scoring=&quot;roc_auc&quot;, cv=3) # Print avg. AUC print(&quot;3-fold AUC: &quot;, np.mean(cross_val_scores)) # 3-fold AUC: 0.998637406769937 . . 4.3 Tuning XGBoost hyperparameters . 4.3.1 Bringing it all together . Alright, it’s time to bring together everything you’ve learned so far! In this final exercise of the course, you will combine your work from the previous exercises into one end-to-end XGBoost pipeline to really cement your understanding of preprocessing and pipelines in XGBoost. . Your work from the previous 3 exercises, where you preprocessed the data and set up your pipeline, has been pre-loaded. Your job is to perform a randomized search and identify the best hyperparameters. . # Create the parameter grid gbm_param_grid = { &#39;clf__learning_rate&#39;: np.arange(0.05, 1, 0.05), &#39;clf__max_depth&#39;: np.arange(3, 10, 1), &#39;clf__n_estimators&#39;: np.arange(50, 200, 50) } # Perform RandomizedSearchCV randomized_roc_auc = RandomizedSearchCV(estimator=pipeline, param_distributions=gbm_param_grid, n_iter=2, scoring=&#39;roc_auc&#39;, verbose=1) # Fit the estimator randomized_roc_auc.fit(X, y) # Compute metrics print(randomized_roc_auc.best_score_) print(randomized_roc_auc.best_estimator_) . Fitting 3 folds for each of 2 candidates, totalling 6 fits 0.9975202094090647 Pipeline(memory=None, steps=[(&#39;featureunion&#39;, FeatureUnion(n_jobs=None, transformer_list=[(&#39;num_mapper&#39;, DataFrameMapper(default=False, df_out=True, features=[([&#39;age&#39;], Imputer(axis=0, copy=True, missing_values=&#39;NaN&#39;, strategy=&#39;median&#39;, verbose=0)), ([&#39;bp&#39;], Imputer(axis=0, copy=True, missing_values=&#39;NaN&#39;, strategy=&#39;median&#39;, verbose=0)), ([&#39;sg&#39;], Imputer(axis=0, copy=... XGBClassifier(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.9000000000000001, max_delta_step=0, max_depth=5, min_child_weight=1, missing=None, n_estimators=150, n_jobs=1, nthread=None, objective=&#39;binary:logistic&#39;, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=None, subsample=1, verbosity=1))], verbose=False) . . 4.4 Final Thoughts . . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/extreme-gradient-boosting-with-xgboost.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/extreme-gradient-boosting-with-xgboost.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post30": {
            "title": "Exploratory Data Analysis in Python",
            "content": "Exploratory Data Analysis in Python . This is the memo of Exploratory Data Analysis in Python from DataCamp. . You can find the original course HERE . . reference . ### Course Description . How do we get from data to answers? Exploratory data analysis is a process for exploring datasets, answering questions, and visualizing results. This course presents the tools you need to clean and validate data, to visualize distributions and relationships between variables, and to use regression models to predict and explain. You’ll explore data related to demographics and health, including the National Survey of Family Growth and the General Social Survey. But the methods you learn apply to all areas of science, engineering, and business. You’ll use Pandas, a powerful library for working with data, and other core Python libraries including NumPy and SciPy, StatsModels for regression, and Matplotlib for visualization. With these tools and skills, you will be prepared to work with real data, make discoveries, and present compelling results. . ### . Read, clean, and validate | Distributions | Relationships | Multivariate Thinking | 1. Read, clean, and validate . . 1.1 DataFrames and Series . What’s the average birth weight for babies in the US? . 1.1.1 Read the codebook . When you work with datasets like the NSFG, it is important to read the documentation carefully. If you interpret a variable incorrectly, you can generate nonsense results and never realize it. So, before we start coding, I want to make sure you are familiar with the NSFG codebook, which describes every variable. . Follow this link to get to the interactive codebook. | Type “birthweight” in the search field, UNSELECT the checkbox that says “Search variable name only”, and press “Search”. You should see a list of variables related to birthweight. | Click on “BIRTHWGT_OZ1” and read the documentation of this variable. For your convenience, it is also displayed here: | . . How many respondents refused to answer this question? . 1 . 1.1.2 Exploring the NSFG data . # Display the number of rows and columns nsfg.shape # (9358, 10) # Display the names of the columns nsfg.columns # Index([&#39;caseid&#39;, &#39;outcome&#39;, &#39;birthwgt_lb1&#39;, &#39;birthwgt_oz1&#39;, &#39;prglngth&#39;, &#39;nbrnaliv&#39;, &#39;agecon&#39;, &#39;agepreg&#39;, &#39;hpagelb&#39;, &#39;wgt2013_2015&#39;], dtype=&#39;object&#39;) # Select column birthwgt_oz1: ounces ounces = nsfg[&#39;birthwgt_oz1&#39;] # Print the first 5 elements of ounces print(ounces.head()) . nsfg.head() caseid outcome birthwgt_lb1 birthwgt_oz1 prglngth nbrnaliv agecon agepreg hpagelb wgt2013_2015 0 60418 1 5.0 4.0 40 1.0 2000 2075.0 22.0 3554.964843 1 60418 1 4.0 12.0 36 1.0 2291 2358.0 25.0 3554.964843 2 60418 1 5.0 4.0 36 1.0 3241 3308.0 52.0 3554.964843 3 60419 6 NaN NaN 33 NaN 3650 NaN NaN 2484.535358 4 60420 1 8.0 13.0 41 1.0 2191 2266.0 24.0 2903.782914 . . 1.2 Clean and Validate . 1.2.1 Validate a variable . In the NSFG dataset, the variable &#39;outcome&#39; encodes the outcome of each pregnancy as shown below: . | value | label | | — | — | | 1 | Live birth | | 2 | Induced abortion | | 3 | Stillbirth | | 4 | Miscarriage | | 5 | Ectopic pregnancy | | 6 | Current pregnancy | . The nsfg DataFrame has been pre-loaded for you. Explore it in the IPython Shell and use the methods Allen showed you in the video to answer the following question: How many pregnancies in this dataset ended with a live birth? . nsfg.outcome.value_counts() 1 6489 4 1469 2 947 6 249 5 118 3 86 Name: outcome, dtype: int64 . 1.2.2 Clean a variable . In the NSFG dataset, the variable &#39;nbrnaliv&#39; records the number of babies born alive at the end of a pregnancy. . If you use .value_counts() to view the responses, you’ll see that the value 8 appears once, and if you consult the codebook, you’ll see that this value indicates that the respondent refused to answer the question. . Your job in this exercise is to replace this value with np.nan . Recall from the video how Allen replaced the values 98 and 99 in the ounces column using the .replace() method: . ounces.replace([98, 99], np.nan, inplace=True) . # Replace the value 8 with NaN nsfg[&#39;nbrnaliv&#39;].replace([8], np.nan, inplace=True) # Print the values and their frequencies print(nsfg[&#39;nbrnaliv&#39;].value_counts()) . 1.0 6379 2.0 100 3.0 5 Name: nbrnaliv, dtype: int64 . If you are careful about this kind of cleaning and validation, it will save time (in the long run) and avoid potentially serious errors. . 1.2.3 Compute a variable . For each pregnancy in the NSFG dataset, the variable &#39;agecon&#39; encodes the respondent’s age at conception, and &#39;agepreg&#39; the respondent’s age at the end of the pregnancy. . Both variables are recorded as integers with two implicit decimal places, so the value 2575 means that the respondent’s age was 25.75 . . # Select the columns and divide by 100 agecon = nsfg[&#39;agecon&#39;] / 100 agepreg = nsfg[&#39;agepreg&#39;] / 100 # Compute the difference preg_length = agepreg - agecon # Compute summary statistics print(preg_length.describe()) . count 9109.000000 mean 0.552069 std 0.271479 min 0.000000 25% 0.250000 50% 0.670000 75% 0.750000 max 0.920000 dtype: float64 . . 1.3 Filter and visualize . 1.3.1 Make a histogram . Histograms are one of the most useful tools in exploratory data analysis. They quickly give you an overview of the distribution of a variable, that is, what values the variable can have, and how many times each value appears. . As we saw in a previous exercise, the NSFG dataset includes a variable &#39;agecon&#39; that records age at conception for each pregnancy. Here, you’re going to plot a histogram of this variable. You’ll use the bins parameter that you saw in the video, and also a new parameter – histtype – which you can read more about here in the matplotlib documentation. Learning how to read documentation is an essential skill. If you want to learn more about matplotlib , you can check out DataCamp’s Introduction to Matplotlib course. . # Plot the histogram plt.hist(agecon, bins=20) # Label the axes plt.xlabel(&#39;Age at conception&#39;) plt.ylabel(&#39;Number of pregnancies&#39;) # Show the figure plt.show() . . # Plot the histogram plt.hist(agecon, bins=20, histtype=&#39;step&#39;) # Label the axes plt.xlabel(&#39;Age at conception&#39;) plt.ylabel(&#39;Number of pregnancies&#39;) # Show the figure plt.show() . . 1.3.2 Compute birth weight . Now let’s pull together the steps in this chapter to compute the average birth weight for full-term babies. . I’ve provided a function, resample_rows_weighted , that takes the NSFG data and resamples it using the sampling weights in wgt2013_2015 . The result is a sample that is representative of the U.S. population. . Then I extract birthwgt_lb1 and birthwgt_oz1 , replace special codes with NaN , and compute total birth weight in pounds, birth_weight . . # Resample the data nsfg = resample_rows_weighted(nsfg, &#39;wgt2013_2015&#39;) # Clean the weight variables pounds = nsfg[&#39;birthwgt_lb1&#39;].replace([98, 99], np.nan) ounces = nsfg[&#39;birthwgt_oz1&#39;].replace([98, 99], np.nan) # Compute total birth weight birth_weight = pounds + ounces/16 . # Create a Boolean Series for full-term babies full_term = nsfg.prglngth &gt;=37 # Select the weights of full-term babies full_term_weight = birth_weight[full_term] # Compute the mean weight of full-term babies print(np.mean(full_term_weight)) # 7.392597951914515 . 1.3.3 Filter . In the previous exercise, you computed the mean birth weight for full-term babies; you filtered out preterm babies because their distribution of weight is different. . The distribution of weight is also different for multiple births, like twins and triplets. In this exercise, you’ll filter them out, too, and see what effect it has on the mean. . # Filter full-term babies full_term = nsfg[&#39;prglngth&#39;] &gt;= 37 # Filter single births single = nsfg[&#39;nbrnaliv&#39;] == 1 # Compute birth weight for single full-term babies single_full_term_weight = birth_weight[single &amp; full_term] print(&#39;Single full-term mean:&#39;, single_full_term_weight.mean()) # Single full-term mean: 7.40297320308299 # Compute birth weight for multiple full-term babies mult_full_term_weight = birth_weight[~single &amp; full_term] print(&#39;Multiple full-term mean:&#39;, mult_full_term_weight.mean()) # Multiple full-term mean: 5.784722222222222 . 2. Distributions . . 2.1 Probability mass functions . . 2.1.1 Make a PMF . The GSS dataset has been pre-loaded for you into a DataFrame called gss . You can explore it in the IPython Shell to get familiar with it. . In this exercise, you’ll focus on one variable in this dataset, &#39;year&#39; , which represents the year each respondent was interviewed. . The Pmf class you saw in the video has already been created for you. You can access it outside of DataCamp via the empiricaldist library. . gss year sex age cohort race educ realinc wtssall 0 1972 1 26.0 1946.0 1 18.0 13537.0000 0.889300 1 1972 2 38.0 1934.0 1 12.0 18951.0000 0.444600 ... ... ... ... ... ... ... ... ... 62462 2016 2 61.0 1955.0 1 16.0 65520.0000 0.956994 62463 2016 2 67.0 1949.0 1 13.0 NaN 1.564363 62464 2016 2 57.0 1959.0 1 12.0 9945.0000 0.956994 62465 2016 2 56.0 1960.0 1 12.0 38610.0000 0.478497 [62466 rows x 8 columns] . # Compute the PMF for year pmf_year = Pmf(gss.year, normalize=False) # Print the result print(pmf_year) . 1972 1613 1973 1504 ... 2014 2538 2016 2867 Name: Pmf, dtype: int64 . 2.1.2 Plot a PMF . Now let’s plot a PMF for the age of the respondents in the GSS dataset. The variable &#39;age&#39; contains respondents’ age in years. . # Select the age column age = gss[&#39;age&#39;] # Make a PMF of age pmf_age = Pmf(age) # Plot the PMF pmf_age.bar() # Label the axes plt.xlabel(&#39;Age&#39;) plt.ylabel(&#39;PMF&#39;) plt.show() . . . 2.2 Cumulative distribution functions . . 2.2.1 Make a CDF . In this exercise, you’ll make a CDF and use it to determine the fraction of respondents in the GSS dataset who are OLDER than 30. . The GSS dataset has been preloaded for you into a DataFrame called gss . . As with the Pmf class from the previous lesson, the Cdf class you just saw in the video has been created for you, and you can access it outside of DataCamp via the empiricaldist library. . # Select the age column age = gss[&#39;age&#39;] # Compute the CDF of age cdf_age = Cdf(age) # Calculate the CDF of 30 print(cdf_age[30]) # 0.2539137136526388 . 2.2.2 Compute IQR . Recall from the video that the interquartile range (IQR) is the difference between the 75th and 25th percentiles. It is a measure of variability that is robust in the presence of errors or extreme values. . In this exercise, you’ll compute the interquartile range of income in the GSS dataset. Income is stored in the &#39;realinc&#39; column, and the CDF of income has already been computed and stored in cdf_income . . np.percentile(gss.realinc.sort_values().dropna(),75) # 43426.0 cdf_income.inverse(0.75) # array(43426.) . # Calculate the 75th percentile percentile_75th = cdf_income.inverse(0.75) # Calculate the 25th percentile percentile_25th = cdf_income.inverse(0.25) # Calculate the interquartile range iqr = percentile_75th - percentile_25th # Print the interquartile range print(iqr) . 2.2.3 Plot a CDF . The distribution of income in almost every country is long-tailed; that is, there are a small number of people with very high incomes. . In the GSS dataset, the variable &#39;realinc&#39; represents total household income, converted to 1986 dollars. We can get a sense of the shape of this distribution by plotting the CDF. . # Select realinc income = gss.realinc # Make the CDF cdf_income = Cdf(income) # Plot it cdf_income.plot() # Label the axes plt.xlabel(&#39;Income (1986 USD)&#39;) plt.ylabel(&#39;CDF&#39;) plt.show() . . . 2.3 Comparing distributions . 2.3.1 Distribution of education . Let’s begin comparing incomes for different levels of education in the GSS dataset, which has been pre-loaded for you into a DataFrame called gss . The variable educ represents the respondent’s years of education. . What fraction of respondents report that they have 12 years of education or fewer? . Cdf(gss.educ) 0.0 0.002311 1.0 0.002921 ... 12.0 0.532261 ... 19.0 0.979231 20.0 1.000000 Name: Cdf, dtype: float64 Cdf(gss.educ)(12) # array(0.53226117) . 2.3.2 Extract education levels . Let’s create Boolean Series to identify respondents with different levels of education. . In the U.S, 12 years of education usually means the respondent has completed high school (secondary education). A respondent with 14 years of education has probably completed an associate degree (two years of college); someone with 16 years has probably completed a bachelor’s degree (four years of college). . # Select educ educ = gss[&#39;educ&#39;] # Bachelor&#39;s degree bach = (educ &gt;= 16) # Associate degree assc = (educ &gt;= 14) &amp; (educ &lt; 16) # High school (12 or fewer years of education) high = (educ &lt;= 12) print(high.mean()) # 0.5308807991547402 . 2.3.3 Plot income CDFs . Let’s now see what the distribution of income looks like for people with different education levels. You can do this by plotting the CDFs. Recall how Allen plotted the income CDFs of respondents interviewed before and after 1995: . Cdf(income[pre95]).plot(label=&#39;Before 1995&#39;) Cdf(income[~pre95]).plot(label=&#39;After 1995&#39;) . You can assume that Boolean Series have been defined, as in the previous exercise, to identify respondents with different education levels: high , assc , and bach . . income = gss[&#39;realinc&#39;] # Plot the CDFs Cdf(income[high]).plot(label=&#39;High school&#39;) Cdf(income[assc]).plot(label=&#39;Associate&#39;) Cdf(income[bach]).plot(label=&#39;Bachelor&#39;) # Label the axes plt.xlabel(&#39;Income (1986 USD)&#39;) plt.ylabel(&#39;CDF&#39;) plt.legend() plt.show() . . It might not be surprising that people with more education have higher incomes, but looking at these distributions, we can see where the differences are. . . 2.4 Modeling distributions . . 2.4.1 Distribution of income . In many datasets, the distribution of income is approximately lognormal, which means that the logarithms of the incomes fit a normal distribution. We’ll see whether that’s true for the GSS data. As a first step, you’ll compute the mean and standard deviation of the log of incomes using NumPy’s np.log10() function. . Then, you’ll use the computed mean and standard deviation to make a norm object using the scipy.stats.norm() function. . # Extract realinc and compute its log income = gss[&#39;realinc&#39;] log_income = np.log10(income) # Compute mean and standard deviation mean = np.mean(log_income) std = np.std(log_income) print(mean, std) # 4.371148677934171 0.42900437330100427 # Make a norm object from scipy.stats import norm dist = norm(mean,std) . 2.4.2 Comparing CDFs . To see whether the distribution of income is well modeled by a lognormal distribution, we’ll compare the CDF of the logarithm of the data to a normal distribution with the same mean and standard deviation. . dist is a scipy.stats.norm object with the same mean and standard deviation as the data. It provides .cdf() , which evaluates the normal cumulative distribution function. . Be careful with capitalization: Cdf() , with an uppercase C , creates Cdf objects. dist.cdf() , with a lowercase c , evaluates the normal cumulative distribution function. . # Evaluate the model CDF xs = np.linspace(2, 5.5) ys = dist.cdf(xs) # Plot the model CDF plt.clf() plt.plot(xs, ys, color=&#39;gray&#39;) # Create and plot the Cdf of log_income Cdf(log_income).plot() # Label the axes plt.xlabel(&#39;log10 of realinc&#39;) plt.ylabel(&#39;CDF&#39;) plt.show() . . The lognormal model is a pretty good fit for the data, but clearly not a perfect match. That’s what real data is like; sometimes it doesn’t fit the model. . 2.4.3 Comparing PDFs . In the previous exercise, we used CDFs to see if the distribution of income is lognormal. We can make the same comparison using a PDF and KDE. That’s what you’ll do in this exercise! . Just as all norm objects have a .cdf() method, they also have a .pdf() method. . To create a KDE plot, you can use Seaborn’s kdeplot() function. To learn more about this function and Seaborn, you can check out DataCamp’s Data Visualization with Seaborn course. Here, Seaborn has been imported for you as sns . . # Evaluate the normal PDF xs = np.linspace(2, 5.5) ys = dist.pdf(xs) # Plot the model PDF plt.clf() plt.plot(xs, ys, color=&#39;gray&#39;) # Plot the data KDE sns.kdeplot(log_income) # Label the axes plt.xlabel(&#39;log10 of realinc&#39;) plt.ylabel(&#39;PDF&#39;) plt.show() . . 3. Relationships . . 3.1 Exploring relationships . . 3.1.1 PMF of age . Do people tend to gain weight as they get older? We can answer this question by visualizing the relationship between weight and age. But before we make a scatter plot, it is a good idea to visualize distributions one variable at a time. Here, you’ll visualize age using a bar chart first. Recall that all PMF objects have a .bar() method to make a bar chart. . The BRFSS dataset includes a variable, &#39;AGE&#39; (note the capitalization!), which represents each respondent’s age. To protect respondents’ privacy, ages are rounded off into 5-year bins. &#39;AGE&#39; contains the midpoint of the bins. . # Extract age age = brfss.AGE # Plot the PMF Pmf(age).bar() # Label the axes plt.xlabel(&#39;Age in years&#39;) plt.ylabel(&#39;PMF&#39;) plt.show() . . 3.1.2 Scatter plot . Now let’s make a scatterplot of weight versus age . To make the code run faster, I’ve selected only the first 1000 rows from the brfss DataFrame. . weight and age have already been extracted for you. Your job is to use plt.plot() to make a scatter plot. . # Select the first 1000 respondents brfss = brfss[:1000] # Extract age and weight age = brfss[&#39;AGE&#39;] weight = brfss[&#39;WTKG3&#39;] # Make a scatter plot plt.plot(age,weight,&#39;o&#39;,alpha=0.1) plt.xlabel(&#39;Age in years&#39;) plt.ylabel(&#39;Weight in kg&#39;) plt.show() . . 3.1.3 Jittering . In the previous exercise, the ages fall in columns because they’ve been rounded into 5-year bins. If we jitter them, the scatter plot will show the relationship more clearly. Recall how Allen jittered height and weight in the video: . height_jitter = height + np.random.normal(0, 2, size=len(brfss)) weight_jitter = weight + np.random.normal(0, 2, size=len(brfss)) . # Select the first 1000 respondents brfss = brfss[:1000] # Add jittering to age age = brfss[&#39;AGE&#39;] + np.random.normal(0,2.5,size=len(brfss)) # Extract weight weight = brfss[&#39;WTKG3&#39;] # Make a scatter plot plt.plot(age,weight,&#39;o&#39;,alpha=0.2,markersize=5) plt.xlabel(&#39;Age in years&#39;) plt.ylabel(&#39;Weight in kg&#39;) plt.show() . . By smoothing out the ages and avoiding saturation, we get the best view of the data. But in this case the nature of the relationship is still hard to see. . . 3.2 Visualizing relationships . . 3.2.1 Height and weight . Previously we looked at a scatter plot of height and weight, and saw that taller people tend to be heavier. Now let’s take a closer look using a box plot. The brfss DataFrame contains a variable &#39;_HTMG10&#39; that represents height in centimeters, binned into 10 cm groups. . Recall how Allen created the box plot of &#39;AGE&#39; and &#39;WTKG3&#39; in the video, with the y-axis on a logarithmic scale: . sns.boxplot(x=&#39;AGE&#39;, y=&#39;WTKG3&#39;, data=data, whis=10) plt.yscale(&#39;log&#39;) . # Drop rows with missing data data = brfss.dropna(subset=[&#39;_HTMG10&#39;, &#39;WTKG3&#39;]) # Make a box plot sns.boxplot(&#39;_HTMG10&#39;,&#39;WTKG3&#39;,whis=10,data=data) # Plot the y-axis on a log scale plt.yscale(&#39;log&#39;) # Remove unneeded lines and label axes sns.despine(left=True, bottom=True) plt.xlabel(&#39;Height in cm&#39;) plt.ylabel(&#39;Weight in kg&#39;) plt.show() . . 3.2.2 Distribution of income . In the next two exercises we’ll look at relationships between income and other variables. In the BRFSS, income is represented as a categorical variable; that is, respondents are assigned to one of 8 income categories. The variable name is &#39;INCOME2&#39; . Before we connect income with anything else, let’s look at the distribution by computing the PMF. Recall that all Pmf objects have a .bar() method. . # Extract income income = brfss.INCOME2 # Plot the PMF Pmf(income).bar() # Label the axes plt.xlabel(&#39;Income level&#39;) plt.ylabel(&#39;PMF&#39;) plt.show() . Almost half of the respondents are in the top income category, so this dataset doesn’t distinguish between the highest incomes and the median. But maybe it can tell us something about people with incomes below the median. . 3.2.3 Income and height . Let’s now use a violin plot to visualize the relationship between income and height. . # Drop rows with missing data data = brfss.dropna(subset=[&#39;INCOME2&#39;, &#39;HTM4&#39;]) # Make a violin plot sns.violinplot(&#39;INCOME2&#39;,&#39;HTM4&#39;,inner=None,data=data) # Remove unneeded lines and label axes sns.despine(left=True, bottom=True) plt.xlabel(&#39;Income level&#39;) plt.ylabel(&#39;Height in cm&#39;) plt.show() . . It looks like there is a weak positive relationsip between income and height, at least for incomes below the median. . . 3.3 Correlation . 3.3.1 Computing correlations . The purpose of the BRFSS is to explore health risk factors, so it includes questions about diet. The variable &#39;_VEGESU1&#39; represents the number of servings of vegetables respondents reported eating per day. . Let’s see how this variable relates to age and income. . # Select columns columns = [&#39;AGE&#39;, &#39;INCOME2&#39;, &#39;_VEGESU1&#39;] subset = brfss[columns] # Compute the correlation matrix print(subset.corr()) . AGE INCOME2 _VEGESU1 AGE 1.000000 -0.015158 -0.009834 INCOME2 -0.015158 1.000000 0.119670 _VEGESU1 -0.009834 0.119670 1.000000 . 3.3.2 Interpreting correlations . In the previous exercise, the correlation between income and vegetable consumption is about 0.12 . The correlation between age and vegetable consumption is about -0.01 . . The following are correct interpretations of these results: . People with higher incomes eat more vegetables. | There could be a strong nonlinear relationship between age and vegetable consumption. | . The correlation between income and vegetable consumption is small ( 0.12 ), but it suggests that there is a week relationship. . But a correlation( -0.01) close to 0 does mean there is no relationship. . 3.4 Simple regression . . 3.4.1 Income and vegetables . As we saw in a previous exercise, the variable &#39;_VEGESU1&#39; represents the number of vegetable servings respondents reported eating per day. . Let’s estimate the slope of the relationship between vegetable consumption and income. . from scipy.stats import linregress # Extract the variables subset = brfss.dropna(subset=[&#39;INCOME2&#39;, &#39;_VEGESU1&#39;]) xs = subset[&#39;INCOME2&#39;] ys = subset[&#39;_VEGESU1&#39;] # Compute the linear regression res = linregress(xs,ys) print(res) . LinregressResult(slope=0.06988048092105019, intercept=1.5287786243363106, rvalue=0.11967005884864107, pvalue=1.378503916247615e-238, stderr=0.002110976356332332) # rvalue: correlation coefficient . 3.4.2 Fit a line . Continuing from the previous exercise: . Assume that xs and ys contain income codes and daily vegetable consumption, respectively, and | res contains the results of a simple linear regression of ys onto xs . | . Now, you’re going to compute the line of best fit. NumPy has been imported for you as np . . # Plot the scatter plot plt.clf() x_jitter = xs + np.random.normal(0, 0.15, len(xs)) plt.plot(x_jitter, ys, &#39;o&#39;, alpha=0.2) # Plot the line of best fit fx = np.array([xs.min(), xs.max()]) fy = res.slope * fx + res.intercept plt.plot(fx, fy, &#39;-&#39;, alpha=0.7) plt.xlabel(&#39;Income code&#39;) plt.ylabel(&#39;Vegetable servings per day&#39;) plt.ylim([0, 6]) plt.show() . . 4. Multivariate Thinking . . 4.1 Limits of simple regression . . 4.1.1 Regression and causation . In the BRFSS dataset, there is a strong relationship between vegetable consumption and income. The income of people who eat 8 servings of vegetables per day is double the income of people who eat none, on average. . Which of the following conclusions can we draw from this data? . A. Eating a good diet leads to better health and higher income. | B. People with higher income can afford a better diet. | C. People with high income are more likely to be vegetarians. | . None of them. . This data is consistent with all of these conclusions, but it does not provide conclusive evidence for any of them. . 4.1.2 Using StatsModels . Let’s run the same regression using SciPy and StatsModels, and confirm we get the same results. . from scipy.stats import linregress import statsmodels.formula.api as smf # Run regression with linregress subset = brfss.dropna(subset=[&#39;INCOME2&#39;, &#39;_VEGESU1&#39;]) xs = subset[&#39;INCOME2&#39;] ys = subset[&#39;_VEGESU1&#39;] res = linregress(xs,ys) print(res) # Run regression with StatsModels results = smf.ols(&#39;_VEGESU1 ~ INCOME2&#39;, data = brfss).fit() print(results.params) . LinregressResult(slope=0.06988048092105019, intercept=1.5287786243363106, rvalue=0.11967005884864107, pvalue=1.378503916247615e-238, stderr=0.002110976356332332) Intercept 1.528779 INCOME2 0.069880 dtype: float64 . When you start working with a new library, checks like this help ensure that you are doing it right. . . 4.2 Multiple regression . . 4.2.1 Plot income and education . To get a closer look at the relationship between income and education, let’s use the variable &#39;educ&#39; to group the data, then plot mean income in each group. . # Group by educ grouped = gss.groupby(&#39;educ&#39;) # Compute mean income in each group mean_income_by_educ = grouped[&#39;realinc&#39;].mean() # Plot mean income as a scatter plot plt.plot(mean_income_by_educ, &#39;o&#39;, alpha=0.5) # Label the axes plt.xlabel(&#39;Education (years)&#39;) plt.ylabel(&#39;Income (1986 $)&#39;) plt.show() . . 4.2.2 Non-linear model of education . The graph in the previous exercise suggests that the relationship between income and education is non-linear. So let’s try fitting a non-linear model. . import statsmodels.formula.api as smf # Add a new column with educ squared gss[&#39;educ2&#39;] = gss[&#39;educ&#39;] ** 2 # Run a regression model with educ, educ2, age, and age2 results = smf.ols(&#39;realinc ~ educ + educ2 + age + age2&#39;,data=gss).fit() # Print the estimated parameters print(results.params) . Intercept -23241.884034 educ -528.309369 educ2 159.966740 age 1696.717149 age2 -17.196984 dtype: float64 . The slope associated with educ2 is positive, so the model curves upward. . . 4.3 Visualizing regression results . . 4.3.1 Making predictions . At this point, we have a model that predicts income using age, education, and sex. . Let’s see what it predicts for different levels of education, holding age constant. . # Run a regression model with educ, educ2, age, and age2 results = smf.ols(&#39;realinc ~ educ + educ2 + age + age2&#39;, data=gss).fit() # Make the DataFrame df = pd.DataFrame() df[&#39;educ&#39;] = np.linspace(0,20) df[&#39;age&#39;] = 30 df[&#39;educ2&#39;] = df[&#39;educ&#39;]**2 df[&#39;age2&#39;] = df[&#39;age&#39;]**2 # Generate and plot the predictions pred = results.predict(df) print(pred.head()) . 0 12182.344976 1 11993.358518 2 11857.672098 3 11775.285717 4 11746.199374 dtype: float64 . 4.3.2 Visualizing predictions . Now let’s visualize the results from the previous exercise! . # Plot mean income in each age group plt.clf() grouped = gss.groupby(&#39;educ&#39;) mean_income_by_educ = grouped[&#39;realinc&#39;].mean() plt.plot(mean_income_by_educ,&#39;o&#39;,alpha=0.5) # Plot the predictions pred = results.predict(df) plt.plot(df[&#39;educ&#39;], pred, label=&#39;Age 30&#39;) # Label axes plt.xlabel(&#39;Education (years)&#39;) plt.ylabel(&#39;Income (1986 $)&#39;) plt.legend() plt.show() . . Looks like this model captures the relationship pretty well. . . 4.4 Logistic regression . . 4.4.1 Predicting a binary variable . Let’s use logistic regression to predict a binary variable. Specifically, we’ll use age, sex, and education level to predict support for legalizing cannabis (marijuana) in the U.S. . In the GSS dataset, the variable grass records the answer to the question “Do you think the use of marijuana should be made legal or not?” . # Recode grass gss[&#39;grass&#39;].replace(2, 0, inplace=True) # Run logistic regression results = smf.logit(&#39;grass ~ age + age2 + educ + educ2 + C(sex)&#39;, data=gss).fit() results.params . Intercept -1.685223 C(sex)[T.2] -0.384611 age -0.034756 age2 0.000192 educ 0.221860 educ2 -0.004163 dtype: float64 . # Make a DataFrame with a range of ages df = pd.DataFrame() df[&#39;age&#39;] = np.linspace(18, 89) df[&#39;age2&#39;] = df[&#39;age&#39;]**2 # Set the education level to 12 df[&#39;educ&#39;] = 12 df[&#39;educ2&#39;] = df[&#39;educ&#39;]**2 # Generate predictions for men and women df[&#39;sex&#39;] = 1 pred1 = results.predict(df) df[&#39;sex&#39;] = 2 pred2 = results.predict(df) grouped = gss.groupby(&#39;age&#39;) favor_by_age = grouped[&#39;grass&#39;].mean() plt.clf() plt.plot(favor_by_age, &#39;o&#39;, alpha=0.5) plt.plot(df[&#39;age&#39;], pred1, label=&#39;Male&#39;) plt.plot(df[&#39;age&#39;], pred2, label=&#39;Female&#39;) plt.xlabel(&#39;Age&#39;) plt.ylabel(&#39;Probability of favoring legalization&#39;) plt.legend() plt.show() . . . 4.5 Next Step . . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/exploratory-data-analysis-in-python.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/exploratory-data-analysis-in-python.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post31": {
            "title": "Dimensionality Reduction in Python",
            "content": "Dimensionality Reduction in Python . This is the memo of the 7th course (23 courses in all) of ‘Machine Learning Scientist with Python’ skill track. . You can find the original course HERE . . ### Course Description . High-dimensional datasets can be overwhelming and leave you not knowing where to start. Typically, you’d visually explore a new dataset first, but when you have too many dimensions the classical approaches will seem insufficient. Fortunately, there are visualization techniques designed specifically for high dimensional data and you’ll be introduced to these in this course. After exploring the data, you’ll often find that many features hold little information because they don’t show any variance or because they are duplicates of other features. You’ll learn how to detect these features and drop them from the dataset so that you can focus on the informative ones. In a next step, you might want to build a model on these features, and it may turn out that some don’t have any effect on the thing you’re trying to predict. You’ll learn how to detect and drop these irrelevant features too, in order to reduce dimensionality and thus complexity. Finally, you’ll learn how feature extraction techniques can reduce dimensionality for you through the calculation of uncorrelated principal components. . ### . Exploring high dimensional data | Feature selection I, selecting for feature information | Feature selection II, selecting for model accuracy | Feature extraction | 1. Exploring high dimensional data . . You’ll be introduced to the concept of dimensionality reduction and will learn when an why this is important. You’ll learn the difference between feature selection and feature extraction and will apply both techniques for data exploration. The chapter ends with a lesson on t-SNE, a powerful feature extraction technique that will allow you to visualize a high-dimensional dataset. . 1.1 Introduction . . 1.1.1 Finding the number of dimensions in a dataset . A larger sample of the Pokemon dataset has been loaded for you as the Pandas dataframe pokemon_df . . How many dimensions, or columns are in this dataset? . pokemon_df.shape (160, 7) . 1.1.2 Removing features without variance . A sample of the Pokemon dataset has been loaded as pokemon_df . To get an idea of which features have little variance you should use the IPython Shell to calculate summary statistics on this sample. Then adjust the code to create a smaller, easier to understand, dataset. . pokemon_df.describe() HP Attack Defense Generation count 160.00000 160.00000 160.000000 160.0 mean 64.61250 74.98125 70.175000 1.0 std 27.92127 29.18009 28.883533 0.0 min 10.00000 5.00000 5.000000 1.0 25% 45.00000 52.00000 50.000000 1.0 50% 60.00000 71.00000 65.000000 1.0 75% 80.00000 95.00000 85.000000 1.0 max 250.00000 155.00000 180.000000 1.0 . pokemon_df.describe(exclude=&#39;number&#39;) Name Type Legendary count 160 160 160 unique 160 15 1 top Weepinbell Water False freq 1 31 160 . # Leave this list as is number_cols = [&#39;HP&#39;, &#39;Attack&#39;, &#39;Defense&#39;] # Remove the feature without variance from this list non_number_cols = [&#39;Name&#39;, &#39;Type&#39;] # Create a new dataframe by subselecting the chosen features df_selected = pokemon_df[number_cols + non_number_cols] # Prints the first 5 lines of the new dataframe print(df_selected.head()) . HP Attack Defense Name Type 0 45 49 49 Bulbasaur Grass 1 60 62 63 Ivysaur Grass 2 80 82 83 Venusaur Grass 3 80 100 123 VenusaurMega Venusaur Grass 4 39 52 43 Charmander Fire . . 1.2 Feature selection vs feature extraction . . 1.2.1 Visually detecting redundant features . Data visualization is a crucial step in any data exploration. Let’s use Seaborn to explore some samples of the US Army ANSUR body measurement dataset. . Two data samples have been pre-loaded as ansur_df_1 and ansur_df_2 . . Seaborn has been imported as sns . . # Create a pairplot and color the points using the &#39;Gender&#39; feature sns.pairplot(ansur_df_1, hue=&#39;Gender&#39;, diag_kind=&#39;hist&#39;) # Show the plot plt.show() . . # Remove one of the redundant features reduced_df = ansur_df_1.drop(&#39;stature_m&#39;, axis=1) # Create a pairplot and color the points using the &#39;Gender&#39; feature sns.pairplot(reduced_df, hue=&#39;Gender&#39;) # Show the plot plt.show() . . # Create a pairplot and color the points using the &#39;Gender&#39; feature sns.pairplot(ansur_df_2, hue=&#39;Gender&#39;, diag_kind=&#39;hist&#39;) # Show the plot plt.show() . . # Remove the redundant feature reduced_df = ansur_df_2.drop(&#39;n_legs&#39;, axis=1) # Create a pairplot and color the points using the &#39;Gender&#39; feature sns.pairplot(reduced_df, hue=&#39;Gender&#39;, diag_kind=&#39;hist&#39;) # Show the plot plt.show() . . The body height (inches) and stature (meters) hold the same information in a different unit + all the individuals in the second sample have two legs. . 1.2.2 Advantage of feature selection . What advantage does feature selection have over feature extraction? . The selected features remain unchanged, and are therefore easy to interpret. . . 1.3 t-SNE visualization of high-dimensional data . . 1.3.1 t-SNE intuition . t-SNE is super powerful, but do you know exactly when to use it? . When you want to visually explore the patterns in a high dimensional dataset. . 1.3.2 Fitting t-SNE to the ANSUR data . t-SNE is a great technique for visual exploration of high dimensional datasets. In this exercise, you’ll apply it to the ANSUR dataset. You’ll remove non-numeric columns from the pre-loaded dataset df and fit TSNE to his numeric dataset. . # Non-numerical columns in the dataset non_numeric = [&#39;Branch&#39;, &#39;Gender&#39;, &#39;Component&#39;] # Drop the non-numerical columns from df df_numeric = df.drop(non_numeric, axis=1) # Create a t-SNE model with learning rate 50 m = TSNE(learning_rate=50) # Fit and transform the t-SNE model on the numeric dataset tsne_features = m.fit_transform(df_numeric) print(tsne_features.shape) (6068, 2) df.shape (6068, 94) . 1.3.3 t-SNE visualisation of dimensionality . Time to look at the results of your hard work. In this exercise, you will visualize the output of t-SNE dimensionality reduction on the combined male and female Ansur dataset. You’ll create 3 scatterplots of the 2 t-SNE features ( &#39;x&#39; and &#39;y&#39; ) which were added to the dataset df . In each scatterplot you’ll color the points according to a different categorical variable. . seaborn has already been imported as sns and matplotlib.pyplot as plt . . # Color the points according to Army Component sns.scatterplot(x=&quot;x&quot;, y=&quot;y&quot;, hue=&#39;Component&#39;, data=df) # Show the plot plt.show() . . # Color the points by Army Branch sns.scatterplot(x=&quot;x&quot;, y=&quot;y&quot;, hue=&#39;Branch&#39;, data=df) # Show the plot plt.show() . . # Color the points by Gender sns.scatterplot(x=&quot;x&quot;, y=&quot;y&quot;, hue=&#39;Gender&#39;, data=df) # Show the plot plt.show() . . There is a Male and a Female cluster. t-SNE found these gender differences in body shape without being told about them explicitly! From the second plot you learned there are more males in the Combat Arms Branch. . 2. Feature selection I, selecting for feature information . . 2.1 The curse of dimensionality . 2.1.1 Train – test split . In this chapter, you will keep working with the ANSUR dataset. Before you can build a model on your dataset, you should first decide on which feature you want to predict. In this case, you’re trying to predict gender. . You need to extract the column holding this feature from the dataset and then split the data into a training and test set. The training set will be used to train the model and the test set will be used to check its performance on unseen data. . ansur_df has been pre-loaded for you. . # Import train_test_split() from sklearn.model_selection import train_test_split # Select the Gender column as the feature to be predicted (y) y = ansur_df[&#39;Gender&#39;] # Remove the Gender column to create the training data X = ansur_df.drop(&#39;Gender&#39;, axis=1) # Perform a 70% train and 30% test data split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) print(&quot;{} rows in test set vs. {} in training set. {} Features.&quot;.format(X_test.shape[0], X_train.shape[0], X_test.shape[1])) # 300 rows in test set vs. 700 in training set. 91 Features. . 2.1.2 Fitting and testing the model . In the previous exercise, you split the dataset into X_train , X_test , y_train , and y_test . These datasets have been pre-loaded for you. You’ll now create a support vector machine classifier model ( SVC() ) and fit that to the training data. You’ll then calculate the accuracy on both the test and training set to detect overfitting. . # Import SVC from sklearn.svm and accuracy_score from sklearn.metrics from sklearn.svm import SVC from sklearn.metrics import accuracy_score # Create an instance of the Support Vector Classification class svc = SVC() # Fit the model to the training data svc.fit(X_train, y_train) # Calculate accuracy scores on both train and test data accuracy_train = accuracy_score(y_train, svc.predict(X_train)) accuracy_test = accuracy_score(y_test, svc.predict(X_test)) print(&quot;{0:.1%} accuracy on test set vs. {1:.1%} on training set&quot;.format(accuracy_test, accuracy_train)) # 49.7% accuracy on test set vs. 100.0% on training set . Looks like the model badly overfits on the training data. On unseen data it performs worse than a random selector would. . 2.1.3 Accuracy after dimensionality reduction . You’ll reduce the overfit with the help of dimensionality reduction. In this case, you’ll apply a rather drastic form of dimensionality reduction by only selecting a single column that has some good information to distinguish between genders. You’ll repeat the train-test split, model fit and prediction steps to compare the accuracy on test vs. training data. . # Assign just the &#39;neckcircumferencebase&#39; column from ansur_df to X X = ansur_df[[&#39;neckcircumferencebase&#39;]] # Split the data, instantiate a classifier and fit the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) svc = SVC() svc.fit(X_train, y_train) # Calculate accuracy scores on both train and test data accuracy_train = accuracy_score(y_train, svc.predict(X_train)) accuracy_test = accuracy_score(y_test, svc.predict(X_test)) print(&quot;{0:.1%} accuracy on test set vs. {1:.1%} on training set&quot;.format(accuracy_test, accuracy_train)) # 93.3% accuracy on test set vs. 94.9% on training set . Wow, what just happened!? On the full dataset the model is rubbish but with a single feature we can make good predictions? This is an example of the curse of dimensionality! The model badly overfits when we feed it too many features. It overlooks that neck circumference by itself is pretty different for males and females. . . 2.2 Features with missing values or little variance . . 2.2.1 Finding a good variance threshold . You’ll be working on a slightly modified subsample of the ANSUR dataset with just head measurements pre-loaded as head_df . . # Create the boxplot head_df.boxplot() plt.show() . . # Normalize the data normalized_df = head_df / head_df.mean() normalized_df.boxplot() plt.show() . . # Normalize the data normalized_df = head_df / head_df.mean() # Print the variances of the normalized data print(normalized_df.var()) . headbreadth 1.678952e-03 headcircumference 1.029623e-03 headlength 1.867872e-03 tragiontopofhead 2.639840e-03 n_hairs 1.002552e-08 measurement_error 3.231707e-27 dtype: float64 . Inspect the printed variances. If you want to remove the 2 very low variance features. What would be a good variance threshold? . 1.0e-03 . 2.2.2 Features with low variance . In the previous exercise you established that 0.001 is a good threshold to filter out low variance features in head_df after normalization. Now use the VarianceThreshold feature selector to remove these features. . from sklearn.feature_selection import VarianceThreshold # Create a VarianceThreshold feature selector sel = VarianceThreshold(threshold=10**-3) # Fit the selector to normalized head_df sel.fit(head_df / head_df.mean()) # Create a boolean mask mask = sel.get_support() # Apply the mask to create a reduced dataframe reduced_df = head_df.loc[:, mask] print(&quot;Dimensionality reduced from {} to {}.&quot;.format(head_df.shape[1], reduced_df.shape[1])) # Dimensionality reduced from 6 to 4. . 2.2.3 Removing features with many missing values . You’ll apply feature selection on the Boston Public Schools dataset which has been pre-loaded as school_df . Calculate the missing value ratio per feature and then create a mask to remove features with many missing values. . school_df.isna().sum() / len(school_df) x 0.000000 y 0.000000 objectid_1 0.000000 objectid 0.000000 bldg_id 0.000000 bldg_name 0.000000 address 0.000000 city 0.000000 zipcode 0.000000 csp_sch_id 0.000000 sch_id 0.000000 sch_name 0.000000 sch_label 0.000000 sch_type 0.000000 shared 0.877863 complex 0.984733 label 0.000000 tlt 0.000000 pl 0.000000 point_x 0.000000 point_y 0.000000 dtype: float64 . # Create a boolean mask on whether each feature less than 50% missing values. mask = school_df.isna().sum() / len(school_df) &lt; 0.5 # Create a reduced dataset by applying the mask reduced_df = school_df.loc[:,mask] print(school_df.shape) print(reduced_df.shape) # (131, 21) # (131, 19) . . 2.3 Pairwise correlation . . 2.3.1 Correlation intuition . The correlation coefficient of A to B is equal to that of B to A. . 2.3.2 Inspecting the correlation matrix . A sample of the ANSUR body measurements dataset has been pre-loaded as ansur_df . Use the terminal to create a correlation matrix for this dataset. . What is the correlation coefficient between wrist and ankle circumference? . ansur_df.corr() Elbow rest height Wrist circumference Ankle circumference Buttock height Crotch height Elbow rest height 1.000000 0.294753 0.301963 -0.007013 -0.026090 Wrist circumference 0.294753 1.000000 0.702178 0.576679 0.606582 Ankle circumference 0.301963 0.702178 1.000000 0.367548 0.386502 Buttock height -0.007013 0.576679 0.367548 1.000000 0.929411 Crotch height -0.026090 0.606582 0.386502 0.929411 1.000000 . 0.702178 . 2.3.3 Visualizing the correlation matrix . Reading the correlation matrix of ansur_df in its raw, numeric format doesn’t allow us to get a quick overview. Let’s improve this by removing redundant values and visualizing the matrix using seaborn. . # Create the correlation matrix corr = ansur_df.corr() # Draw the heatmap sns.heatmap(corr, cmap=cmap, center=0, linewidths=1, annot=True, fmt=&quot;.2f&quot;) plt.show() . . # Create the correlation matrix corr = ansur_df.corr() # Generate a mask for the upper triangle mask = np.triu(np.ones_like(corr, dtype=bool)) # Add the mask to the heatmap sns.heatmap(corr, mask=mask, cmap=cmap, center=0, linewidths=1, annot=True, fmt=&quot;.2f&quot;) plt.show() . . . 2.4 Removing highly correlated features . . 2.4.1 Filtering out highly correlated features . You’re going to automate the removal of highly correlated features in the numeric ANSUR dataset. You’ll calculate the correlation matrix and filter out columns that have a correlation coefficient of more than 0.95 or less than -0.95. . Since each correlation coefficient occurs twice in the matrix (correlation of A to B equals correlation of B to A) you’ll want to ignore half of the correlation matrix so that only one of the two correlated features is removed. Use a mask trick for this purpose. . # Calculate the correlation matrix and take the absolute value corr_matrix = ansur_df.corr().abs() # Create a True/False mask and apply it mask = np.triu(np.ones_like(corr_matrix, dtype=bool)) tri_df = corr_matrix.mask(mask) # List column names of highly correlated features (r &gt; 0.95) to_drop = [c for c in tri_df.columns if any(tri_df[c] &gt; 0.95)] # Drop the features in the to_drop list reduced_df = ansur_df.drop(to_drop, axis=1) print(&quot;The reduced dataframe has {} columns.&quot;.format(reduced_df.shape[1])) # The reduced dataframe has 88 columns. . You’ve automated the removal of highly correlated features. . 2.4.2 Nuclear energy and pool drownings . The dataset that has been pre-loaded for you as weird_df contains actual data provided by the US Centers for Disease Control &amp; Prevention and Department of Energy. . Let’s see if we can find a pattern. . # Put nuclear energy production on the x-axis and the number of pool drownings on the y-axis sns.scatterplot(x=&#39;nuclear_energy&#39;, y=&#39;pool_drownings&#39;, data=weird_df) plt.show() . . # Print out the correlation matrix of weird_df print(weird_df.corr()) . pool_drownings nuclear_energy pool_drownings 1.000000 0.901179 nuclear_energy 0.901179 1.000000 . What can you conclude from the strong correlation (r=0.9) between these features? . Not much, correlation does not imply causation. . 3. Feature selection II, selecting for model accuracy . . 3.1 Selecting features for model performance . . 3.1.1 Building a diabetes classifier . You’ll be using the Pima Indians diabetes dataset to predict whether a person has diabetes using logistic regression. There are 8 features and one target in this dataset. The data has been split into a training and test set and pre-loaded for you as X_train , y_train , X_test , and y_test . . A StandardScaler() instance has been predefined as scaler and a LogisticRegression() one as lr . . # Fit the scaler on the training features and transform these in one go X_train_std = scaler.fit_transform(X_train) # Fit the logistic regression model on the scaled training data lr.fit(X_train_std, y_train) # Scale the test features X_test_std = scaler.transform(X_test) # Predict diabetes presence on the scaled test set y_pred = lr.predict(X_test_std) # Prints accuracy metrics and feature coefficients print(&quot;{0:.1%} accuracy on test set.&quot;.format(accuracy_score(y_test, y_pred))) print(dict(zip(X.columns, abs(lr.coef_[0]).round(2)))) . 79.6% accuracy on test set. {&#39;family&#39;: 0.34, &#39;diastolic&#39;: 0.03, &#39;glucose&#39;: 1.23, &#39;triceps&#39;: 0.24, &#39;age&#39;: 0.34, &#39;insulin&#39;: 0.19, &#39;bmi&#39;: 0.38, &#39;pregnant&#39;: 0.04} . 3.1.2 Manual Recursive Feature Elimination . Now that we’ve created a diabetes classifier, let’s see if we can reduce the number of features without hurting the model accuracy too much. . On the second line of code the features are selected from the original dataframe. Adjust this selection. . A StandardScaler() instance has been predefined as scaler and a LogisticRegression() one as lr . . # Remove the feature with the lowest model coefficient X = diabetes_df[[&#39;pregnant&#39;, &#39;glucose&#39;, &#39;triceps&#39;, &#39;insulin&#39;, &#39;bmi&#39;, &#39;family&#39;, &#39;age&#39;]] 80.6% accuracy on test set. {&#39;family&#39;: 0.34, &#39;glucose&#39;: 1.23, &#39;triceps&#39;: 0.24, &#39;age&#39;: 0.35, &#39;insulin&#39;: 0.2, &#39;bmi&#39;: 0.39, &#39;pregnant&#39;: 0.05} . # Remove the 2 features with the lowest model coefficients X = diabetes_df[[&#39;glucose&#39;, &#39;triceps&#39;, &#39;bmi&#39;, &#39;family&#39;, &#39;age&#39;]] 79.6% accuracy on test set. {&#39;family&#39;: 0.34, &#39;age&#39;: 0.37, &#39;bmi&#39;: 0.34, &#39;glucose&#39;: 1.13, &#39;triceps&#39;: 0.25} . # Only keep the feature with the highest coefficient X = diabetes_df[[&#39;glucose&#39;]] 76.5% accuracy on test set. {&#39;glucose&#39;: 1.27} . # Performs a 25-75% train test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0) # Scales features and fits the logistic regression model to the data lr.fit(scaler.fit_transform(X_train), y_train) # Calculates the accuracy on the test set and prints coefficients acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test))) print(&quot;{0:.1%} accuracy on test set.&quot;.format(acc)) print(dict(zip(X.columns, abs(lr.coef_[0]).round(2)))) . Removing all but one feature only reduced the accuracy by a few percent. . 3.1.3 Automatic Recursive Feature Elimination . Now let’s automate this recursive process. Wrap a Recursive Feature Eliminator (RFE) around our logistic regression estimator and pass it the desired number of features. . # Create the RFE with a LogisticRegression estimator and 3 features to select rfe = RFE(estimator=LogisticRegression(), n_features_to_select=3, verbose=1) # Fits the eliminator to the data rfe.fit(X_train, y_train) # Print the features and their ranking (high = dropped early on) print(dict(zip(X.columns, rfe.ranking_))) # Print the features that are not eliminated print(X.columns[rfe.support_]) # Calculates the test set accuracy acc = accuracy_score(y_test, rfe.predict(X_test)) print(&quot;{0:.1%} accuracy on test set.&quot;.format(acc)) . Fitting estimator with 8 features. Fitting estimator with 7 features. Fitting estimator with 6 features. Fitting estimator with 5 features. Fitting estimator with 4 features. {&#39;family&#39;: 2, &#39;diastolic&#39;: 6, &#39;glucose&#39;: 1, &#39;triceps&#39;: 3, &#39;age&#39;: 1, &#39;insulin&#39;: 4, &#39;bmi&#39;: 1, &#39;pregnant&#39;: 5} Index([&#39;glucose&#39;, &#39;bmi&#39;, &#39;age&#39;], dtype=&#39;object&#39;) 80.6% accuracy on test set. . When we eliminate all but the 3 most relevant features we get a 80.6% accuracy on the test set. . . 3.2 Tree-based feature selection . . 3.2.1 Building a random forest model . You’ll again work on the Pima Indians dataset to predict whether an individual has diabetes. This time using a random forest classifier. You’ll fit the model on the training data after performing the train-test split and consult the feature importance values. . # Perform a 75% training and 25% test data split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0) # Fit the random forest model to the training data rf = RandomForestClassifier(random_state=0) rf.fit(X_train, y_train) # Calculate the accuracy acc = accuracy_score(y_test, rf.predict(X_test)) # Print the importances per feature print(dict(zip(X.columns, rf.feature_importances_.round(2)))) # Print accuracy print(&quot;{0:.1%} accuracy on test set.&quot;.format(acc)) . {&#39;family&#39;: 0.12, &#39;diastolic&#39;: 0.08, &#39;glucose&#39;: 0.21, &#39;triceps&#39;: 0.11, &#39;age&#39;: 0.16, &#39;insulin&#39;: 0.13, &#39;bmi&#39;: 0.09, &#39;pregnant&#39;: 0.09} 77.6% accuracy on test set. . The random forest model gets 78% accuracy on the test set and &#39;glucose&#39; is the most important feature ( 0.21 ). . 3.2.2 Random forest for feature selection . # Create a mask for features importances above the threshold mask = rf.feature_importances_ &gt; 0.15 # Apply the mask to the feature dataset X reduced_X = X.loc[:,mask] # prints out the selected column names print(reduced_X.columns) # Index([&#39;glucose&#39;, &#39;age&#39;], dtype=&#39;object&#39;) . Only the features &#39;glucose&#39; and &#39;age&#39; were considered sufficiently important. . 3.2.3 Recursive Feature Elimination with random forests . You’ll wrap a Recursive Feature Eliminator around a random forest model to remove features step by step. This method is more conservative compared to selecting features after applying a single importance threshold. Since dropping one feature can influence the relative importances of the others. . # Wrap the feature eliminator around the random forest model rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, verbose=1) # Fit the model to the training data rfe.fit(X_train, y_train) # Create a mask using an attribute of rfe mask = rfe.support_ # Apply the mask to the feature dataset X and print the result reduced_X = X.loc[:,mask] print(reduced_X.columns) . Fitting estimator with 8 features. Fitting estimator with 7 features. Fitting estimator with 6 features. Fitting estimator with 5 features. Fitting estimator with 4 features. Fitting estimator with 3 features. Index([&#39;glucose&#39;, &#39;bmi&#39;], dtype=&#39;object&#39;) . # Set the feature eliminator to remove 2 features on each step rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, step=2, verbose=1) # Fit the model to the training data rfe.fit(X_train, y_train) # Create a mask mask = rfe.support_ # Apply the mask to the feature dataset X and print the result reduced_X = X.loc[:, mask] print(reduced_X.columns) . Fitting estimator with 8 features. Fitting estimator with 6 features. Fitting estimator with 4 features. Index([&#39;glucose&#39;, &#39;insulin&#39;], dtype=&#39;object&#39;) . Compared to the quick and dirty single threshold method from the previous exercise one of the selected features is different. . . 3.3 Regularized linear regression . 3.3.1 Creating a LASSO regressor . You’ll be working on the numeric ANSUR body measurements dataset to predict a persons Body Mass Index (BMI) using the pre-imported Lasso() regressor. BMI is a metric derived from body height and weight but those two features have been removed from the dataset to give the model a challenge. . You’ll standardize the data first using the StandardScaler() that has been instantiated for you as scaler to make sure all coefficients face a comparable regularizing force trying to bring them down. . # Set the test size to 30% to get a 70-30% train test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) # Fit the scaler on the training features and transform these in one go X_train_std = scaler.fit_transform(X_train) # Create the Lasso model la = Lasso() # Fit it to the standardized training data la.fit(X_train_std, y_train) . 3.3.2 Lasso model results . Now that you’ve trained the Lasso model, you’ll score its predictive capacity (R2) on the test set and count how many features are ignored because their coefficient is reduced to zero. . # Transform the test set with the pre-fitted scaler X_test_std = scaler.transform(X_test) # Calculate the coefficient of determination (R squared) on X_test_std r_squared = la.score(X_test_std, y_test) print(&quot;The model can predict {0:.1%} of the variance in the test set.&quot;.format(r_squared)) # Create a list that has True values when coefficients equal 0 zero_coef = la.coef_ == 0 # Calculate how many features have a zero coefficient n_ignored = sum(zero_coef) print(&quot;The model has ignored {} out of {} features.&quot;.format(n_ignored, len(la.coef_))) . The model can predict 84.7% of the variance in the test set. The model has ignored 82 out of 91 features. . We can predict almost 85% of the variance in the BMI value using just 9 out of 91 of the features. The R^2 could be higher though. . 3.3.3 Adjusting the regularization strength . Your current Lasso model has an R2R2 score of 84.7%. When a model applies overly powerful regularization it can suffer from high bias, hurting its predictive power. . Let’s improve the balance between predictive power and model simplicity by tweaking the alpha parameter. . # Find the highest alpha value with R-squared above 98% la = Lasso(alpha=0.1, random_state=0) # Fits the model and calculates performance stats la.fit(X_train_std, y_train) r_squared = la.score(X_test_std, y_test) n_ignored_features = sum(la.coef_ == 0) # Print peformance stats print(&quot;The model can predict {0:.1%} of the variance in the test set.&quot;.format(r_squared)) print(&quot;{} out of {} features were ignored.&quot;.format(n_ignored_features, len(la.coef_))) . The model can predict 98.3% of the variance in the test set. 64 out of 91 features were ignored. . With this more appropriate regularization strength we can predict 98% of the variance in the BMI value while ignoring 2/3 of the features. . . 3.4 Combining feature selectors . . 3.4.1 Creating a LassoCV regressor . You’ll be predicting biceps circumference on a subsample of the male ANSUR dataset using the LassoCV() regressor that automatically tunes the regularization strength (alpha value) using Cross-Validation. . from sklearn.linear_model import LassoCV # Create and fit the LassoCV model on the training set lcv = LassoCV() lcv.fit(X_train, y_train) print(&#39;Optimal alpha = {0:.3f}&#39;.format(lcv.alpha_)) # Calculate R squared on the test set r_squared = lcv.score(X_test, y_test) print(&#39;The model explains {0:.1%} of the test set variance&#39;.format(r_squared)) # Create a mask for coefficients not equal to zero lcv_mask = lcv.coef_ != 0 print(&#39;{} features out of {} selected&#39;.format(sum(lcv_mask), len(lcv_mask))) . Optimal alpha = 0.089 The model explains 88.2% of the test set variance 26 features out of 32 selected . We got a decent R squared and removed 6 features. We’ll save the lcv_mask for later on. . 3.4.2 Ensemble models for extra votes . The LassoCV() model selected 26 out of 32 features. Not bad, but not a spectacular dimensionality reduction either. Let’s use two more models to select the 10 features they consider most important using the Recursive Feature Eliminator (RFE). . from sklearn.feature_selection import RFE from sklearn.ensemble import GradientBoostingRegressor # Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step rfe_gb = RFE(estimator=GradientBoostingRegressor(), n_features_to_select=10, step=3, verbose=1) rfe_gb.fit(X_train, y_train) # Calculate the R squared on the test set r_squared = rfe_gb.score(X_test, y_test) print(&#39;The model can explain {0:.1%} of the variance in the test set&#39;.format(r_squared)) # Assign the support array to gb_mask gb_mask = rfe_gb.support_ . Fitting estimator with 32 features. Fitting estimator with 29 features. Fitting estimator with 26 features. Fitting estimator with 23 features. Fitting estimator with 20 features. Fitting estimator with 17 features. Fitting estimator with 14 features. Fitting estimator with 11 features. The model can explain 85.6% of the variance in the test set . from sklearn.feature_selection import RFE from sklearn.ensemble import RandomForestRegressor # Select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step rfe_rf = RFE(estimator=RandomForestRegressor(), n_features_to_select=10, step=3, verbose=1) rfe_rf.fit(X_train, y_train) # Calculate the R squared on the test set r_squared = rfe_rf.score(X_test, y_test) print(&#39;The model can explain {0:.1%} of the variance in the test set&#39;.format(r_squared)) # Assign the support array to gb_mask rf_mask = rfe_rf.support_ . Fitting estimator with 32 features. Fitting estimator with 29 features. Fitting estimator with 26 features. Fitting estimator with 23 features. Fitting estimator with 20 features. Fitting estimator with 17 features. Fitting estimator with 14 features. Fitting estimator with 11 features. The model can explain 84.0% of the variance in the test set . Inluding the Lasso linear model from the previous exercise, we now have the votes from 3 models on which features are important. . 3.4.3 Combining 3 feature selectors . We’ll combine the votes of the 3 models you built in the previous exercises, to decide which features are important into a meta mask. We’ll then use this mask to reduce dimensionality and see how a simple linear regressor performs on the reduced dataset. . # Sum the votes of the three models votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0) # Create a mask for features selected by all 3 models meta_mask = votes &gt;= 3 # Apply the dimensionality reduction on X X_reduced = X.loc[:, meta_mask] # Plug the reduced dataset into a linear regression pipeline X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0) lm.fit(scaler.fit_transform(X_train), y_train) r_squared = lm.score(scaler.transform(X_test), y_test) print(&#39;The model can explain {0:.1%} of the variance in the test set using {1:} features.&#39;.format(r_squared, len(lm.coef_))) # The model can explain 86.8% of the variance in the test set using 7 features. . Using the votes from 3 models you were able to select just 7 features that allowed a simple linear model to get a high accuracy! . 4. Feature extraction . . 4.1 Feature extraction . . 4.1.1 Manual feature extraction I . You want to compare prices for specific products between stores. The features in the pre-loaded dataset sales_df are: storeID , product , quantity and revenue . The quantity and revenue features tell you how many items of a particular product were sold in a store and what the total revenue was. For the purpose of your analysis it’s more interesting to know the average price per product. . storeID product quantity revenue 0 A Apples 1811 9300.6 1 A Bananas 1003 3375.2 2 A Oranges 1604 8528.5 3 B Apples 1785 9181.0 4 B Bananas 944 3680.2 . # Calculate the price from the quantity sold and revenue sales_df[&#39;price&#39;] = sales_df[&#39;revenue&#39;] / sales_df[&#39;quantity&#39;] # Drop the quantity and revenue features reduced_df = sales_df.drop([&#39;revenue&#39;, &#39;quantity&#39;], axis=1) print(reduced_df.head()) . storeID product price 0 A Apples 5.135616 1 A Bananas 3.365105 2 A Oranges 5.317020 3 B Apples 5.143417 4 B Bananas 3.898517 . When you understand the dataset well, always check if you can calculate relevant features and drop irrelevant ones. . 4.1.2 Manual feature extraction II . You’re working on a variant of the ANSUR dataset, height_df , where a person’s height was measured 3 times. Add a feature with the mean height to the dataset and then drop the 3 original features. . weight_kg height_1 height_2 height_3 0 81.5 1.78 1.80 1.80 1 72.6 1.70 1.70 1.69 2 92.9 1.74 1.75 1.73 3 79.4 1.66 1.68 1.67 4 94.6 1.91 1.93 1.90 . # Calculate the mean height height_df[&#39;height&#39;] = height_df[[&#39;height_1&#39;,&#39;height_2&#39;,&#39;height_3&#39;]].mean(axis=1) # Drop the 3 original height features reduced_df = height_df.drop([&#39;height_1&#39;,&#39;height_2&#39;,&#39;height_3&#39;], axis=1) print(reduced_df.head()) . weight_kg height 0 81.5 1.793333 1 72.6 1.696667 2 92.9 1.740000 3 79.4 1.670000 4 94.6 1.913333 . 4.1.3 Principal component intuition . . After standardizing the lower and upper arm lengths from the ANSUR dataset we’ve added two perpendicular vectors that are aligned with the main directions of variance. We can describe each point in the dataset as a combination of these two vectors multiplied with a value each. These values are then called principal components. . People with a negative component for the yellow vector have long forearms relative to their upper arms. . . 4.2 Principal component analysis . . 4.2.1 Calculating Principal Components . You’ll visually inspect a 4 feature sample of the ANSUR dataset before and after PCA using Seaborn’s pairplot() . This will allow you to inspect the pairwise correlations between the features. . # Create a pairplot to inspect ansur_df sns.pairplot(ansur_df) plt.show() . . from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA # Create the scaler scaler = StandardScaler() ansur_std = scaler.fit_transform(ansur_df) # Create the PCA instance and fit and transform the data with pca pca = PCA() pc = pca.fit_transform(ansur_std) pc_df = pd.DataFrame(pc, columns=[&#39;PC 1&#39;, &#39;PC 2&#39;, &#39;PC 3&#39;, &#39;PC 4&#39;]) # Create a pairplot of the principal component dataframe sns.pairplot(pc_df) plt.show() . . Notice how, in contrast to the input features, none of the principal components are correlated to one another. . 4.2.2 PCA on a larger dataset . You’ll now apply PCA on a somewhat larger ANSUR datasample with 13 dimensions, once again pre-loaded as ansur_df . The fitted model will be used in the next exercise. Since we are not using the principal components themselves there is no need to transform the data, instead, it is sufficient to fit pca to the data. . from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA # Scale the data scaler = StandardScaler() ansur_std = scaler.fit_transform(ansur_df) # Apply PCA pca = PCA() pca.fit(ansur_std) . You’ve fitted PCA on our 13 feature datasample. Now let’s see how the components explain the variance. . 4.2.3 PCA explained variance . You’ll be inspecting the variance explained by the different principal components of the pca instance you created in the previous exercise. . # Inspect the explained variance ratio per component print(pca.explained_variance_ratio_) [0.61449404 0.19893965 0.06803095 0.03770499 0.03031502 0.0171759 0.01072762 0.00656681 0.00634743 0.00436015 0.0026586 0.00202617 0.00065268] . # Print the cumulative sum of the explained variance ratio print(pca.explained_variance_ratio_.cumsum()) [0.61449404 0.81343368 0.88146463 0.91916962 0.94948464 0.96666054 0.97738816 0.98395496 0.99030239 0.99466254 0.99732115 0.99934732 1. ] . What’s the lowest number of principal components you should keep if you don’t want to lose more than 10% of explained variance during dimensionality reduction? . 4 principal components . Using no more than 4 principal components we can explain more than 90% of the variance in the 13 feature dataset. . . 4.3 PCA applications . . 4.3.1 Understanding the components . You’ll apply PCA to the numeric features of the Pokemon dataset, poke_df , using a pipeline to combine the feature scaling and PCA in one go. You’ll then interpret the meanings of the first two components. . # Build the pipeline pipe = Pipeline([(&#39;scaler&#39;, StandardScaler()), (&#39;reducer&#39;, PCA(n_components=2))]) # Fit it to the dataset and extract the component vectors pipe.fit(poke_df) vectors = pipe.steps[1][1].components_.round(2) # Print feature effects print(&#39;PC 1 effects = &#39; + str(dict(zip(poke_df.columns, vectors[0])))) print(&#39;PC 2 effects = &#39; + str(dict(zip(poke_df.columns, vectors[1])))) PC 1 effects = {&#39;Speed&#39;: 0.34, &#39;Sp. Def&#39;: 0.45, &#39;Defense&#39;: 0.36, &#39;Sp. Atk&#39;: 0.46, &#39;HP&#39;: 0.39, &#39;Attack&#39;: 0.44} PC 2 effects = {&#39;Speed&#39;: -0.67, &#39;Sp. Def&#39;: 0.24, &#39;Defense&#39;: 0.63, &#39;Sp. Atk&#39;: -0.31, &#39;HP&#39;: 0.08, &#39;Attack&#39;: -0.01} . PC1: All features have a similar positive effect. PC 1 can be interpreted as a measure of overall quality (high stats). | PC2: Defense has a strong positive effect on the second component and speed a strong negative one. This component quantifies an agility vs. armor &amp; protection trade-off. | . You’ve used the pipeline for the first time and understand how the features relate to the components. . 4.3.2 PCA for feature exploration . You’ll use the PCA pipeline you’ve built in the previous exercise to visually explore how some categorical features relate to the variance in poke_df . These categorical features ( Type &amp; Legendary ) can be found in a separate dataframe poke_cat_df . . poke_df.head() HP Attack Defense Sp. Atk Sp. Def Speed 0 45 49 49 65 65 45 1 60 62 63 80 80 60 2 80 82 83 100 100 80 3 80 100 123 122 120 80 4 39 52 43 60 50 65 . pipe = Pipeline([(&#39;scaler&#39;, StandardScaler()), (&#39;reducer&#39;, PCA(n_components=2))]) # Fit the pipeline to poke_df and transform the data pc = pipe.fit_transform(poke_df) . pc [[-1.5563747 -0.02148212] [-0.36286656 -0.05026854] [ 1.28015158 -0.06272022] ... [ 2.45821626 -0.51588158] [ 3.5303971 -0.95106516] [ 2.23378629 0.53762985]] . # Add the 2 components to poke_cat_df poke_cat_df[&#39;PC 1&#39;] = pc[:, 0] poke_cat_df[&#39;PC 2&#39;] = pc[:, 1] . # Use the Type feature to color the PC 1 vs PC 2 scatterplot sns.scatterplot(data=poke_cat_df, x=&#39;PC 1&#39;, y=&#39;PC 2&#39;, hue=&#39;Type&#39;) plt.show() . . # Use the Legendary feature to color the PC 1 vs PC 2 scatterplot sns.scatterplot(data=poke_cat_df, x=&#39;PC 1&#39;, y=&#39;PC 2&#39;, hue=&#39;Legendary&#39;) plt.show() . . Looks like the different types are scattered all over the place while the legendary pokemon always score high for PC 1 meaning they have high stats overall. Their spread along the PC 2 axis tells us they aren’t consistently fast and vulnerable or slow and armored. . 4.3.3 PCA in a model pipeline . We just saw that legendary pokemon tend to have higher stats overall. Let’s see if we can add a classifier to our pipeline that detects legendary versus non-legendary pokemon based on the principal components. . # Build the pipeline pipe = Pipeline([ (&#39;scaler&#39;, StandardScaler()), (&#39;reducer&#39;, PCA(n_components=3)), (&#39;classifier&#39;, RandomForestClassifier(random_state=0))]) # Fit the pipeline to the training data pipe.fit(X_train, y_train) # Score the accuracy on the test set accuracy = pipe.score(X_test, y_test) # Prints the explained variance ratio and accuracy print(pipe.steps[1][1].explained_variance_ratio_) print(&#39;{0:.1%} test set accuracy&#39;.format(accuracy)) . PCA(n_components=2) [0.45624044 0.17767414] 95.8% test set accuracy . PCA(n_components=3) [0.45624044 0.17767414 0.12858833] 95.0% test set accuracy . Looks like adding the third component does not increase the model accuracy, even though it adds information to the dataset. . . 4.4 Principal Component selection . . 4.4.1 Selecting the proportion of variance to keep . You’ll let PCA determine the number of components to calculate based on an explained variance threshold that you decide. . # Let PCA select 90% of the variance pipe = Pipeline([(&#39;scaler&#39;, StandardScaler()), (&#39;reducer&#39;, PCA(n_components=0.9))]) # Fit the pipe to the data pipe.fit(ansur_df) print(&#39;{} components selected&#39;.format(len(pipe.steps[1][1].components_))) . PCA(n_components=0.8) 11 components selected PCA(n_components=0.9) 23 components selected . We need to more than double the components to go from 80% to 90% explained variance. . 4.4.2 Choosing the number of components . You’ll now make a more informed decision on the number of principal components to reduce your data to using the “elbow in the plot” technique. . # Pipeline a scaler and pca selecting 10 components pipe = Pipeline([(&#39;scaler&#39;, StandardScaler()), (&#39;reducer&#39;, PCA(n_components=10))]) # Fit the pipe to the data pipe.fit(ansur_df) # Plot the explained variance ratio plt.plot(pipe.steps[1][1].explained_variance_ratio_) plt.xlabel(&#39;Principal component index&#39;) plt.ylabel(&#39;Explained variance ratio&#39;) plt.show() . . To how many components can you reduce the dataset without compromising too much on explained variance? Note that the x-axis is zero indexed. . The ‘elbow’ in the plot is at 3 components (the 3rd component has index 2). . 4.4.3 PCA for image compression . You’ll reduce the size of 16 images with hand written digits (MNIST dataset) using PCA. . The samples are 28 by 28 pixel gray scale images that have been flattened to arrays with 784 elements each (28 x 28 = 784) and added to the 2D numpy array X_test . Each of the 784 pixels has a value between 0 and 255 and can be regarded as a feature. . A pipeline with a scaler and PCA model to select 78 components has been pre-loaded for you as pipe . This pipeline has already been fitted to the entire MNIST dataset except for the 16 samples in X_test . . Finally, a function plot_digits has been created for you that will plot 16 images in a grid. . # Plot the MNIST sample data plot_digits(X_test) . . pipe Pipeline(memory=None, steps=[(&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with_std=True)), (&#39;reducer&#39;, PCA(copy=True, iterated_power=&#39;auto&#39;, n_components=78, random_state=None, svd_solver=&#39;auto&#39;, tol=0.0, whiten=False))]) . # Transform the input data to principal components pc = pipe.transform(X_test) # Prints the number of features per dataset print(&quot;X_test has {} features&quot;.format(X_test.shape[1])) print(&quot;pc has {} features&quot;.format(pc.shape[1])) # X_test has 784 features # pc has 78 features # Inverse transform the components to original feature space X_rebuilt = pipe.inverse_transform(pc) # Prints the number of features print(&quot;X_rebuilt has {} features&quot;.format(X_rebuilt.shape[1])) # X_rebuilt has 784 features # Plot the reconstructed data plot_digits(X_rebuilt) . . You’ve reduced the size of the data 10 fold but were able to reconstruct images with reasonable quality. . . ### The End . . . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/dimensionality-reduction-in-python.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/dimensionality-reduction-in-python.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post32": {
            "title": "Deep Learning in Python",
            "content": "Deep Learning in Python . This is the memo of the 25th course of ‘Data Scientist with Python’ track. . You can find the original course HERE . . . 1. Basics of deep learning and neural networks . . 1.1 Introduction to deep learning . . . . . 1.2 Forward propagation . . #### Coding the forward propagation algorithm . In this exercise, you’ll write code to do forward propagation (prediction) for your first neural network: . . Each data point is a customer. The first input is how many accounts they have, and the second input is how many children they have. The model will predict how many transactions the user makes in the next year. . You will use this data throughout the first 2 chapters of this course. . input_data # array([3, 5]) weights # {&#39;node_0&#39;: array([2, 4]), &#39;node_1&#39;: array([ 4, -5]), &#39;output&#39;: array([2, 7])} . input_data * weights[&#39;node_0&#39;] # array([ 6, 20]) np.array([3, 5]) * np.array([2, 4]) # array([ 6, 20]) (input_data * weights[&#39;node_0&#39;]).sum() # 26 . # Calculate node 0 value: node_0_value node_0_value = (input_data * weights[&#39;node_0&#39;]).sum() # Calculate node 1 value: node_1_value node_1_value = (input_data * weights[&#39;node_1&#39;]).sum() # Put node values into array: hidden_layer_outputs hidden_layer_outputs = np.array([node_0_value, node_1_value]) # Calculate output: output output = (hidden_layer_outputs * weights[&#39;output&#39;]).sum() # Print output print(output) # -39 . It looks like the network generated a prediction of -39 . . . 1.3 Activation functions . . . . #### The Rectified Linear Activation Function . An “activation function” is a function applied at each node. It converts the node’s input into some output. . The rectified linear activation function (called ReLU ) has been shown to lead to very high-performance networks. This function takes a single number as an input, returning 0 if the input is negative, and the input if the input is positive. . Here are some examples: . relu(3) = 3 . relu(-3) = 0 . def relu(input): &#39;&#39;&#39;Define your relu activation function here&#39;&#39;&#39; # Calculate the value for the output of the relu function: output output = max(input, 0) # Return the value just calculated return(output) # Calculate node 0 value: node_0_output node_0_input = (input_data * weights[&#39;node_0&#39;]).sum() node_0_output = relu(node_0_input) # Calculate node 1 value: node_1_output node_1_input = (input_data * weights[&#39;node_1&#39;]).sum() node_1_output = relu(node_1_input) # Put node values into array: hidden_layer_outputs hidden_layer_outputs = np.array([node_0_output, node_1_output]) # Calculate model output (do not apply relu) model_output = (hidden_layer_outputs * weights[&#39;output&#39;]).sum() # Print model output print(model_output) # 52 . You predicted 52 transactions. Without this activation function, you would have predicted a negative number! . The real power of activation functions will come soon when you start tuning model weights. . #### Applying the network to many observations/rows of data . input_data [array([3, 5]), array([ 1, -1]), array([0, 0]), array([8, 4])] weights {&#39;node_0&#39;: array([2, 4]), &#39;node_1&#39;: array([ 4, -5]), &#39;output&#39;: array([2, 7])} . def relu(input): &#39;&#39;&#39;Define relu activation function&#39;&#39;&#39; return(max(input, 0)) # Define predict_with_network() def predict_with_network(input_data_row, weights): # Calculate node 0 value node_0_input = (input_data_row * weights[&#39;node_0&#39;]).sum() node_0_output = relu(node_0_input) # Calculate node 1 value node_1_input = (input_data_row * weights[&#39;node_1&#39;]).sum() node_1_output = relu(node_1_input) # Put node values into array: hidden_layer_outputs hidden_layer_outputs = np.array([node_0_output, node_1_output]) # Calculate model output input_to_final_layer = (hidden_layer_outputs * weights[&#39;output&#39;]).sum() model_output = relu(input_to_final_layer) # Return model output return(model_output) # Create empty list to store prediction results results = [] for input_data_row in input_data: # Append prediction to results results.append(predict_with_network(input_data_row, weights)) # Print results print(results) # [52, 63, 0, 148] . . 1.4 Deeper networks . . . #### Forward propagation in a deeper network . You now have a model with 2 hidden layers. The values for an input data point are shown inside the input nodes. The weights are shown on the edges/lines. What prediction would this model make on this data point? . Assume the activation function at each node is the identity function . That is, each node’s output will be the same as its input. So the value of the bottom node in the first hidden layer is -1, and not 0, as it would be if the ReLU activation function was used. . . | Hidden Layer 1 | Hidden Layer 2 | Prediction | | 6 | -1 | | | | | 0 | | -1 | 5 | | . #### Multi-layer neural networks . In this exercise, you’ll write code to do forward propagation for a neural network with 2 hidden layers. Each hidden layer has two nodes. . input_data array([3, 5]) weights {&#39;node_0_0&#39;: array([2, 4]), &#39;node_0_1&#39;: array([ 4, -5]), &#39;node_1_0&#39;: array([-1, 2]), &#39;node_1_1&#39;: array([1, 2]), &#39;output&#39;: array([2, 7])} . def predict_with_network(input_data): # Calculate node 0 in the first hidden layer node_0_0_input = (input_data * weights[&#39;node_0_0&#39;]).sum() node_0_0_output = relu(node_0_0_input) # Calculate node 1 in the first hidden layer node_0_1_input = (input_data * weights[&#39;node_0_1&#39;]).sum() node_0_1_output = relu(node_0_1_input) # Put node values into array: hidden_0_outputs hidden_0_outputs = np.array([node_0_0_output, node_0_1_output]) # Calculate node 0 in the second hidden layer node_1_0_input = (hidden_0_outputs * weights[&#39;node_1_0&#39;]).sum() node_1_0_output = relu(node_1_0_input) # Calculate node 1 in the second hidden layer node_1_1_input = (hidden_0_outputs * weights[&#39;node_1_1&#39;]).sum() node_1_1_output = relu(node_1_1_input) # Put node values into array: hidden_1_outputs hidden_1_outputs = np.array([node_1_0_output, node_1_1_output]) # Calculate model output: model_output model_output = (hidden_1_outputs * weights[&#39;output&#39;]).sum() # Return model_output return(model_output) output = predict_with_network(input_data) print(output) # 182 . #### Representations are learned . How are the weights that determine the features/interactions in Neural Networks created? . The model training process sets them to optimize predictive accuracy. . #### Levels of representation . Which layers of a model capture more complex or “higher level” interactions? . The last layers capture the most complex interactions. . . 2. Optimizing a neural network with backward propagation . . 2.1 The need for optimization . #### Calculating model errors . What is the error (predicted – actual) for the following network when the input data is [3, 2] and the actual value of the target (what you are trying to predict) is 5? . . prediction = (32 + 21) * 2 + (30 + 20)*2 . =16 . error = 16 – 5 = 11 . #### Understanding how weights change model accuracy . Imagine you have to make a prediction for a single data point. The actual value of the target is 7. The weight going from node_0 to the output is 2, as shown below. . If you increased it slightly, changing it to 2.01, would the predictions become more accurate, less accurate, or stay the same? . . prediction_before = 16 . error_before = 16 – 7 = 9 . prediction_after = (32.01 + 21) * 2 + (30 + 20)*2 . =16.x . error_after = 9.x . Increasing the weight to 2.01 would increase the resulting error from 9 to 9.08 , making the predictions less accurate. . #### Coding how weight changes affect accuracy . Now you’ll get to change weights in a real network and see how they affect model accuracy! . . Have a look at the following neural network: . Its weights have been pre-loaded as weights_0 . Your task in this exercise is to update a single weight in weights_0 to create weights_1 , which gives a perfect prediction (in which the predicted value is equal to target_actual 3). # The data point you will make a prediction for input_data = np.array([0, 3]) # Sample weights weights_0 = {&#39;node_0&#39;: [2, 1], &#39;node_1&#39;: [1, 2], &#39;output&#39;: [1, 1] } # The actual target value, used to calculate the error target_actual = 3 # Make prediction using original weights model_output_0 = predict_with_network(input_data, weights_0) # Calculate error: error_0 error_0 = model_output_0 - target_actual # Create weights that cause the network to make perfect prediction (3): weights_1 weights_1 = {&#39;node_0&#39;: [2, 1], &#39;node_1&#39;: [1, 2], &#39;output&#39;: [1, 0] } # Make prediction using new weights: model_output_1 model_output_1 = predict_with_network(input_data, weights_1) # Calculate error: error_1 error_1 = model_output_1 - target_actual # Print error_0 and error_1 print(error_0) print(error_1) # 6 # 0 . #### Scaling up to multiple data points . You’ve seen how different weights will have different accuracies on a single prediction. But usually, you’ll want to measure model accuracy on many points. . You’ll now write code to compare model accuracies for two different sets of weights, which have been stored as weights_0 and weights_1 . . input_data [array([0, 3]), array([1, 2]), array([-1, -2]), array([4, 0])] target_actuals [1, 3, 5, 7] weights_0 {&#39;node_0&#39;: array([2, 1]), &#39;node_1&#39;: array([1, 2]), &#39;output&#39;: array([1, 1])} weights_1 {&#39;node_0&#39;: array([2, 1]), &#39;node_1&#39;: array([1. , 1.5]), &#39;output&#39;: array([1. , 1.5])} . from sklearn.metrics import mean_squared_error # Create model_output_0 model_output_0 = [] # Create model_output_1 model_output_1 = [] # Loop over input_data for row in input_data: # Append prediction to model_output_0 model_output_0.append(predict_with_network(row, weights_0)) # Append prediction to model_output_1 model_output_1.append(predict_with_network(row, weights_1)) # Calculate the mean squared error for model_output_0: mse_0 mse_0 = mean_squared_error(target_actuals, model_output_0) # Calculate the mean squared error for model_output_1: mse_1 mse_1 = mean_squared_error(target_actuals, model_output_1) # Print mse_0 and mse_1 print(&quot;Mean squared error with weights_0: %f&quot; %mse_0) print(&quot;Mean squared error with weights_1: %f&quot; %mse_1) # Mean squared error with weights_0: 37.500000 # Mean squared error with weights_1: 49.890625 . It looks like model_output_1 has a higher mean squared error. . . 2.2 Gradient descent . . ex. learning rate = 0.01 . w.r.t. = with respect to . new weight = 2 – -24 * 0.01 = 2.24 . #### Calculating slopes . You’re now going to practice calculating slopes. . When plotting the mean-squared error loss function against predictions, the slope is 2 * x * (y-xb) , or 2 * input_data * error . . Note that x and b may have multiple numbers ( x is a vector for each data point, and b is a vector). In this case, the output will also be a vector, which is exactly what you want. . You’re ready to write the code to calculate this slope while using a single data point. . input_data array([1, 2, 3]) weights array([0, 2, 1]) target 0 . # Calculate the predictions: preds preds = (weights * input_data).sum() # Calculate the error: error error = target - preds # Calculate the slope: slope slope = 2 * input_data * error # Print the slope print(slope) # [-14 -28 -42] . You can now use this slope to improve the weights of the model! . #### Improving model weights . You’ve just calculated the slopes you need. Now it’s time to use those slopes to improve your model. . If you add the slopes to your weights, you will move in the right direction. However, it’s possible to move too far in that direction. . So you will want to take a small step in that direction first, using a lower learning rate, and verify that the model is improving. . # Set the learning rate: learning_rate learning_rate = 0.01 # Calculate the predictions: preds preds = (weights * input_data).sum() # weights # array([0, 2, 1]) # Calculate the error: error error = preds - target # Calculate the slope: slope slope = 2 * input_data * error # slope # array([14, 28, 42]) # Update the weights: weights_updated weights_updated = weights - learning_rate * slope # weights_updated # array([-0.14, 1.72, 0.58]) # Get updated predictions: preds_updated preds_updated = (weights_updated * input_data).sum() # Calculate updated error: error_updated error_updated = preds_updated - target # Print the original error print(error) # Print the updated error print(error_updated) # 7 # 5.04 . Updating the model weights did indeed decrease the error! . #### Making multiple updates to weights . You’re now going to make multiple updates so you can dramatically improve your model weights, and see how the predictions improve with each update. . get_slope? Signature: get_slope(input_data, target, weights) Docstring: &lt;no docstring&gt; File: /tmp/tmpt3wthzls/&lt;ipython-input-1-7b11d278e306&gt; Type: function get_mse? Signature: get_mse(input_data, target, weights) Docstring: &lt;no docstring&gt; File: /tmp/tmpt3wthzls/&lt;ipython-input-1-7b11d278e306&gt; Type: function . n_updates = 20 mse_hist = [] # Iterate over the number of updates for i in range(n_updates): # Calculate the slope: slope slope = get_slope(input_data, target, weights) # Update the weights: weights weights = weights - slope * 0.01 # Calculate mse with new weights: mse mse = get_mse(input_data, target, weights) # Append the mse to mse_hist mse_hist.append(mse) # Plot the mse history plt.plot(mse_hist) plt.xlabel(&#39;Iterations&#39;) plt.ylabel(&#39;Mean Squared Error&#39;) plt.show() . . As you can see, the mean squared error decreases as the number of iterations go up. . . 2.3 Back propagation . . #### The relationship between forward and backward propagation . If you have gone through 4 iterations of calculating slopes (using backward propagation) and then updated weights. . How many times must you have done forward propagation? . 4 . Each time you generate predictions using forward propagation, you update the weights using backward propagation. . #### Thinking about backward propagation . If your predictions were all exactly right, and your errors were all exactly 0, the slope of the loss function with respect to your predictions would also be 0. . In that circumstance,  the updates to all weights in the network would also be 0. . . 2.4 Backpropagation in practice . . slope = 2 * imput * error . 6 and 18 are slopes just calculated in the above graph . x &lt;= 0: slope = 0 . x &gt; 0: slope = 1 . gradient = input(white) * slope(red) * ReLU_slope(=1 here) . gradient_0 = 061 = 0 . gradient_3 = 1181 = 18 . #### A round of backpropagation . In the network shown below, we have done forward propagation, and node values calculated as part of forward propagation are shown in white. . The weights are shown in black. . Layers after the question mark show the slopes calculated as part of back-prop, rather than the forward-prop values. Those slope values are shown in purple. . This network again uses the ReLU activation function, so the slope of the activation function is 1 for any node receiving a positive value as input. . Assume the node being examined had a positive value (so the activation function’s slope is 1). . . What is the slope needed to update the weight with the question mark? . . gradient = input(white) * slope(purple) * ReLU_slope(=1 here) . = 231 = 6 . . 3. Building deep learning models with keras . . 3.1 Creating a keras model . . #### Understanding your data . You will soon start building models in Keras to predict wages based on various professional and demographic factors. . Before you start building a model, it’s good to understand your data by performing some exploratory analysis. . df.head() wage_per_hour union education_yrs experience_yrs age female marr 0 5.10 0 8 21 35 1 1 1 4.95 0 9 42 57 1 1 2 6.67 0 12 1 19 0 0 3 4.00 0 12 4 22 0 0 4 7.50 0 12 17 35 0 1 south manufacturing construction 0 0 1 0 1 0 1 0 2 0 1 0 3 0 0 0 4 0 0 0 . #### Specifying a model . Now you’ll get to work with your first model in Keras, and will immediately be able to run more complex neural network models on larger datasets compared to the first two chapters. . To start, you’ll take the skeleton of a neural network and add a hidden layer and an output layer. You’ll then fit that model and see Keras do the optimization so your model continually gets better. . predictors[:3] array([[ 0, 8, 21, 35, 1, 1, 0, 1, 0], [ 0, 9, 42, 57, 1, 1, 0, 1, 0], [ 0, 12, 1, 19, 0, 0, 0, 1, 0]]) . # Import necessary modules import keras from keras.layers import Dense from keras.models import Sequential # Save the number of columns in predictors: n_cols n_cols = predictors.shape[1] # Set up the model: model model = Sequential() # Add the first layer model.add(Dense(50, activation=&#39;relu&#39;, input_shape=(n_cols,))) # Add the second layer model.add(Dense(32, activation=&#39;relu&#39;)) # Add the output layer model.add(Dense(1)) . Now that you’ve specified the model, the next step is to compile it. . . 3.2 Compiling and fitting a model . #### Compiling the model . You’re now going to compile the model you specified earlier. To compile the model, you need to specify the optimizer and loss function to use. . The Adam optimizer is an excellent choice. You can read more about it as well as other keras optimizers here , and if you are really curious to learn more, you can read the original paper that introduced the Adam optimizer. . In this exercise, you’ll use the Adam optimizer and the mean squared error loss function. Go for it! . # Import necessary modules import keras from keras.layers import Dense from keras.models import Sequential # Specify the model n_cols = predictors.shape[1] model = Sequential() model.add(Dense(50, activation=&#39;relu&#39;, input_shape = (n_cols,))) model.add(Dense(32, activation=&#39;relu&#39;)) model.add(Dense(1)) # Compile the model model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;) # Verify that model contains information from compiling print(&quot;Loss function: &quot; + model.loss) # Loss function: mean_squared_error . #### Fitting the model . # Import necessary modules import keras from keras.layers import Dense from keras.models import Sequential # Specify the model n_cols = predictors.shape[1] model = Sequential() model.add(Dense(50, activation=&#39;relu&#39;, input_shape = (n_cols,))) model.add(Dense(32, activation=&#39;relu&#39;)) model.add(Dense(1)) # Compile the model model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;) # Fit the model model.fit(predictors, target) . Epoch 1/10 32/534 [&gt;.............................] - ETA: 1s - loss: 146.0927 534/534 [==============================] - 0s - loss: 78.1405 Epoch 2/10 32/534 [&gt;.............................] - ETA: 0s - loss: 85.0537 534/534 [==============================] - 0s - loss: 30.3265 Epoch 3/10 32/534 [&gt;.............................] - ETA: 0s - loss: 21.0463 534/534 [==============================] - 0s - loss: 27.0886 Epoch 4/10 32/534 [&gt;.............................] - ETA: 0s - loss: 16.8466 534/534 [==============================] - 0s - loss: 25.1240 Epoch 5/10 32/534 [&gt;.............................] - ETA: 0s - loss: 23.2123 534/534 [==============================] - 0s - loss: 24.0247 Epoch 6/10 32/534 [&gt;.............................] - ETA: 0s - loss: 13.3941 534/534 [==============================] - 0s - loss: 23.2055 Epoch 7/10 32/534 [&gt;.............................] - ETA: 0s - loss: 28.1707 534/534 [==============================] - 0s - loss: 22.4556 Epoch 8/10 32/534 [&gt;.............................] - ETA: 0s - loss: 11.3898 534/534 [==============================] - 0s - loss: 22.0805 Epoch 9/10 32/534 [&gt;.............................] - ETA: 0s - loss: 21.9370 480/534 [=========================&gt;....] - ETA: 0s - loss: 21.9982 534/534 [==============================] - 0s - loss: 21.7470 Epoch 10/10 32/534 [&gt;.............................] - ETA: 0s - loss: 5.4697 534/534 [==============================] - 0s - loss: 21.5538 &lt;keras.callbacks.History at 0x7f0fc2b49390&gt; . You now know how to specify, compile, and fit a deep learning model using keras! . . 3.3 Classification models . #### Understanding your classification data . Now you will start modeling with a new dataset for a classification problem. This data includes information about passengers on the Titanic. . You will use predictors such as age , fare and where each passenger embarked from to predict who will survive. This data is from a tutorial on data science competitions . Look here for descriptions of the features. . df.head(3) survived pclass ... embarked_from_queenstown embarked_from_southampton 0 0 3 ... 0 1 1 1 1 ... 0 0 2 1 3 ... 0 1 [3 rows x 11 columns] df.columns Index([&#39;survived&#39;, &#39;pclass&#39;, &#39;age&#39;, &#39;sibsp&#39;, &#39;parch&#39;, &#39;fare&#39;, &#39;male&#39;, &#39;age_was_missing&#39;, &#39;embarked_from_cherbourg&#39;, &#39;embarked_from_queenstown&#39;, &#39;embarked_from_southampton&#39;], dtype=&#39;object&#39;) . #### Last steps in classification models . You’ll now create a classification model using the titanic dataset. . Here, you’ll use the &#39;sgd&#39; optimizer, which stands for Stochastic Gradient Descent . You’ll now create a classification model using the titanic dataset. . # Import necessary modules import keras from keras.layers import Dense from keras.models import Sequential from keras.utils import to_categorical # Convert the target to categorical: target target = to_categorical(df.survived) # Set up the model model = Sequential() # Add the first layer model.add(Dense(32, activation=&#39;relu&#39;, input_shape=(n_cols,))) # Add the output layer model.add(Dense(2, activation=&#39;softmax&#39;)) # Compile the model model.compile(optimizer=&#39;sgd&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) # Fit the model model.fit(predictors, target) . Epoch 1/10 32/891 [&gt;.............................] - ETA: 0s - loss: 7.6250 - acc: 0.2188 576/891 [==================&gt;...........] - ETA: 0s - loss: 2.6143 - acc: 0.6024 891/891 [==============================] - 0s - loss: 2.5170 - acc: 0.5948 ... Epoch 10/10 32/891 [&gt;.............................] - ETA: 0s - loss: 0.4892 - acc: 0.7500 736/891 [=======================&gt;......] - ETA: 0s - loss: 0.6318 - acc: 0.6807 891/891 [==============================] - 0s - loss: 0.6444 - acc: 0.6779 . This simple model is generating an accuracy of 68! . . 3.4 Using models . . #### Making predictions . In this exercise, your predictions will be probabilities, which is the most common way for data scientists to communicate their predictions to colleagues. . # Specify, compile, and fit the model model = Sequential() model.add(Dense(32, activation=&#39;relu&#39;, input_shape = (n_cols,))) model.add(Dense(2, activation=&#39;softmax&#39;)) model.compile(optimizer=&#39;sgd&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.fit(predictors, target) # Calculate predictions: predictions predictions = model.predict(pred_data) # Calculate predicted probability of survival: predicted_prob_true predicted_prob_true = predictions[:,1] # print predicted_prob_true print(predicted_prob_true) . predicted_prob_true array([0.20054096, 0.3806974 , 0.6795431 , 0.45789802, 0.16493829, ... 0.12394663], dtype=float32) . You’re now ready to begin learning how to fine-tune your models. . . 4. Fine-tuning keras models . . 4.1 Understanding model optimization . #### Diagnosing optimization problems . All of the following could prevent a model from showing an improved loss in its first few epochs. . Learning rate too low. | Learning rate too high. | Poor choice of activation function. | . #### Changing optimization parameters . It’s time to get your hands dirty with optimization. You’ll now try optimizing a model at a very low learning rate, a very high learning rate, and a “just right” learning rate. . You’ll want to look at the results after running this exercise, remembering that a low value for the loss function is good. . For these exercises, we’ve pre-loaded the predictors and target values from your previous classification models (predicting who would survive on the Titanic). . You’ll want the optimization to start from scratch every time you change the learning rate, to give a fair comparison of how each learning rate did in your results. So we have created a function get_new_model() that creates an unoptimized model to optimize. . # Import the SGD optimizer from keras.optimizers import SGD # Create list of learning rates: lr_to_test lr_to_test = [0.000001, 0.01, 1] # Loop over learning rates for lr in lr_to_test: print(&#39; n nTesting model with learning rate: %f n&#39;%lr ) # Build new model to test, unaffected by previous models model = get_new_model() # Create SGD optimizer with specified learning rate: my_optimizer my_optimizer = SGD(lr=lr) # Compile the model model.compile(optimizer=my_optimizer, loss=&#39;categorical_crossentropy&#39;) # Fit the model model.fit(predictors, target) . Testing model with learning rate: 0.000001 Epoch 1/10 32/891 [&gt;.............................] - ETA: 1s - loss: 3.6053 640/891 [====================&gt;.........] - ETA: 0s - loss: 1.9211 891/891 [==============================] - 0s - loss: 1.6579 ... Epoch 10/10 32/891 [&gt;.............................] - ETA: 0s - loss: 0.5917 672/891 [=====================&gt;........] - ETA: 0s - loss: 0.5966 891/891 [==============================] - 0s - loss: 0.6034 Testing model with learning rate: 0.010000 Epoch 1/10 32/891 [&gt;.............................] - ETA: 1s - loss: 1.0910 576/891 [==================&gt;...........] - ETA: 0s - loss: 1.8064 891/891 [==============================] - 0s - loss: 1.4091 ... Epoch 10/10 32/891 [&gt;.............................] - ETA: 0s - loss: 0.6419 672/891 [=====================&gt;........] - ETA: 0s - loss: 0.5787 891/891 [==============================] - 0s - loss: 0.5823 Testing model with learning rate: 1.000000 Epoch 1/10 32/891 [&gt;.............................] - ETA: 1s - loss: 1.0273 608/891 [===================&gt;..........] - ETA: 0s - loss: 1.9649 891/891 [==============================] - 0s - loss: 1.8966 ... Epoch 10/10 32/891 [&gt;.............................] - ETA: 0s - loss: 0.7226 672/891 [=====================&gt;........] - ETA: 0s - loss: 0.6031 891/891 [==============================] - 0s - loss: 0.6060 . . 4.2 Model validation . #### Evaluating model accuracy on validation dataset . Now it’s your turn to monitor model accuracy with a validation data set. A model definition has been provided as model . Your job is to add the code to compile it and then fit it. You’ll check the validation score in each epoch. . # Save the number of columns in predictors: n_cols n_cols = predictors.shape[1] input_shape = (n_cols,) # Specify the model model = Sequential() model.add(Dense(100, activation=&#39;relu&#39;, input_shape = input_shape)) model.add(Dense(100, activation=&#39;relu&#39;)) model.add(Dense(2, activation=&#39;softmax&#39;)) # Compile the model model.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) # Fit the model hist = model.fit(predictors, target, validation_split=0.3) . Train on 623 samples, validate on 268 samples Epoch 1/10 32/623 [&gt;.............................] - ETA: 0s - loss: 3.3028 - acc: 0.4062 608/623 [============================&gt;.] - ETA: 0s - loss: 1.3320 - acc: 0.5938 623/623 [==============================] - 0s - loss: 1.3096 - acc: 0.6003 - val_loss: 0.6805 - val_acc: 0.7201 ... Epoch 10/10 32/623 [&gt;.............................] - ETA: 0s - loss: 0.4873 - acc: 0.7812 320/623 [==============&gt;...............] - ETA: 0s - loss: 0.5953 - acc: 0.7063 623/623 [==============================] - 0s - loss: 0.6169 - acc: 0.6870 - val_loss: 0.5339 - val_acc: 0.7351 . #### Early stopping: Optimizing the optimization . Now that you know how to monitor your model performance throughout optimization, you can use early stopping to stop optimization when it isn’t helping any more. Since the optimization stops automatically when it isn’t helping, you can also set a high value for epochs in your call to .fit() . . # Import EarlyStopping from keras.callbacks import EarlyStopping # Save the number of columns in predictors: n_cols n_cols = predictors.shape[1] input_shape = (n_cols,) # Specify the model model = Sequential() model.add(Dense(100, activation=&#39;relu&#39;, input_shape = input_shape)) model.add(Dense(100, activation=&#39;relu&#39;)) model.add(Dense(2, activation=&#39;softmax&#39;)) # Compile the model model.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) # Define early_stopping_monitor early_stopping_monitor = EarlyStopping(patience=2) # Fit the model model.fit(predictors, target, epochs=30, validation_split=0.3, callbacks=[early_stopping_monitor]) . Train on 623 samples, validate on 268 samples Epoch 1/30 32/623 [&gt;.............................] - ETA: 0s - loss: 5.6563 - acc: 0.4688 608/623 [============================&gt;.] - ETA: 0s - loss: 1.6536 - acc: 0.5609 623/623 [==============================] - 0s - loss: 1.6406 - acc: 0.5650 - val_loss: 1.0856 - val_acc: 0.6567 ... Epoch 6/30 32/623 [&gt;.............................] - ETA: 0s - loss: 0.4607 - acc: 0.7812 608/623 [============================&gt;.] - ETA: 0s - loss: 0.6208 - acc: 0.7007 623/623 [==============================] - 0s - loss: 0.6231 - acc: 0.6982 - val_loss: 0.6149 - val_acc: 0.6828 Epoch 7/30 32/623 [&gt;.............................] - ETA: 0s - loss: 0.6697 - acc: 0.6875 608/623 [============================&gt;.] - ETA: 0s - loss: 0.6483 - acc: 0.7072 623/623 [==============================] - 0s - loss: 0.6488 - acc: 0.7063 - val_loss: 0.7276 - val_acc: 0.6493 . Because optimization will automatically stop when it is no longer helpful, it is okay to specify the maximum number of epochs as 30 rather than using the default of 10 that you’ve used so far. Here, it seems like the optimization stopped after 7 epochs. . #### Experimenting with wider networks . Now you know everything you need to begin experimenting with different models! . A model called model_1 has been pre-loaded. You can see a summary of this model printed in the IPython Shell. This is a relatively small network, with only 10 units in each hidden layer. . In this exercise you’ll create a new model called model_2 which is similar to model_1 , except it has 100 units in each hidden layer. . # Define early_stopping_monitor early_stopping_monitor = EarlyStopping(patience=2) # Create the new model: model_2 model_2 = Sequential() # Add the first and second layers model_2.add(Dense(100, activation=&#39;relu&#39;, input_shape=input_shape)) model_2.add(Dense(100, activation=&#39;relu&#39;)) # Add the output layer model_2.add(Dense(2, activation=&#39;softmax&#39;)) # Compile model_2 model_2.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) # Fit model_1 model_1_training = model_1.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False) # Fit model_2 model_2_training = model_2.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False) # Create the plot plt.plot(model_1_training.history[&#39;val_loss&#39;], &#39;r&#39;, model_2_training.history[&#39;val_loss&#39;], &#39;b&#39;) plt.xlabel(&#39;Epochs&#39;) plt.ylabel(&#39;Validation score&#39;) plt.show() . model_1.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_1 (Dense) (None, 10) 110 _________________________________________________________________ dense_2 (Dense) (None, 10) 110 _________________________________________________________________ dense_3 (Dense) (None, 2) 22 ================================================================= Total params: 242.0 Trainable params: 242 Non-trainable params: 0.0 _________________________________________________________________ model_2.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_4 (Dense) (None, 100) 1100 _________________________________________________________________ dense_5 (Dense) (None, 100) 10100 _________________________________________________________________ dense_6 (Dense) (None, 2) 202 ================================================================= Total params: 11,402.0 Trainable params: 11,402 Non-trainable params: 0.0 _________________________________________________________________ . . The blue model is the one you made, the red is the original model. Your model had a lower loss value, so it is the better model. . #### Adding layers to a network . You’ve seen how to experiment with wider networks. In this exercise, you’ll try a deeper network (more hidden layers). . # The input shape to use in the first hidden layer input_shape = (n_cols,) # Create the new model: model_2 model_2 = Sequential() # Add the first, second, and third hidden layers model_2.add(Dense(50, activation=&#39;relu&#39;, input_shape=input_shape)) model_2.add(Dense(50, activation=&#39;relu&#39;)) model_2.add(Dense(50, activation=&#39;relu&#39;)) # Add the output layer model_2.add(Dense(2, activation=&#39;softmax&#39;)) # Compile model_2 model_2.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) # Fit model 1 model_1_training = model_1.fit(predictors, target, epochs=20, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=False) # Fit model 2 model_2_training = model_2.fit(predictors, target, epochs=20, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=False) # Create the plot plt.plot(model_1_training.history[&#39;val_loss&#39;], &#39;r&#39;, model_2_training.history[&#39;val_loss&#39;], &#39;b&#39;) plt.xlabel(&#39;Epochs&#39;) plt.ylabel(&#39;Validation score&#39;) plt.show() . model_1.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_1 (Dense) (None, 50) 550 _________________________________________________________________ dense_2 (Dense) (None, 2) 102 ================================================================= Total params: 652.0 Trainable params: 652 Non-trainable params: 0.0 _________________________________________________________________ model_2.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_3 (Dense) (None, 50) 550 _________________________________________________________________ dense_4 (Dense) (None, 50) 2550 _________________________________________________________________ dense_5 (Dense) (None, 50) 2550 _________________________________________________________________ dense_6 (Dense) (None, 2) 102 ================================================================= Total params: 5,752.0 Trainable params: 5,752 Non-trainable params: 0.0 _________________________________________________________________ . . . 4.3 Thinking about model capacity . . . . #### Experimenting with model structures . You’ve just run an experiment where you compared two networks that were identical except that the 2nd network had an extra hidden layer. . You see that this 2nd network (the deeper network) had better performance. Given that, How to get an even better performance? . Increasing the number of units in each hidden layer would be a good next step to try achieving even better performance. . . 4.4 Stepping up to images . . #### Building your own digit recognition model . You’ve reached the final exercise of the course – you now know everything you need to build an accurate model to recognize handwritten digits! . To add an extra challenge, we’ve loaded only 2500 images, rather than 60000 which you will see in some published results. Deep learning models perform better with more data, however, they also take longer to train, especially when they start becoming more complex. . If you have a computer with a CUDA compatible GPU, you can take advantage of it to improve computation time. If you don’t have a GPU, no problem! You can set up a deep learning environment in the cloud that can run your models on a GPU. Here is a blog post by Dan that explains how to do this – check it out after completing this exercise! It is a great next step as you continue your deep learning journey. . Ready to take your deep learning to the next level? Check out Advanced Deep Learning with Keras in Python to see how the Keras functional API lets you build domain knowledge to solve new types of problems. Once you know how to use the functional API, take a look at “Convolutional Neural Networks for Image Processing” to learn image-specific applications of Keras. . # feature of 28 * 28 = 784 image of a handwriting digit image. # each value is a number between 0 ~ 255, stands for the darkness of that pixel X array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], dtype=float32) X.shape (2500, 784) # target: 0 ~ 9 y array([[0., 0., 0., ..., 0., 0., 0.], [0., 1., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 1., 0.], [0., 1., 0., ..., 0., 0., 0.]]) y.shape (2500, 10) . # Create the model: model model = Sequential() # Add the first hidden layer model.add(Dense(50, activation=&#39;relu&#39;, input_shape=(X.shape[1],))) # Add the second hidden layer model.add(Dense(50, activation=&#39;relu&#39;)) # Add the output layer model.add(Dense(y.shape[1], activation=&#39;softmax&#39;)) # Compile the model model.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) # Fit the model model.fit(X, y, validation_split=0.3) . Train on 1750 samples, validate on 750 samples Epoch 1/10 32/1750 [..............................] - ETA: 3s - loss: 2.1979 - acc: 0.2188 480/1750 [=======&gt;......................] - ETA: 0s - loss: 2.1655 - acc: 0.2333 960/1750 [===============&gt;..............] - ETA: 0s - loss: 1.9699 - acc: 0.3354 1440/1750 [=======================&gt;......] - ETA: 0s - loss: 1.7895 - acc: 0.4153 1750/1750 [==============================] - 0s - loss: 1.6672 - acc: 0.4737 - val_loss: 1.0023 - val_acc: 0.7707 ... Epoch 10/10 32/1750 [..............................] - ETA: 0s - loss: 0.1482 - acc: 1.0000 480/1750 [=======&gt;......................] - ETA: 0s - loss: 0.1109 - acc: 0.9792 960/1750 [===============&gt;..............] - ETA: 0s - loss: 0.1046 - acc: 0.9812 1440/1750 [=======================&gt;......] - ETA: 0s - loss: 0.1028 - acc: 0.9812 1696/1750 [============================&gt;.] - ETA: 0s - loss: 0.1014 - acc: 0.9817 1750/1750 [==============================] - 0s - loss: 0.0999 - acc: 0.9823 - val_loss: 0.3186 - val_acc: 0.9053 . You’ve done something pretty amazing. You should see better than 90% accuracy recognizing handwritten digits, even while using a small training set of only 1750 images! . The End. . Thank you for reading. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/deep-learning-in-python.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/deep-learning-in-python.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post33": {
            "title": "Clustering Methods with SciPy",
            "content": "Clustering Methods with SciPy . This is the memo of the 6th course (23 courses in all) of ‘Machine Learning Scientist with Python’ skill track. . You can find the original course HERE . . ### Course Description . You have probably come across Google News, which automatically groups similar news articles under a topic. Have you ever wondered what process runs in the background to arrive at these groups? In this course, you will be introduced to unsupervised learning through clustering using the SciPy library in Python. This course covers pre-processing of data and application of hierarchical and k-means clustering. Through the course, you will explore player statistics from a popular football video game, FIFA 18. After completing the course, you will be able to quickly apply various clustering algorithms on data, visualize the clusters formed and analyze results. . ### . Introduction to Clustering | Hierarchical Clustering | K-Means Clustering | Clustering in Real World | . 1. Introduction to Clustering . . 1.1 Unsupervised learning: basics . . 1.1.1 Unsupervised learning in real world . Which of the following examples can be solved with unsupervised learning? . A list of tweets to be classified based on their sentiment, the data has tweets associated with a positive or negative sentiment. | A spam recognition system that marks incoming emails as spam, the data has emails marked as spam and not spam. | Segmentation of learners at DataCamp based on courses they complete. The training data has no labels.press | . 1.1.2 Pokémon sightings . There have been reports of sightings of rare, legendary Pokémon. You have been asked to investigate! Plot the coordinates of sightings to find out where the Pokémon might be. The X and Y coordinates of the points are stored in list x and y , respectively. . # Import plotting class from matplotlib library from matplotlib import pyplot as plt # Create a scatter plot plt.scatter(x, y) # Display the scatter plot plt.show() . . Notice the areas where the sightings are dense. This indicates that there is not one, but two legendary Pokémon out there! . . 1.2 Basics of cluster analysis . . #### Hierarchical clustering algorithms . . #### K-means clustering algorithms . . 1.2.1 Pokémon sightings: hierarchical clustering . We are going to continue the investigation into the sightings of legendary Pokémon from the previous exercise. Remember that in the scatter plot of the previous exercise, you identified two areas where Pokémon sightings were dense. This means that the points seem to separate into two clusters. In this exercise, you will form two clusters of the sightings using hierarchical clustering. . df.head() x y 0 9 8 1 6 4 2 2 10 3 3 6 4 1 0 . # Import linkage and fcluster functions from scipy.cluster.hierarchy import linkage, fcluster # Use the linkage() function to compute distance Z = linkage(df, &#39;ward&#39;) # Generate cluster labels df[&#39;cluster_labels&#39;] = fcluster(Z, 2, criterion=&#39;maxclust&#39;) # Plot the points with seaborn sns.scatterplot(x=&#39;x&#39;, y=&#39;y&#39;, hue=&#39;cluster_labels&#39;, data=df) plt.show() . type(Z) numpy.ndarray Z[:3] array([[10., 13., 0., 2.], [15., 19., 0., 2.], [ 1., 5., 1., 2.]]) df x y cluster_labels 0 9 8 2 1 6 4 2 ... 8 1 6 2 9 7 1 2 10 23 29 1 11 26 25 1 ... . . Notice that the cluster labels are plotted with different colors. You will notice that the resulting plot has an extra cluster labelled 0 in the legend. This will be explained later in the course. . 1.2.2 Pokémon sightings: k-means clustering . We are going to continue the investigation into the sightings of legendary Pokémon from the previous exercise. Just like the previous exercise, we will use the same example of Pokémon sightings. In this exercise, you will form clusters of the sightings using k-means clustering. . # Import kmeans and vq functions from scipy.cluster.vq import kmeans, vq # Compute cluster centers centroids,_ = kmeans(df, 2) # Assign cluster labels df[&#39;cluster_labels&#39;], _ = vq(df, centroids) # Plot the points with seaborn sns.scatterplot(x=&#39;x&#39;, y=&#39;y&#39;, hue=&#39;cluster_labels&#39;, data=df) plt.show() . centroids array([[23.7, 28. ], [ 4.3, 5.9]]) . . Notice that in this case, the results of both types of clustering are similar. We will look at distinctly different results later in the course. . . 1.3 Data preparation for cluster analysis . . 1.3.1 Normalize basic list data . Now that you are aware of normalization, let us try to normalize some data. goals_for is a list of goals scored by a football team in their last ten matches. Let us standardize the data using the whiten() function. . # Import the whiten function from scipy.cluster.vq import whiten goals_for = [4,3,2,3,1,1,2,0,1,4] # Use the whiten() function to standardize the data scaled_data = whiten(goals_for) print(scaled_data) # [3.07692308 2.30769231 1.53846154 2.30769231 0.76923077 0.76923077 1.53846154 0. 0.76923077 3.07692308] . 1.3.2 Visualize normalized data . After normalizing your data, you can compare the scaled data to the original data to see the difference. The variables from the last exercise, goals_for and scaled_data are already available to you. . # Plot original data plt.plot(goals_for, label=&#39;original&#39;) # Plot scaled data plt.plot(scaled_data, label=&#39;scaled&#39;) # Show the legend in the plot plt.legend() # Display the plot plt.show() . . 1.3.3 Normalization of small numbers . In earlier examples, you have normalization of whole numbers. In this exercise, you will look at the treatment of fractional numbers – the change of interest rates in the country of Bangalla over the years. . # Prepare data rate_cuts = [0.0025, 0.001, -0.0005, -0.001, -0.0005, 0.0025, -0.001, -0.0015, -0.001, 0.0005] # Use the whiten() function to standardize the data scaled_data = whiten(rate_cuts) # Plot original data plt.plot(rate_cuts, label=&#39;original&#39;) # Plot scaled data plt.plot(scaled_data, label=&#39;scaled&#39;) plt.legend() plt.show() . . Notice how the changes in the original data are negligible as compared to the scaled data . 1.3.4 FIFA 18: Normalize data . FIFA 18 is a football video game that was released in 2017 for PC and consoles. The dataset that you are about to work on contains data on the 1000 top individual players in the game. You will explore various features of the data as we move ahead in the course. In this exercise, you will work with two columns, eur_wage , the wage of a player in Euros and eur_value , their current transfer market value. . The data for this exercise is stored in a Pandas dataframe, fifa . whiten from scipy.cluster.vq and matplotlib.pyplot as plt have been pre-loaded. . # Scale wage and value fifa[&#39;scaled_wage&#39;] = whiten(fifa[&#39;eur_wage&#39;]) fifa[&#39;scaled_value&#39;] = whiten(fifa[&#39;eur_value&#39;]) # Plot the two columns in a scatter plot fifa.plot(x=&#39;scaled_wage&#39;, y=&#39;scaled_value&#39;, kind = &#39;scatter&#39;) plt.show() # Check mean and standard deviation of scaled values print(fifa[[&#39;scaled_wage&#39;, &#39;scaled_value&#39;]].describe()) . scaled_wage scaled_value count 1000.00 1000.00 mean 1.12 1.31 std 1.00 1.00 min 0.00 0.00 25% 0.47 0.73 50% 0.85 1.02 75% 1.41 1.54 max 9.11 8.98 . . As you can see the scaled values have a standard deviation of 1. . 2. Hierarchical Clustering . . 2.1 Basics of hierarchical clustering . . . 2.1.1 Hierarchical clustering: ward method . It is time for Comic-Con! Comic-Con is an annual comic-based convention held in major cities in the world. You have the data of last year’s footfall, the number of people at the convention ground at a given time. . You would like to decide the location of your stall to maximize sales. Using the ward method, apply hierarchical clustering to find the two points of attraction in the area. . comic_con x_coordinate y_coordinate x_scaled y_scaled 0 17 4 0.509349 0.090010 1 20 6 0.599234 0.135015 2 35 0 1.048660 0.000000 3 14 0 0.419464 0.000000 4 37 4 1.108583 0.090010 5 33 3 0.988736 0.067507 6 14 1 0.419464 0.022502 . # Import the fcluster and linkage functions from scipy.cluster.hierarchy import linkage, fcluster # Use the linkage() function distance_matrix = linkage(comic_con[[&#39;x_scaled&#39;, &#39;y_scaled&#39;]], method = &#39;ward&#39;, metric = &#39;euclidean&#39;) # Assign cluster labels comic_con[&#39;cluster_labels&#39;] = fcluster(distance_matrix, 2, criterion=&#39;maxclust&#39;) # Plot clusters sns.scatterplot(x=&#39;x_scaled&#39;, y=&#39;y_scaled&#39;, hue=&#39;cluster_labels&#39;, data = comic_con) plt.show() . . Notice the two clusters correspond to the points of attractions in the figure towards the bottom (a stage) and the top right (an interesting stall). . 2.1.2 Hierarchical clustering: single method . Let us use the same footfall dataset and check if any changes are seen if we use a different method for clustering. . # Import the fcluster and linkage functions from scipy.cluster.hierarchy import fcluster, linkage # Use the linkage() function distance_matrix = linkage(comic_con[[&#39;x_scaled&#39;, &#39;y_scaled&#39;]], method = &#39;single&#39;, metric = &#39;euclidean&#39;) # Assign cluster labels comic_con[&#39;cluster_labels&#39;] = fcluster(distance_matrix, 2, criterion=&#39;maxclust&#39;) # Plot clusters sns.scatterplot(x=&#39;x_scaled&#39;, y=&#39;y_scaled&#39;, hue=&#39;cluster_labels&#39;, data = comic_con) plt.show() . Notice that in this example, the clusters formed are not different from the ones created using the ward method. . 2.1.3 Hierarchical clustering: complete method . For the third and final time, let us use the same footfall dataset and check if any changes are seen if we use a different method for clustering. . # Import the fcluster and linkage functions from scipy.cluster.hierarchy import linkage, fcluster # Use the linkage() function distance_matrix = linkage(comic_con[[&#39;x_scaled&#39;, &#39;y_scaled&#39;]], method=&#39;complete&#39;, metric=&#39;euclidean&#39;) # Assign cluster labels comic_con[&#39;cluster_labels&#39;] = fcluster(distance_matrix,2,criterion=&#39;maxclust&#39;) # Plot clusters sns.scatterplot(x=&#39;x_scaled&#39;, y=&#39;y_scaled&#39;, hue=&#39;cluster_labels&#39;, data = comic_con) plt.show() . Coincidentally, the clusters formed are not different from the ward or single methods. Next, let us learn how to visualize clusters. . . 2.2 Visualize clusters . . 2.2.1 Visualize clusters with matplotlib . We have discussed that visualizations are necessary to assess the clusters that are formed and spot trends in your data. Let us now focus on visualizing the footfall dataset from Comic-Con using the matplotlib module. . # Import the pyplot class from matplotlib import pyplot as plt # Define a colors dictionary for clusters colors = {1:&#39;red&#39;, 2:&#39;blue&#39;} # Plot a scatter plot comic_con.plot.scatter(x = &#39;x_scaled&#39;, y = &#39;y_scaled&#39;, c = comic_con[&#39;cluster_labels&#39;].apply(lambda x: colors[x])) plt.show() . . 2.2.2 Visualize clusters with seaborn . Let us now visualize the footfall dataset from Comic Con using the seaborn module. Visualizing clusters using seaborn is easier with the inbuild hue function for cluster labels. . # Import the seaborn module import seaborn as sns # Plot a scatter plot using seaborn sns.scatterplot(x=&#39;x_scaled&#39;, y=&#39;y_scaled&#39;, hue=&#39;cluster_labels&#39;, data = comic_con) plt.show() . . Notice the legend is automatically shown when using the hue argument. . . 2.3 How many clusters? . . 2.3.1 Create a dendrogram . Dendrograms are branching diagrams that show the merging of clusters as we move through the distance matrix. Let us use the Comic Con footfall data to create a dendrogram. . # Import the dendrogram function from scipy.cluster.hierarchy import dendrogram # Create a dendrogram dn = dendrogram(distance_matrix) # Display the dendogram plt.show() . . . 2.4 Limitations of hierarchical clustering . . 2.4.1 FIFA 18: exploring defenders . In the FIFA 18 dataset, various attributes of players are present. Two such attributes are: . sliding tackle a number between 0-99 which signifies how accurate a player is able to perform sliding tackles | aggression a number between 0-99 which signifies the commitment and will of a player | . These are typically high in defense-minded players. In this exercise, you will perform clustering based on these attributes in the data. . fifa.head() sliding_tackle aggression ... scaled_aggression cluster_labels 0 23 63 ... 3.72 3 1 26 48 ... 2.84 3 2 33 56 ... 3.31 3 3 38 78 ... 4.61 3 4 11 29 ... 1.71 2 [5 rows x 5 columns] . # Fit the data into a hierarchical clustering algorithm distance_matrix = linkage(fifa[[&#39;scaled_sliding_tackle&#39;, &#39;scaled_aggression&#39;]], &#39;ward&#39;) # Assign cluster labels to each row of data fifa[&#39;cluster_labels&#39;] = fcluster(distance_matrix, 3, criterion=&#39;maxclust&#39;) # Display cluster centers of each cluster print(fifa[[&#39;scaled_sliding_tackle&#39;, &#39;scaled_aggression&#39;, &#39;cluster_labels&#39;]].groupby(&#39;cluster_labels&#39;).mean()) # Create a scatter plot through seaborn sns.scatterplot(x=&#39;scaled_sliding_tackle&#39;, y=&#39;scaled_aggression&#39;, hue=&#39;cluster_labels&#39;, data=fifa) plt.show() . scaled_sliding_tackle scaled_aggression cluster_labels 1 2.99 4.35 2 0.74 1.94 3 1.34 3.62 . . 3. K-Means Clustering . . 3.1 Basics of k-means clustering . . 3.1.1 K-means clustering: first exercise . This exercise will familiarize you with the usage of k-means clustering on a dataset. Let us use the Comic Con dataset and check how k-means clustering works on it. . Recall the two steps of k-means clustering: . Define cluster centers through kmeans() function. It has two required arguments: observations and number of clusters. | Assign cluster labels through the vq() function. It has two required arguments: observations and cluster centers. | . # Import the kmeans and vq functions from scipy.cluster.vq import kmeans, vq # Generate cluster centers cluster_centers, distortion = kmeans(comic_con[[&#39;x_scaled&#39;,&#39;y_scaled&#39;]], 2) # Assign cluster labels comic_con[&#39;cluster_labels&#39;], distortion_list = vq(comic_con[[&#39;x_scaled&#39;,&#39;y_scaled&#39;]],cluster_centers) # Plot clusters sns.scatterplot(x=&#39;x_scaled&#39;, y=&#39;y_scaled&#39;, hue=&#39;cluster_labels&#39;, data = comic_con) plt.show() . . Notice that the clusters formed are exactly the same as hierarchical clustering that you did in the previous chapter. . 3.1.2 Runtime of k-means clustering . Recall that it took a significantly long time to run hierarchical clustering. How long does it take to run the kmeans() function on the FIFA dataset? . %timeit kmeans(fifa[[&#39;scaled_sliding_tackle&#39;,&#39;scaled_aggression&#39;]],3) # 10 loops, best of 3: 69.7 ms per loop %timeit linkage(fifa[[&#39;scaled_sliding_tackle&#39;,&#39;scaled_aggression&#39;]], method = &#39;ward&#39;, metric = &#39;euclidean&#39;) # 1 loop, best of 3: 703 ms per loop . . 3.2 How many clusters? . . 3.2.1 Elbow method on distinct clusters . Let us use the comic con data set to see how the elbow plot looks on a data set with distinct, well-defined clusters. You may want to display the data points before proceeding with the exercise. . distortions = [] num_clusters = range(1, 7) # Create a list of distortions from the kmeans function for i in num_clusters: cluster_centers, distortion = kmeans(comic_con[[&#39;x_scaled&#39;,&#39;y_scaled&#39;]],i) distortions.append(distortion) # Create a data frame with two lists - num_clusters, distortions elbow_plot = pd.DataFrame({&#39;num_clusters&#39;: num_clusters, &#39;distortions&#39;: distortions}) # Creat a line plot of num_clusters and distortions sns.lineplot(x=&#39;num_clusters&#39;, y=&#39;distortions&#39;, data = elbow_plot) plt.xticks(num_clusters) plt.show() . . From the elbow plot, there are 2 clusters in the data. . 3.2.2 Elbow method on uniform data . In the earlier exercise, you constructed an elbow plot on data with well-defined clusters. Let us now see how the elbow plot looks on a data set with uniformly distributed points. You may want to display the data points on the console before proceeding with the exercise. . distortions = [] num_clusters = range(2, 7) # Create a list of distortions from the kmeans function for i in num_clusters: cluster_centers, distortion = kmeans(uniform_data[[&#39;x_scaled&#39;,&#39;y_scaled&#39;]],i) distortions.append(distortion) # Create a data frame with two lists - number of clusters and distortions elbow_plot = pd.DataFrame({&#39;num_clusters&#39;: num_clusters, &#39;distortions&#39;: distortions}) # Creat a line plot of num_clusters and distortions sns.lineplot(x=&#39;num_clusters&#39;, y=&#39;distortions&#39;, data = elbow_plot) plt.xticks(num_clusters) plt.show() . . From the elbow plot, we can not determine how many clusters in the data. . . 3.3 Limitations of k-means clustering . . 3.3.1 Impact of seeds on distinct clusters . You noticed the impact of seeds on a dataset that did not have well-defined groups of clusters. In this exercise, you will explore whether seeds impact the clusters in the Comic Con data, where the clusters are well-defined. . # Import random class from numpy import random # Initialize seed random.seed(0) random.seed([1, 2, 1000]) # Run kmeans clustering cluster_centers, distortion = kmeans(comic_con[[&#39;x_scaled&#39;, &#39;y_scaled&#39;]], 2) comic_con[&#39;cluster_labels&#39;], distortion_list = vq(comic_con[[&#39;x_scaled&#39;, &#39;y_scaled&#39;]], cluster_centers) # Plot the scatterplot sns.scatterplot(x=&#39;x_scaled&#39;, y=&#39;y_scaled&#39;, hue=&#39;cluster_labels&#39;, data = comic_con) plt.show() . . Notice that the plots have not changed after changing the seed as the clusters are well-defined. . 3.3.2 Uniform clustering patterns . Now that you are familiar with the impact of seeds, let us look at the bias in k-means clustering towards the formation of uniform clusters. . Let us use a mouse-like dataset for our next exercise. A mouse-like dataset is a group of points that resemble the head of a mouse: it has three clusters of points arranged in circles, one each for the face and two ears of a mouse. . Here is how a typical mouse-like dataset looks like ( Source ). . . # Import the kmeans and vq functions from scipy.cluster.vq import kmeans, vq # Generate cluster centers cluster_centers, distortion = kmeans(mouse[[&#39;x_scaled&#39;,&#39;y_scaled&#39;]],3) # Assign cluster labels mouse[&#39;cluster_labels&#39;], distortion_list = vq(mouse[[&#39;x_scaled&#39;,&#39;y_scaled&#39;]],cluster_centers) # Plot clusters sns.scatterplot(x=&#39;x_scaled&#39;, y=&#39;y_scaled&#39;, hue=&#39;cluster_labels&#39;, data = mouse) plt.show() . . Notice that kmeans is unable to capture the three visible clusters clearly, and the two clusters towards the top have taken in some points along the boundary. This happens due to the underlying assumption in kmeans algorithm to minimize distortions which leads to clusters that are similar in terms of area. . 3.3.3 FIFA 18: defenders revisited . In the FIFA 18 dataset, various attributes of players are present. Two such attributes are: . defending a number which signifies the defending attributes of a player | physical a number which signifies the physical attributes of a player | . These are typically defense-minded players. In this exercise, you will perform clustering based on these attributes in the data. . # Set up a random seed in numpy random.seed([1000,2000]) # Fit the data into a k-means algorithm cluster_centers,_ = kmeans(fifa[[&#39;scaled_def&#39;, &#39;scaled_phy&#39;]], 3) # Assign cluster labels fifa[&#39;cluster_labels&#39;], _ = vq(fifa[[&#39;scaled_def&#39;, &#39;scaled_phy&#39;]], cluster_centers) # Display cluster centers print(fifa[[&#39;scaled_def&#39;, &#39;scaled_phy&#39;, &#39;cluster_labels&#39;]].groupby(&#39;cluster_labels&#39;).mean()) # Create a scatter plot through seaborn sns.scatterplot(x=&#39;scaled_def&#39;, y=&#39;scaled_phy&#39;, hue=&#39;cluster_labels&#39;, data=fifa) plt.show() . scaled_def scaled_phy cluster_labels 0 3.74 8.87 1 1.87 7.08 2 2.10 8.94 . . Notice that the seed has an impact on clustering as the data is uniformly distributed. . 4. Clustering in Real World . . 4.1 Dominant colors in images . . 4.1.1 Extract RGB values from image . There are broadly three steps to find the dominant colors in an image: . Extract RGB values into three lists. | Perform k-means clustering on scaled RGB values. | Display the colors of cluster centers. | . To extract RGB values, we use the imread() function of the image class of matplotlib . Empty lists, r , g and b have been initialized. . For the purpose of finding dominant colors, we will be using the following image. . . # Import image class of matplotlib from matplotlib import image as img # Read batman image and print dimensions batman_image = img.imread(&#39;batman.jpg&#39;) print(batman_image.shape) # (57, 90, 3) # Store RGB values of all pixels in lists r, g and b for rows in batman_image: for temp_r, temp_g, temp_b in rows: r.append(temp_r) g.append(temp_g) b.append(temp_b) . You have successfully extracted the RGB values of the image into three lists, one for each color channel. . #### 4.1.2 How many dominant colors? . The RGB values are stored in a data frame, batman_df . The RGB values have been standardized used the whiten() function, stored in columns, scaled_red , scaled_blue and scaled_green . . Construct an elbow plot with the data frame. How many dominant colors are present? . distortions = [] num_clusters = range(1, 7) # Create a list of distortions from the kmeans function for i in num_clusters: cluster_centers, distortion = kmeans(batman_df[[&#39;scaled_red&#39;, &#39;scaled_blue&#39;, &#39;scaled_green&#39;]], i) distortions.append(distortion) # Create a data frame with two lists, num_clusters and distortions elbow_plot = pd.DataFrame({&#39;num_clusters&#39;:num_clusters,&#39;distortions&#39;:distortions}) # Create a line plot of num_clusters and distortions sns.lineplot(x=&#39;num_clusters&#39;, y=&#39;distortions&#39;, data = elbow_plot) plt.xticks(num_clusters) plt.show() . . Notice that there are three distinct colors present in the image, which is supported by the elbow plot. . 4.1.3 Display dominant colors . To display the dominant colors, convert the colors of the cluster centers to their raw values and then converted them to the range of 0-1, using the following formula: converted_pixel = standardized_pixel * pixel_std / 255 . # Get standard deviations of each color r_std, g_std, b_std = batman_df[[&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;]].std() for cluster_center in cluster_centers: scaled_r, scaled_g, scaled_b = cluster_center # Convert each standardized value to scaled value colors.append(( scaled_r * r_std / 255, scaled_g * g_std / 255, scaled_b * b_std / 255 )) # Display colors of cluster centers plt.imshow([colors]) plt.show() . . Notice the three colors resemble the three that are indicative from visual inspection of the image. . . 4.2 Document clustering . . 4.2.1 TF-IDF(term frequency–inverse document frequency) of movie plots . Let us use the plots of randomly selected movies to perform document clustering on. Before performing clustering on documents, they need to be cleaned of any unwanted noise (such as special characters and stop words) and converted into a sparse matrix through TF-IDF of the documents. . Use the TfidfVectorizer class to perform the TF-IDF of movie plots stored in the list plots . The remove_noise() function is available to use as a tokenizer in the TfidfVectorizer class. The .fit_transform() method fits the data into the TfidfVectorizer objects and then generates the TF-IDF sparse matrix. . plots[:1] [&#39;Cable Hogue is isolated in the desert, awaiting his partners, Taggart and Bowen, ... ... A coyote wanders into the abandoned Cable Springs. But the coyote has a collar – possibly symbolising the taming of the wilderness.&#39;] . # Import TfidfVectorizer class from sklearn from sklearn.feature_extraction.text import TfidfVectorizer # Initialize TfidfVectorizer tfidf_vectorizer = TfidfVectorizer(min_df=0.1, max_df=0.75, max_features=50, tokenizer=remove_noise) # Use the .fit_transform() method on the list plots tfidf_matrix = tfidf_vectorizer.fit_transform(plots) . 4.2.2 Top terms in movie clusters . Now that you have created a sparse matrix, generate cluster centers and print the top three terms in each cluster. Use the .todense() method to convert the sparse matrix, tfidf_matrix to a normal matrix for the kmeans() function to process. Then, use the .get_feature_names() method to get a list of terms in the tfidf_vectorizer object. The zip() function in Python joins two lists. . The tfidf_vectorizer object and sparse matrix, tfidf_matrix , from the previous have been retained in this exercise. kmeans has been imported from SciPy. . With a higher number of data points, the clusters formed would be defined more clearly. However, this requires some computational power, making it difficult to accomplish in an exercise here. . num_clusters = 2 # Generate cluster centers through the kmeans function cluster_centers, distortion = kmeans(tfidf_matrix.todense(),num_clusters) # Generate terms from the tfidf_vectorizer object terms = tfidf_vectorizer.get_feature_names() for i in range(num_clusters): # Sort the terms and print top 3 terms center_terms = dict(zip(terms, cluster_centers[i])) sorted_terms = sorted(center_terms, key=center_terms.get, reverse=True) print(sorted_terms[:3]) # [&#39;back&#39;, &#39;father&#39;, &#39;one&#39;] # [&#39;man&#39;, &#39;police&#39;, &#39;killed&#39;] . Notice positive, warm words in the first cluster and words referring to action in the second cluster. . . 4.3 Clustering with multiple features . . 4.3.1 Clustering with many features . What should you do if you have too many features for clustering? . Reduce features using a technique like Factor Analysis . 4.3.2 Basic checks on clusters . In the FIFA 18 dataset, we have concentrated on defenders in previous exercises. Let us try to focus on attacking attributes of a player. Pace ( pac ), Dribbling ( dri ) and Shooting ( sho ) are features that are present in attack minded players. In this exercise, k-means clustering has already been applied on the data using the scaled values of these three attributes. Try some basic checks on the clusters so formed. . The data is stored in a Pandas data frame, fifa . The scaled column names are present in a list scaled_features . The cluster labels are stored in the cluster_labels column. Recall the .count() and .mean() methods in Pandas help you find the number of observations and mean of observations in a data frame. . # Print the size of the clusters print(fifa.groupby(&#39;cluster_labels&#39;)[&#39;ID&#39;].count()) # Print the mean value of wages in each cluster print(fifa.groupby(&#39;cluster_labels&#39;)[&#39;eur_wage&#39;].mean()) . cluster_labels 0 83 1 107 2 60 Name: ID, dtype: int64 cluster_labels 0 132108.43 1 130308.41 2 117583.33 Name: eur_wage, dtype: float64 . In this example, the cluster sizes are not very different, and there are no significant differences that can be seen in the wages. Further analysis is required to validate these clusters. . 4.3.3 FIFA 18: what makes a complete player? . The overall level of a player in FIFA 18 is defined by six characteristics: pace ( pac ), shooting ( sho ), passing ( pas ), dribbling ( dri ), defending ( def ), physical ( phy ). . # Create centroids with kmeans for 2 clusters cluster_centers,_ = kmeans(fifa[scaled_features], 2) # Assign cluster labels and print cluster centers fifa[&#39;cluster_labels&#39;], _ = vq(fifa[scaled_features], cluster_centers) print(fifa.groupby(&#39;cluster_labels&#39;)[scaled_features].mean()) # Plot cluster centers to visualize clusters fifa.groupby(&#39;cluster_labels&#39;)[scaled_features].mean().plot(legend=True, kind=&#39;bar&#39;) plt.show() # Get the name column of top 5 players in each cluster for cluster in fifa[&#39;cluster_labels&#39;].unique(): print(cluster, fifa[fifa[&#39;cluster_labels&#39;] == cluster][&#39;name&#39;].values[:5]) . scaled_pac scaled_sho scaled_pas scaled_dri scaled_def cluster_labels 0 6.68 5.43 8.46 8.51 2.50 1 5.44 3.66 7.17 6.76 3.97 scaled_phy cluster_labels 0 8.34 1 9.21 0 [&#39;Cristiano Ronaldo&#39; &#39;L. Messi&#39; &#39;Neymar&#39; &#39;L. Suárez&#39; &#39;M. Neuer&#39;] 1 [&#39;Sergio Ramos&#39; &#39;G. Chiellini&#39; &#39;D. Godín&#39; &#39;Thiago Silva&#39; &#39;M. Hummels&#39;] . . Notice the top players in each cluster are representative of the overall characteristics of the cluster – one of the clusters primarily represents attackers, whereas the other represents defenders. . Surprisingly, a top goalkeeper Manuel Neuer is seen in the attackers group, but he is known for going out of the box and participating in open play, which are reflected in his FIFA 18 attributes. . . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/clustering-methods-with-scipy.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/clustering-methods-with-scipy.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post34": {
            "title": "Case Studies in Statistical Thinking",
            "content": "Case Studies in Statistical Thinking . This is the memo of the 5th course (5 courses in all) of ‘Statistics Fundamentals with Python’ skill track. . You can find the original course HERE . . ### . Fish sleep and bacteria growth: A review of Statistical Thinking I and II | Analysis of results of the 2015 FINA World Swimming Championships | The “Current Controversy” of the 2013 World Championships | Statistical seismology and the Parkfield region | Earthquakes and oil mining in Oklahoma | . 1. Fish sleep and bacteria growth: A review of Statistical Thinking I and II . . 1.1 Activity of zebrafish and melatonin . #### EDA: Plot ECDFs of active bout length . An active bout is a stretch of time where a fish is constantly moving. Plot an ECDF of active bout length for the mutant and wild type fish for the seventh night of their lives. The data sets are in the numpy arrays bout_lengths_wt and bout_lengths_mut . The bout lengths are in units of minutes. . # Import the dc_stat_think module as dcst import dc_stat_think as dcst # Generate x and y values for plotting ECDFs x_wt, y_wt = dcst.ecdf(bout_lengths_wt) x_mut, y_mut = dcst.ecdf(bout_lengths_mut) # Plot the ECDFs _ = plt.plot(x_wt, y_wt, marker=&#39;.&#39;, linestyle=&#39;none&#39;) _ = plt.plot(x_mut, y_mut, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Make a legend, label axes, and show plot _ = plt.legend((&#39;wt&#39;, &#39;mut&#39;)) _ = plt.xlabel(&#39;active bout length (min)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) plt.show() . . There is an outlier of one active bout for a mutant fish, and the ECDF exposes this clearly. It is important to know about, but we will not focus on it going forward, though. . #### Interpreting ECDFs and the story . While a more detailed analysis of distributions is often warranted for careful analyses, you can already get a feel for the distributions and the story behind the data by eyeballing the ECDFs. Which of the following would be the most reasonable statement to make about how the active bout lengths are distributed and what kind of process might be behind exiting the active bout to rest? . If you need a refresher, here are videos from Statistical Thinking I about stories behind probability distributions. . Discrete Uniform and Binomial | Poisson processes and Poisson distribution | Normal distribution | Exponential Distribution | . The bout lengths appear Exponentially distributed, which implies that exiting an active bout to rest is a Poisson process; the fish have no apparent memory about when they became active. . While not exactly Exponentially distributed, the ECDF has no left tail, and no discernible inflection point, which is very much like the Exponential CDF. . . 1.2 Bootstrap confidence intervals . #### Parameter estimation: active bout length . Compute the mean active bout length for wild type and mutant, with 95% bootstrap confidence interval. The data sets are again available in the numpy arrays bout_lengths_wt and bout_lengths_mut . The dc_stat_think module has been imported as dcst . . # Compute mean active bout length mean_wt = np.mean(bout_lengths_wt) mean_mut = np.mean(bout_lengths_mut) # Draw bootstrap replicates bs_reps_wt = dcst.draw_bs_reps(bout_lengths_wt, np.mean, size=10000) bs_reps_mut = dcst.draw_bs_reps(bout_lengths_mut, np.mean, size=10000) # Compute 95% confidence intervals conf_int_wt = np.percentile(bs_reps_wt, [2.5, 97.5]) conf_int_mut = np.percentile(bs_reps_mut, [2.5, 97.5]) # Print the results print(&quot;&quot;&quot; wt: mean = {0:.3f} min., conf. int. = [{1:.1f}, {2:.1f}] min. mut: mean = {3:.3f} min., conf. int. = [{4:.1f}, {5:.1f}] min. &quot;&quot;&quot;.format(mean_wt, *conf_int_wt, mean_mut, *conf_int_mut)) # wt: mean = 3.874 min., conf. int. = [3.6, 4.1] min. # mut: mean = 6.543 min., conf. int. = [6.1, 7.0] min. . The confidence intervals are quite separated. Nonetheless, we will proceed to perform hypothesis tests. . . 1.3 Permutation and bootstrap hypothesis tests . . #### Permutation test: wild type versus heterozygote . Test the hypothesis that the heterozygote and wild type bout lengths are identically distributed using a permutation test. . # Compute the difference of means: diff_means_exp diff_means_exp = np.mean(bout_lengths_het) - np.mean(bout_lengths_wt) # Draw permutation replicates: perm_reps perm_reps = dcst.draw_perm_reps(bout_lengths_het, bout_lengths_wt, dcst.diff_of_means, size=10000) # Compute the p-value: p-val p_val = np.sum(perm_reps &gt;= diff_means_exp) / len(perm_reps) # Print the result print(&#39;p =&#39;, p_val) # p = 0.001 . A p-value of 0.001 suggests that the observed difference in means is unlikely to occur if heterozygotic and wild type fish have active bout lengths that are identically distributed. . #### Bootstrap hypothesis test . The permutation test has a pretty restrictive hypothesis, that the heterozygotic and wild type bout lengths are identically distributed. Now, use a bootstrap hypothesis test to test the hypothesis that the means are equal, making no assumptions about the distributions. . # Concatenate arrays: bout_lengths_concat bout_lengths_concat = np.concatenate((bout_lengths_wt, bout_lengths_het)) # Compute mean of all bout_lengths: mean_bout_length mean_bout_length = np.mean(bout_lengths_concat) # Generate shifted arrays wt_shifted = bout_lengths_wt - np.mean(bout_lengths_wt) + mean_bout_length het_shifted = bout_lengths_het - np.mean(bout_lengths_het) + mean_bout_length # Compute 10,000 bootstrap replicates from shifted arrays bs_reps_wt = dcst.draw_bs_reps(wt_shifted, np.mean, size=10000) bs_reps_het = dcst.draw_bs_reps(het_shifted, np.mean, size=10000) # Get replicates of difference of means: bs_replicates bs_reps = bs_reps_het - bs_reps_wt # Compute and print p-value: p p = np.sum(bs_reps &gt;= diff_means_exp) / len(bs_reps) print(&#39;p-value =&#39;, p) # p-value = 0.0004 . We get a result of similar magnitude as the permutation test, though slightly smaller, probably because the heterozygote bout length distribution has a heavier tail to the right. . . 1.4 Linear regressions and pairs bootstrap . #### Assessing the growth rate . To compute the growth rate, you can do a linear regression of the logarithm of the total bacterial area versus time. Compute the growth rate and get a 95% confidence interval using pairs bootstrap. The time points, in units of hours, are stored in the numpy array t and the bacterial area, in units of square micrometers, is stored in bac_area . . # Compute logarithm of the bacterial area: log_bac_area log_bac_area = np.log(bac_area) # Compute the slope and intercept: growth_rate, log_a0 growth_rate, log_a0 = np.polyfit(t, log_bac_area, 1) # Draw 10,000 pairs bootstrap replicates: growth_rate_bs_reps, log_a0_bs_reps growth_rate_bs_reps, log_a0_bs_reps = dcst.draw_bs_pairs_linreg(t, log_bac_area, size=10000) # Compute confidence intervals: growth_rate_conf_int growth_rate_conf_int = np.percentile(growth_rate_bs_reps, [2.5, 97.5]) # Print the result to the screen print(&quot;&quot;&quot; Growth rate: {0:.4f} sq. µm/hour 95% conf int: [{1:.4f}, {2:.4f}] sq. µm/hour &quot;&quot;&quot;.format(growth_rate, *growth_rate_conf_int)) # Growth rate: 0.2301 sq. µm/hour # 95% conf int: [0.2266, 0.2336] sq. µm/hour . Under these conditions, the bacteria add about 0.23 square micrometers worth of mass each hour. The error bar is very tight, which we will see graphically in the next exercise. . #### Plotting the growth curve . You saw in the previous exercise that the confidence interval on the growth curve is very tight. You will explore this graphically here by plotting several bootstrap lines along with the growth curve. You will use the plt.semilogy() function to make the plot with the y-axis on a log scale. This means that you will need to transform your theoretical linear regression curve for plotting by exponentiating it. . # Plot data points in a semilog-y plot with axis labeles _ = plt.semilogy(t, bac_area, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Generate x-values for the bootstrap lines: t_bs t_bs = np.array([0, 14]) # Plot the first 100 bootstrap lines for i in range(100): y = np.exp(growth_rate_bs_reps[i] * t_bs + log_a0_bs_reps[i]) _ = plt.semilogy(t_bs, y, linewidth=0.5, alpha=0.05, color=&#39;red&#39;) # Label axes and show plot _ = plt.xlabel(&#39;time (hr)&#39;) _ = plt.ylabel(&#39;area (sq. µm)&#39;) plt.show() . . You can see that the bootstrap replicates do not stray much. This is due to the exquisitly exponential nature of the bacterial growth under these experimental conditions. . 2. Analysis of results of the 2015 FINA World Swimming Championships . . 2.1 Introduction to swimming data . . #### Graphical EDA of men’s 200 free heats . In the heats, all contestants swim, the very fast and the very slow. To explore how the swim times are distributed, plot an ECDF of the men’s 200 freestyle. . # Generate x and y values for ECDF: x, y x, y = dcst.ecdf(mens_200_free_heats) # Plot the ECDF as dots plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Label axes and show plot plt.xlabel(&#39;time (s)&#39;) plt.ylabel(&#39;ECDF&#39;) plt.show() . . Graphical EDA is always a great start. We see that fast swimmers are below 115 seconds, with a smattering of slow swimmers past that, including one very slow swimmer. . #### 200 m free time with confidence interval . Now, you will practice parameter estimation and computation of confidence intervals by computing the mean and median swim time for the men’s 200 freestyle heats. The median is useful because it is immune to heavy tails in the distribution of swim times, such as the slow swimmers in the heats. mens_200_free_heats is still in your namespace. . # Compute mean and median swim times mean_time = np.mean(mens_200_free_heats) median_time = np.median(mens_200_free_heats) # Draw 10,000 bootstrap replicates of the mean and median bs_reps_mean = dcst.draw_bs_reps(mens_200_free_heats, np.mean, size=10000) bs_reps_median = dcst.draw_bs_reps(mens_200_free_heats, np.median, size=10000) # Compute the 95% confidence intervals conf_int_mean = np.percentile(bs_reps_mean, [2.5, 97.5]) conf_int_median = np.percentile(bs_reps_median, [2.5, 97.5]) # Print the result to the screen print(&quot;&quot;&quot; mean time: {0:.2f} sec. 95% conf int of mean: [{1:.2f}, {2:.2f}] sec. median time: {3:.2f} sec. 95% conf int of median: [{4:.2f}, {5:.2f}] sec. &quot;&quot;&quot;.format(mean_time, *conf_int_mean, median_time, *conf_int_median)) . Indeed, the mean swim time is longer than the median because of the effect of the very slow swimmers. . . 2.2 Do swimmers go faster in the finals? . . #### EDA: finals versus semifinals . First, you will get an understanding of how athletes’ performance changes from the semifinals to the finals by computing the fractional improvement from the semifinals to finals and plotting an ECDF of all of these values. . The arrays final_times and semi_times contain the swim times of the respective rounds. The arrays are aligned such that final_times[i] and semi_times[i] are for the same swimmer/event. If you are interested in the strokes/events, you can check out the data frame df in your namespace, which has more detailed information, but is not used in the analysis. . df.head() athleteid stroke distance final_swimtime lastname semi_swimtime 0 100537 FREE 100 52.52 CAMPBELL 53.00 1 100537 FREE 50 24.12 CAMPBELL 24.32 2 100631 FREE 100 52.82 CAMPBELL 52.84 3 100631 FREE 50 24.36 CAMPBELL 24.22 4 100650 FLY 100 57.67 MCKEON 57.59 . # Compute fractional difference in time between finals and semis f = (semi_times - final_times) / semi_times # Generate x and y values for the ECDF: x, y x, y = dcst.ecdf(f) # Make a plot of the ECDF plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Label axes and show plot _ = plt.xlabel(&#39;f&#39;) _ = plt.ylabel(&#39;ECDF&#39;) plt.show() . . The median of the ECDF is just above zero. But at first glance, it does not look like there is much of any difference between semifinals and finals. We’ll check this carefully in the next exercises. . #### Parameter estimates of difference between finals and semifinals . Compute the mean fractional improvement from the semifinals to finals, along with a 95% confidence interval of the mean. The Numpy array f that you computed in the last exercise is in your namespace. . # Mean fractional time difference: f_mean f_mean = np.mean(f) # Get bootstrap reps of mean: bs_reps bs_reps = dcst.draw_bs_reps(f, func=np.mean, size=10000) # Compute confidence intervals: conf_int conf_int = np.percentile(bs_reps, [2.5, 97.5]) # Report print(&quot;&quot;&quot; mean frac. diff.: {0:.5f} 95% conf int of mean frac. diff.: [{1:.5f}, {2:.5f}]&quot;&quot;&quot;.format(f_mean, *conf_int)) # mean frac. diff.: 0.00040 # 95% conf int of mean frac. diff.: [-0.00092, 0.00176] . It looks like the mean finals time is just faster than the mean semifinal time, and they very well may be the same. We’ll test this hypothesis next. . #### How to do the permutation test . Based on our EDA and parameter estimates, it is tough to discern improvement from the semifinals to finals. In the next exercise, you will test the hypothesis that there is no difference in performance between the semifinals and finals. A permutation test is fitting for this. We will use the mean value of f as the test statistic. . Step of the permutation test: . Take an array of semifinal times and an array of final times for each swimmer for each stroke/distance pair. | Go through each array, and for each index, swap the entry in the respective final and semifinal array with a 50% probability. | Use the resulting final and semifinal arrays to compute f and then the mean of f . | . #### Generating permutation samples . As you worked out in the last exercise, we need to generate a permutation sample by randomly swapping corresponding entries in the semi_times and final_times array. Write a function with signature swap_random(a, b) that returns arrays where random indices have the entries in a and b swapped. . def swap_random(a, b): &quot;&quot;&quot;Randomly swap entries in two arrays.&quot;&quot;&quot; # Indices to swap swap_inds = np.random.random(size=len(a)) &lt; 0.5 # Make copies of arrays a and b for output a_out = np.copy(a) b_out = np.copy(b) # Swap values a_out[swap_inds] = b[swap_inds] b_out[swap_inds] = a[swap_inds] return a_out, b_out . Now you have this function in hand to do the permutation test. . #### Hypothesis test: Do women swim the same way in semis and finals? . Test the hypothesis that performance in the finals and semifinals are identical using the mean of the fractional improvement as your test statistic. The test statistic under the null hypothesis is considered to be at least as extreme as what was observed if it is greater than or equal to f_mean , which is already in your namespace. . The semifinal and final times are contained in the numpy arrays semi_times and final_times . . # Set up array of permutation replicates perm_reps = np.empty(1000) for i in range(1000): # Generate a permutation sample semi_perm, final_perm = swap_random(semi_times, final_times) # Compute f from the permutation sample f = (semi_perm - final_perm) / semi_perm # Compute and store permutation replicate perm_reps[i] = np.mean(f) # Compute and print p-value print(&#39;p =&#39;, np.sum(perm_reps &gt;= f_mean) / 1000) . That was a little tricky… Nice work! The p-value is large, about 0.27, which suggests that the results of the 2015 World Championships are consistent with there being no difference in performance between the finals and semifinals. . . 2.3 How does the performance of swimmers decline over long events? . . #### EDA: Plot all your data . To get a graphical overview of a data set, it is often useful to plot all of your data. In this exercise, plot all of the splits for all female swimmers in the 800 meter heats. . # Plot the splits for each swimmer for splitset in splits: _ = plt.plot(split_number, splitset, linewidth=1, color=&#39;lightgray&#39;) # Compute the mean split times mean_splits = np.mean(splits, axis=0) # Plot the mean split times _ = plt.plot(split_number, mean_splits, linewidth=3, markersize=12) # Label axes and show plot _ = plt.xlabel(&#39;split number&#39;) _ = plt.ylabel(&#39;split time (s)&#39;) plt.show() . . You can see that there is wide variability in the splits among the swimmers, and what appears to be a slight trend toward slower split times. . #### Linear regression of average split time . We will assume that the swimmers slow down in a linear fashion over the course of the 800 m event. The slowdown per split is then the slope of the mean split time versus split number plot. Perform a linear regression to estimate the slowdown per split and compute a pairs bootstrap 95% confidence interval on the slowdown. Also show a plot of the best fit line. . # Perform regression slowdown, split_3 = np.polyfit(split_number, mean_splits, deg=1) # Compute pairs bootstrap bs_reps, _ = dcst.draw_bs_pairs_linreg(split_number, mean_splits, size=10000) # Compute confidence interval conf_int = np.percentile(bs_reps, [2.5, 97.5]) # Plot the data with regressions line _ = plt.plot(split_number, mean_splits, marker=&#39;.&#39;, linestyle=&#39;none&#39;) _ = plt.plot(split_number, slowdown * split_number + split_3, &#39;-&#39;) # Label axes and show plot _ = plt.xlabel(&#39;split number&#39;) _ = plt.ylabel(&#39;split time (s)&#39;) plt.show() # Print the slowdown per split print(&quot;&quot;&quot; mean slowdown: {0:.3f} sec./split 95% conf int of mean slowdown: [{1:.3f}, {2:.3f}] sec./split&quot;&quot;&quot;.format( slowdown, *conf_int)) # mean slowdown: 0.065 sec./split # 95% conf int of mean slowdown: [0.051, 0.078] sec./split . . There is a small (about 6 hundreths of a second), but discernible, slowdown per split. We’ll do a hypothesis test next. . #### Hypothesis test: are they slowing down? . Now we will test the null hypothesis that the swimmer’s split time is not at all correlated with the distance they are at in the swim. We will use the Pearson correlation coefficient (computed using dcst.pearson_r() ) as the test statistic. . # Observed correlation rho = dcst.pearson_r(split_number, mean_splits) # Initialize permutation reps perm_reps_rho = np.empty(10000) # Make permutation reps for i in range(10000): # Scramble the split number array scrambled_split_number = np.random.permutation(split_number) # Compute the Pearson correlation coefficient perm_reps_rho[i] = dcst.pearson_r(scrambled_split_number, mean_splits) # Compute and print p-value p_val = np.sum(perm_reps_rho &gt;= rho) / len(perm_reps_rho) print(&#39;p =&#39;, p_val) . The tiny effect is very real! With 10,000 replicates, we never got a correlation as big as observed under the hypothesis that the swimmers do not change speed as the race progresses. . 3. The “Current Controversy” of the 2013 World Championships . . 3.1 Introduction to the current controversy . . #### A metric for improvement . In your first analysis, you will investigate how times of swimmers in 50 m events change as they move between low numbered lanes (1-3) to high numbered lanes (6-8) in the semifinals and finals. We showed in the previous chapter that there is little difference between semifinal and final performance, so you will neglect any differences due to it being the final versus the semifinal. . You want to use as much data as you can, so use all four strokes for both the men’s and women’s competitions. As such, what would be a good metric for improvement from one round to the next for an individual swimmer, where t a is the swim time in a low numbered lane and t b is the swim time in a high numbered lane? . The fractional improvement of swim time, ( t a – t b ) / t a . . This is a good metric; it is the fractional improvement, and therefore independent of the basal speed (which is itself dependent on stroke and gender). . #### ECDF of improvement from low to high lanes . Now that you have a metric for improvement going from low- to high-numbered lanes, plot an ECDF of this metric. . # Compute the fractional improvement of being in high lane: f f = (swimtime_low_lanes - swimtime_high_lanes) / swimtime_low_lanes # Make x and y values for ECDF: x, y x, y = dcst.ecdf(f) # Plot the ECDFs as dots _ = plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Label the axes and show the plot _ = plt.xlabel(&#39;f&#39;) _ = plt.ylabel(&#39;ECDF&#39;) plt.show() . . This is starting to paint a picture of lane bias. The ECDF demonstrates that all but three of the 26 swimmers swam faster in the high numbered lanes. . #### Estimation of mean improvement . You will now estimate how big this current effect is. Compute the mean fractional improvement for being in a high-numbered lane versus a low-numbered lane, along with a 95% confidence interval of the mean. . # Compute the mean difference: f_mean f_mean = np.mean(f) # Draw 10,000 bootstrap replicates: bs_reps bs_reps = dcst.draw_bs_reps(f, np.mean, size=10000) # Compute 95% confidence interval: conf_int conf_int = np.percentile(bs_reps, [2.5, 97.5]) # Print the result print(&quot;&quot;&quot; mean frac. diff.: {0:.5f} 95% conf int of mean frac. diff.: [{1:.5f}, {2:.5f}]&quot;&quot;&quot;.format(f_mean, *conf_int)) # mean frac. diff.: 0.01051 # 95% conf int of mean frac. diff.: [0.00612, 0.01591] . It sure looks like swimmers are faster in lanes 6-8. . #### How should we test the hypothesis? . You are interested in the presence of lane bias toward higher lanes, presumably due to a slight current in the pool. A natural null hypothesis to test, then, is that the mean fractional improvement going from low to high lane numbers is zero. Which of the following is a good way to simulate this null hypothesis? . As a reminder, the arrays swimtime_low_lanes and swimtime_high_lanes contain the swim times for lanes 1-3 and 6-8, respectively, and we define the fractional improvement as f = (swimtime_low_lanes - swimtime_high_lanes) / swimtime_low_lanes . . Subtract the mean of f from f to generate f_shift . Then, take bootstrap replicate of the mean from this f_shift . . #### Hypothesis test: Does lane assignment affect performance? . Perform a bootstrap hypothesis test of the null hypothesis that the mean fractional improvement going from low-numbered lanes to high-numbered lanes is zero. Take the fractional improvement as your test statistic, and “at least as extreme as” to mean that the test statistic under the null hypothesis is greater than or equal to what was observed. . # Shift f: f_shift f_shift = f - f_mean # Draw 100,000 bootstrap replicates of the mean: bs_reps bs_reps = dcst.draw_bs_reps(f_shift, np.mean, size=100000) # Compute and report the p-value p_val = np.sum(bs_reps &gt;= f_mean) / 100000 print(&#39;p =&#39;, p_val) . A p-value of 0.0003 is quite small and suggests that the mean fractional improvment is greater than zero. . #### Did the 2015 event have this problem? . You would like to know if this is a typical problem with pools in competitive swimming. To address this question, perform a similar analysis for the results of the 2015 FINA World Championships. That is, compute the mean fractional improvement for going from lanes 1-3 to lanes 6-8 for the 2015 competition, along with a 95% confidence interval on the mean. Also test the hypothesis that the mean fractional improvement is zero. . The arrays swimtime_low_lanes_15 and swimtime_high_lanes_15 have the pertinent data. . # Compute f and its mean f = (swimtime_low_lanes_15 - swimtime_high_lanes_15) / swimtime_low_lanes_15 f_mean = np.mean(f) # Draw 10,000 bootstrap replicates bs_reps = dcst.draw_bs_reps(f, np.mean, size=10000) # Compute 95% confidence interval conf_int = np.percentile(bs_reps, [2.5, 97.5]) # Shift f f_shift = f - f_mean # Draw 100,000 bootstrap replicates of the mean bs_reps = dcst.draw_bs_reps(f_shift, np.mean, size=100000) # Compute the p-value p_val = np.sum(bs_reps &gt;= f_mean) / 100000 # Print the results print(&quot;&quot;&quot; mean frac. diff.: {0:.5f} 95% conf int of mean frac. diff.: [{1:.5f}, {2:.5f}] p-value: {3:.5f}&quot;&quot;&quot;.format(f_mean, *conf_int, p_val)) # mean frac. diff.: 0.00079 # 95% conf int of mean frac. diff.: [-0.00198, 0.00341] # p-value: 0.28179 . Both the confidence interval an the p-value suggest that there was no lane bias in 2015. . . 3.2 The zigzag effect . . #### Which splits should we consider? . As you proceed to quantitatively analyze the zigzag effect in the 1500 m, which splits should you include in our analysis? For reference, the plot of the zigzag effect from the video is shown to the right. . You should include all splits except the first two and the last two. You should neglect the last two because swimmers stop pacing themselves and “kick” for the final stretch. The first two are different because they involve jumping off the starting blocks and more underwater swimming than others. . You want to use splits where the swimmers are swimming as consistently as they can. . #### EDA: mean differences between odd and even splits . To investigate the differences between odd and even splits, you first need to define a difference metric. In previous exercises, you investigated the improvement of moving from a low-numbered lane to a high-numbered lane, defining f = ( t a – t b ) / t a . There, the t a in the denominator served as our reference time for improvement. Here, you are considering both improvement and decline in performance depending on the direction of swimming, so you want the reference to be an average. So, we will define the fractional difference as f = 2( t a – t b ) / ( t a + t b ). . Your task here is to plot the mean fractional difference between odd and even splits versus lane number. I have already calculated the mean fractional differences for the 2013 and 2015 Worlds for you, and they are stored in f_13 and f_15 . The corresponding lane numbers are in the array lanes . . # Plot the the fractional difference for 2013 and 2015 plt.plot(lanes, f_13, marker=&#39;.&#39;, markersize=12, linestyle=&#39;none&#39;) plt.plot(lanes, f_15, marker=&#39;.&#39;, markersize=12, linestyle=&#39;none&#39;) # Add a legend _ = plt.legend((2013, 2015)) # Label axes and show plot plt.xlabel(&#39;lane&#39;) plt.ylabel(&#39;frac. diff. (odd - even)&#39;) plt.show() . . EDA has exposed a strong slope in 2013 compared to 2015! . #### How does the current effect depend on lane position? . To quantify the effect of lane number on performance, perform a linear regression on the f_13 versus lanes data. Do a pairs bootstrap calculation to get a 95% confidence interval. Finally, make a plot of the regression. The arrays lanes and f_13 are in your namespace. . Note that we could compute error bars on the mean fractional differences and use them in the regression, but that is beyond the scope of this course. . # Compute the slope and intercept of the frac diff/lane curve slope, intercept = np.polyfit(lanes, f_13, 1) # Compute bootstrap replicates bs_reps_slope, bs_reps_int = dcst.draw_bs_pairs_linreg(lanes, f_13, size=10000) # Compute 95% confidence interval of slope conf_int = np.percentile(bs_reps_slope, [2.5, 97.5]) # Print slope and confidence interval print(&quot;&quot;&quot; slope: {0:.5f} per lane 95% conf int: [{1:.5f}, {2:.5f}] per lane&quot;&quot;&quot;.format(slope, *conf_int)) # x-values for plotting regression lines x = np.array([1, 8]) # Plot 100 bootstrap replicate lines for i in range(100): _ = plt.plot(x, bs_reps_slope[i] * x + bs_reps_int[i], color=&#39;red&#39;, alpha=0.2, linewidth=0.5) # Update the plot plt.draw() plt.show() # slope: 0.00447 per lane # 95% conf int: [0.00394, 0.00501] per lane . . The slope is a fractional difference of about 0.4% per lane. This is quite a substantial difference at this elite level of swimming where races can be decided by tiny differences. . #### Hypothesis test: can this be by chance? . The EDA and linear regression analysis is pretty conclusive. Nonetheless, you will top off the analysis of the zigzag effect by testing the hypothesis that lane assignment has nothing to do with the mean fractional difference between even and odd lanes using a permutation test. You will use the Pearson correlation coefficient, which you can compute with dcst.pearson_r() as the test statistic. The variables lanes and f_13 are already in your namespace. . # Compute observed correlation: rho rho = dcst.pearson_r(lanes, f_13) # Initialize permutation reps: perm_reps_rho perm_reps_rho = np.empty(10000) # Make permutation reps for i in range(10000): # Scramble the lanes array: scrambled_lanes scrambled_lanes = np.random.permutation(lanes) # Compute the Pearson correlation coefficient perm_reps_rho[i] = dcst.pearson_r(scrambled_lanes, f_13) # Compute and print p-value p_val = np.sum(perm_reps_rho &gt;= rho) / 10000 print(&#39;p =&#39;, p_val) # p = 0.0 . The p-value is very small, as you would expect from the confidence interval of the last exercise. . . 3.3 Recap of swimming analysis . . 4. Statistical seismology and the Parkfield region . . 4.1 Introduction to statistical seismology and the Parkfield experiment . . Gutenberg–Richter law . 4.1.1 Parkfield earthquake magnitudes . As usual, you will start with EDA and plot the ECDF of the magnitudes of earthquakes detected in the Parkfield region from 1950 to 2016. The magnitudes of all earthquakes in the region from the ANSS ComCat are stored in the Numpy array mags . . When you do it this time, though, take a shortcut in generating the ECDF. You may recall that putting an asterisk before an argument in a function splits what follows into separate arguments. Since dcst.ecdf() returns two values, we can pass them as the x , y positional arguments to plt.plot() as plt.plot(*dcst.ecdf(data_you_want_to_plot)) . . You will use this shortcut in this exercise and going forward. . # Make the plot plt.plot(*dcst.ecdf(mags), marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Label axes and show plot plt.xlabel(&#39;magnitude&#39;) plt.ylabel(&#39;ECDF&#39;) plt.show() . . Note the distinctive roll-off at magnitudes below 1.0. . 4.1.2 Computing the b-value . The b -value is a common metric for the seismicity of a region. You can imagine you would like to calculate it often when working with earthquake data. For tasks like this that you will do often, it is best to write a function! So, write a function with signature b_value(mags, mt, perc=[2.5, 97.5], n_reps=None) that returns the b -value and (optionally, if n_reps is not None ) its confidence interval for a set of magnitudes, mags . The completeness threshold is given by mt . The perc keyword argument gives the percentiles for the lower and upper bounds of the confidence interval, and n_reps is the number of bootstrap replicates to use in computing the confidence interval. . def b_value(mags, mt, perc=[2.5, 97.5], n_reps=None): &quot;&quot;&quot;Compute the b-value and optionally its confidence interval.&quot;&quot;&quot; # Extract magnitudes above completeness threshold: m m = mags[mags &gt;= mt] # Compute b-value: b b = (np.mean(m) - mt) * np.log(10) # Draw bootstrap replicates if n_reps is None: return b else: m_bs_reps = dcst.draw_bs_reps(m, np.mean, size=n_reps) # Compute b-value from replicates: b_bs_reps b_bs_reps = (m_bs_reps - mt) * np.log(10) # Compute confidence interval: conf_int conf_int = np.percentile(b_bs_reps, perc) return b, conf_int . You now have a very handy function for computing b-values. You’ll use it in this and the next chapter. . 4.1.3 The b-value for Parkfield . The ECDF is effective at exposing roll-off, as you could see below magnitude 1. Because there are plenty of earthquakes above magnitude 3, you can use m t = 3 as your completeness threshold. With this completeness threshold, compute the b -value for the Parkfield region from 1950 to 2016, along with the 95% confidence interval. Print the results to the screen. The variable mags with all the magnitudes is in your namespace. . Overlay the theoretical Exponential CDF to verify that the Parkfield region follows the Gutenberg-Richter Law. . # Compute b-value and confidence interval b, conf_int = b_value(mags, mt, perc=[2.5, 97.5], n_reps=10000) # Generate samples to for theoretical ECDF m_theor = np.random.exponential(b/np.log(10), size=100000) + mt # Plot the theoretical CDF _ = plt.plot(*dcst.ecdf(m_theor)) # Plot the ECDF (slicing mags &gt;= mt) _ = plt.plot(*dcst.ecdf(mags[mags &gt;= mt]), marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Pretty up and show the plot _ = plt.xlabel(&#39;magnitude&#39;) _ = plt.ylabel(&#39;ECDF&#39;) _ = plt.xlim(2.8, 6.2) plt.show() # Report the results print(&quot;&quot;&quot; b-value: {0:.2f} 95% conf int: [{1:.2f}, {2:.2f}]&quot;&quot;&quot;.format(b, *conf_int)) # b-value: 1.08 # 95% conf int: [0.94, 1.24] . . Parkfield seems to follow the Gutenberg-Richter law very well. The b-value of about 1 is typical for regions along fault zones. . . 4.2 Timing of major earthquakes and the Parkfield sequence . . 4.2.1 Interearthquake time estimates for Parkfield . In this exercise, you will first compute the best estimates for the parameters for the Exponential and Gaussian models for interearthquake times. You will then plot the theoretical CDFs for the respective models along with the formal ECDF of the actual Parkfield interearthquake times. . # Compute the mean time gap: mean_time_gap mean_time_gap = np.mean(time_gap) # Standard deviation of the time gap: std_time_gap std_time_gap = np.std(time_gap) # Generate theoretical Exponential distribution of timings: time_gap_exp time_gap_exp = np.random.exponential(scale=mean_time_gap, size=10000) # Generate theoretical Normal distribution of timings: time_gap_norm time_gap_norm = np.random.normal(loc=mean_time_gap, scale=std_time_gap, size=10000) # Plot theoretical CDFs _ = plt.plot(*dcst.ecdf(time_gap_exp)) _ = plt.plot(*dcst.ecdf(time_gap_norm)) # Plot Parkfield ECDF _ = plt.plot(*dcst.ecdf(time_gap, formal=True, min_x=-10, max_x=50)) # Add legend _ = plt.legend((&#39;Exp.&#39;, &#39;Norm.&#39;), loc=&#39;upper left&#39;) # Label axes, set limits and show plot _ = plt.xlabel(&#39;time gap (years)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) _ = plt.xlim(-10, 50) plt.show() . . By eye, the Gaussian model seems to describe the observed data best. We will investigate the consequences of this in the next exercise, and see if we can reject the Exponential model in coming exercises. . 4.2.2 When will the next big Parkfield quake be? . The last big earthquake in the Parkfield region was on the evening of September 27, 2004 local time. Your task is to get an estimate as to when the next Parkfield quake will be, assuming the Exponential model and also the Gaussian model. In both cases, the best estimate is given by the mean time gap, which you computed in the last exercise to be 24.62 years, meaning that the next earthquake would be in 2029. Compute 95% confidence intervals on when the next earthquake will be assuming an Exponential distribution parametrized by mean_time_gap you computed in the last exercise. Do the same assuming a Normal distribution parametrized by mean_time_gap and std_time_gap . . # Draw samples from the Exponential distribution: exp_samples exp_samples = np.random.exponential(scale=mean_time_gap, size=100000) # Draw samples from the Normal distribution: norm_samples norm_samples = np.random.normal(loc=mean_time_gap, scale=std_time_gap, size=100000) # No earthquake as of today, so only keep samples that are long enough exp_samples = exp_samples[exp_samples &gt; today - last_quake] norm_samples = norm_samples[norm_samples &gt; today - last_quake] # Compute the confidence intervals with medians conf_int_exp = np.percentile(exp_samples, [2.5, 50, 97.5]) + last_quake conf_int_norm = np.percentile(norm_samples, [2.5, 50, 97.5]) + last_quake # Print the results print(&#39;Exponential:&#39;, conf_int_exp) print(&#39; Normal:&#39;, conf_int_norm) # Exponential: [2020.43020248 2036.77538201 2110.14809932] # Normal: [2020.64362947 2030.72447973 2046.46834012] . Great work! The models given decidedly different predictions. The Gaussian model says the next earthquake is almost sure to be in the next few decades, but the Exponential model says we may very well have to wait longer. . . 4.3 How are the Parkfield interearthquake times distributed? . . 4.3.1 Computing the value of a formal ECDF . To be able to do the Kolmogorov-Smirnov test, we need to compute the value of a formal ECDF at arbitrary points. In other words, we need a function, ecdf_formal(x, data) that returns the value of the formal ECDF derived from the data set data for each value in the array x . Two of the functions accomplish this. One will not. Of the two that do the calculation correctly, one is faster. Label each. . As a reminder, the ECDF is formally defined as ECDF( x ) = (number of samples ≤ x ) / (total number of samples). You also might want to check out the doc string of np.searchsorted() . . # a) def ecdf_formal(x, data): return np.searchsorted(np.sort(data), x) / len(data) # b) def ecdf_formal(x, data): return np.searchsorted(np.sort(data), x, side=&#39;right&#39;) / len(data) # c) def ecdf_formal(x, data): output = np.empty(len(x)) data = np.sort(data) for i, x_val in x: j = 0 while j &lt; len(data) and x_val &gt;= data[j]: j += 1 output[i] = j return output / len(data) . (a) Incorrect; (b) Correct, fast; (c) Correct, slow. . 4.3.2 Computing the K-S statistic . Write a function to compute the Kolmogorov-Smirnov statistic from two datasets, data1 and data2 , in which data2 consists of samples from the theoretical distribution you are comparing your data to. Note that this means we are using hacker stats to compute the K-S statistic for a dataset and a theoretical distribution, not the K-S statistic for two empirical datasets. Conveniently, the function you just selected for computing values of the formal ECDF is given as dcst.ecdf_formal() . . def ks_stat(data1, data2): # Compute ECDF from data: x, y x, y = dcst.ecdf(data1) # Compute corresponding values of the target CDF cdf = dcst.ecdf_formal(x, data2) # Compute distances between concave corners and CDF D_top = y - cdf # Compute distance between convex corners and CDF D_bottom = cdf - y + 1/len(data1) return np.max((D_top, D_bottom)) . 4.3.3 Drawing K-S replicates . Now, you need a function to draw Kolmogorov-Smirnov replicates out of a target distribution, f . Construct a function with signature draw_ks_reps(n, f, args=(), size=10000, n_reps=10000) to do so. Here, n is the number of data points, and f is the function you will use to generate samples from the target CDF. For example, to test against an Exponential distribution, you would pass np.random.exponential as f . This function usually takes arguments, which must be passed as a tuple. So, if you wanted to take samples from an Exponential distribution with mean x_mean , you would use the args=(x_mean,) keyword. The keyword arguments size and n_reps respectively represent the number of samples to take from the target distribution and the number of replicates to draw. . def draw_ks_reps(n, f, args=(), size=10000, n_reps=10000): # Generate samples from target distribution x_f = f(*args, size=size) # Initialize K-S replicates reps = np.empty(n_reps) # Draw replicates for i in range(n_reps): # Draw samples for comparison x_samp = f(*args, size=n) # Compute K-S statistic reps[i] = dcst.ks_stat(x_samp, x_f) return reps . 4.3.4 The K-S test for Exponentiality . Test the null hypothesis that the interearthquake times of the Parkfield sequence are Exponentially distributed. That is, earthquakes happen at random with no memory of when the last one was. Note This calculation is computationally intensive (you will draw more than 10 8 random numbers), so it will take about 10 seconds to complete. # Draw target distribution: x_f x_f = np.random.exponential(scale=mean_time_gap, size=10000) # Compute K-S stat: d d = dcst.ks_stat(x_f, time_gap) # Draw K-S replicates: reps reps = dcst.draw_ks_reps(len(time_gap), np.random.exponential, args=(mean_time_gap,), size=10000, n_reps=10000) # Compute and print p-value p_val = np.sum(reps &gt;= d) / 10000 print(&#39;p =&#39;, p_val) # p = 0.2199 . That’s a p-value above 0.2. This means that the Parkfield sequence is not outside the realm of possibility if earthquakes there are a Poisson process. This does not mean that they are generated by a Poisson process, but that the observed sequence is not incongruous with that model. The upshot is that it is really hard to say when the next Parkfield quake will be. . 5. Earthquakes and oil mining in Oklahoma . . 5.1 Variations in earthquake frequency and seismicity . 5.1.1 EDA: Plotting earthquakes over time . Make a plot where the y -axis is the magnitude and the x -axis is the time of all earthquakes in Oklahoma between 1980 and the first half of 2017. Each dot in the plot represents a single earthquake. The time of the earthquakes, as decimal years, is stored in the Numpy array time , and the magnitudes in the Numpy array mags . . # Plot time vs. magnitude plt.plot(time, mags, marker=&#39;.&#39;, linestyle=&#39;none&#39;, alpha=0.1) # Label axes and show the plot plt.xlabel(&#39;time (year)&#39;) plt.ylabel(&#39;magnitude&#39;) plt.show() . . 5.1.2 Estimates of the mean interearthquake times . The graphical EDA in the last exercise shows an obvious change in earthquake frequency around 2010. To compare, compute the mean time between earthquakes of magnitude 3 and larger from 1980 through 2009 and also from 2010 through mid-2017. Also include 95% confidence intervals of the mean. The variables dt_pre and dt_post respectively contain the time gap between all earthquakes of magnitude at least 3 from pre-2010 and post-2010 in units of days. . # Compute mean interearthquake time mean_dt_pre = np.mean(dt_pre) mean_dt_post = np.mean(dt_post) # Draw 10,000 bootstrap replicates of the mean bs_reps_pre = dcst.draw_bs_reps(dt_pre, np.mean, size=10000) bs_reps_post = dcst.draw_bs_reps(dt_post, np.mean, size=10000) # Compute the confidence interval conf_int_pre = np.percentile(bs_reps_pre, [2.5, 97.5]) conf_int_post = np.percentile(bs_reps_post, [2.5, 97.5]) # Print the results print(&quot;&quot;&quot;1980 through 2009 mean time gap: {0:.2f} days 95% conf int: [{1:.2f}, {2:.2f}] days&quot;&quot;&quot;.format(mean_dt_pre, *conf_int_pre)) print(&quot;&quot;&quot; 2010 through mid-2017 mean time gap: {0:.2f} days 95% conf int: [{1:.2f}, {2:.2f}] days&quot;&quot;&quot;.format(mean_dt_post, *conf_int_post)) . 1980 through 2009 mean time gap: 204.61 days 95% conf int: [140.30, 276.13] days 2010 through mid-2017 mean time gap: 1.12 days 95% conf int: [0.97, 1.29] days . There is almost a 200-fold increase in earthquake frequency after 2010. . 5.1.3 Hypothesis test: did earthquake frequency change? . Obviously, there was a massive increase in earthquake frequency once wastewater injection began. Nonetheless, you will still do a hypothesis test for practice. You will not test the hypothesis that the interearthquake times have the same distribution before and after 2010, since wastewater injection may affect the distribution. Instead, you will assume that they have the same mean. So, compute the p-value associated with the hypothesis that the pre- and post-2010 interearthquake times have the same mean, using the mean of pre-2010 time gaps minus the mean of post-2010 time gaps as your test statistic. . # Compute the observed test statistic mean_dt_diff = mean_dt_pre - mean_dt_post # Shift the post-2010 data to have the same mean as the pre-2010 data dt_post_shift = dt_post - mean_dt_post + mean_dt_pre # Compute 10,000 bootstrap replicates from arrays bs_reps_pre = dcst.draw_bs_reps(dt_pre, np.mean, size=10000) bs_reps_post = dcst.draw_bs_reps(dt_post_shift, np.mean, size=10000) # Get replicates of difference of means bs_reps = bs_reps_pre - bs_reps_post # Compute and print the p-value p_val = np.sum(bs_reps &gt;= mean_dt_diff) / 10000 print(&#39;p =&#39;, p_val) # p = 0.0 . In 10,000 samples, not one had a test statistic greater than was was observed. The p-value is, predictably based on what we have done so far, is tiny! . 5.1.4 How to display your analysis . In the last three exercises, you generated a plot, computed means/confidence intervals, and did a hypothesis test. If you were to present your results to others, which of the following is the most effective order of emphasis, from greatest-to-least, you should put on the respective results? . plot, mean/confidence interval, hypothesis test . The plot graphically shows all data, and the scale of the effect is evident. The mean and confidence interval quantify how big the effect is. The hypothesis test, by this point, is so obvious it is useless. . . 5.2 Earthquake magnitudes in Oklahoma . . 5.2.1 EDA: Comparing magnitudes before and after 2010 . Make an ECDF of earthquake magnitudes from 1980 through 2009. On the same plot, show an ECDF of magnitudes of earthquakes from 2010 through mid-2017. The time of the earthquakes, as decimal years, are stored in the Numpy array time and the magnitudes in the Numpy array mags . . # Get magnitudes before and after 2010 mags_pre = mags[time &lt; 2010] mags_post = mags[time &gt;= 2010] # Generate ECDFs plt.plot(*dcst.ecdf(mags_pre), marker=&#39;.&#39;, linestyle=&#39;none&#39;) plt.plot(*dcst.ecdf(mags_post), marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Label axes and show plot _ = plt.xlabel(&#39;magnitude&#39;) _ = plt.ylabel(&#39;ECDF&#39;) plt.legend((&#39;1980 though 2009&#39;, &#39;2010 through mid-2017&#39;), loc=&#39;upper left&#39;) plt.show() . . Both curves seem to follow the Gutenberg-Richter Law, but with different completeness thresholds, probably due to improvements in sensing capabilities in more recent years. . 5.2.2 Quantification of the b-values . Based on the plot you generated in the previous exercise, you can safely use a completeness threshold of mt = 3 . Using this threshold, compute b -values for the period between 1980 and 2009 and for 2010 through mid-2017. The function b_value() you wrote last chapter, which computes the b -value and confidence interval from a set of magnitudes and completeness threshold, is available in your namespace, as are the numpy arrays mags_pre and mags_post from the last exercise, and mt . . # Compute b-value and confidence interval for pre-2010 b_pre, conf_int_pre = b_value(mags_pre, mt, perc=[2.5, 97.5], n_reps=10000) # Compute b-value and confidence interval for post-2010 b_post, conf_int_post = b_value(mags_post, mt, perc=[2.5, 97.5], n_reps=10000) # Report the results print(&quot;&quot;&quot; 1980 through 2009 b-value: {0:.2f} 95% conf int: [{1:.2f}, {2:.2f}] 2010 through mid-2017 b-value: {3:.2f} 95% conf int: [{4:.2f}, {5:.2f}] &quot;&quot;&quot;.format(b_pre, *conf_int_pre, b_post, *conf_int_post)) . 1980 through 2009 b-value: 0.74 95% conf int: [0.54, 0.96] 2010 through mid-2017 b-value: 0.62 95% conf int: [0.60, 0.65] . The confidence interval for the b -value for recent earthquakes is tighter than for earlier ones because there are many more recent ones. Still, the confidence intervals overlap, and we can perform a hypothesis test to see if we might get these results if the b -values are actually the same. . 5.2.3 How should we do a hypothesis test on differences of the b-value? . We wish to test the hypothesis that the b -value in Oklahoma from 1980 through 2009 is the same as that from 2010 through mid-2017. Which of the first five statements is false? If none of them are false, select the last choice. . You should only include earthquakes that have magnitudes above the completeness threshold. A value of 3 is reasonable. | You should perform a permutation test because asserting a null hypothesis that the b -values are the same implicitly assumes that the magnitudes are identically distributed, specifically Exponentially, by the Gutenberg-Richter Law. | A reasonable test statistic is the difference between the mean post-2010 magnitude and the mean pre-2010 magnitude. | You do not need to worry about the fact that there were far fewer earthquakes before 2010 than there were after. That is to say, there are fewer earthquakes before 2010, but sufficiently many to do a permutation test. | You do not need to worry about the fact that the two time intervals are of different length. | None of the above statements are false. | . For instructional purposes, here are reasons why each is true: Option 1 is true because below the completeness threshold, we are not comparing earthquakes before and after 2010, but observed earthquakes before and after 2010. We do not have a complete data set below the completeness threshold. . Option 2 is true because we really are assuming the Gutenberg-Richter law holds, in part because we are only considering earthquakes above the completeness threshold. We are using a model (the G-R law) to deal with missing data. So, since both sets of quakes follow the same statistical model, and that model has a single parameter, a permutation test is appropriate. . Option 3 is true, even though you may be thinking that the mean values are not the b -values, and that you should be using the difference in b -value as your test statistic. However, the difference in mean magnitude is directly proportional to the difference in b -value, so the result of the hypothesis test will be identical if we use b -values of mean magnitudes. . Option 4 is true because even though they have different numbers of earthquakes, you are only interested in summary statistics about their magnitude. There were 53 earthquakes between 1980 and 2009 with magnitude 3 or greater, so we have enough to compute a reliable mean. . Option 5 is true because, provided the time interval is long enough, the b -value is independent of the time interval, just like the mean of Exponentially distributed values is independent of how many there are, provided there are not too few. . 5.2.4 Hypothesis test: are the b-values different? . Perform the hypothesis test sketched out on the previous exercise. The variables mags_pre and mags_post are already loaded into your namespace, as is mt = 3 . . # Only magnitudes above completeness threshold mags_pre = mags_pre[mags_pre &gt;= mt] mags_post = mags_post[mags_post &gt;= mt] # Observed difference in mean magnitudes: diff_obs diff_obs = np.mean(mags_post) - np.mean(mags_pre) # Generate permutation replicates: perm_reps perm_reps = dcst.draw_perm_reps(mags_post, mags_pre, dcst.diff_of_means, size=10000) # Compute and print p-value p_val = np.sum(perm_reps &lt; diff_obs) / 10000 print(&#39;p =&#39;, p_val) # p = 0.0993 . A p-value around 0.1 suggests that the observed magnitudes are commensurate with there being no change in b -value after wastewater injection began. . 5.2.5 What can you conclude from this analysis? . All but one of the following constitute reasonable conclusions from our analysis of earthquakes. Which one does not? . The seismicity, as measured by the b -value, is comparable before and after wastewater injection. | Earthquakes are over 100 times more frequent in Oklahoma after widespread wastewater injection began. | Oklahoma has a smaller b -value than the Parkfield region, so the Parkfield region has more earthquakes. | Oklahoma has a b -value smaller than the Parkfield region, so a randomly selected earthquake above magnitude 3 in Oklahoma more likely than not has a smaller magnitude than one above magnitude 3 randomly selected from the Parkfield region. | . One cannot conclude information about frequency of earthquakes from the b -value alone. It is also true that from 2010-mid 2017, Oklahoma had twice as many earthquakes of magnitude 3 and higher than the entire state of California! . . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/case-studies-in-statistical-thinking.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/case-studies-in-statistical-thinking.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post35": {
            "title": "Unsupervised Learning in Python",
            "content": "Unsupervised Learning in Python . This is the memo of the 23th course of ‘Data Scientist with Python’ track. . You can find the original course HERE . . . 1. Clustering for dataset exploration . . 1.1 Unsupervised learning . #### How many clusters? . You are given an array points of size 300×2, where each row gives the (x, y) co-ordinates of a point on a map. Make a scatter plot of these points, and use the scatter plot to guess how many clusters there are. . points[:3] array([[ 0.06544649, -0.76866376], [-1.52901547, -0.42953079], [ 1.70993371, 0.69885253]]) xs=points[:,0] ys=points[:,1] plt.scatter(xs,ys) plt.show() . . #### Clustering 2D points . From the scatter plot of the previous exercise, you saw that the points seem to separate into 3 clusters. You’ll now create a KMeans model to find 3 clusters, and fit it to the data points from the previous exercise. After the model has been fit, you’ll obtain the cluster labels for some new points using the .predict() method. . # Import KMeans from sklearn.cluster import KMeans # Create a KMeans instance with 3 clusters: model model = KMeans(n_clusters=3) # Fit model to points model.fit(points) # Determine the cluster labels of new_points: labels labels = model.predict(new_points) # Print cluster labels of new_points print(labels) [1 2 0 1 2 1 2 2 2 0 1 2 2 0 0 2 0 0 2 2 0 2 1 2 1 0 2 0 0 1 1 2 2 2 0 1 2 ... 0 2 2 1] . You’ve successfully performed k-Means clustering and predicted the labels of new points. But it is not easy to inspect the clustering by just looking at the printed labels. A visualization would be far more useful. In the next exercise, you’ll inspect your clustering with a scatter plot! . #### Inspect your clustering . # Import pyplot import matplotlib.pyplot as plt # Assign the columns of new_points: xs and ys xs = new_points[:,0] ys = new_points[:,1] # Make a scatter plot of xs and ys, using labels to define the colors plt.scatter(xs, ys, c=labels, alpha=0.5) # Assign the cluster centers: centroids centroids = model.cluster_centers_ # Assign the columns of centroids: centroids_x, centroids_y centroids_x = centroids[:,0] centroids_y = centroids[:,1] # Make a scatter plot of centroids_x and centroids_y plt.scatter(centroids_x, centroids_y, marker=&#39;D&#39;, s=50) plt.show() . . The clustering looks great! But how can you be sure that 3 clusters is the correct choice? In other words, how can you evaluate the quality of a clustering? . . 1.2 Evaluating a clustering . #### How many clusters of grain? . samples array([[15.26 , 14.84 , 0.871 , ..., 3.312 , 2.221 , 5.22 ], ..., [12.3 , 13.34 , 0.8684, ..., 2.974 , 5.637 , 5.063 ]]) . ks = range(1, 6) inertias = [] for k in ks: # Create a KMeans instance with k clusters: model model = KMeans(n_clusters=k) # Fit model to samples model.fit(samples) # Append the inertia to the list of inertias inertias.append(model.inertia_) # Plot ks vs inertias plt.plot(ks, inertias, &#39;-o&#39;) plt.xlabel(&#39;number of clusters, k&#39;) plt.ylabel(&#39;inertia&#39;) plt.xticks(ks) plt.show() . . The inertia decreases very slowly from 3 clusters to 4, so it looks like 3 clusters would be a good choice for this data. . #### Evaluating the grain clustering . In the previous exercise, you observed from the inertia plot that 3 is a good number of clusters for the grain data. In fact, the grain samples come from a mix of 3 different grain varieties: “Kama”, “Rosa” and “Canadian”. In this exercise, cluster the grain samples into three clusters, and compare the clusters to the grain varieties using a cross-tabulation. . You have the array samples of grain samples, and a list varieties giving the grain variety for each sample. . varieties [&#39;Kama wheat&#39;, &#39;Kama wheat&#39;, ... &#39;Canadian wheat&#39;, &#39;Canadian wheat&#39;] . # Create a KMeans model with 3 clusters: model model = KMeans(n_clusters=3) # Use fit_predict to fit model and obtain cluster labels: labels labels = model.fit_predict(samples) # Create a DataFrame with labels and varieties as columns: df df = pd.DataFrame({&#39;labels&#39;: labels, &#39;varieties&#39;: varieties}) # Create crosstab: ct ct = pd.crosstab(df[&#39;labels&#39;], df[&#39;varieties&#39;]) # Display ct print(ct) . varieties Canadian wheat Kama wheat Rosa wheat labels 0 0 1 60 1 68 9 0 2 2 60 10 . The cross-tabulation shows that the 3 varieties of grain separate really well into 3 clusters. But depending on the type of data you are working with, the clustering may not always be this good. Is there anything you can do in such situations to improve your clustering? . . 1.3 Transforming features for better clusterings . #### Scaling fish data for clustering . You are given an array samples giving measurements of fish. Each row represents an individual fish. The measurements, such as weight in grams, length in centimeters, and the percentage ratio of height to length, have very different scales. In order to cluster this data effectively, you’ll need to standardize these features first. In this exercise, you’ll build a pipeline to standardize and cluster the data. . These fish measurement data were sourced from the Journal of Statistics Education . . # Perform the necessary imports from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans # Create scaler: scaler scaler = StandardScaler() # Create KMeans instance: kmeans kmeans = KMeans(n_clusters=4) # Create pipeline: pipeline pipeline = make_pipeline(scaler, kmeans) . #### Clustering the fish data . samples array([[ 242. , 23.2, 25.4, 30. , 38.4, 13.4], [ 290. , 24. , 26.3, 31.2, 40. , 13.8], [ 340. , 23.9, 26.5, 31.1, 39.8, 15.1], ... . # Import pandas import pandas as pd # Fit the pipeline to samples pipeline.fit(samples) # Calculate the cluster labels: labels labels = pipeline.predict(samples) # Create a DataFrame with labels and species as columns: df df = pd.DataFrame({&#39;labels&#39;:labels, &#39;species&#39;:species}) # Create crosstab: ct ct = pd.crosstab(df[&#39;labels&#39;], df[&#39;species&#39;]) # Display ct print(ct) . species Bream Pike Roach Smelt labels 0 0 0 0 13 1 33 0 1 0 2 0 17 0 0 3 1 0 19 1 . It looks like the fish data separates really well into 4 clusters! . #### Clustering stocks using KMeans . In this exercise, you’ll cluster companies using their daily stock price movements (i.e. the dollar difference between the closing and opening prices for each trading day). You are given a NumPy array movements of daily price movements from 2010 to 2015 (obtained from Yahoo! Finance), where each row corresponds to a company, and each column corresponds to a trading day. . Some stocks are more expensive than others. To account for this, include a Normalizer at the beginning of your pipeline. The Normalizer will separately transform each company’s stock price to a relative scale before the clustering begins. . Note that Normalizer() is different to StandardScaler() , which you used in the previous exercise. While StandardScaler() standardizes features (such as the features of the fish data from the previous exercise) by removing the mean and scaling to unit variance, Normalizer() rescales each sample – here, each company’s stock price – independently of the other. . movements array([[ 5.8000000e-01, -2.2000500e-01, -3.4099980e+00, ..., -5.3599620e+00, 8.4001900e-01, -1.9589981e+01], ..., [ 1.5999900e-01, 1.0001000e-02, 0.0000000e+00, ..., -6.0001000e-02, 2.5999800e-01, 9.9998000e-02]]) . # Import Normalizer from sklearn.preprocessing import Normalizer # Create a normalizer: normalizer normalizer = Normalizer() # Create a KMeans model with 10 clusters: kmeans kmeans = KMeans(n_clusters=10) # Make a pipeline chaining normalizer and kmeans: pipeline pipeline = make_pipeline(normalizer, kmeans) # Fit pipeline to the daily price movements pipeline.fit(movements) . Now that your pipeline has been set up, you can find out which stocks move together in the next exercise! . #### Which stocks move together? . In the previous exercise, you clustered companies by their daily stock price movements. So which company have stock prices that tend to change in the same way? You’ll now inspect the cluster labels from your clustering to find out. . # Import pandas import pandas as pd # Predict the cluster labels: labels labels = pipeline.predict(movements) # Create a DataFrame aligning labels and companies: df df = pd.DataFrame({&#39;labels&#39;: labels, &#39;companies&#39;: companies}) # Display df sorted by cluster label print(df.sort_values(&#39;labels&#39;)) . companies labels 59 Yahoo 0 15 Ford 0 35 Navistar 0 26 JPMorgan Chase 1 16 General Electrics 1 58 Xerox 1 11 Cisco 1 18 Goldman Sachs 1 20 Home Depot 1 5 Bank of America 1 3 American express 1 55 Wells Fargo 1 1 AIG 1 38 Pepsi 2 ... . In the next chapter, you’ll learn about how to communicate results such as this through visualizations. . . 2. Visualization with hierarchical clustering and t-SNE . . ### Visualizing hierarchies . #### Hierarchical clustering of the grain data . Use the linkage() function to obtain a hierarchical clustering of the grain samples, and use dendrogram() to visualize the result. . # Perform the necessary imports from scipy.cluster.hierarchy import linkage, dendrogram import matplotlib.pyplot as plt # Calculate the linkage: mergings mergings = linkage(samples, method=&#39;complete&#39;) # Plot the dendrogram, using varieties as labels dendrogram(mergings, labels=varieties, leaf_rotation=90, leaf_font_size=6, ) plt.show() . . Dendrograms are a great way to illustrate the arrangement of the clusters produced by hierarchical clustering. . #### Hierarchies of stocks . # Import normalize from sklearn.preprocessing import normalize # Normalize the movements: normalized_movements normalized_movements = normalize(movements) # Calculate the linkage: mergings mergings = linkage(normalized_movements, method=&#39;complete&#39;) # Plot the dendrogram dendrogram(mergings, labels=companies, leaf_rotation=90, leaf_font_size=6) plt.show() . . . ### Cluster labels in hierarchical clustering . #### Different linkage, different hierarchical clustering! . Perform a hierarchical clustering of the voting countries with &#39;single&#39; linkage, and compare the resulting dendrogram with the one in the video. Different linkage, different hierarchical clustering! . # Perform the necessary imports import matplotlib.pyplot as plt from scipy.cluster.hierarchy import linkage, dendrogram # Calculate the linkage: mergings mergings = linkage(samples, method=&#39;single&#39;) # Plot the dendrogram dendrogram(mergings, labels=country_names, leaf_rotation=90, leaf_font_size=6) plt.show() . . As you can see, performing single linkage hierarchical clustering produces a different dendrogram! . ### Extracting the cluster labels . Use the fcluster() function to extract the cluster labels for this intermediate clustering, and compare the labels with the grain varieties using a cross-tabulation. . # Perform the necessary imports import pandas as pd from scipy.cluster.hierarchy import fcluster # Use fcluster to extract labels: labels labels = fcluster(mergings, 6, criterion=&#39;distance&#39;) # Create a DataFrame with labels and varieties as columns: df df = pd.DataFrame({&#39;labels&#39;: labels, &#39;varieties&#39;: varieties}) # Create crosstab: ct ct = pd.crosstab(df[&#39;labels&#39;], df[&#39;varieties&#39;]) # Display ct print(ct) . varieties Canadian wheat Kama wheat Rosa wheat labels 1 14 3 0 2 0 0 14 3 0 11 0 . You’ve now mastered the fundamentals of k-Means and agglomerative hierarchical clustering. Next, you’ll learn about t-SNE, which is a powerful tool for visualizing high dimensional data. . . ### t-SNE for 2-dimensional maps . #### t-SNE visualization of grain dataset . t-distributed stochastic neighbor embedding . In this exercise, you’ll apply t-SNE to the grain samples data and inspect the resulting t-SNE features using a scatter plot. . samples[:3] array([[15.26 , 14.84 , 0.871 , 5.763 , 3.312 , 2.221 , 5.22 ], [14.88 , 14.57 , 0.8811, 5.554 , 3.333 , 1.018 , 4.956 ], [14.29 , 14.09 , 0.905 , 5.291 , 3.337 , 2.699 , 4.825 ]]) variety_numbers[:3] [1, 1, 1] . # Import TSNE from sklearn.manifold import TSNE # Create a TSNE instance: model model = TSNE(learning_rate=200) # Apply fit_transform to samples: tsne_features tsne_features = model.fit_transform(samples) # Select the 0th feature: xs xs = tsne_features[:,0] # Select the 1st feature: ys ys = tsne_features[:,1] # Scatter plot, coloring by variety_numbers plt.scatter(xs, ys, c=variety_numbers) plt.show() . . As you can see, the t-SNE visualization manages to separate the 3 varieties of grain samples. . #### A t-SNE map of the stock market . t-SNE provides great visualizations when the individual samples can be labeled. In this exercise, you’ll apply t-SNE to the company stock price data. A scatter plot of the resulting t-SNE features, labeled by the company names, gives you a map of the stock market! . # Import TSNE from sklearn.manifold import TSNE # Create a TSNE instance: model model = TSNE(learning_rate=50) # Apply fit_transform to normalized_movements: tsne_features tsne_features = model.fit_transform(normalized_movements) # Select the 0th feature: xs xs = tsne_features[:,0] # Select the 1th feature: ys ys = tsne_features[:,1] # Scatter plot plt.scatter(xs, ys, alpha=0.5) # Annotate the points for x, y, company in zip(xs, ys, companies): plt.annotate(company, (x, y), fontsize=5, alpha=0.75) plt.show() . . It’s visualizations such as this that make t-SNE such a powerful tool for extracting quick insights from high dimensional data. . . 3. Decorrelating your data and dimension reduction . . ### Visualizing the PCA( principal component analysis ) transformation . #### Correlated data in nature . You are given an array grains giving the width and length of samples of grain. You suspect that width and length will be correlated. To confirm this, make a scatter plot of width vs length and measure their Pearson correlation. . # Perform the necessary imports import matplotlib.pyplot as plt from scipy.stats import pearsonr # Assign the 0th column of grains: width width = grains[:,0] # Assign the 1st column of grains: length length = grains[:,1] # Scatter plot width vs length plt.scatter(width, length) plt.axis(&#39;equal&#39;) plt.xlabel(&#39;grains width&#39;) plt.ylabel(&#39;grains length&#39;) plt.show() # Calculate the Pearson correlation correlation, pvalue = pearsonr(width, length) # Display the correlation print(correlation) # 0.8604149377143467 . . The width and length of the grain samples are highly correlated. . #### Decorrelating the grain measurements with PCA . You’ll use PCA to decorrelate these measurements, then plot the decorrelated points and measure their Pearson correlation. . # Import PCA from sklearn.decomposition import PCA # Create PCA instance: model model = PCA() # Apply the fit_transform method of model to grains: pca_features pca_features = model.fit_transform(grains) # Assign 0th column of pca_features: xs xs = pca_features[:,0] # Assign 1st column of pca_features: ys ys = pca_features[:,1] # Scatter plot xs vs ys plt.scatter(xs, ys) plt.axis(&#39;equal&#39;) plt.xlabel(&#39;pca_features 0&#39;) plt.ylabel(&#39;pca_features 1&#39;) plt.show() # Calculate the Pearson correlation of xs and ys correlation, pvalue = pearsonr(xs, ys) # Display the correlation print(correlation) . . ### Intrinsic dimension . #### The first principal component . The first principal component of the data is the direction in which the data varies the most. In this exercise, your job is to use PCA to find the first principal component of the length and width measurements of the grain samples, and represent it as an arrow on the scatter plot. . # Make a scatter plot of the untransformed points plt.scatter(grains[:,0], grains[:,1]) # Add labels plt.xlabel(&#39;width&#39;) plt.ylabel(&#39;length&#39;) # Create a PCA instance: model model = PCA() # Fit model to points model.fit(grains) # Get the mean of the grain samples: mean mean = model.mean_ # Get the first principal component: first_pc first_pc = model.components_[0,:] # Plot first_pc as an arrow, starting at mean plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color=&#39;red&#39;, width=0.01) # Keep axes on same scale plt.axis(&#39;equal&#39;) plt.show() . . This is the direction in which the grain data varies the most. . #### Variance of the PCA features . The fish dataset is 6-dimensional. But what is its intrinsic dimension? Make a plot of the variances of the PCA features to find out. As before, samples is a 2D array, where each row represents a fish. You’ll need to standardize the features first. . samples[:3] array([[242. , 23.2, 25.4, 30. , 38.4, 13.4], [290. , 24. , 26.3, 31.2, 40. , 13.8], [340. , 23.9, 26.5, 31.1, 39.8, 15.1]]) . # Perform the necessary imports from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline import matplotlib.pyplot as plt # Create scaler: scaler scaler = StandardScaler() # Create a PCA instance: pca pca = PCA() # Create pipeline: pipeline pipeline = make_pipeline(scaler, pca) # Fit the pipeline to &#39;samples&#39; pipeline.fit(samples) # Plot the explained variances features = range(pca.n_components_) plt.bar(features, pca.explained_variance_) plt.xlabel(&#39;PCA feature&#39;) plt.ylabel(&#39;variance&#39;) plt.xticks(features) plt.show() . . It looks like PCA features 0 and 1 have significant variance. . . ### Dimension reduction with PCA . Use PCA for dimensionality reduction of the fish measurements, retaining only the 2 most important components. . scaled_samples[:3] array([[-0.50109735, -0.36878558, -0.34323399, -0.23781518, 1.0032125 , 0.25373964], [-0.37434344, -0.29750241, -0.26893461, -0.14634781, 1.15869615, 0.44376493], [-0.24230812, -0.30641281, -0.25242364, -0.15397009, 1.13926069, 1.0613471 ]]) . # Import PCA from sklearn.decomposition import PCA # Create a PCA model with 2 components: pca pca = PCA(n_components=2) # Fit the PCA instance to the scaled samples pca.fit(scaled_samples) # Transform the scaled samples: pca_features pca_features = pca.transform(scaled_samples) # Print the shape of pca_features print(pca_features.shape) (85, 2) . pca_features[:3] array([[-0.57640502, -0.94649159], [-0.36852393, -1.17103598], [-0.28028168, -1.59709224]]) . You’ve successfully reduced the dimensionality from 6 to 2. . #### A tf-idf word-frequency array . In this exercise, you’ll create a tf-idf word frequency array for a toy collection of documents. For this, use the TfidfVectorizer from sklearn. It transforms a list of documents into a word frequency array, which it outputs as a csr_matrix. It has fit() and transform() methods like other sklearn objects. . term frequency–inverse document frequency . documents [&#39;cats say meow&#39;, &#39;dogs say woof&#39;, &#39;dogs chase cats&#39;] . # Import TfidfVectorizer from sklearn.feature_extraction.text import TfidfVectorizer # Create a TfidfVectorizer: tfidf tfidf = TfidfVectorizer() # Apply fit_transform to document: csr_mat csr_mat = tfidf.fit_transform(documents) # Print result of toarray() method print(csr_mat.toarray()) # Get the words: words words = tfidf.get_feature_names() # Print words print(words) . [[0.51785612 0. 0. 0.68091856 0.51785612 0. ] [0. 0. 0.51785612 0. 0.51785612 0.68091856] [0.51785612 0.68091856 0.51785612 0. 0. 0. ]] [&#39;cats&#39;, &#39;chase&#39;, &#39;dogs&#39;, &#39;meow&#39;, &#39;say&#39;, &#39;woof&#39;] . . #### Clustering Wikipedia part I . You saw in the video that TruncatedSVD is able to perform PCA on sparse arrays in csr_matrix format, such as word-frequency arrays. Combine your knowledge of TruncatedSVD and k-means to cluster some popular pages from Wikipedia. In this exercise, build the pipeline. In the next exercise, you’ll apply it to the word-frequency array of some Wikipedia articles. . Create a Pipeline object consisting of a TruncatedSVD followed by KMeans. (This time, we’ve precomputed the word-frequency matrix for you, so there’s no need for a TfidfVectorizer). . The Wikipedia dataset you will be working with was obtained from here . . # Perform the necessary imports from sklearn.decomposition import TruncatedSVD from sklearn.cluster import KMeans from sklearn.pipeline import make_pipeline # Create a TruncatedSVD instance: svd svd = TruncatedSVD(n_components=50) # Create a KMeans instance: kmeans kmeans = KMeans(n_clusters=6) # Create a pipeline: pipeline pipeline = make_pipeline(svd, kmeans) . #### Clustering Wikipedia part II . You are given an array articles of tf-idf word-frequencies of some popular Wikipedia articles, and a list titles of their titles. Use your pipeline to cluster the Wikipedia articles. . type(articles) scipy.sparse.csr.csr_matrix articles &lt;60x13125 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39; with 42091 stored elements in Compressed Sparse Row format&gt; titles[:3] [&#39;HTTP 404&#39;, &#39;Alexa Internet&#39;, &#39;Internet Explorer&#39;] df.shape (13125, 60) df.head(1) HTTP 404 Alexa Internet Internet Explorer HTTP cookie Google Search 0 0.0 0.0 0.0 0.0 0.0 Tumblr Hypertext Transfer Protocol Social search Firefox LinkedIn 0 0.0 0.0 0.0 0.0 0.0 ... Chad Kroeger Nate Ruess The Wanted Stevie Nicks 0 ... 0.0 0.0 0.0 0.008878 Arctic Monkeys Black Sabbath Skrillex Red Hot Chili Peppers Sepsis 0 0.0 0.0 0.049502 0.0 0.0 Adam Levine 0 0.0 [1 rows x 60 columns] . # Import pandas import pandas as pd # Fit the pipeline to articles pipeline.fit(articles) # Calculate the cluster labels: labels labels = pipeline.predict(articles) # Create a DataFrame aligning labels and titles: df df = pd.DataFrame({&#39;label&#39;: labels, &#39;article&#39;: titles}) # Display df sorted by cluster label print(df.sort_values(&#39;label&#39;)) . article label 59 Adam Levine 0 57 Red Hot Chili Peppers 0 56 Skrillex 0 55 Black Sabbath 0 54 Arctic Monkeys 0 53 Stevie Nicks 0 52 The Wanted 0 51 Nate Ruess 0 50 Chad Kroeger 0 58 Sepsis 0 30 France national football team 1 31 Cristiano Ronaldo 1 32 Arsenal F.C. 1 33 Radamel Falcao 1 37 Football 1 35 Colombia national football team 1 36 2014 FIFA World Cup qualification 1 38 Neymar 1 39 Franck Ribéry 1 34 Zlatan Ibrahimović 1 26 Mila Kunis 2 28 Anne Hathaway 2 27 Dakota Fanning 2 25 Russell Crowe 2 29 Jennifer Aniston 2 23 Catherine Zeta-Jones 2 22 Denzel Washington 2 21 Michael Fassbender 2 20 Angelina Jolie 2 24 Jessica Biel 2 . . 4. Discovering interpretable features . . ### Non-negative matrix factorization (NMF) . #### NMF applied to Wikipedia articles . articles &lt;60x13125 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39; with 42091 stored elements in Compressed Sparse Row format&gt; print(articles) (0, 16) 0.024688249778400003 (0, 32) 0.0239370711117 (0, 33) 0.0210896267411 (0, 137) 0.012295430569100001 . # Import NMF from sklearn.decomposition import NMF # Create an NMF instance: model model = NMF(n_components=6) # Fit the model to articles model.fit(articles) # Transform the articles: nmf_features nmf_features = model.transform(articles) # Print the NMF features print(nmf_features) . [[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 4.40531625e-01] [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.66689786e-01] ... [3.78224082e-01 1.43978958e-02 0.00000000e+00 9.84935436e-02 1.35904928e-02 0.00000000e+00]] . these NMF features don’t make much sense at this point, but you will explore them in the next exercise! . #### NMF features of the Wikipedia articles . When investigating the features, notice that for both actors, the NMF feature 3 has by far the highest value. This means that both articles are reconstructed using mainly the 3rd NMF component. . # Import pandas import pandas as pd # Create a pandas DataFrame: df df = pd.DataFrame(nmf_features, index=titles) # Print the row for &#39;Anne Hathaway&#39; print(df.loc[&#39;Anne Hathaway&#39;]) # Print the row for &#39;Denzel Washington&#39; print(df.loc[&#39;Denzel Washington&#39;]) . 0 0.003845 1 0.000000 2 0.000000 3 0.575711 4 0.000000 5 0.000000 Name: Anne Hathaway, dtype: float64 0 0.000000 1 0.005601 2 0.000000 3 0.422380 4 0.000000 5 0.000000 Name: Denzel Washington, dtype: float64 df.head() 0 1 2 3 4 5 HTTP 404 0.000000 0.0 0.0 0.0 0.0 0.440465 Alexa Internet 0.000000 0.0 0.0 0.0 0.0 0.566605 Internet Explorer 0.003821 0.0 0.0 0.0 0.0 0.398646 HTTP cookie 0.000000 0.0 0.0 0.0 0.0 0.381740 Google Search 0.000000 0.0 0.0 0.0 0.0 0.485517 . Notice that for both actors, the NMF feature 3 has by far the highest value. This means that both articles are reconstructed using mainly the 3rd NMF component. Because NMF components represent topics (for instance, acting!). . ### NMF learns interpretable parts . #### NMF learns topics of documents . When NMF is applied to documents, the components correspond to topics of documents, and the NMF features reconstruct the documents from the topics. . Verify this for yourself for the NMF model that you built earlier using the Wikipedia articles. Previously, you saw that the 3rd NMF feature value was high for the articles about actors Anne Hathaway and Denzel Washington. . In this exercise, identify the topic of the corresponding NMF component. . model.components_ array([[1.13754523e-02, 1.20974422e-03, 0.00000000e+00, ..., 0.00000000e+00, 4.23594130e-04, 0.00000000e+00], [0.00000000e+00, 9.57177268e-06, 5.66343849e-03, ..., 2.81289906e-03, 2.97179984e-04, 0.00000000e+00], [0.00000000e+00, 8.30814049e-06, 0.00000000e+00, ..., 0.00000000e+00, 1.43192324e-04, 0.00000000e+00], [4.14811200e-03, 0.00000000e+00, 3.05595648e-03, ..., 1.74191620e-03, 6.71969911e-03, 0.00000000e+00], [0.00000000e+00, 5.68399302e-04, 4.91797182e-03, ..., 1.91632504e-04, 1.35146218e-03, 0.00000000e+00], [1.38501597e-04, 0.00000000e+00, 8.74840829e-03, ..., 2.40081634e-03, 1.68211026e-03, 0.00000000e+00]]) words[:3] [&#39;aaron&#39;, &#39;abandon&#39;, &#39;abandoned&#39;] . # Import pandas import pandas as pd # Create a DataFrame: components_df components_df = pd.DataFrame(model.components_, columns=words) # Print the shape of the DataFrame print(components_df.shape) # (6, 13125) # Select row 3: component component = components_df.iloc[3,:] # Print result of nlargest print(component.nlargest()) film 0.627877 award 0.253131 starred 0.245284 role 0.211451 actress 0.186398 Name: 3, dtype: float64 . components_df aaron abandon abandoned abandoning abandonment abbas abbey 0 0.011375 0.001210 0.000000 0.001739 0.000136 0.0 0.0 1 0.000000 0.000010 0.005663 0.000000 0.000002 0.0 0.0 2 0.000000 0.000008 0.000000 0.000000 0.004692 0.0 0.0 3 0.004148 0.000000 0.003056 0.000000 0.000614 0.0 0.0 4 0.000000 0.000568 0.004918 0.000000 0.000000 0.0 0.0 5 0.000139 0.000000 0.008748 0.000000 0.000185 0.0 0.0 abbreviated abbreviation abc ... zealand zenith zeppelin 0 0.002463 2.445684e-07 0.000834 ... 0.025781 0.0 0.008324 1 0.000566 5.002620e-04 0.000000 ... 0.008106 0.0 0.000000 2 0.000758 1.604283e-05 0.000000 ... 0.008730 0.0 0.000000 3 0.002436 8.143270e-05 0.003985 ... 0.012594 0.0 0.000000 4 0.000089 4.259695e-05 0.000000 ... 0.001809 0.0 0.000000 5 0.008629 1.530385e-05 0.000000 ... 0.000000 0.0 0.000000 zero zeus zimbabwe zinc zone zones zoo 0 0.000000 0.0 0.0 0.0 0.000000 0.000424 0.0 1 0.001710 0.0 0.0 0.0 0.002813 0.000297 0.0 2 0.001317 0.0 0.0 0.0 0.000000 0.000143 0.0 3 0.000000 0.0 0.0 0.0 0.001742 0.006720 0.0 4 0.000017 0.0 0.0 0.0 0.000192 0.001351 0.0 5 0.000000 0.0 0.0 0.0 0.002401 0.001682 0.0 [6 rows x 13125 columns] . . #### Explore the LED digits dataset . In the following exercises, you’ll use NMF to decompose grayscale images into their commonly occurring patterns. . samples array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]]) . # Import pyplot from matplotlib import pyplot as plt # Select the 0th row: digit digit = samples[0,:] # Print digit print(digit) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] # Reshape digit to a 13x8 array: bitmap bitmap = digit.reshape(13,8) # Print bitmap print(bitmap) [[0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 1. 1. 1. 1. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0.]] # Use plt.imshow to display bitmap plt.imshow(bitmap, cmap=&#39;gray&#39;, interpolation=&#39;nearest&#39;) plt.colorbar() plt.show() . . You’ll explore this dataset further in the next exercise and see for yourself how NMF can learn the parts of images. . #### NMF learns the parts of images . Now use what you’ve learned about NMF to decompose the digits dataset. You are again given the digit images as a 2D array samples . This time, you are also provided with a function show_as_image() that displays the image encoded by any 1D array: . def show_as_image(sample): bitmap = sample.reshape((13, 8)) plt.figure() plt.imshow(bitmap, cmap=&#39;gray&#39;, interpolation=&#39;nearest&#39;) plt.colorbar() plt.show() . After you are done, take a moment to look through the plots and notice how NMF has expressed the digit as a sum of the components! . # Import NMF from sklearn.decomposition import NMF # Create an NMF model: model model = NMF(n_components=7) # Apply fit_transform to samples: features features = model.fit_transform(samples) # Call show_as_image on each component for component in model.components_: show_as_image(component) # Assign the 0th row of features: digit_features digit_features = features[0,:] # Print digit_features print(digit_features) . features Out[3]: array([[4.76823559e-01, 0.00000000e+00, 0.00000000e+00, 5.90605054e-01, 4.81559442e-01, 0.00000000e+00, 7.37557191e-16], ... [0.00000000e+00, 0.00000000e+00, 5.21027460e-01, 0.00000000e+00, 4.81559442e-01, 4.93832117e-01, 0.00000000e+00]]) . . . #### PCA doesn’t learn parts . Unlike NMF, PCA doesn’t learn the parts of things. Its components do not correspond to topics (in the case of documents) or to parts of images, when trained on images. Verify this for yourself by inspecting the components of a PCA model fit to the dataset of LED digit images from the previous exercise. . # Import PCA from sklearn.decomposition import PCA # Create a PCA instance: model model = PCA(n_components=7) # Apply fit_transform to samples: features features = model.fit_transform(samples) # Call show_as_image on each component for component in model.components_: show_as_image(component) . . Notice that the components of PCA do not represent meaningful parts of images of LED digits! . ### Building recommender systems using NMF . #### Which articles are similar to ‘Cristiano Ronaldo’? . You learned how to use NMF features and the cosine similarity to find similar articles. Apply this to your NMF model for popular Wikipedia articles, by finding the articles most similar to the article about the footballer Cristiano Ronaldo. . # Perform the necessary imports import pandas as pd from sklearn.preprocessing import normalize # Normalize the NMF features: norm_features norm_features = normalize(nmf_features) # Create a DataFrame: df df = pd.DataFrame(norm_features, index=titles) # Select the row corresponding to &#39;Cristiano Ronaldo&#39;: article article = df.loc[&#39;Cristiano Ronaldo&#39;,:] # Compute the dot products: similarities similarities = df.dot(article) # Display those with the largest cosine similarity print(similarities.nlargest()) . Cristiano Ronaldo 1.000000 Franck Ribéry 0.999972 Radamel Falcao 0.999942 Zlatan Ibrahimović 0.999942 France national football team 0.999923 dtype: float64 df.head() 0 1 2 3 4 5 HTTP 404 0.000000 0.0 0.0 0.0 0.0 1.000000 Alexa Internet 0.000000 0.0 0.0 0.0 0.0 1.000000 Internet Explorer 0.009583 0.0 0.0 0.0 0.0 0.999954 HTTP cookie 0.000000 0.0 0.0 0.0 0.0 1.000000 Google Search 0.000000 0.0 0.0 0.0 0.0 1.000000 article 0 0.002523 1 0.999942 2 0.000859 3 0.010274 4 0.001947 5 0.000724 Name: Cristiano Ronaldo, dtype: float64 similarities Out[15]: HTTP 404 0.000724 Alexa Internet 0.000724 Internet Explorer 0.000748 ... France national football team 0.999923 Cristiano Ronaldo 1.000000 Arsenal F.C. 0.997739 Radamel Falcao 0.999942 Zlatan Ibrahimović 0.999942 Colombia national football team 0.999897 2014 FIFA World Cup qualification 0.998443 Football 0.974915 Neymar 0.999021 Franck Ribéry 0.999972 ... Sepsis 0.041880 Adam Levine 0.041873 dtype: float64 . Although you may need to know a little about football (or soccer, depending on where you’re from!) to be able to evaluate for yourself the quality of the computed similarities! . . #### Recommend musical artists part I . In this exercise and the next, you’ll use what you’ve learned about NMF to recommend popular music artists! You are given a sparse array artists whose rows correspond to artists and whose column correspond to users. The entries give the number of times each artist was listened to by each user. . In this exercise, build a pipeline and transform the array into normalized NMF features. The first step in the pipeline, MaxAbsScaler , transforms the data so that all users have the same influence on the model, regardless of how many different artists they’ve listened to. In the next exercise, you’ll use the resulting normalized NMF features for recommendation! . artists &lt;111x500 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39; with 2894 stored elements in Compressed Sparse Row format&gt; print(artists) (0, 2) 105.0 (0, 15) 165.0 (0, 20) 91.0 . # Perform the necessary imports from sklearn.decomposition import NMF from sklearn.preprocessing import Normalizer, MaxAbsScaler from sklearn.pipeline import make_pipeline # Create a MaxAbsScaler: scaler scaler = MaxAbsScaler() # Create an NMF model: nmf nmf = NMF(n_components=20) # Create a Normalizer: normalizer normalizer = Normalizer() # Create a pipeline: pipeline pipeline = make_pipeline(scaler, nmf, normalizer) # Apply fit_transform to artists: norm_features norm_features = pipeline.fit_transform(artists) . #### Recommend musical artists part II . Suppose you were a big fan of Bruce Springsteen – which other musicial artists might you like? Use your NMF features from the previous exercise and the cosine similarity to find similar musical artists. . # Import pandas import pandas as pd # Create a DataFrame: df df = pd.DataFrame(norm_features, index=artist_names) # Select row of &#39;Bruce Springsteen&#39;: artist artist = df.loc[&#39;Bruce Springsteen&#39;] # Compute cosine similarities: similarities similarities = df.dot(artist) # Display those with highest cosine similarity print(similarities.nlargest()) . Bruce Springsteen 1.000000 Neil Young 0.955896 Van Morrison 0.872452 Leonard Cohen 0.864763 Bob Dylan 0.859047 dtype: float64 . The End. . Thank you for reading. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/unsupervised-learning-in-python.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/unsupervised-learning-in-python.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post36": {
            "title": "Supervised Learning with scikit-learn",
            "content": "Supervised Learning with scikit-learn . This is the memo of the 21th course of ‘Data Scientist with Python’ track. . You can find the original course HERE . . . Classification . | ### Machine learning introduction . What is machine learning? Giving computers the ability to learn to make decisions from data without being explicitly programmed . Examples of machine learning: Learning to predict whether an email is spam or not (supervised) . Clustering Wikipedia entries into different categories (unsupervised) . #### Types of Machine Learning . supervised learning | unsupervised learning | reinforcement learning | . Supervised learning: . Predictor variables/ features and a target variable . Aim: Predict the target variable, given the predictor variables . Classification: Target variable consists of categories . Regression: Target variable is continuous . Unsupervised learning: . Uncovering hidden patterns from unlabeled data . Example of unsupervised learning: . Grouping customers into distinct categories (Clustering) . Reinforcement learning: Software agents interact with an environment . Learn how to optimize their behavior . Given a system of rewards and punishments . Applications . Economics . Genetics . Game playing (AlphaGo) . #### Naming conventions . Features = predictor variables = independent variables | Target variable = dependent variable = response variable | . . #### Features of Supervised learning . Automate time-consuming or expensive manual tasks (ex. Doctor’s diagnosis) | Make predictions about the future (ex. Will a customer click on an ad or not) | Need labeled data (Historical data with labels etc.) | . #### Popular libraries . scikit-learning (basic) | TensorFlow | keras | . . ### Exploratory data analysis . #### Numerical EDA . In this chapter, you’ll be working with a dataset obtained from the UCI Machine Learning Repository consisting of votes made by US House of Representatives Congressmen. . Your goal will be to predict their party affiliation (‘Democrat’ or ‘Republican’) based on how they voted on certain key issues. . Here, it’s worth noting that we have preprocessed this dataset to deal with missing values. This is so that your focus can be directed towards understanding how to train and evaluate supervised learning models. . Once you have mastered these fundamentals, you will be introduced to preprocessing techniques in Chapter 4 and have the chance to apply them there yourself – including on this very same dataset! . Before thinking about what supervised learning models you can apply to this, however, you need to perform Exploratory data analysis (EDA) in order to understand the structure of the data. . df.head() party infants water budget physician salvador religious 0 republican 0 1 0 1 1 1 1 republican 0 1 0 1 1 1 2 democrat 0 1 1 0 1 1 3 democrat 0 1 1 0 1 1 4 democrat 1 1 1 0 1 1 satellite aid missile immigration synfuels education superfund 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 1 2 0 0 0 0 1 0 1 3 0 0 0 0 1 0 1 4 0 0 0 0 1 0 1 crime duty_free_exports eaa_rsa 0 1 0 1 1 1 0 1 2 1 0 0 3 0 0 1 4 1 1 1 . df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 435 entries, 0 to 434 Data columns (total 17 columns): party 435 non-null object infants 435 non-null int64 water 435 non-null int64 budget 435 non-null int64 physician 435 non-null int64 salvador 435 non-null int64 religious 435 non-null int64 satellite 435 non-null int64 aid 435 non-null int64 missile 435 non-null int64 immigration 435 non-null int64 synfuels 435 non-null int64 education 435 non-null int64 superfund 435 non-null int64 crime 435 non-null int64 duty_free_exports 435 non-null int64 eaa_rsa 435 non-null int64 dtypes: int64(16), object(1) memory usage: 57.9+ KB . ### Visual EDA . . Above is a countplot of the &#39;education&#39; bill, generated from the following code: . plt.figure() sns.countplot(x=&#39;education&#39;, hue=&#39;party&#39;, data=df, palette=&#39;RdBu&#39;) plt.xticks([0,1], [&#39;No&#39;, &#39;Yes&#39;]) plt.show() . In sns.countplot() , we specify the x-axis data to be &#39;education&#39; , and hue to be &#39;party&#39; . Recall that &#39;party&#39; is also our target variable. So the resulting plot shows the difference in voting behavior between the two parties for the &#39;education&#39; bill, with each party colored differently. We manually specified the color to be &#39;RdBu&#39; , as the Republican party has been traditionally associated with red, and the Democratic party with blue. . It seems like Democrats voted resoundingly against this bill, compared to Republicans. . plt.figure() sns.countplot(x=&#39;missile&#39;, hue=&#39;party&#39;, data=df, palette=&#39;RdBu&#39;) plt.xticks([0,1], [&#39;No&#39;, &#39;Yes&#39;]) plt.show() . . Democrats vote resoundingly in favor of missile, compared to Republicans. . . ### The classification challenge . #### k-Nearest Neighbors: Fit . k-nearest neighbors algorithm . # Import KNeighborsClassifier from sklearn.neighbors from sklearn.neighbors import KNeighborsClassifier # Create arrays for the features and the response variable y = df[&#39;party&#39;].values X = df.drop(&#39;party&#39;, axis=1).values # Create a k-NN classifier with 6 neighbors knn = KNeighborsClassifier(n_neighbors=6) # Fit the classifier to the data knn.fit(X, y) . #### k-Nearest Neighbors: Predict . # Import KNeighborsClassifier from sklearn.neighbors from sklearn.neighbors import KNeighborsClassifier # Create arrays for the features and the response variable y = df[&#39;party&#39;] X = df.drop(&#39;party&#39;, axis=1).values # Create a k-NN classifier with 6 neighbors: knn knn = KNeighborsClassifier(n_neighbors=6) # Fit the classifier to the data knn.fit(X, y) # Predict the labels for the training data X y_pred = knn.predict(X) # Predict and print the label for the new data point X_new new_prediction = knn.predict(X_new) print(&quot;Prediction: {}&quot;.format(new_prediction)) # Prediction: [&#39;democrat&#39;] . How sure can you be of its predictions? In other words, how can you measure its performance? . . ### Measuring model performance . #### The digits recognition dataset: MNIST . In the following exercises, you’ll be working with the MNIST digits recognition dataset, which has 10 classes, the digits 0 through 9! A reduced version of the MNIST dataset is one of scikit-learn’s included datasets, and that is the one we will use in this exercise. . Each sample in this scikit-learn dataset is an 8×8 image representing a handwritten digit. Each pixel is represented by an integer in the range 0 to 16, indicating varying levels of black. . It is a famous dataset in machine learning and computer vision, and frequently used as a benchmark to evaluate the performance of a new model. . # Import necessary modules from sklearn import datasets import matplotlib.pyplot as plt # Load the digits dataset: digits digits = datasets.load_digits() # Print the keys and DESCR of the dataset print(digits.keys()) #dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;target_names&#39;, &#39;images&#39;, &#39;DESCR&#39;]) print(digits.DESCR) /* Optical Recognition of Handwritten Digits Data Set =================================================== Notes -- Data Set Characteristics: :Number of Instances: 5620 :Number of Attributes: 64 :Attribute Information: 8x8 image of integer pixels in the range 0..16. :Missing Attribute Values: None :Creator: E. Alpaydin (alpaydin &#39;@&#39; boun.edu.tr) :Date: July; 1998 ... */ # Print the shape of the images and data keys print(digits.images.shape) (1797, 8, 8) print(digits.data.shape) (1797, 64) # Display digit 1010 plt.imshow(digits.images[1010], cmap=plt.cm.gray_r, interpolation=&#39;nearest&#39;) plt.show() . . #### Train/Test Split + Fit/Predict/Accuracy . # Import necessary modules from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import train_test_split # Create feature and target arrays X = digits.data y = digits.target # Split into training and test set # Stratify the split according to the labels so that they are distributed in the training and test sets as they are in the original dataset. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y) # Create a k-NN classifier with 7 neighbors: knn knn = KNeighborsClassifier(n_neighbors=7) # Fit the classifier to the training data knn.fit(X_train, y_train) # Print the accuracy print(knn.score(X_test, y_test)) # 0.983333333333 . Incredibly, this out of the box k-NN classifier with 7 neighbors has learned from the training data and predicted the labels of the images in the test set with 98% accuracy, and it did so in less than a second! This is one illustration of how incredibly useful machine learning techniques can be. . . #### Overfitting and underfitting . In this exercise, you will compute and plot the training and testing accuracy scores for a variety of different neighbor values. By observing how the accuracy scores differ for the training and testing sets with different values of k, you will develop your intuition for overfitting and underfitting. . # Setup arrays to store train and test accuracies neighbors = np.arange(1, 9) train_accuracy = np.empty(len(neighbors)) test_accuracy = np.empty(len(neighbors)) # Loop over different values of k for i, k in enumerate(neighbors): # Setup a k-NN Classifier with k neighbors: knn knn = KNeighborsClassifier(n_neighbors=k) # Fit the classifier to the training data knn.fit(X_train, y_train) #Compute accuracy on the training set train_accuracy[i] = knn.score(X_train, y_train) #Compute accuracy on the testing set test_accuracy[i] = knn.score(X_test, y_test) # Generate plot plt.title(&#39;k-NN: Varying Number of Neighbors&#39;) plt.plot(neighbors, test_accuracy, label = &#39;Testing Accuracy&#39;) plt.plot(neighbors, train_accuracy, label = &#39;Training Accuracy&#39;) plt.legend() plt.xlabel(&#39;Number of Neighbors&#39;) plt.ylabel(&#39;Accuracy&#39;) plt.show() . . It looks like the test accuracy is highest when using 3 and 5 neighbors. Using 8 neighbors or more seems to result in a simple model that underfits the data. . . 2. Regression . . ### Introduction to regression . #### Importing data for supervised learning . In this chapter, you will work with Gapminder data that we have consolidated into one CSV file available in the workspace as &#39;gapminder.csv&#39; . Specifically, your goal will be to use this data to predict the life expectancy in a given country based on features such as the country’s GDP, fertility rate, and population. . # Import numpy and pandas import numpy as np import pandas as pd # Read the CSV file into a DataFrame: df df = pd.read_csv(&#39;gapminder.csv&#39;) # Create arrays for features and target variable y = df[&#39;life&#39;].values X = df[&#39;fertility&#39;].values # Print the dimensions of X and y before reshaping print(&quot;Dimensions of y before reshaping: {}&quot;.format(y.shape)) print(&quot;Dimensions of X before reshaping: {}&quot;.format(X.shape)) # Dimensions of y before reshaping: (139,) # Dimensions of X before reshaping: (139,) # Reshape X and y y = y.reshape(-1,1) X = X.reshape(-1,1) # Print the dimensions of X and y after reshaping print(&quot;Dimensions of y after reshaping: {}&quot;.format(y.shape)) print(&quot;Dimensions of X after reshaping: {}&quot;.format(X.shape)) # Dimensions of y after reshaping: (139, 1) # Dimensions of X after reshaping: (139, 1) . #### Exploring the Gapminder data . df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 139 entries, 0 to 138 Data columns (total 9 columns): population 139 non-null float64 fertility 139 non-null float64 HIV 139 non-null float64 CO2 139 non-null float64 BMI_male 139 non-null float64 GDP 139 non-null float64 BMI_female 139 non-null float64 life 139 non-null float64 child_mortality 139 non-null float64 dtypes: float64(9) memory usage: 9.9 KB . sns.heatmap(df.corr(), square=True, cmap=&#39;RdYlGn&#39;) plt.show() . . ### The basics of linear regression . We suppose that y and x have a linear relationship that can be model by . y = ax + b . An linear regression is to find a, b that minimize the sum of the squared residual (= Ordinary Least Squares, OLS) . Why squared residual? . Residuals may be positive and negative. . They cancel each other. square residual can solve this problem. . . green lines are residuals . When we have n variables of x, . y = a1x1 + a2x2 + … an*xn + b . we find a1, a2, … an, b that minimize the sum of the squared residual. . #### Fit &amp; predict for regression . In this exercise, you will use the &#39;fertility&#39; feature of the Gapminder dataset. Since the goal is to predict life expectancy, the target variable here is &#39;life&#39; . . You will also compute and print the R2 score using sckit-learn’s .score() method. . # Import LinearRegression from sklearn.linear_model import LinearRegression # Create the regressor: reg reg = LinearRegression() # Create the prediction space prediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1,1) # Fit the model to the data reg.fit(X_fertility, y) # Compute predictions over the prediction space: y_pred y_pred = reg.predict(prediction_space) # Print R^2 print(reg.score(X_fertility, y)) 0.619244216774 # Plot regression line plt.plot(prediction_space, y_pred, color=&#39;black&#39;, linewidth=3) plt.show() . . #### Train/test split for regression . In this exercise, you will split the Gapminder dataset into training and testing sets, and then fit and predict a linear regression over all features. In addition to computing the R2 score, you will also compute the Root Mean Squared Error (RMSE), which is another commonly used metric to evaluate regression models. . # Import necessary modules from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split # Create training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42) # Create the regressor: reg_all reg_all = LinearRegression() # Fit the regressor to the training data reg_all.fit(X_train, y_train) # Predict on the test data: y_pred y_pred = reg_all.predict(X_test) # Compute and print R^2 and RMSE print(&quot;R^2: {}&quot;.format(reg_all.score(X_test, y_test))) # R^2: 0.838046873142936 rmse = np.sqrt(mean_squared_error(y_test, y_pred)) print(&quot;Root Mean Squared Error: {}&quot;.format(rmse)) # Root Mean Squared Error: 3.2476010800377213 . ### Cross-validation . What is cross validation? . https://en.wikipedia.org/wiki/Cross-validation_(statistics) . #### 5-fold cross-validation . In this exercise, you will practice 5-fold cross validation on the Gapminder data. By default, scikit-learn’s cross_val_score() function uses R2R2 as the metric of choice for regression. . # Import the necessary modules from sklearn.linear_model import LinearRegression from sklearn.model_selection import cross_val_score # Create a linear regression object: reg reg = LinearRegression() # Compute 5-fold cross-validation scores: cv_scores cv_scores = cross_val_score(reg, X, y, cv=5) # Print the 5-fold cross-validation scores print(cv_scores) # [ 0.81720569 0.82917058 0.90214134 0.80633989 0.94495637] print(&quot;Average 5-Fold CV Score: {}&quot;.format(np.mean(cv_scores))) # Average 5-Fold CV Score: 0.8599627722793232 . #### K-Fold CV comparison . Cross validation is essential but do not forget that the more folds you use, the more computationally expensive cross-validation becomes. In this exercise, you will explore this for yourself. Your job is to perform 3-fold cross-validation and then 10-fold cross-validation on the Gapminder dataset. . # Import necessary modules from sklearn.linear_model import LinearRegression from sklearn.model_selection import cross_val_score # Create a linear regression object: reg reg = LinearRegression() # Perform 3-fold CV cvscores_3 = cross_val_score(reg, X, y, cv=3) print(np.mean(cvscores_3)) # 0.871871278262 # Perform 10-fold CV cvscores_10 = cross_val_score(reg, X, y, cv=10) print(np.mean(cvscores_10)) # 0.843612862013 . %timeit cross_val_score(reg, X, y, cv=3) 100 loops, best of 3: 8.73 ms per loop %timeit cross_val_score(reg, X, y, cv=10) 10 loops, best of 3: 27.5 ms per loop . . ### Regularized regression . #### Regularization I: Lasso . In this exercise, you will fit a lasso regression to the Gapminder data you have been working with and plot the coefficients. Just as with the Boston data, you will find that the coefficients of some features are shrunk to 0, with only the most important ones remaining. . df.columns Index([&#39;population&#39;, &#39;fertility&#39;, &#39;HIV&#39;, &#39;CO2&#39;, &#39;BMI_male&#39;, &#39;GDP&#39;, &#39;BMI_female&#39;, &#39;life&#39;, &#39;child_mortality&#39;], dtype=&#39;object&#39;) X: [&#39;population&#39;, &#39;fertility&#39;, &#39;HIV&#39;, &#39;CO2&#39;, &#39;BMI_male&#39;, &#39;GDP&#39;, &#39;BMI_female&#39;, &#39;child_mortality&#39;] y: life . # Import Lasso from sklearn.linear_model import Lasso # Instantiate a lasso regressor: lasso lasso = Lasso(alpha=0.4, normalize=True) # Fit the regressor to the data lasso.fit(X, y) # Compute and print the coefficients lasso_coef = lasso.fit(X, y).coef_ print(lasso_coef) # [-0. -0. -0. 0. 0. 0. -0. -0.07087587] # Plot the coefficients plt.plot(range(len(df_columns)), lasso_coef) plt.xticks(range(len(df_columns)), df_columns.values, rotation=60) plt.margins(0.02) plt.show() . . According to the lasso algorithm, it seems like &#39;child_mortality&#39; is the most important feature when predicting life expectancy. . #### Regularization II: Ridge . Lasso is great for feature selection, but when building regression models, Ridge regression should be your first choice. . Recall that lasso performs regularization by adding to the loss function a penalty term of the absolute value of each coefficient multiplied by some alpha. This is also known as L1L1 regularization because the regularization term is the L1L1 norm of the coefficients. This is not the only way to regularize, however. . If instead you took the sum of the squared values of the coefficients multiplied by some alpha – like in Ridge regression – you would be computing the L2L2norm. In this exercise, you will practice fitting ridge regression models over a range of different alphas, and plot cross-validated R2R2 scores for each, using this function that we have defined for you, which plots the R2R2 score as well as standard error for each alpha: . def display_plot(cv_scores, cv_scores_std): fig = plt.figure() ax = fig.add_subplot(1,1,1) ax.plot(alpha_space, cv_scores) std_error = cv_scores_std / np.sqrt(10) ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2) ax.set_ylabel(&#39;CV Score +/- Std Error&#39;) ax.set_xlabel(&#39;Alpha&#39;) ax.axhline(np.max(cv_scores), linestyle=&#39;--&#39;, color=&#39;.5&#39;) ax.set_xlim([alpha_space[0], alpha_space[-1]]) ax.set_xscale(&#39;log&#39;) plt.show() . Don’t worry about the specifics of the above function works. The motivation behind this exercise is for you to see how the R2R2 score varies with different alphas, and to understand the importance of selecting the right value for alpha. You’ll learn how to tune alpha in the next chapter. . # Import necessary modules from sklearn.linear_model import Ridge from sklearn.model_selection import cross_val_score # Setup the array of alphas and lists to store scores alpha_space = np.logspace(-4, 0, 50) ridge_scores = [] ridge_scores_std = [] # Create a ridge regressor: ridge ridge = Ridge(normalize=True) # Compute scores over range of alphas for alpha in alpha_space: # Specify the alpha value to use: ridge.alpha ridge.alpha = alpha # Perform 10-fold CV: ridge_cv_scores ridge_cv_scores = cross_val_score(ridge, X, y, cv=10) # Append the mean of ridge_cv_scores to ridge_scores ridge_scores.append(np.mean(ridge_cv_scores)) # Append the std of ridge_cv_scores to ridge_scores_std ridge_scores_std.append(np.std(ridge_cv_scores)) # Display the plot display_plot(ridge_scores, ridge_scores_std) . . Notice how the cross-validation scores change with different alphas. . . 3. Fine-tuning your model . . ### confusion matrix . What is confusion matrix . https://en.wikipedia.org/wiki/Confusion_matrix . . sensitivity , recall , hit rate , or true positive rate (TPR) . . specificity , selectivity or true negative rate (TNR) . . precision or positive predictive value (PPV) . . accuracy (ACC) . . F1 score is the harmonic mean of precision and sensitivity . #### illustration for TPR, TNR and PPV . . source . #### Metrics for classification . In this exercise, you will dive more deeply into evaluating the performance of binary classifiers by computing a confusion matrix and generating a classification report. . Here, you’ll work with the PIMA Indians dataset obtained from the UCI Machine Learning Repository. The goal is to predict whether or not a given female patient will contract diabetes based on features such as BMI, age, and number of pregnancies. . Therefore, it is a binary classification problem. A target value of 0 indicates that the patient does not have diabetes, while a value of 1 indicates that the patient does have diabetes. . df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 768 entries, 0 to 767 Data columns (total 9 columns): pregnancies 768 non-null int64 glucose 768 non-null int64 diastolic 768 non-null int64 triceps 768 non-null float64 insulin 768 non-null float64 bmi 768 non-null float64 dpf 768 non-null float64 age 768 non-null int64 diabetes 768 non-null int64 dtypes: float64(4), int64(5) memory usage: 54.1 KB . # Import necessary modules from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix # Create training and test set X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Instantiate a k-NN classifier: knn knn = KNeighborsClassifier(n_neighbors=6) # Fit the classifier to the training data knn.fit(X_train, y_train) # Predict the labels of the test data: y_pred y_pred = knn.predict(X_test) # Generate the confusion matrix and classification report print(confusion_matrix(y_test, y_pred)) print(classification_report(y_test, y_pred)) . [[176 30] [ 52 50]] precision recall f1-score support 0 0.77 0.85 0.81 206 1 0.62 0.49 0.55 102 avg / total 0.72 0.73 0.72 308 . . ### Logistic regression and the ROC curve . What is logistic regression? . https://en.wikipedia.org/wiki/Logistic_regression . What is ROC? . Receiver operating characteristic . Further Reading: scikit-learn document . . #### Building a logistic regression model . X.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 768 entries, 0 to 767 Data columns (total 8 columns): pregnancies 768 non-null int64 glucose 768 non-null int64 diastolic 768 non-null int64 triceps 768 non-null float64 insulin 768 non-null float64 bmi 768 non-null float64 dpf 768 non-null float64 age 768 non-null int64 dtypes: float64(4), int64(4) memory usage: 48.1 KB y 0 1 1 0 2 1 .. 765 0 766 1 767 0 Name: diabetes, dtype: int64 . # Import the necessary modules from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import classification_report, confusion_matrix # Create training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42) # Create the classifier: logreg logreg = LogisticRegression() # Fit the classifier to the training data logreg.fit(X_train, y_train) # Predict the labels of the test set: y_pred y_pred = logreg.predict(X_test) # Compute and print the confusion matrix and classification report print(confusion_matrix(y_test, y_pred)) print(classification_report(y_test, y_pred)) . [[176 30] [ 35 67]] precision recall f1-score support 0 0.83 0.85 0.84 206 1 0.69 0.66 0.67 102 avg / total 0.79 0.79 0.79 308 . #### Plotting an ROC curve . .predict_proba() . # Import necessary modules from sklearn.metrics import roc_curve # Compute predicted probabilities: y_pred_prob y_pred_prob = logreg.predict_proba(X_test)[:,1] # Generate ROC curve values: fpr, tpr, thresholds fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob) # Plot ROC curve plt.plot([0, 1], [0, 1], &#39;k--&#39;) plt.plot(fpr, tpr) plt.xlabel(&#39;False Positive Rate&#39;) plt.ylabel(&#39;True Positive Rate&#39;) plt.title(&#39;ROC Curve&#39;) plt.show() . logreg.predict_proba(X_test) # False, True # 0, 1 # Negative, Positive array([[ 0.60409835, 0.39590165], [ 0.76042394, 0.23957606], [ 0.79670177, 0.20329823], ... [ 0.84686912, 0.15313088], [ 0.97617225, 0.02382775], [ 0.40380502, 0.59619498]]) . . #### Precision-recall Curve . There are other ways to visually evaluate model performance. One such way is the precision-recall curve, which is generated by plotting the precision and recall for different thresholds. . Note that here, the class is positive (1) if the individual has diabetes. . . A recall of 1 corresponds to a classifier with a low threshold in which all females who contract diabetes were correctly classified as such, at the expense of many misclassifications of those who did not have diabetes. | Precision is undefined for a classifier which makes no positive predictions, that is, classifies everyone as not having diabetes. | When the threshold is very close to 1, precision is also 1, because the classifier is absolutely certain about its predictions. | . recall or sensitivity, TPR = 1 means all true positive are detected. We can predict all to positive to get a recall of 1. . precision, PPV = 1 means no false positive are detected. We can predict less positive to get a higher precision. . #### Area under the ROC curve . #### AUC( Area Under the Curve ) computation . # diabetes data set df.head() pregnancies glucose diastolic triceps insulin bmi dpf age 0 6 148 72 35.00000 155.548223 33.6 0.627 50 1 1 85 66 29.00000 155.548223 26.6 0.351 31 2 8 183 64 29.15342 155.548223 23.3 0.672 32 3 1 89 66 23.00000 94.000000 28.1 0.167 21 4 0 137 40 35.00000 168.000000 43.1 2.288 33 diabetes 0 1 1 0 2 1 3 0 4 1 . # Import necessary modules from sklearn.metrics import roc_auc_score from sklearn.model_selection import cross_val_score # Compute predicted probabilities: y_pred_prob y_pred_prob = logreg.predict_proba(X_test)[:,1] # Compute and print AUC score print(&quot;AUC: {}&quot;.format(roc_auc_score(y_test, y_pred_prob))) # Compute cross-validated AUC scores: cv_auc cv_auc = cross_val_score(logreg, X, y, cv=5, scoring=&#39;roc_auc&#39;) # Print list of AUC scores print(&quot;AUC scores computed using 5-fold cross-validation: {}&quot;.format(cv_auc)) . AUC: 0.8254806777079764 AUC scores computed using 5-fold cross-validation: [ 0.80148148 0.8062963 0.81481481 0.86245283 0.8554717 ] . . ### Hyperparameter tuning . #### Hyperparameter tuning with GridSearchCV . You will now practice this yourself, but by using logistic regression on the diabetes dataset. . Like the alpha parameter of lasso and ridge regularization that you saw earlier, logistic regression also has a regularization parameter: CC. CC controls the inverse of the regularization strength, and this is what you will tune in this exercise. A large CC can lead to an overfit model, while a small CC can lead to an underfit model. . The hyperparameter space for CC has been setup for you. Your job is to use GridSearchCV and logistic regression to find the optimal CC in this hyperparameter space. . # diabetes data set . # Import necessary modules from sklearn.linear_model import LogisticRegression from sklearn.model_selection import GridSearchCV # Setup the hyperparameter grid c_space = np.logspace(-5, 8, 15) param_grid = {&#39;C&#39;: c_space} # Instantiate a logistic regression classifier: logreg logreg = LogisticRegression() # Instantiate the GridSearchCV object: logreg_cv logreg_cv = GridSearchCV(logreg, param_grid, cv=5) # Fit it to the data logreg_cv.fit(X, y) # Print the tuned parameters and score print(&quot;Tuned Logistic Regression Parameters: {}&quot;.format(logreg_cv.best_params_)) print(&quot;Best score is {}&quot;.format(logreg_cv.best_score_)) . Tuned Logistic Regression Parameters: {&#39;C&#39;: 3.7275937203149381} Best score is 0.7708333333333334 . #### Hyperparameter tuning with RandomizedSearchCV . GridSearchCV can be computationally expensive, especially if you are searching over a large hyperparameter space and dealing with multiple hyperparameters. A solution to this is to use RandomizedSearchCV , in which not all hyperparameter values are tried out. Instead, a fixed number of hyperparameter settings is sampled from specified probability distributions. You’ll practice using RandomizedSearchCV in this exercise and see how this works. . # diabetes data set . # Import necessary modules from scipy.stats import randint from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import RandomizedSearchCV # Setup the parameters and distributions to sample from: param_dist param_dist = {&quot;max_depth&quot;: [3, None], &quot;max_features&quot;: randint(1, 9), &quot;min_samples_leaf&quot;: randint(1, 9), &quot;criterion&quot;: [&quot;gini&quot;, &quot;entropy&quot;]} # Instantiate a Decision Tree classifier: tree tree = DecisionTreeClassifier() # Instantiate the RandomizedSearchCV object: tree_cv tree_cv = RandomizedSearchCV(tree, param_dist, cv=5) # Fit it to the data tree_cv.fit(X, y) # Print the tuned parameters and score print(&quot;Tuned Decision Tree Parameters: {}&quot;.format(tree_cv.best_params_)) print(&quot;Best score is {}&quot;.format(tree_cv.best_score_)) . Tuned Decision Tree Parameters: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 3, &#39;max_features&#39;: 7, &#39;min_samples_leaf&#39;: 1} Best score is 0.7317708333333334 . Note that RandomizedSearchCV will never outperform GridSearchCV . Instead, it is valuable because it saves on computation time. . ### Hold-out set for final evaluation . #### Hold-out set in practice I: Classification . In addition to CC, logistic regression has a &#39;penalty&#39; hyperparameter which specifies whether to use &#39;l1&#39; or &#39;l2&#39; regularization. Your job in this exercise is to create a hold-out set, tune the &#39;C&#39; and &#39;penalty&#39; hyperparameters of a logistic regression classifier using GridSearchCV on the training set. . # diabetes data set . # Import necessary modules from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.model_selection import GridSearchCV # Create the hyperparameter grid c_space = np.logspace(-5, 8, 15) param_grid = {&#39;C&#39;: c_space, &#39;penalty&#39;: [&#39;l1&#39;, &#39;l2&#39;]} # Instantiate the logistic regression classifier: logreg logreg = LogisticRegression() # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Instantiate the GridSearchCV object: logreg_cv logreg_cv = GridSearchCV(logreg, param_grid, cv=5) # Fit it to the training data logreg_cv.fit(X_train, y_train) # Print the optimal parameters and best score print(&quot;Tuned Logistic Regression Parameter: {}&quot;.format(logreg_cv.best_params_)) print(&quot;Tuned Logistic Regression Accuracy: {}&quot;.format(logreg_cv.best_score_)) . Tuned Logistic Regression Parameter: {&#39;C&#39;: 0.43939705607607948, &#39;penalty&#39;: &#39;l1&#39;} Tuned Logistic Regression Accuracy: 0.7652173913043478 . #### Hold-out set in practice II: Regression . Remember lasso and ridge regression from the previous chapter? Lasso used the L1 penalty to regularize, while ridge used the L2 penalty. There is another type of regularized regression known as the elastic net. In elastic net regularization, the penalty term is a linear combination of the L1 and L2 penalties: . a∗L1+b∗L2 . In scikit-learn, this term is represented by the &#39;l1_ratio&#39; parameter: An &#39;l1_ratio&#39; of 1 corresponds to an L1L1 penalty, and anything lower is a combination of L1L1 and L2L2. . In this exercise, you will GridSearchCV to tune the &#39;l1_ratio&#39; of an elastic net model trained on the Gapminder data. As in the previous exercise, use a hold-out set to evaluate your model’s performance. . df.head() population fertility HIV CO2 BMI_male GDP BMI_female life 0 34811059.0 2.73 0.1 3.328945 24.59620 12314.0 129.9049 75.3 1 19842251.0 6.43 2.0 1.474353 22.25083 7103.0 130.1247 58.3 2 40381860.0 2.24 0.5 4.785170 27.50170 14646.0 118.8915 75.5 3 2975029.0 1.40 0.1 1.804106 25.35542 7383.0 132.8108 72.5 4 21370348.0 1.96 0.1 18.016313 27.56373 41312.0 117.3755 81.5 child_mortality 0 29.5 1 192.0 2 15.4 3 20.0 4 5.2 . # Import necessary modules from sklearn.linear_model import ElasticNet from sklearn.metrics import mean_squared_error from sklearn.model_selection import GridSearchCV, train_test_split # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Create the hyperparameter grid l1_space = np.linspace(0, 1, 30) param_grid = {&#39;l1_ratio&#39;: l1_space} # Instantiate the ElasticNet regressor: elastic_net elastic_net = ElasticNet() # Setup the GridSearchCV object: gm_cv gm_cv = GridSearchCV(elastic_net, param_grid, cv=5) # Fit it to the training data gm_cv.fit(X_train, y_train) # Predict on the test set and compute metrics y_pred = gm_cv.predict(X_test) r2 = gm_cv.score(X_test, y_test) mse = mean_squared_error(y_test, y_pred) print(&quot;Tuned ElasticNet l1 ratio: {}&quot;.format(gm_cv.best_params_)) print(&quot;Tuned ElasticNet R squared: {}&quot;.format(r2)) print(&quot;Tuned ElasticNet MSE: {}&quot;.format(mse)) . Tuned ElasticNet l1 ratio: {&#39;l1_ratio&#39;: 0.20689655172413793} Tuned ElasticNet R squared: 0.8668305372460283 Tuned ElasticNet MSE: 10.05791413339844 . . Preprocessing and pipelines . ### Preprocessing data . #### Exploring categorical features . df.head() population fertility HIV CO2 BMI_male GDP BMI_female life 0 34811059.0 2.73 0.1 3.328945 24.59620 12314.0 129.9049 75.3 1 19842251.0 6.43 2.0 1.474353 22.25083 7103.0 130.1247 58.3 2 40381860.0 2.24 0.5 4.785170 27.50170 14646.0 118.8915 75.5 3 2975029.0 1.40 0.1 1.804106 25.35542 7383.0 132.8108 72.5 4 21370348.0 1.96 0.1 18.016313 27.56373 41312.0 117.3755 81.5 child_mortality Region 0 29.5 Middle East &amp; North Africa 1 192.0 Sub-Saharan Africa 2 15.4 America 3 20.0 Europe &amp; Central Asia 4 5.2 East Asia &amp; Pacific . # Import pandas import pandas as pd # Read &#39;gapminder.csv&#39; into a DataFrame: df df = pd.read_csv(&#39;gapminder.csv&#39;) # Create a boxplot of life expectancy per region df.boxplot(&#39;life&#39;, &#39;Region&#39;, rot=60) # Show the plot plt.show() . . #### Creating dummy variables . # Create dummy variables: df_region df_region = pd.get_dummies(df) # Print the columns of df_region print(df_region.columns) # Create dummy variables with drop_first=True: df_region df_region = pd.get_dummies(df, drop_first=True) # Print the new columns of df_region print(df_region.columns) . Index([&#39;population&#39;, &#39;fertility&#39;, &#39;HIV&#39;, &#39;CO2&#39;, &#39;BMI_male&#39;, &#39;GDP&#39;, &#39;BMI_female&#39;, &#39;life&#39;, &#39;child_mortality&#39;, &#39;Region_America&#39;, &#39;Region_East Asia &amp; Pacific&#39;, &#39;Region_Europe &amp; Central Asia&#39;, &#39;Region_Middle East &amp; North Africa&#39;, &#39;Region_South Asia&#39;, &#39;Region_Sub-Saharan Africa&#39;], dtype=&#39;object&#39;) # Region_America has been dropped Index([&#39;population&#39;, &#39;fertility&#39;, &#39;HIV&#39;, &#39;CO2&#39;, &#39;BMI_male&#39;, &#39;GDP&#39;, &#39;BMI_female&#39;, &#39;life&#39;, &#39;child_mortality&#39;, &#39;Region_East Asia &amp; Pacific&#39;, &#39;Region_Europe &amp; Central Asia&#39;, &#39;Region_Middle East &amp; North Africa&#39;, &#39;Region_South Asia&#39;, &#39;Region_Sub-Saharan Africa&#39;], dtype=&#39;object&#39;) df_region.head() population fertility HIV CO2 BMI_male GDP BMI_female life 0 34811059.0 2.73 0.1 3.328945 24.59620 12314.0 129.9049 75.3 1 19842251.0 6.43 2.0 1.474353 22.25083 7103.0 130.1247 58.3 2 40381860.0 2.24 0.5 4.785170 27.50170 14646.0 118.8915 75.5 child_mortality Region_East Asia &amp; Pacific Region_Europe &amp; Central Asia 0 29.5 0 0 1 192.0 0 0 2 15.4 0 0 Region_Middle East &amp; North Africa Region_South Asia 0 1 0 1 0 0 2 0 0 Region_Sub-Saharan Africa 0 0 1 1 2 0 . #### Regression with categorical features . # Import necessary modules from sklearn.linear_model import Ridge from sklearn.model_selection import cross_val_score # Instantiate a ridge regressor: ridge ridge = Ridge(alpha=0.5, normalize=True) # Perform 5-fold cross-validation: ridge_cv ridge_cv = cross_val_score(ridge, X, y, cv=5) # Print the cross-validated scores print(ridge_cv) [ 0.86808336 0.80623545 0.84004203 0.7754344 0.87503712] . . ### Handling missing data . #### Dropping missing data . Now, it’s time for you to take care of these yourself! . The unprocessed dataset has been loaded into a DataFrame df . Explore it in the IPython Shell with the .head() method. You will see that there are certain data points labeled with a &#39;?&#39; . These denote missing values. As you saw in the video, different datasets encode missing values in different ways. Sometimes it may be a &#39;9999&#39; , other times a 0 – real-world data can be very messy! If you’re lucky, the missing values will already be encoded as NaN . We use NaN because it is an efficient and simplified way of internally representing missing data, and it lets us take advantage of pandas methods such as .dropna() and .fillna() , as well as scikit-learn’s Imputation transformer Imputer() . . In this exercise, your job is to convert the &#39;?&#39; s to NaNs, and then drop the rows that contain them from the DataFrame. . df.head(3) party infants water budget physician salvador religious satellite aid 0 republican 0 1 0 1 1 1 0 0 1 republican 0 1 0 1 1 1 0 0 2 democrat ? 1 1 ? 1 1 0 0 missile immigration synfuels education superfund crime duty_free_exports 0 0 1 ? 1 1 1 0 1 0 0 0 1 1 1 0 2 0 0 1 0 1 1 0 eaa_rsa 0 1 1 ? 2 0 . # Convert &#39;?&#39; to NaN df[df == &#39;?&#39;] = np.nan # Print the number of NaNs print(df.isnull().sum()) # Print shape of original DataFrame print(&quot;Shape of Original DataFrame: {}&quot;.format(df.shape)) # Drop missing values and print shape of new DataFrame df = df.dropna() # Print shape of new DataFrame print(&quot;Shape of DataFrame After Dropping All Rows with Missing Values: {}&quot;.format(df.shape)) . party 0 infants 12 water 48 budget 11 physician 11 salvador 15 religious 11 satellite 14 aid 15 missile 22 immigration 7 synfuels 21 education 31 superfund 25 crime 17 duty_free_exports 28 eaa_rsa 104 dtype: int64 Shape of Original DataFrame: (435, 17) Shape of DataFrame After Dropping All Rows with Missing Values: (232, 17) . When many values in your dataset are missing, if you drop them, you may end up throwing away valuable information along with the missing data. It’s better instead to develop an imputation strategy. This is where domain knowledge is useful, but in the absence of it, you can impute missing values with the mean or the median of the row or column that the missing value is in. . #### Imputing missing data in a ML Pipeline I . As you’ve come to appreciate, there are many steps to building a model, from creating training and test sets, to fitting a classifier or regressor, to tuning its parameters, to evaluating its performance on new data. Imputation can be seen as the first step of this machine learning process, the entirety of which can be viewed within the context of a pipeline. Scikit-learn provides a pipeline constructor that allows you to piece together these steps into one process and thereby simplify your workflow. . You’ll now practice setting up a pipeline with two steps: the imputation step, followed by the instantiation of a classifier. You’ve seen three classifiers in this course so far: k-NN, logistic regression, and the decision tree. You will now be introduced to a fourth one – the Support Vector Machine, or SVM . . # Import the Imputer module from sklearn.preprocessing import Imputer from sklearn.svm import SVC # Setup the Imputation transformer: imp # axis=0 for column imp = Imputer(missing_values=&#39;NaN&#39;, strategy=&#39;most_frequent&#39;, axis=0) # Instantiate the SVC classifier: clf clf = SVC() # Setup the pipeline with the required steps: steps steps = [(&#39;imputation&#39;, imp), (&#39;SVM&#39;, clf)] . #### Imputing missing data in a ML Pipeline II . Having setup the steps of the pipeline in the previous exercise, you will now use it on the voting dataset to classify a Congressman’s party affiliation. . What makes pipelines so incredibly useful is the simple interface that they provide. You can use the .fit() and .predict() methods on pipelines just as you did with your classifiers and regressors! . # Import necessary modules from sklearn.preprocessing import Imputer from sklearn.pipeline import Pipeline from sklearn.svm import SVC # Setup the pipeline steps: steps steps = [(&#39;imputation&#39;, Imputer(missing_values=&#39;NaN&#39;, strategy=&#39;most_frequent&#39;, axis=0)), (&#39;SVM&#39;, SVC())] # Create the pipeline: pipeline pipeline = Pipeline(steps) # Create training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Fit the pipeline to the train set pipeline.fit(X_train, y_train) # Predict the labels of the test set y_pred = pipeline.predict(X_test) # Compute metrics print(classification_report(y_test, y_pred)) . precision recall f1-score support democrat 0.99 0.96 0.98 85 republican 0.94 0.98 0.96 46 avg / total 0.97 0.97 0.97 131 . ### Centering and scaling . #### Centering and scaling your data . You will now explore scaling for yourself on a new dataset – White Wine Quality ! . We have used the &#39;quality&#39; feature of the wine to create a binary target variable: If &#39;quality&#39; is less than 5 , the target variable is 1 , and otherwise, it is 0 . . Notice how some features seem to have different units of measurement. &#39;density&#39; , for instance, takes values between 0.98 and 1.04, while &#39;total sulfur dioxide&#39; ranges from 9 to 440. As a result, it may be worth scaling the features here. Your job in this exercise is to scale the features and compute the mean and standard deviation of the unscaled features compared to the scaled features. . # white wine quality data set df.head(3) fixed acidity volatile acidity citric acid residual sugar chlorides 0 7.0 0.27 0.36 20.7 0.045 1 6.3 0.30 0.34 1.6 0.049 2 8.1 0.28 0.40 6.9 0.050 free sulfur dioxide total sulfur dioxide density pH sulphates 0 45.0 170.0 1.0010 3.00 0.45 1 14.0 132.0 0.9940 3.30 0.49 2 30.0 97.0 0.9951 3.26 0.44 alcohol quality 0 8.8 6 1 9.5 6 2 10.1 6 . # Import scale from sklearn.preprocessing import scale # Scale the features: X_scaled X_scaled = scale(X) # Print the mean and standard deviation of the unscaled features print(&quot;Mean of Unscaled Features: {}&quot;.format(np.mean(X))) print(&quot;Standard Deviation of Unscaled Features: {}&quot;.format(np.std(X))) # Print the mean and standard deviation of the scaled features print(&quot;Mean of Scaled Features: {}&quot;.format(np.mean(X_scaled))) print(&quot;Standard Deviation of Scaled Features: {}&quot;.format(np.std(X_scaled))) . Mean of Unscaled Features: 18.432687072460002 Standard Deviation of Unscaled Features: 41.54494764094571 Mean of Scaled Features: 2.7314972981668206e-15 Standard Deviation of Scaled Features: 0.9999999999999999 . #### Centering and scaling in a pipeline . With regard to whether or not scaling is effective, the proof is in the pudding! See for yourself whether or not scaling the features of the White Wine Quality dataset has any impact on its performance. . You will use a k-NN classifier as part of a pipeline that includes scaling, and for the purposes of comparison, a k-NN classifier trained on the unscaled data has been provided. . # Import the necessary modules from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline # Setup the pipeline steps: steps steps = [(&#39;scaler&#39;, StandardScaler()), (&#39;knn&#39;, KNeighborsClassifier())] # Create the pipeline: pipeline pipeline = Pipeline(steps) # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Fit the pipeline to the training set: knn_scaled knn_scaled = pipeline.fit(X_train, y_train) # Instantiate and fit a k-NN classifier to the unscaled data knn_unscaled = KNeighborsClassifier().fit(X_train, y_train) # Compute and print metrics print(&#39;Accuracy with Scaling: {}&#39;.format(knn_scaled.score(X_test, y_test))) print(&#39;Accuracy without Scaling: {}&#39;.format(knn_unscaled.score(X_test, y_test))) Accuracy with Scaling: 0.7700680272108843 Accuracy without Scaling: 0.6979591836734694 . It looks like scaling has significantly improved model performance! . . #### Bringing it all together I: Pipeline for classification . It is time now to piece together everything you have learned so far into a pipeline for classification! Your job in this exercise is to build a pipeline that includes scaling and hyperparameter tuning to classify wine quality. . You’ll return to using the SVM classifier you were briefly introduced to earlier in this chapter. The hyperparameters you will tune are C and gamma. C controls the regularization strength. It is analogous to the C you tuned for logistic regression in Chapter 3, while gamma controls the kernel coefficient: . # Setup the pipeline steps = [(&#39;scaler&#39;, StandardScaler()), (&#39;SVM&#39;, SVC())] pipeline = Pipeline(steps) # Specify the hyperparameter space parameters = {&#39;SVM__C&#39;:[1, 10, 100], &#39;SVM__gamma&#39;:[0.1, 0.01]} # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21) # Instantiate the GridSearchCV object: cv cv = GridSearchCV(pipeline, parameters) # Fit to the training set cv.fit(X_train, y_train) # Predict the labels of the test set: y_pred y_pred = cv.predict(X_test) # Compute and print metrics print(&quot;Accuracy: {}&quot;.format(cv.score(X_test, y_test))) print(classification_report(y_test, y_pred)) print(&quot;Tuned Model Parameters: {}&quot;.format(cv.best_params_)) . Accuracy: 0.7795918367346939 precision recall f1-score support False 0.83 0.85 0.84 662 True 0.67 0.63 0.65 318 avg / total 0.78 0.78 0.78 980 Tuned Model Parameters: {&#39;SVM__C&#39;: 10, &#39;SVM__gamma&#39;: 0.1} . #### Bringing it all together II: Pipeline for regression . Your job is to build a pipeline that imputes the missing data, scales the features, and fits an ElasticNet to the Gapminder data. You will then tune the l1_ratio of your ElasticNet using GridSearchCV. . # Setup the pipeline steps: steps steps = [(&#39;imputation&#39;, Imputer(missing_values=&#39;NaN&#39;, strategy=&#39;mean&#39;, axis=0)), (&#39;scaler&#39;, StandardScaler()), (&#39;elasticnet&#39;, ElasticNet())] # Create the pipeline: pipeline pipeline = Pipeline(steps) # Specify the hyperparameter space parameters = {&#39;elasticnet__l1_ratio&#39;:np.linspace(0,1,30)} # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Create the GridSearchCV object: gm_cv gm_cv = GridSearchCV(pipeline, parameters) # Fit to the training set gm_cv.fit(X_train, y_train) # Compute and print the metrics r2 = gm_cv.score(X_test, y_test) print(&quot;Tuned ElasticNet Alpha: {}&quot;.format(gm_cv.best_params_)) print(&quot;Tuned ElasticNet R squared: {}&quot;.format(r2)) # Tuned ElasticNet Alpha: {&#39;elasticnet__l1_ratio&#39;: 1.0} # Tuned ElasticNet R squared: 0.8862016570888217 . The End. . Thank you for reading. . . Classification . | ### Machine learning introduction . What is machine learning? Giving computers the ability to learn to make decisions from data without being explicitly programmed . Examples of machine learning: Learning to predict whether an email is spam or not (supervised) . Clustering Wikipedia entries into different categories (unsupervised) . #### Types of Machine Learning . supervised learning | unsupervised learning | reinforcement learning | . Supervised learning: . Predictor variables/ features and a target variable . Aim: Predict the target variable, given the predictor variables . Classification: Target variable consists of categories . Regression: Target variable is continuous . Unsupervised learning: . Uncovering hidden patterns from unlabeled data . Example of unsupervised learning: . Grouping customers into distinct categories (Clustering) . Reinforcement learning: Software agents interact with an environment . Learn how to optimize their behavior . Given a system of rewards and punishments . Applications . Economics . Genetics . Game playing (AlphaGo) . #### Naming conventions . Features = predictor variables = independent variables | Target variable = dependent variable = response variable | . . #### Features of Supervised learning . Automate time-consuming or expensive manual tasks (ex. Doctor’s diagnosis) | Make predictions about the future (ex. Will a customer click on an ad or not) | Need labeled data (Historical data with labels etc.) | . #### Popular libraries . scikit-learning (basic) | TensorFlow | keras | . . ### Exploratory data analysis . #### Numerical EDA . In this chapter, you’ll be working with a dataset obtained from the UCI Machine Learning Repository consisting of votes made by US House of Representatives Congressmen. . Your goal will be to predict their party affiliation (‘Democrat’ or ‘Republican’) based on how they voted on certain key issues. . Here, it’s worth noting that we have preprocessed this dataset to deal with missing values. This is so that your focus can be directed towards understanding how to train and evaluate supervised learning models. . Once you have mastered these fundamentals, you will be introduced to preprocessing techniques in Chapter 4 and have the chance to apply them there yourself – including on this very same dataset! . Before thinking about what supervised learning models you can apply to this, however, you need to perform Exploratory data analysis (EDA) in order to understand the structure of the data. . df.head() party infants water budget physician salvador religious 0 republican 0 1 0 1 1 1 1 republican 0 1 0 1 1 1 2 democrat 0 1 1 0 1 1 3 democrat 0 1 1 0 1 1 4 democrat 1 1 1 0 1 1 satellite aid missile immigration synfuels education superfund 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 1 2 0 0 0 0 1 0 1 3 0 0 0 0 1 0 1 4 0 0 0 0 1 0 1 crime duty_free_exports eaa_rsa 0 1 0 1 1 1 0 1 2 1 0 0 3 0 0 1 4 1 1 1 . df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 435 entries, 0 to 434 Data columns (total 17 columns): party 435 non-null object infants 435 non-null int64 water 435 non-null int64 budget 435 non-null int64 physician 435 non-null int64 salvador 435 non-null int64 religious 435 non-null int64 satellite 435 non-null int64 aid 435 non-null int64 missile 435 non-null int64 immigration 435 non-null int64 synfuels 435 non-null int64 education 435 non-null int64 superfund 435 non-null int64 crime 435 non-null int64 duty_free_exports 435 non-null int64 eaa_rsa 435 non-null int64 dtypes: int64(16), object(1) memory usage: 57.9+ KB . ### Visual EDA . . Above is a countplot of the &#39;education&#39; bill, generated from the following code: . plt.figure() sns.countplot(x=&#39;education&#39;, hue=&#39;party&#39;, data=df, palette=&#39;RdBu&#39;) plt.xticks([0,1], [&#39;No&#39;, &#39;Yes&#39;]) plt.show() . In sns.countplot() , we specify the x-axis data to be &#39;education&#39; , and hue to be &#39;party&#39; . Recall that &#39;party&#39; is also our target variable. So the resulting plot shows the difference in voting behavior between the two parties for the &#39;education&#39; bill, with each party colored differently. We manually specified the color to be &#39;RdBu&#39; , as the Republican party has been traditionally associated with red, and the Democratic party with blue. . It seems like Democrats voted resoundingly against this bill, compared to Republicans. . plt.figure() sns.countplot(x=&#39;missile&#39;, hue=&#39;party&#39;, data=df, palette=&#39;RdBu&#39;) plt.xticks([0,1], [&#39;No&#39;, &#39;Yes&#39;]) plt.show() . . Democrats vote resoundingly in favor of missile, compared to Republicans. . . ### The classification challenge . #### k-Nearest Neighbors: Fit . k-nearest neighbors algorithm . # Import KNeighborsClassifier from sklearn.neighbors from sklearn.neighbors import KNeighborsClassifier # Create arrays for the features and the response variable y = df[&#39;party&#39;].values X = df.drop(&#39;party&#39;, axis=1).values # Create a k-NN classifier with 6 neighbors knn = KNeighborsClassifier(n_neighbors=6) # Fit the classifier to the data knn.fit(X, y) . #### k-Nearest Neighbors: Predict . # Import KNeighborsClassifier from sklearn.neighbors from sklearn.neighbors import KNeighborsClassifier # Create arrays for the features and the response variable y = df[&#39;party&#39;] X = df.drop(&#39;party&#39;, axis=1).values # Create a k-NN classifier with 6 neighbors: knn knn = KNeighborsClassifier(n_neighbors=6) # Fit the classifier to the data knn.fit(X, y) # Predict the labels for the training data X y_pred = knn.predict(X) # Predict and print the label for the new data point X_new new_prediction = knn.predict(X_new) print(&quot;Prediction: {}&quot;.format(new_prediction)) # Prediction: [&#39;democrat&#39;] . How sure can you be of its predictions? In other words, how can you measure its performance? . . ### Measuring model performance . #### The digits recognition dataset: MNIST . In the following exercises, you’ll be working with the MNIST digits recognition dataset, which has 10 classes, the digits 0 through 9! A reduced version of the MNIST dataset is one of scikit-learn’s included datasets, and that is the one we will use in this exercise. . Each sample in this scikit-learn dataset is an 8×8 image representing a handwritten digit. Each pixel is represented by an integer in the range 0 to 16, indicating varying levels of black. . It is a famous dataset in machine learning and computer vision, and frequently used as a benchmark to evaluate the performance of a new model. . # Import necessary modules from sklearn import datasets import matplotlib.pyplot as plt # Load the digits dataset: digits digits = datasets.load_digits() # Print the keys and DESCR of the dataset print(digits.keys()) #dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;target_names&#39;, &#39;images&#39;, &#39;DESCR&#39;]) print(digits.DESCR) /* Optical Recognition of Handwritten Digits Data Set =================================================== Notes -- Data Set Characteristics: :Number of Instances: 5620 :Number of Attributes: 64 :Attribute Information: 8x8 image of integer pixels in the range 0..16. :Missing Attribute Values: None :Creator: E. Alpaydin (alpaydin &#39;@&#39; boun.edu.tr) :Date: July; 1998 ... */ # Print the shape of the images and data keys print(digits.images.shape) (1797, 8, 8) print(digits.data.shape) (1797, 64) # Display digit 1010 plt.imshow(digits.images[1010], cmap=plt.cm.gray_r, interpolation=&#39;nearest&#39;) plt.show() . . #### Train/Test Split + Fit/Predict/Accuracy . # Import necessary modules from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import train_test_split # Create feature and target arrays X = digits.data y = digits.target # Split into training and test set # Stratify the split according to the labels so that they are distributed in the training and test sets as they are in the original dataset. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y) # Create a k-NN classifier with 7 neighbors: knn knn = KNeighborsClassifier(n_neighbors=7) # Fit the classifier to the training data knn.fit(X_train, y_train) # Print the accuracy print(knn.score(X_test, y_test)) # 0.983333333333 . Incredibly, this out of the box k-NN classifier with 7 neighbors has learned from the training data and predicted the labels of the images in the test set with 98% accuracy, and it did so in less than a second! This is one illustration of how incredibly useful machine learning techniques can be. . . #### Overfitting and underfitting . In this exercise, you will compute and plot the training and testing accuracy scores for a variety of different neighbor values. By observing how the accuracy scores differ for the training and testing sets with different values of k, you will develop your intuition for overfitting and underfitting. . # Setup arrays to store train and test accuracies neighbors = np.arange(1, 9) train_accuracy = np.empty(len(neighbors)) test_accuracy = np.empty(len(neighbors)) # Loop over different values of k for i, k in enumerate(neighbors): # Setup a k-NN Classifier with k neighbors: knn knn = KNeighborsClassifier(n_neighbors=k) # Fit the classifier to the training data knn.fit(X_train, y_train) #Compute accuracy on the training set train_accuracy[i] = knn.score(X_train, y_train) #Compute accuracy on the testing set test_accuracy[i] = knn.score(X_test, y_test) # Generate plot plt.title(&#39;k-NN: Varying Number of Neighbors&#39;) plt.plot(neighbors, test_accuracy, label = &#39;Testing Accuracy&#39;) plt.plot(neighbors, train_accuracy, label = &#39;Training Accuracy&#39;) plt.legend() plt.xlabel(&#39;Number of Neighbors&#39;) plt.ylabel(&#39;Accuracy&#39;) plt.show() . . It looks like the test accuracy is highest when using 3 and 5 neighbors. Using 8 neighbors or more seems to result in a simple model that underfits the data. . . 2. Regression . . ### Introduction to regression . #### Importing data for supervised learning . In this chapter, you will work with Gapminder data that we have consolidated into one CSV file available in the workspace as &#39;gapminder.csv&#39; . Specifically, your goal will be to use this data to predict the life expectancy in a given country based on features such as the country’s GDP, fertility rate, and population. . # Import numpy and pandas import numpy as np import pandas as pd # Read the CSV file into a DataFrame: df df = pd.read_csv(&#39;gapminder.csv&#39;) # Create arrays for features and target variable y = df[&#39;life&#39;].values X = df[&#39;fertility&#39;].values # Print the dimensions of X and y before reshaping print(&quot;Dimensions of y before reshaping: {}&quot;.format(y.shape)) print(&quot;Dimensions of X before reshaping: {}&quot;.format(X.shape)) # Dimensions of y before reshaping: (139,) # Dimensions of X before reshaping: (139,) # Reshape X and y y = y.reshape(-1,1) X = X.reshape(-1,1) # Print the dimensions of X and y after reshaping print(&quot;Dimensions of y after reshaping: {}&quot;.format(y.shape)) print(&quot;Dimensions of X after reshaping: {}&quot;.format(X.shape)) # Dimensions of y after reshaping: (139, 1) # Dimensions of X after reshaping: (139, 1) . #### Exploring the Gapminder data . df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 139 entries, 0 to 138 Data columns (total 9 columns): population 139 non-null float64 fertility 139 non-null float64 HIV 139 non-null float64 CO2 139 non-null float64 BMI_male 139 non-null float64 GDP 139 non-null float64 BMI_female 139 non-null float64 life 139 non-null float64 child_mortality 139 non-null float64 dtypes: float64(9) memory usage: 9.9 KB . sns.heatmap(df.corr(), square=True, cmap=&#39;RdYlGn&#39;) plt.show() . . ### The basics of linear regression . We suppose that y and x have a linear relationship that can be model by . y = ax + b . An linear regression is to find a, b that minimize the sum of the squared residual (= Ordinary Least Squares, OLS) . Why squared residual? . Residuals may be positive and negative. . They cancel each other. square residual can solve this problem. . . green lines are residuals . When we have n variables of x, . y = a1x1 + a2x2 + … an*xn + b . we find a1, a2, … an, b that minimize the sum of the squared residual. . #### Fit &amp; predict for regression . In this exercise, you will use the &#39;fertility&#39; feature of the Gapminder dataset. Since the goal is to predict life expectancy, the target variable here is &#39;life&#39; . . You will also compute and print the R2 score using sckit-learn’s .score() method. . # Import LinearRegression from sklearn.linear_model import LinearRegression # Create the regressor: reg reg = LinearRegression() # Create the prediction space prediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1,1) # Fit the model to the data reg.fit(X_fertility, y) # Compute predictions over the prediction space: y_pred y_pred = reg.predict(prediction_space) # Print R^2 print(reg.score(X_fertility, y)) 0.619244216774 # Plot regression line plt.plot(prediction_space, y_pred, color=&#39;black&#39;, linewidth=3) plt.show() . . #### Train/test split for regression . In this exercise, you will split the Gapminder dataset into training and testing sets, and then fit and predict a linear regression over all features. In addition to computing the R2 score, you will also compute the Root Mean Squared Error (RMSE), which is another commonly used metric to evaluate regression models. . # Import necessary modules from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split # Create training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42) # Create the regressor: reg_all reg_all = LinearRegression() # Fit the regressor to the training data reg_all.fit(X_train, y_train) # Predict on the test data: y_pred y_pred = reg_all.predict(X_test) # Compute and print R^2 and RMSE print(&quot;R^2: {}&quot;.format(reg_all.score(X_test, y_test))) # R^2: 0.838046873142936 rmse = np.sqrt(mean_squared_error(y_test, y_pred)) print(&quot;Root Mean Squared Error: {}&quot;.format(rmse)) # Root Mean Squared Error: 3.2476010800377213 . ### Cross-validation . What is cross validation? . https://en.wikipedia.org/wiki/Cross-validation_(statistics) . #### 5-fold cross-validation . In this exercise, you will practice 5-fold cross validation on the Gapminder data. By default, scikit-learn’s cross_val_score() function uses R2R2 as the metric of choice for regression. . # Import the necessary modules from sklearn.linear_model import LinearRegression from sklearn.model_selection import cross_val_score # Create a linear regression object: reg reg = LinearRegression() # Compute 5-fold cross-validation scores: cv_scores cv_scores = cross_val_score(reg, X, y, cv=5) # Print the 5-fold cross-validation scores print(cv_scores) # [ 0.81720569 0.82917058 0.90214134 0.80633989 0.94495637] print(&quot;Average 5-Fold CV Score: {}&quot;.format(np.mean(cv_scores))) # Average 5-Fold CV Score: 0.8599627722793232 . #### K-Fold CV comparison . Cross validation is essential but do not forget that the more folds you use, the more computationally expensive cross-validation becomes. In this exercise, you will explore this for yourself. Your job is to perform 3-fold cross-validation and then 10-fold cross-validation on the Gapminder dataset. . # Import necessary modules from sklearn.linear_model import LinearRegression from sklearn.model_selection import cross_val_score # Create a linear regression object: reg reg = LinearRegression() # Perform 3-fold CV cvscores_3 = cross_val_score(reg, X, y, cv=3) print(np.mean(cvscores_3)) # 0.871871278262 # Perform 10-fold CV cvscores_10 = cross_val_score(reg, X, y, cv=10) print(np.mean(cvscores_10)) # 0.843612862013 . %timeit cross_val_score(reg, X, y, cv=3) 100 loops, best of 3: 8.73 ms per loop %timeit cross_val_score(reg, X, y, cv=10) 10 loops, best of 3: 27.5 ms per loop . . ### Regularized regression . #### Regularization I: Lasso . In this exercise, you will fit a lasso regression to the Gapminder data you have been working with and plot the coefficients. Just as with the Boston data, you will find that the coefficients of some features are shrunk to 0, with only the most important ones remaining. . df.columns Index([&#39;population&#39;, &#39;fertility&#39;, &#39;HIV&#39;, &#39;CO2&#39;, &#39;BMI_male&#39;, &#39;GDP&#39;, &#39;BMI_female&#39;, &#39;life&#39;, &#39;child_mortality&#39;], dtype=&#39;object&#39;) X: [&#39;population&#39;, &#39;fertility&#39;, &#39;HIV&#39;, &#39;CO2&#39;, &#39;BMI_male&#39;, &#39;GDP&#39;, &#39;BMI_female&#39;, &#39;child_mortality&#39;] y: life . # Import Lasso from sklearn.linear_model import Lasso # Instantiate a lasso regressor: lasso lasso = Lasso(alpha=0.4, normalize=True) # Fit the regressor to the data lasso.fit(X, y) # Compute and print the coefficients lasso_coef = lasso.fit(X, y).coef_ print(lasso_coef) # [-0. -0. -0. 0. 0. 0. -0. -0.07087587] # Plot the coefficients plt.plot(range(len(df_columns)), lasso_coef) plt.xticks(range(len(df_columns)), df_columns.values, rotation=60) plt.margins(0.02) plt.show() . . According to the lasso algorithm, it seems like &#39;child_mortality&#39; is the most important feature when predicting life expectancy. . #### Regularization II: Ridge . Lasso is great for feature selection, but when building regression models, Ridge regression should be your first choice. . Recall that lasso performs regularization by adding to the loss function a penalty term of the absolute value of each coefficient multiplied by some alpha. This is also known as L1L1 regularization because the regularization term is the L1L1 norm of the coefficients. This is not the only way to regularize, however. . If instead you took the sum of the squared values of the coefficients multiplied by some alpha – like in Ridge regression – you would be computing the L2L2norm. In this exercise, you will practice fitting ridge regression models over a range of different alphas, and plot cross-validated R2R2 scores for each, using this function that we have defined for you, which plots the R2R2 score as well as standard error for each alpha: . def display_plot(cv_scores, cv_scores_std): fig = plt.figure() ax = fig.add_subplot(1,1,1) ax.plot(alpha_space, cv_scores) std_error = cv_scores_std / np.sqrt(10) ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2) ax.set_ylabel(&#39;CV Score +/- Std Error&#39;) ax.set_xlabel(&#39;Alpha&#39;) ax.axhline(np.max(cv_scores), linestyle=&#39;--&#39;, color=&#39;.5&#39;) ax.set_xlim([alpha_space[0], alpha_space[-1]]) ax.set_xscale(&#39;log&#39;) plt.show() . Don’t worry about the specifics of the above function works. The motivation behind this exercise is for you to see how the R2R2 score varies with different alphas, and to understand the importance of selecting the right value for alpha. You’ll learn how to tune alpha in the next chapter. . # Import necessary modules from sklearn.linear_model import Ridge from sklearn.model_selection import cross_val_score # Setup the array of alphas and lists to store scores alpha_space = np.logspace(-4, 0, 50) ridge_scores = [] ridge_scores_std = [] # Create a ridge regressor: ridge ridge = Ridge(normalize=True) # Compute scores over range of alphas for alpha in alpha_space: # Specify the alpha value to use: ridge.alpha ridge.alpha = alpha # Perform 10-fold CV: ridge_cv_scores ridge_cv_scores = cross_val_score(ridge, X, y, cv=10) # Append the mean of ridge_cv_scores to ridge_scores ridge_scores.append(np.mean(ridge_cv_scores)) # Append the std of ridge_cv_scores to ridge_scores_std ridge_scores_std.append(np.std(ridge_cv_scores)) # Display the plot display_plot(ridge_scores, ridge_scores_std) . . Notice how the cross-validation scores change with different alphas. . . 3. Fine-tuning your model . . ### confusion matrix . What is confusion matrix . https://en.wikipedia.org/wiki/Confusion_matrix . . sensitivity , recall , hit rate , or true positive rate (TPR) . . specificity , selectivity or true negative rate (TNR) . . precision or positive predictive value (PPV) . . accuracy (ACC) . . F1 score is the harmonic mean of precision and sensitivity . #### illustration for TPR, TNR and PPV . . source . #### Metrics for classification . In this exercise, you will dive more deeply into evaluating the performance of binary classifiers by computing a confusion matrix and generating a classification report. . Here, you’ll work with the PIMA Indians dataset obtained from the UCI Machine Learning Repository. The goal is to predict whether or not a given female patient will contract diabetes based on features such as BMI, age, and number of pregnancies. . Therefore, it is a binary classification problem. A target value of 0 indicates that the patient does not have diabetes, while a value of 1 indicates that the patient does have diabetes. . df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 768 entries, 0 to 767 Data columns (total 9 columns): pregnancies 768 non-null int64 glucose 768 non-null int64 diastolic 768 non-null int64 triceps 768 non-null float64 insulin 768 non-null float64 bmi 768 non-null float64 dpf 768 non-null float64 age 768 non-null int64 diabetes 768 non-null int64 dtypes: float64(4), int64(5) memory usage: 54.1 KB . # Import necessary modules from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix # Create training and test set X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Instantiate a k-NN classifier: knn knn = KNeighborsClassifier(n_neighbors=6) # Fit the classifier to the training data knn.fit(X_train, y_train) # Predict the labels of the test data: y_pred y_pred = knn.predict(X_test) # Generate the confusion matrix and classification report print(confusion_matrix(y_test, y_pred)) print(classification_report(y_test, y_pred)) . [[176 30] [ 52 50]] precision recall f1-score support 0 0.77 0.85 0.81 206 1 0.62 0.49 0.55 102 avg / total 0.72 0.73 0.72 308 . . ### Logistic regression and the ROC curve . What is logistic regression? . https://en.wikipedia.org/wiki/Logistic_regression . What is ROC? . Receiver operating characteristic . Further Reading: scikit-learn document . . #### Building a logistic regression model . X.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 768 entries, 0 to 767 Data columns (total 8 columns): pregnancies 768 non-null int64 glucose 768 non-null int64 diastolic 768 non-null int64 triceps 768 non-null float64 insulin 768 non-null float64 bmi 768 non-null float64 dpf 768 non-null float64 age 768 non-null int64 dtypes: float64(4), int64(4) memory usage: 48.1 KB y 0 1 1 0 2 1 .. 765 0 766 1 767 0 Name: diabetes, dtype: int64 . # Import the necessary modules from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import classification_report, confusion_matrix # Create training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42) # Create the classifier: logreg logreg = LogisticRegression() # Fit the classifier to the training data logreg.fit(X_train, y_train) # Predict the labels of the test set: y_pred y_pred = logreg.predict(X_test) # Compute and print the confusion matrix and classification report print(confusion_matrix(y_test, y_pred)) print(classification_report(y_test, y_pred)) . [[176 30] [ 35 67]] precision recall f1-score support 0 0.83 0.85 0.84 206 1 0.69 0.66 0.67 102 avg / total 0.79 0.79 0.79 308 . #### Plotting an ROC curve . .predict_proba() . # Import necessary modules from sklearn.metrics import roc_curve # Compute predicted probabilities: y_pred_prob y_pred_prob = logreg.predict_proba(X_test)[:,1] # Generate ROC curve values: fpr, tpr, thresholds fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob) # Plot ROC curve plt.plot([0, 1], [0, 1], &#39;k--&#39;) plt.plot(fpr, tpr) plt.xlabel(&#39;False Positive Rate&#39;) plt.ylabel(&#39;True Positive Rate&#39;) plt.title(&#39;ROC Curve&#39;) plt.show() . logreg.predict_proba(X_test) # False, True # 0, 1 # Negative, Positive array([[ 0.60409835, 0.39590165], [ 0.76042394, 0.23957606], [ 0.79670177, 0.20329823], ... [ 0.84686912, 0.15313088], [ 0.97617225, 0.02382775], [ 0.40380502, 0.59619498]]) . . #### Precision-recall Curve . There are other ways to visually evaluate model performance. One such way is the precision-recall curve, which is generated by plotting the precision and recall for different thresholds. . Note that here, the class is positive (1) if the individual has diabetes. . . A recall of 1 corresponds to a classifier with a low threshold in which all females who contract diabetes were correctly classified as such, at the expense of many misclassifications of those who did not have diabetes. | Precision is undefined for a classifier which makes no positive predictions, that is, classifies everyone as not having diabetes. | When the threshold is very close to 1, precision is also 1, because the classifier is absolutely certain about its predictions. | . recall or sensitivity, TPR = 1 means all true positive are detected. We can predict all to positive to get a recall of 1. . precision, PPV = 1 means no false positive are detected. We can predict less positive to get a higher precision. . #### Area under the ROC curve . #### AUC( Area Under the Curve ) computation . # diabetes data set df.head() pregnancies glucose diastolic triceps insulin bmi dpf age 0 6 148 72 35.00000 155.548223 33.6 0.627 50 1 1 85 66 29.00000 155.548223 26.6 0.351 31 2 8 183 64 29.15342 155.548223 23.3 0.672 32 3 1 89 66 23.00000 94.000000 28.1 0.167 21 4 0 137 40 35.00000 168.000000 43.1 2.288 33 diabetes 0 1 1 0 2 1 3 0 4 1 . # Import necessary modules from sklearn.metrics import roc_auc_score from sklearn.model_selection import cross_val_score # Compute predicted probabilities: y_pred_prob y_pred_prob = logreg.predict_proba(X_test)[:,1] # Compute and print AUC score print(&quot;AUC: {}&quot;.format(roc_auc_score(y_test, y_pred_prob))) # Compute cross-validated AUC scores: cv_auc cv_auc = cross_val_score(logreg, X, y, cv=5, scoring=&#39;roc_auc&#39;) # Print list of AUC scores print(&quot;AUC scores computed using 5-fold cross-validation: {}&quot;.format(cv_auc)) . AUC: 0.8254806777079764 AUC scores computed using 5-fold cross-validation: [ 0.80148148 0.8062963 0.81481481 0.86245283 0.8554717 ] . . ### Hyperparameter tuning . #### Hyperparameter tuning with GridSearchCV . You will now practice this yourself, but by using logistic regression on the diabetes dataset. . Like the alpha parameter of lasso and ridge regularization that you saw earlier, logistic regression also has a regularization parameter: CC. CC controls the inverse of the regularization strength, and this is what you will tune in this exercise. A large CC can lead to an overfit model, while a small CC can lead to an underfit model. . The hyperparameter space for CC has been setup for you. Your job is to use GridSearchCV and logistic regression to find the optimal CC in this hyperparameter space. . # diabetes data set . # Import necessary modules from sklearn.linear_model import LogisticRegression from sklearn.model_selection import GridSearchCV # Setup the hyperparameter grid c_space = np.logspace(-5, 8, 15) param_grid = {&#39;C&#39;: c_space} # Instantiate a logistic regression classifier: logreg logreg = LogisticRegression() # Instantiate the GridSearchCV object: logreg_cv logreg_cv = GridSearchCV(logreg, param_grid, cv=5) # Fit it to the data logreg_cv.fit(X, y) # Print the tuned parameters and score print(&quot;Tuned Logistic Regression Parameters: {}&quot;.format(logreg_cv.best_params_)) print(&quot;Best score is {}&quot;.format(logreg_cv.best_score_)) . Tuned Logistic Regression Parameters: {&#39;C&#39;: 3.7275937203149381} Best score is 0.7708333333333334 . #### Hyperparameter tuning with RandomizedSearchCV . GridSearchCV can be computationally expensive, especially if you are searching over a large hyperparameter space and dealing with multiple hyperparameters. A solution to this is to use RandomizedSearchCV , in which not all hyperparameter values are tried out. Instead, a fixed number of hyperparameter settings is sampled from specified probability distributions. You’ll practice using RandomizedSearchCV in this exercise and see how this works. . # diabetes data set . # Import necessary modules from scipy.stats import randint from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import RandomizedSearchCV # Setup the parameters and distributions to sample from: param_dist param_dist = {&quot;max_depth&quot;: [3, None], &quot;max_features&quot;: randint(1, 9), &quot;min_samples_leaf&quot;: randint(1, 9), &quot;criterion&quot;: [&quot;gini&quot;, &quot;entropy&quot;]} # Instantiate a Decision Tree classifier: tree tree = DecisionTreeClassifier() # Instantiate the RandomizedSearchCV object: tree_cv tree_cv = RandomizedSearchCV(tree, param_dist, cv=5) # Fit it to the data tree_cv.fit(X, y) # Print the tuned parameters and score print(&quot;Tuned Decision Tree Parameters: {}&quot;.format(tree_cv.best_params_)) print(&quot;Best score is {}&quot;.format(tree_cv.best_score_)) . Tuned Decision Tree Parameters: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 3, &#39;max_features&#39;: 7, &#39;min_samples_leaf&#39;: 1} Best score is 0.7317708333333334 . Note that RandomizedSearchCV will never outperform GridSearchCV . Instead, it is valuable because it saves on computation time. . ### Hold-out set for final evaluation . #### Hold-out set in practice I: Classification . In addition to CC, logistic regression has a &#39;penalty&#39; hyperparameter which specifies whether to use &#39;l1&#39; or &#39;l2&#39; regularization. Your job in this exercise is to create a hold-out set, tune the &#39;C&#39; and &#39;penalty&#39; hyperparameters of a logistic regression classifier using GridSearchCV on the training set. . # diabetes data set . # Import necessary modules from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.model_selection import GridSearchCV # Create the hyperparameter grid c_space = np.logspace(-5, 8, 15) param_grid = {&#39;C&#39;: c_space, &#39;penalty&#39;: [&#39;l1&#39;, &#39;l2&#39;]} # Instantiate the logistic regression classifier: logreg logreg = LogisticRegression() # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Instantiate the GridSearchCV object: logreg_cv logreg_cv = GridSearchCV(logreg, param_grid, cv=5) # Fit it to the training data logreg_cv.fit(X_train, y_train) # Print the optimal parameters and best score print(&quot;Tuned Logistic Regression Parameter: {}&quot;.format(logreg_cv.best_params_)) print(&quot;Tuned Logistic Regression Accuracy: {}&quot;.format(logreg_cv.best_score_)) . Tuned Logistic Regression Parameter: {&#39;C&#39;: 0.43939705607607948, &#39;penalty&#39;: &#39;l1&#39;} Tuned Logistic Regression Accuracy: 0.7652173913043478 . #### Hold-out set in practice II: Regression . Remember lasso and ridge regression from the previous chapter? Lasso used the L1 penalty to regularize, while ridge used the L2 penalty. There is another type of regularized regression known as the elastic net. In elastic net regularization, the penalty term is a linear combination of the L1 and L2 penalties: . a∗L1+b∗L2 . In scikit-learn, this term is represented by the &#39;l1_ratio&#39; parameter: An &#39;l1_ratio&#39; of 1 corresponds to an L1L1 penalty, and anything lower is a combination of L1L1 and L2L2. . In this exercise, you will GridSearchCV to tune the &#39;l1_ratio&#39; of an elastic net model trained on the Gapminder data. As in the previous exercise, use a hold-out set to evaluate your model’s performance. . df.head() population fertility HIV CO2 BMI_male GDP BMI_female life 0 34811059.0 2.73 0.1 3.328945 24.59620 12314.0 129.9049 75.3 1 19842251.0 6.43 2.0 1.474353 22.25083 7103.0 130.1247 58.3 2 40381860.0 2.24 0.5 4.785170 27.50170 14646.0 118.8915 75.5 3 2975029.0 1.40 0.1 1.804106 25.35542 7383.0 132.8108 72.5 4 21370348.0 1.96 0.1 18.016313 27.56373 41312.0 117.3755 81.5 child_mortality 0 29.5 1 192.0 2 15.4 3 20.0 4 5.2 . # Import necessary modules from sklearn.linear_model import ElasticNet from sklearn.metrics import mean_squared_error from sklearn.model_selection import GridSearchCV, train_test_split # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Create the hyperparameter grid l1_space = np.linspace(0, 1, 30) param_grid = {&#39;l1_ratio&#39;: l1_space} # Instantiate the ElasticNet regressor: elastic_net elastic_net = ElasticNet() # Setup the GridSearchCV object: gm_cv gm_cv = GridSearchCV(elastic_net, param_grid, cv=5) # Fit it to the training data gm_cv.fit(X_train, y_train) # Predict on the test set and compute metrics y_pred = gm_cv.predict(X_test) r2 = gm_cv.score(X_test, y_test) mse = mean_squared_error(y_test, y_pred) print(&quot;Tuned ElasticNet l1 ratio: {}&quot;.format(gm_cv.best_params_)) print(&quot;Tuned ElasticNet R squared: {}&quot;.format(r2)) print(&quot;Tuned ElasticNet MSE: {}&quot;.format(mse)) . Tuned ElasticNet l1 ratio: {&#39;l1_ratio&#39;: 0.20689655172413793} Tuned ElasticNet R squared: 0.8668305372460283 Tuned ElasticNet MSE: 10.05791413339844 . . Preprocessing and pipelines . ### Preprocessing data . #### Exploring categorical features . df.head() population fertility HIV CO2 BMI_male GDP BMI_female life 0 34811059.0 2.73 0.1 3.328945 24.59620 12314.0 129.9049 75.3 1 19842251.0 6.43 2.0 1.474353 22.25083 7103.0 130.1247 58.3 2 40381860.0 2.24 0.5 4.785170 27.50170 14646.0 118.8915 75.5 3 2975029.0 1.40 0.1 1.804106 25.35542 7383.0 132.8108 72.5 4 21370348.0 1.96 0.1 18.016313 27.56373 41312.0 117.3755 81.5 child_mortality Region 0 29.5 Middle East &amp; North Africa 1 192.0 Sub-Saharan Africa 2 15.4 America 3 20.0 Europe &amp; Central Asia 4 5.2 East Asia &amp; Pacific . # Import pandas import pandas as pd # Read &#39;gapminder.csv&#39; into a DataFrame: df df = pd.read_csv(&#39;gapminder.csv&#39;) # Create a boxplot of life expectancy per region df.boxplot(&#39;life&#39;, &#39;Region&#39;, rot=60) # Show the plot plt.show() . . #### Creating dummy variables . # Create dummy variables: df_region df_region = pd.get_dummies(df) # Print the columns of df_region print(df_region.columns) # Create dummy variables with drop_first=True: df_region df_region = pd.get_dummies(df, drop_first=True) # Print the new columns of df_region print(df_region.columns) . Index([&#39;population&#39;, &#39;fertility&#39;, &#39;HIV&#39;, &#39;CO2&#39;, &#39;BMI_male&#39;, &#39;GDP&#39;, &#39;BMI_female&#39;, &#39;life&#39;, &#39;child_mortality&#39;, &#39;Region_America&#39;, &#39;Region_East Asia &amp; Pacific&#39;, &#39;Region_Europe &amp; Central Asia&#39;, &#39;Region_Middle East &amp; North Africa&#39;, &#39;Region_South Asia&#39;, &#39;Region_Sub-Saharan Africa&#39;], dtype=&#39;object&#39;) # Region_America has been dropped Index([&#39;population&#39;, &#39;fertility&#39;, &#39;HIV&#39;, &#39;CO2&#39;, &#39;BMI_male&#39;, &#39;GDP&#39;, &#39;BMI_female&#39;, &#39;life&#39;, &#39;child_mortality&#39;, &#39;Region_East Asia &amp; Pacific&#39;, &#39;Region_Europe &amp; Central Asia&#39;, &#39;Region_Middle East &amp; North Africa&#39;, &#39;Region_South Asia&#39;, &#39;Region_Sub-Saharan Africa&#39;], dtype=&#39;object&#39;) df_region.head() population fertility HIV CO2 BMI_male GDP BMI_female life 0 34811059.0 2.73 0.1 3.328945 24.59620 12314.0 129.9049 75.3 1 19842251.0 6.43 2.0 1.474353 22.25083 7103.0 130.1247 58.3 2 40381860.0 2.24 0.5 4.785170 27.50170 14646.0 118.8915 75.5 child_mortality Region_East Asia &amp; Pacific Region_Europe &amp; Central Asia 0 29.5 0 0 1 192.0 0 0 2 15.4 0 0 Region_Middle East &amp; North Africa Region_South Asia 0 1 0 1 0 0 2 0 0 Region_Sub-Saharan Africa 0 0 1 1 2 0 . #### Regression with categorical features . # Import necessary modules from sklearn.linear_model import Ridge from sklearn.model_selection import cross_val_score # Instantiate a ridge regressor: ridge ridge = Ridge(alpha=0.5, normalize=True) # Perform 5-fold cross-validation: ridge_cv ridge_cv = cross_val_score(ridge, X, y, cv=5) # Print the cross-validated scores print(ridge_cv) [ 0.86808336 0.80623545 0.84004203 0.7754344 0.87503712] . . ### Handling missing data . #### Dropping missing data . Now, it’s time for you to take care of these yourself! . The unprocessed dataset has been loaded into a DataFrame df . Explore it in the IPython Shell with the .head() method. You will see that there are certain data points labeled with a &#39;?&#39; . These denote missing values. As you saw in the video, different datasets encode missing values in different ways. Sometimes it may be a &#39;9999&#39; , other times a 0 – real-world data can be very messy! If you’re lucky, the missing values will already be encoded as NaN . We use NaN because it is an efficient and simplified way of internally representing missing data, and it lets us take advantage of pandas methods such as .dropna() and .fillna() , as well as scikit-learn’s Imputation transformer Imputer() . . In this exercise, your job is to convert the &#39;?&#39; s to NaNs, and then drop the rows that contain them from the DataFrame. . df.head(3) party infants water budget physician salvador religious satellite aid 0 republican 0 1 0 1 1 1 0 0 1 republican 0 1 0 1 1 1 0 0 2 democrat ? 1 1 ? 1 1 0 0 missile immigration synfuels education superfund crime duty_free_exports 0 0 1 ? 1 1 1 0 1 0 0 0 1 1 1 0 2 0 0 1 0 1 1 0 eaa_rsa 0 1 1 ? 2 0 . # Convert &#39;?&#39; to NaN df[df == &#39;?&#39;] = np.nan # Print the number of NaNs print(df.isnull().sum()) # Print shape of original DataFrame print(&quot;Shape of Original DataFrame: {}&quot;.format(df.shape)) # Drop missing values and print shape of new DataFrame df = df.dropna() # Print shape of new DataFrame print(&quot;Shape of DataFrame After Dropping All Rows with Missing Values: {}&quot;.format(df.shape)) . party 0 infants 12 water 48 budget 11 physician 11 salvador 15 religious 11 satellite 14 aid 15 missile 22 immigration 7 synfuels 21 education 31 superfund 25 crime 17 duty_free_exports 28 eaa_rsa 104 dtype: int64 Shape of Original DataFrame: (435, 17) Shape of DataFrame After Dropping All Rows with Missing Values: (232, 17) . When many values in your dataset are missing, if you drop them, you may end up throwing away valuable information along with the missing data. It’s better instead to develop an imputation strategy. This is where domain knowledge is useful, but in the absence of it, you can impute missing values with the mean or the median of the row or column that the missing value is in. . #### Imputing missing data in a ML Pipeline I . As you’ve come to appreciate, there are many steps to building a model, from creating training and test sets, to fitting a classifier or regressor, to tuning its parameters, to evaluating its performance on new data. Imputation can be seen as the first step of this machine learning process, the entirety of which can be viewed within the context of a pipeline. Scikit-learn provides a pipeline constructor that allows you to piece together these steps into one process and thereby simplify your workflow. . You’ll now practice setting up a pipeline with two steps: the imputation step, followed by the instantiation of a classifier. You’ve seen three classifiers in this course so far: k-NN, logistic regression, and the decision tree. You will now be introduced to a fourth one – the Support Vector Machine, or SVM . . # Import the Imputer module from sklearn.preprocessing import Imputer from sklearn.svm import SVC # Setup the Imputation transformer: imp # axis=0 for column imp = Imputer(missing_values=&#39;NaN&#39;, strategy=&#39;most_frequent&#39;, axis=0) # Instantiate the SVC classifier: clf clf = SVC() # Setup the pipeline with the required steps: steps steps = [(&#39;imputation&#39;, imp), (&#39;SVM&#39;, clf)] . #### Imputing missing data in a ML Pipeline II . Having setup the steps of the pipeline in the previous exercise, you will now use it on the voting dataset to classify a Congressman’s party affiliation. . What makes pipelines so incredibly useful is the simple interface that they provide. You can use the .fit() and .predict() methods on pipelines just as you did with your classifiers and regressors! . # Import necessary modules from sklearn.preprocessing import Imputer from sklearn.pipeline import Pipeline from sklearn.svm import SVC # Setup the pipeline steps: steps steps = [(&#39;imputation&#39;, Imputer(missing_values=&#39;NaN&#39;, strategy=&#39;most_frequent&#39;, axis=0)), (&#39;SVM&#39;, SVC())] # Create the pipeline: pipeline pipeline = Pipeline(steps) # Create training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Fit the pipeline to the train set pipeline.fit(X_train, y_train) # Predict the labels of the test set y_pred = pipeline.predict(X_test) # Compute metrics print(classification_report(y_test, y_pred)) . precision recall f1-score support democrat 0.99 0.96 0.98 85 republican 0.94 0.98 0.96 46 avg / total 0.97 0.97 0.97 131 . ### Centering and scaling . #### Centering and scaling your data . You will now explore scaling for yourself on a new dataset – White Wine Quality ! . We have used the &#39;quality&#39; feature of the wine to create a binary target variable: If &#39;quality&#39; is less than 5 , the target variable is 1 , and otherwise, it is 0 . . Notice how some features seem to have different units of measurement. &#39;density&#39; , for instance, takes values between 0.98 and 1.04, while &#39;total sulfur dioxide&#39; ranges from 9 to 440. As a result, it may be worth scaling the features here. Your job in this exercise is to scale the features and compute the mean and standard deviation of the unscaled features compared to the scaled features. . # white wine quality data set df.head(3) fixed acidity volatile acidity citric acid residual sugar chlorides 0 7.0 0.27 0.36 20.7 0.045 1 6.3 0.30 0.34 1.6 0.049 2 8.1 0.28 0.40 6.9 0.050 free sulfur dioxide total sulfur dioxide density pH sulphates 0 45.0 170.0 1.0010 3.00 0.45 1 14.0 132.0 0.9940 3.30 0.49 2 30.0 97.0 0.9951 3.26 0.44 alcohol quality 0 8.8 6 1 9.5 6 2 10.1 6 . # Import scale from sklearn.preprocessing import scale # Scale the features: X_scaled X_scaled = scale(X) # Print the mean and standard deviation of the unscaled features print(&quot;Mean of Unscaled Features: {}&quot;.format(np.mean(X))) print(&quot;Standard Deviation of Unscaled Features: {}&quot;.format(np.std(X))) # Print the mean and standard deviation of the scaled features print(&quot;Mean of Scaled Features: {}&quot;.format(np.mean(X_scaled))) print(&quot;Standard Deviation of Scaled Features: {}&quot;.format(np.std(X_scaled))) . Mean of Unscaled Features: 18.432687072460002 Standard Deviation of Unscaled Features: 41.54494764094571 Mean of Scaled Features: 2.7314972981668206e-15 Standard Deviation of Scaled Features: 0.9999999999999999 . #### Centering and scaling in a pipeline . With regard to whether or not scaling is effective, the proof is in the pudding! See for yourself whether or not scaling the features of the White Wine Quality dataset has any impact on its performance. . You will use a k-NN classifier as part of a pipeline that includes scaling, and for the purposes of comparison, a k-NN classifier trained on the unscaled data has been provided. . # Import the necessary modules from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline # Setup the pipeline steps: steps steps = [(&#39;scaler&#39;, StandardScaler()), (&#39;knn&#39;, KNeighborsClassifier())] # Create the pipeline: pipeline pipeline = Pipeline(steps) # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Fit the pipeline to the training set: knn_scaled knn_scaled = pipeline.fit(X_train, y_train) # Instantiate and fit a k-NN classifier to the unscaled data knn_unscaled = KNeighborsClassifier().fit(X_train, y_train) # Compute and print metrics print(&#39;Accuracy with Scaling: {}&#39;.format(knn_scaled.score(X_test, y_test))) print(&#39;Accuracy without Scaling: {}&#39;.format(knn_unscaled.score(X_test, y_test))) Accuracy with Scaling: 0.7700680272108843 Accuracy without Scaling: 0.6979591836734694 . It looks like scaling has significantly improved model performance! . . #### Bringing it all together I: Pipeline for classification . It is time now to piece together everything you have learned so far into a pipeline for classification! Your job in this exercise is to build a pipeline that includes scaling and hyperparameter tuning to classify wine quality. . You’ll return to using the SVM classifier you were briefly introduced to earlier in this chapter. The hyperparameters you will tune are C and gamma. C controls the regularization strength. It is analogous to the C you tuned for logistic regression in Chapter 3, while gamma controls the kernel coefficient: . # Setup the pipeline steps = [(&#39;scaler&#39;, StandardScaler()), (&#39;SVM&#39;, SVC())] pipeline = Pipeline(steps) # Specify the hyperparameter space parameters = {&#39;SVM__C&#39;:[1, 10, 100], &#39;SVM__gamma&#39;:[0.1, 0.01]} # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21) # Instantiate the GridSearchCV object: cv cv = GridSearchCV(pipeline, parameters) # Fit to the training set cv.fit(X_train, y_train) # Predict the labels of the test set: y_pred y_pred = cv.predict(X_test) # Compute and print metrics print(&quot;Accuracy: {}&quot;.format(cv.score(X_test, y_test))) print(classification_report(y_test, y_pred)) print(&quot;Tuned Model Parameters: {}&quot;.format(cv.best_params_)) . Accuracy: 0.7795918367346939 precision recall f1-score support False 0.83 0.85 0.84 662 True 0.67 0.63 0.65 318 avg / total 0.78 0.78 0.78 980 Tuned Model Parameters: {&#39;SVM__C&#39;: 10, &#39;SVM__gamma&#39;: 0.1} . #### Bringing it all together II: Pipeline for regression . Your job is to build a pipeline that imputes the missing data, scales the features, and fits an ElasticNet to the Gapminder data. You will then tune the l1_ratio of your ElasticNet using GridSearchCV. . # Setup the pipeline steps: steps steps = [(&#39;imputation&#39;, Imputer(missing_values=&#39;NaN&#39;, strategy=&#39;mean&#39;, axis=0)), (&#39;scaler&#39;, StandardScaler()), (&#39;elasticnet&#39;, ElasticNet())] # Create the pipeline: pipeline pipeline = Pipeline(steps) # Specify the hyperparameter space parameters = {&#39;elasticnet__l1_ratio&#39;:np.linspace(0,1,30)} # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Create the GridSearchCV object: gm_cv gm_cv = GridSearchCV(pipeline, parameters) # Fit to the training set gm_cv.fit(X_train, y_train) # Compute and print the metrics r2 = gm_cv.score(X_test, y_test) print(&quot;Tuned ElasticNet Alpha: {}&quot;.format(gm_cv.best_params_)) print(&quot;Tuned ElasticNet R squared: {}&quot;.format(r2)) # Tuned ElasticNet Alpha: {&#39;elasticnet__l1_ratio&#39;: 1.0} # Tuned ElasticNet R squared: 0.8862016570888217 . The End. . Thank you for reading. . . Classification . | ### Machine learning introduction . What is machine learning? Giving computers the ability to learn to make decisions from data without being explicitly programmed . Examples of machine learning: Learning to predict whether an email is spam or not (supervised) . Clustering Wikipedia entries into different categories (unsupervised) . #### Types of Machine Learning . supervised learning | unsupervised learning | reinforcement learning | . Supervised learning: . Predictor variables/ features and a target variable . Aim: Predict the target variable, given the predictor variables . Classification: Target variable consists of categories . Regression: Target variable is continuous . Unsupervised learning: . Uncovering hidden patterns from unlabeled data . Example of unsupervised learning: . Grouping customers into distinct categories (Clustering) . Reinforcement learning: Software agents interact with an environment . Learn how to optimize their behavior . Given a system of rewards and punishments . Applications . Economics . Genetics . Game playing (AlphaGo) . #### Naming conventions . Features = predictor variables = independent variables | Target variable = dependent variable = response variable | . . #### Features of Supervised learning . Automate time-consuming or expensive manual tasks (ex. Doctor’s diagnosis) | Make predictions about the future (ex. Will a customer click on an ad or not) | Need labeled data (Historical data with labels etc.) | . #### Popular libraries . scikit-learning (basic) | TensorFlow | keras | . . ### Exploratory data analysis . #### Numerical EDA . In this chapter, you’ll be working with a dataset obtained from the UCI Machine Learning Repository consisting of votes made by US House of Representatives Congressmen. . Your goal will be to predict their party affiliation (‘Democrat’ or ‘Republican’) based on how they voted on certain key issues. . Here, it’s worth noting that we have preprocessed this dataset to deal with missing values. This is so that your focus can be directed towards understanding how to train and evaluate supervised learning models. . Once you have mastered these fundamentals, you will be introduced to preprocessing techniques in Chapter 4 and have the chance to apply them there yourself – including on this very same dataset! . Before thinking about what supervised learning models you can apply to this, however, you need to perform Exploratory data analysis (EDA) in order to understand the structure of the data. . df.head() party infants water budget physician salvador religious 0 republican 0 1 0 1 1 1 1 republican 0 1 0 1 1 1 2 democrat 0 1 1 0 1 1 3 democrat 0 1 1 0 1 1 4 democrat 1 1 1 0 1 1 satellite aid missile immigration synfuels education superfund 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 1 2 0 0 0 0 1 0 1 3 0 0 0 0 1 0 1 4 0 0 0 0 1 0 1 crime duty_free_exports eaa_rsa 0 1 0 1 1 1 0 1 2 1 0 0 3 0 0 1 4 1 1 1 . df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 435 entries, 0 to 434 Data columns (total 17 columns): party 435 non-null object infants 435 non-null int64 water 435 non-null int64 budget 435 non-null int64 physician 435 non-null int64 salvador 435 non-null int64 religious 435 non-null int64 satellite 435 non-null int64 aid 435 non-null int64 missile 435 non-null int64 immigration 435 non-null int64 synfuels 435 non-null int64 education 435 non-null int64 superfund 435 non-null int64 crime 435 non-null int64 duty_free_exports 435 non-null int64 eaa_rsa 435 non-null int64 dtypes: int64(16), object(1) memory usage: 57.9+ KB . ### Visual EDA . . Above is a countplot of the &#39;education&#39; bill, generated from the following code: . plt.figure() sns.countplot(x=&#39;education&#39;, hue=&#39;party&#39;, data=df, palette=&#39;RdBu&#39;) plt.xticks([0,1], [&#39;No&#39;, &#39;Yes&#39;]) plt.show() . In sns.countplot() , we specify the x-axis data to be &#39;education&#39; , and hue to be &#39;party&#39; . Recall that &#39;party&#39; is also our target variable. So the resulting plot shows the difference in voting behavior between the two parties for the &#39;education&#39; bill, with each party colored differently. We manually specified the color to be &#39;RdBu&#39; , as the Republican party has been traditionally associated with red, and the Democratic party with blue. . It seems like Democrats voted resoundingly against this bill, compared to Republicans. . plt.figure() sns.countplot(x=&#39;missile&#39;, hue=&#39;party&#39;, data=df, palette=&#39;RdBu&#39;) plt.xticks([0,1], [&#39;No&#39;, &#39;Yes&#39;]) plt.show() . . Democrats vote resoundingly in favor of missile, compared to Republicans. . . ### The classification challenge . #### k-Nearest Neighbors: Fit . k-nearest neighbors algorithm . # Import KNeighborsClassifier from sklearn.neighbors from sklearn.neighbors import KNeighborsClassifier # Create arrays for the features and the response variable y = df[&#39;party&#39;].values X = df.drop(&#39;party&#39;, axis=1).values # Create a k-NN classifier with 6 neighbors knn = KNeighborsClassifier(n_neighbors=6) # Fit the classifier to the data knn.fit(X, y) . #### k-Nearest Neighbors: Predict . # Import KNeighborsClassifier from sklearn.neighbors from sklearn.neighbors import KNeighborsClassifier # Create arrays for the features and the response variable y = df[&#39;party&#39;] X = df.drop(&#39;party&#39;, axis=1).values # Create a k-NN classifier with 6 neighbors: knn knn = KNeighborsClassifier(n_neighbors=6) # Fit the classifier to the data knn.fit(X, y) # Predict the labels for the training data X y_pred = knn.predict(X) # Predict and print the label for the new data point X_new new_prediction = knn.predict(X_new) print(&quot;Prediction: {}&quot;.format(new_prediction)) # Prediction: [&#39;democrat&#39;] . How sure can you be of its predictions? In other words, how can you measure its performance? . . ### Measuring model performance . #### The digits recognition dataset: MNIST . In the following exercises, you’ll be working with the MNIST digits recognition dataset, which has 10 classes, the digits 0 through 9! A reduced version of the MNIST dataset is one of scikit-learn’s included datasets, and that is the one we will use in this exercise. . Each sample in this scikit-learn dataset is an 8×8 image representing a handwritten digit. Each pixel is represented by an integer in the range 0 to 16, indicating varying levels of black. . It is a famous dataset in machine learning and computer vision, and frequently used as a benchmark to evaluate the performance of a new model. . # Import necessary modules from sklearn import datasets import matplotlib.pyplot as plt # Load the digits dataset: digits digits = datasets.load_digits() # Print the keys and DESCR of the dataset print(digits.keys()) #dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;target_names&#39;, &#39;images&#39;, &#39;DESCR&#39;]) print(digits.DESCR) /* Optical Recognition of Handwritten Digits Data Set =================================================== Notes -- Data Set Characteristics: :Number of Instances: 5620 :Number of Attributes: 64 :Attribute Information: 8x8 image of integer pixels in the range 0..16. :Missing Attribute Values: None :Creator: E. Alpaydin (alpaydin &#39;@&#39; boun.edu.tr) :Date: July; 1998 ... */ # Print the shape of the images and data keys print(digits.images.shape) (1797, 8, 8) print(digits.data.shape) (1797, 64) # Display digit 1010 plt.imshow(digits.images[1010], cmap=plt.cm.gray_r, interpolation=&#39;nearest&#39;) plt.show() . . #### Train/Test Split + Fit/Predict/Accuracy . # Import necessary modules from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import train_test_split # Create feature and target arrays X = digits.data y = digits.target # Split into training and test set # Stratify the split according to the labels so that they are distributed in the training and test sets as they are in the original dataset. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y) # Create a k-NN classifier with 7 neighbors: knn knn = KNeighborsClassifier(n_neighbors=7) # Fit the classifier to the training data knn.fit(X_train, y_train) # Print the accuracy print(knn.score(X_test, y_test)) # 0.983333333333 . Incredibly, this out of the box k-NN classifier with 7 neighbors has learned from the training data and predicted the labels of the images in the test set with 98% accuracy, and it did so in less than a second! This is one illustration of how incredibly useful machine learning techniques can be. . . #### Overfitting and underfitting . In this exercise, you will compute and plot the training and testing accuracy scores for a variety of different neighbor values. By observing how the accuracy scores differ for the training and testing sets with different values of k, you will develop your intuition for overfitting and underfitting. . # Setup arrays to store train and test accuracies neighbors = np.arange(1, 9) train_accuracy = np.empty(len(neighbors)) test_accuracy = np.empty(len(neighbors)) # Loop over different values of k for i, k in enumerate(neighbors): # Setup a k-NN Classifier with k neighbors: knn knn = KNeighborsClassifier(n_neighbors=k) # Fit the classifier to the training data knn.fit(X_train, y_train) #Compute accuracy on the training set train_accuracy[i] = knn.score(X_train, y_train) #Compute accuracy on the testing set test_accuracy[i] = knn.score(X_test, y_test) # Generate plot plt.title(&#39;k-NN: Varying Number of Neighbors&#39;) plt.plot(neighbors, test_accuracy, label = &#39;Testing Accuracy&#39;) plt.plot(neighbors, train_accuracy, label = &#39;Training Accuracy&#39;) plt.legend() plt.xlabel(&#39;Number of Neighbors&#39;) plt.ylabel(&#39;Accuracy&#39;) plt.show() . . It looks like the test accuracy is highest when using 3 and 5 neighbors. Using 8 neighbors or more seems to result in a simple model that underfits the data. . . 2. Regression . . ### Introduction to regression . #### Importing data for supervised learning . In this chapter, you will work with Gapminder data that we have consolidated into one CSV file available in the workspace as &#39;gapminder.csv&#39; . Specifically, your goal will be to use this data to predict the life expectancy in a given country based on features such as the country’s GDP, fertility rate, and population. . # Import numpy and pandas import numpy as np import pandas as pd # Read the CSV file into a DataFrame: df df = pd.read_csv(&#39;gapminder.csv&#39;) # Create arrays for features and target variable y = df[&#39;life&#39;].values X = df[&#39;fertility&#39;].values # Print the dimensions of X and y before reshaping print(&quot;Dimensions of y before reshaping: {}&quot;.format(y.shape)) print(&quot;Dimensions of X before reshaping: {}&quot;.format(X.shape)) # Dimensions of y before reshaping: (139,) # Dimensions of X before reshaping: (139,) # Reshape X and y y = y.reshape(-1,1) X = X.reshape(-1,1) # Print the dimensions of X and y after reshaping print(&quot;Dimensions of y after reshaping: {}&quot;.format(y.shape)) print(&quot;Dimensions of X after reshaping: {}&quot;.format(X.shape)) # Dimensions of y after reshaping: (139, 1) # Dimensions of X after reshaping: (139, 1) . #### Exploring the Gapminder data . df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 139 entries, 0 to 138 Data columns (total 9 columns): population 139 non-null float64 fertility 139 non-null float64 HIV 139 non-null float64 CO2 139 non-null float64 BMI_male 139 non-null float64 GDP 139 non-null float64 BMI_female 139 non-null float64 life 139 non-null float64 child_mortality 139 non-null float64 dtypes: float64(9) memory usage: 9.9 KB . sns.heatmap(df.corr(), square=True, cmap=&#39;RdYlGn&#39;) plt.show() . . ### The basics of linear regression . We suppose that y and x have a linear relationship that can be model by . y = ax + b . An linear regression is to find a, b that minimize the sum of the squared residual (= Ordinary Least Squares, OLS) . Why squared residual? . Residuals may be positive and negative. . They cancel each other. square residual can solve this problem. . . green lines are residuals . When we have n variables of x, . y = a1x1 + a2x2 + … an*xn + b . we find a1, a2, … an, b that minimize the sum of the squared residual. . #### Fit &amp; predict for regression . In this exercise, you will use the &#39;fertility&#39; feature of the Gapminder dataset. Since the goal is to predict life expectancy, the target variable here is &#39;life&#39; . . You will also compute and print the R2 score using sckit-learn’s .score() method. . # Import LinearRegression from sklearn.linear_model import LinearRegression # Create the regressor: reg reg = LinearRegression() # Create the prediction space prediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1,1) # Fit the model to the data reg.fit(X_fertility, y) # Compute predictions over the prediction space: y_pred y_pred = reg.predict(prediction_space) # Print R^2 print(reg.score(X_fertility, y)) 0.619244216774 # Plot regression line plt.plot(prediction_space, y_pred, color=&#39;black&#39;, linewidth=3) plt.show() . . #### Train/test split for regression . In this exercise, you will split the Gapminder dataset into training and testing sets, and then fit and predict a linear regression over all features. In addition to computing the R2 score, you will also compute the Root Mean Squared Error (RMSE), which is another commonly used metric to evaluate regression models. . # Import necessary modules from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split # Create training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42) # Create the regressor: reg_all reg_all = LinearRegression() # Fit the regressor to the training data reg_all.fit(X_train, y_train) # Predict on the test data: y_pred y_pred = reg_all.predict(X_test) # Compute and print R^2 and RMSE print(&quot;R^2: {}&quot;.format(reg_all.score(X_test, y_test))) # R^2: 0.838046873142936 rmse = np.sqrt(mean_squared_error(y_test, y_pred)) print(&quot;Root Mean Squared Error: {}&quot;.format(rmse)) # Root Mean Squared Error: 3.2476010800377213 . ### Cross-validation . What is cross validation? . https://en.wikipedia.org/wiki/Cross-validation_(statistics) . #### 5-fold cross-validation . In this exercise, you will practice 5-fold cross validation on the Gapminder data. By default, scikit-learn’s cross_val_score() function uses R2R2 as the metric of choice for regression. . # Import the necessary modules from sklearn.linear_model import LinearRegression from sklearn.model_selection import cross_val_score # Create a linear regression object: reg reg = LinearRegression() # Compute 5-fold cross-validation scores: cv_scores cv_scores = cross_val_score(reg, X, y, cv=5) # Print the 5-fold cross-validation scores print(cv_scores) # [ 0.81720569 0.82917058 0.90214134 0.80633989 0.94495637] print(&quot;Average 5-Fold CV Score: {}&quot;.format(np.mean(cv_scores))) # Average 5-Fold CV Score: 0.8599627722793232 . #### K-Fold CV comparison . Cross validation is essential but do not forget that the more folds you use, the more computationally expensive cross-validation becomes. In this exercise, you will explore this for yourself. Your job is to perform 3-fold cross-validation and then 10-fold cross-validation on the Gapminder dataset. . # Import necessary modules from sklearn.linear_model import LinearRegression from sklearn.model_selection import cross_val_score # Create a linear regression object: reg reg = LinearRegression() # Perform 3-fold CV cvscores_3 = cross_val_score(reg, X, y, cv=3) print(np.mean(cvscores_3)) # 0.871871278262 # Perform 10-fold CV cvscores_10 = cross_val_score(reg, X, y, cv=10) print(np.mean(cvscores_10)) # 0.843612862013 . %timeit cross_val_score(reg, X, y, cv=3) 100 loops, best of 3: 8.73 ms per loop %timeit cross_val_score(reg, X, y, cv=10) 10 loops, best of 3: 27.5 ms per loop . . ### Regularized regression . #### Regularization I: Lasso . In this exercise, you will fit a lasso regression to the Gapminder data you have been working with and plot the coefficients. Just as with the Boston data, you will find that the coefficients of some features are shrunk to 0, with only the most important ones remaining. . df.columns Index([&#39;population&#39;, &#39;fertility&#39;, &#39;HIV&#39;, &#39;CO2&#39;, &#39;BMI_male&#39;, &#39;GDP&#39;, &#39;BMI_female&#39;, &#39;life&#39;, &#39;child_mortality&#39;], dtype=&#39;object&#39;) X: [&#39;population&#39;, &#39;fertility&#39;, &#39;HIV&#39;, &#39;CO2&#39;, &#39;BMI_male&#39;, &#39;GDP&#39;, &#39;BMI_female&#39;, &#39;child_mortality&#39;] y: life . # Import Lasso from sklearn.linear_model import Lasso # Instantiate a lasso regressor: lasso lasso = Lasso(alpha=0.4, normalize=True) # Fit the regressor to the data lasso.fit(X, y) # Compute and print the coefficients lasso_coef = lasso.fit(X, y).coef_ print(lasso_coef) # [-0. -0. -0. 0. 0. 0. -0. -0.07087587] # Plot the coefficients plt.plot(range(len(df_columns)), lasso_coef) plt.xticks(range(len(df_columns)), df_columns.values, rotation=60) plt.margins(0.02) plt.show() . . According to the lasso algorithm, it seems like &#39;child_mortality&#39; is the most important feature when predicting life expectancy. . #### Regularization II: Ridge . Lasso is great for feature selection, but when building regression models, Ridge regression should be your first choice. . Recall that lasso performs regularization by adding to the loss function a penalty term of the absolute value of each coefficient multiplied by some alpha. This is also known as L1L1 regularization because the regularization term is the L1L1 norm of the coefficients. This is not the only way to regularize, however. . If instead you took the sum of the squared values of the coefficients multiplied by some alpha – like in Ridge regression – you would be computing the L2L2norm. In this exercise, you will practice fitting ridge regression models over a range of different alphas, and plot cross-validated R2R2 scores for each, using this function that we have defined for you, which plots the R2R2 score as well as standard error for each alpha: . def display_plot(cv_scores, cv_scores_std): fig = plt.figure() ax = fig.add_subplot(1,1,1) ax.plot(alpha_space, cv_scores) std_error = cv_scores_std / np.sqrt(10) ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2) ax.set_ylabel(&#39;CV Score +/- Std Error&#39;) ax.set_xlabel(&#39;Alpha&#39;) ax.axhline(np.max(cv_scores), linestyle=&#39;--&#39;, color=&#39;.5&#39;) ax.set_xlim([alpha_space[0], alpha_space[-1]]) ax.set_xscale(&#39;log&#39;) plt.show() . Don’t worry about the specifics of the above function works. The motivation behind this exercise is for you to see how the R2R2 score varies with different alphas, and to understand the importance of selecting the right value for alpha. You’ll learn how to tune alpha in the next chapter. . # Import necessary modules from sklearn.linear_model import Ridge from sklearn.model_selection import cross_val_score # Setup the array of alphas and lists to store scores alpha_space = np.logspace(-4, 0, 50) ridge_scores = [] ridge_scores_std = [] # Create a ridge regressor: ridge ridge = Ridge(normalize=True) # Compute scores over range of alphas for alpha in alpha_space: # Specify the alpha value to use: ridge.alpha ridge.alpha = alpha # Perform 10-fold CV: ridge_cv_scores ridge_cv_scores = cross_val_score(ridge, X, y, cv=10) # Append the mean of ridge_cv_scores to ridge_scores ridge_scores.append(np.mean(ridge_cv_scores)) # Append the std of ridge_cv_scores to ridge_scores_std ridge_scores_std.append(np.std(ridge_cv_scores)) # Display the plot display_plot(ridge_scores, ridge_scores_std) . . Notice how the cross-validation scores change with different alphas. . . 3. Fine-tuning your model . . ### confusion matrix . What is confusion matrix . https://en.wikipedia.org/wiki/Confusion_matrix . . sensitivity , recall , hit rate , or true positive rate (TPR) . . specificity , selectivity or true negative rate (TNR) . . precision or positive predictive value (PPV) . . accuracy (ACC) . . F1 score is the harmonic mean of precision and sensitivity . #### illustration for TPR, TNR and PPV . . source . #### Metrics for classification . In this exercise, you will dive more deeply into evaluating the performance of binary classifiers by computing a confusion matrix and generating a classification report. . Here, you’ll work with the PIMA Indians dataset obtained from the UCI Machine Learning Repository. The goal is to predict whether or not a given female patient will contract diabetes based on features such as BMI, age, and number of pregnancies. . Therefore, it is a binary classification problem. A target value of 0 indicates that the patient does not have diabetes, while a value of 1 indicates that the patient does have diabetes. . df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 768 entries, 0 to 767 Data columns (total 9 columns): pregnancies 768 non-null int64 glucose 768 non-null int64 diastolic 768 non-null int64 triceps 768 non-null float64 insulin 768 non-null float64 bmi 768 non-null float64 dpf 768 non-null float64 age 768 non-null int64 diabetes 768 non-null int64 dtypes: float64(4), int64(5) memory usage: 54.1 KB . # Import necessary modules from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix # Create training and test set X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Instantiate a k-NN classifier: knn knn = KNeighborsClassifier(n_neighbors=6) # Fit the classifier to the training data knn.fit(X_train, y_train) # Predict the labels of the test data: y_pred y_pred = knn.predict(X_test) # Generate the confusion matrix and classification report print(confusion_matrix(y_test, y_pred)) print(classification_report(y_test, y_pred)) . [[176 30] [ 52 50]] precision recall f1-score support 0 0.77 0.85 0.81 206 1 0.62 0.49 0.55 102 avg / total 0.72 0.73 0.72 308 . . ### Logistic regression and the ROC curve . What is logistic regression? . https://en.wikipedia.org/wiki/Logistic_regression . What is ROC? . Receiver operating characteristic . Further Reading: scikit-learn document . . #### Building a logistic regression model . X.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 768 entries, 0 to 767 Data columns (total 8 columns): pregnancies 768 non-null int64 glucose 768 non-null int64 diastolic 768 non-null int64 triceps 768 non-null float64 insulin 768 non-null float64 bmi 768 non-null float64 dpf 768 non-null float64 age 768 non-null int64 dtypes: float64(4), int64(4) memory usage: 48.1 KB y 0 1 1 0 2 1 .. 765 0 766 1 767 0 Name: diabetes, dtype: int64 . # Import the necessary modules from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import classification_report, confusion_matrix # Create training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42) # Create the classifier: logreg logreg = LogisticRegression() # Fit the classifier to the training data logreg.fit(X_train, y_train) # Predict the labels of the test set: y_pred y_pred = logreg.predict(X_test) # Compute and print the confusion matrix and classification report print(confusion_matrix(y_test, y_pred)) print(classification_report(y_test, y_pred)) . [[176 30] [ 35 67]] precision recall f1-score support 0 0.83 0.85 0.84 206 1 0.69 0.66 0.67 102 avg / total 0.79 0.79 0.79 308 . #### Plotting an ROC curve . .predict_proba() . # Import necessary modules from sklearn.metrics import roc_curve # Compute predicted probabilities: y_pred_prob y_pred_prob = logreg.predict_proba(X_test)[:,1] # Generate ROC curve values: fpr, tpr, thresholds fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob) # Plot ROC curve plt.plot([0, 1], [0, 1], &#39;k--&#39;) plt.plot(fpr, tpr) plt.xlabel(&#39;False Positive Rate&#39;) plt.ylabel(&#39;True Positive Rate&#39;) plt.title(&#39;ROC Curve&#39;) plt.show() . logreg.predict_proba(X_test) # False, True # 0, 1 # Negative, Positive array([[ 0.60409835, 0.39590165], [ 0.76042394, 0.23957606], [ 0.79670177, 0.20329823], ... [ 0.84686912, 0.15313088], [ 0.97617225, 0.02382775], [ 0.40380502, 0.59619498]]) . . #### Precision-recall Curve . There are other ways to visually evaluate model performance. One such way is the precision-recall curve, which is generated by plotting the precision and recall for different thresholds. . Note that here, the class is positive (1) if the individual has diabetes. . . A recall of 1 corresponds to a classifier with a low threshold in which all females who contract diabetes were correctly classified as such, at the expense of many misclassifications of those who did not have diabetes. | Precision is undefined for a classifier which makes no positive predictions, that is, classifies everyone as not having diabetes. | When the threshold is very close to 1, precision is also 1, because the classifier is absolutely certain about its predictions. | . recall or sensitivity, TPR = 1 means all true positive are detected. We can predict all to positive to get a recall of 1. . precision, PPV = 1 means no false positive are detected. We can predict less positive to get a higher precision. . #### Area under the ROC curve . #### AUC( Area Under the Curve ) computation . # diabetes data set df.head() pregnancies glucose diastolic triceps insulin bmi dpf age 0 6 148 72 35.00000 155.548223 33.6 0.627 50 1 1 85 66 29.00000 155.548223 26.6 0.351 31 2 8 183 64 29.15342 155.548223 23.3 0.672 32 3 1 89 66 23.00000 94.000000 28.1 0.167 21 4 0 137 40 35.00000 168.000000 43.1 2.288 33 diabetes 0 1 1 0 2 1 3 0 4 1 . # Import necessary modules from sklearn.metrics import roc_auc_score from sklearn.model_selection import cross_val_score # Compute predicted probabilities: y_pred_prob y_pred_prob = logreg.predict_proba(X_test)[:,1] # Compute and print AUC score print(&quot;AUC: {}&quot;.format(roc_auc_score(y_test, y_pred_prob))) # Compute cross-validated AUC scores: cv_auc cv_auc = cross_val_score(logreg, X, y, cv=5, scoring=&#39;roc_auc&#39;) # Print list of AUC scores print(&quot;AUC scores computed using 5-fold cross-validation: {}&quot;.format(cv_auc)) . AUC: 0.8254806777079764 AUC scores computed using 5-fold cross-validation: [ 0.80148148 0.8062963 0.81481481 0.86245283 0.8554717 ] . . ### Hyperparameter tuning . #### Hyperparameter tuning with GridSearchCV . You will now practice this yourself, but by using logistic regression on the diabetes dataset. . Like the alpha parameter of lasso and ridge regularization that you saw earlier, logistic regression also has a regularization parameter: CC. CC controls the inverse of the regularization strength, and this is what you will tune in this exercise. A large CC can lead to an overfit model, while a small CC can lead to an underfit model. . The hyperparameter space for CC has been setup for you. Your job is to use GridSearchCV and logistic regression to find the optimal CC in this hyperparameter space. . # diabetes data set . # Import necessary modules from sklearn.linear_model import LogisticRegression from sklearn.model_selection import GridSearchCV # Setup the hyperparameter grid c_space = np.logspace(-5, 8, 15) param_grid = {&#39;C&#39;: c_space} # Instantiate a logistic regression classifier: logreg logreg = LogisticRegression() # Instantiate the GridSearchCV object: logreg_cv logreg_cv = GridSearchCV(logreg, param_grid, cv=5) # Fit it to the data logreg_cv.fit(X, y) # Print the tuned parameters and score print(&quot;Tuned Logistic Regression Parameters: {}&quot;.format(logreg_cv.best_params_)) print(&quot;Best score is {}&quot;.format(logreg_cv.best_score_)) . Tuned Logistic Regression Parameters: {&#39;C&#39;: 3.7275937203149381} Best score is 0.7708333333333334 . #### Hyperparameter tuning with RandomizedSearchCV . GridSearchCV can be computationally expensive, especially if you are searching over a large hyperparameter space and dealing with multiple hyperparameters. A solution to this is to use RandomizedSearchCV , in which not all hyperparameter values are tried out. Instead, a fixed number of hyperparameter settings is sampled from specified probability distributions. You’ll practice using RandomizedSearchCV in this exercise and see how this works. . # diabetes data set . # Import necessary modules from scipy.stats import randint from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import RandomizedSearchCV # Setup the parameters and distributions to sample from: param_dist param_dist = {&quot;max_depth&quot;: [3, None], &quot;max_features&quot;: randint(1, 9), &quot;min_samples_leaf&quot;: randint(1, 9), &quot;criterion&quot;: [&quot;gini&quot;, &quot;entropy&quot;]} # Instantiate a Decision Tree classifier: tree tree = DecisionTreeClassifier() # Instantiate the RandomizedSearchCV object: tree_cv tree_cv = RandomizedSearchCV(tree, param_dist, cv=5) # Fit it to the data tree_cv.fit(X, y) # Print the tuned parameters and score print(&quot;Tuned Decision Tree Parameters: {}&quot;.format(tree_cv.best_params_)) print(&quot;Best score is {}&quot;.format(tree_cv.best_score_)) . Tuned Decision Tree Parameters: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 3, &#39;max_features&#39;: 7, &#39;min_samples_leaf&#39;: 1} Best score is 0.7317708333333334 . Note that RandomizedSearchCV will never outperform GridSearchCV . Instead, it is valuable because it saves on computation time. . ### Hold-out set for final evaluation . #### Hold-out set in practice I: Classification . In addition to CC, logistic regression has a &#39;penalty&#39; hyperparameter which specifies whether to use &#39;l1&#39; or &#39;l2&#39; regularization. Your job in this exercise is to create a hold-out set, tune the &#39;C&#39; and &#39;penalty&#39; hyperparameters of a logistic regression classifier using GridSearchCV on the training set. . # diabetes data set . # Import necessary modules from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.model_selection import GridSearchCV # Create the hyperparameter grid c_space = np.logspace(-5, 8, 15) param_grid = {&#39;C&#39;: c_space, &#39;penalty&#39;: [&#39;l1&#39;, &#39;l2&#39;]} # Instantiate the logistic regression classifier: logreg logreg = LogisticRegression() # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Instantiate the GridSearchCV object: logreg_cv logreg_cv = GridSearchCV(logreg, param_grid, cv=5) # Fit it to the training data logreg_cv.fit(X_train, y_train) # Print the optimal parameters and best score print(&quot;Tuned Logistic Regression Parameter: {}&quot;.format(logreg_cv.best_params_)) print(&quot;Tuned Logistic Regression Accuracy: {}&quot;.format(logreg_cv.best_score_)) . Tuned Logistic Regression Parameter: {&#39;C&#39;: 0.43939705607607948, &#39;penalty&#39;: &#39;l1&#39;} Tuned Logistic Regression Accuracy: 0.7652173913043478 . #### Hold-out set in practice II: Regression . Remember lasso and ridge regression from the previous chapter? Lasso used the L1 penalty to regularize, while ridge used the L2 penalty. There is another type of regularized regression known as the elastic net. In elastic net regularization, the penalty term is a linear combination of the L1 and L2 penalties: . a∗L1+b∗L2 . In scikit-learn, this term is represented by the &#39;l1_ratio&#39; parameter: An &#39;l1_ratio&#39; of 1 corresponds to an L1L1 penalty, and anything lower is a combination of L1L1 and L2L2. . In this exercise, you will GridSearchCV to tune the &#39;l1_ratio&#39; of an elastic net model trained on the Gapminder data. As in the previous exercise, use a hold-out set to evaluate your model’s performance. . df.head() population fertility HIV CO2 BMI_male GDP BMI_female life 0 34811059.0 2.73 0.1 3.328945 24.59620 12314.0 129.9049 75.3 1 19842251.0 6.43 2.0 1.474353 22.25083 7103.0 130.1247 58.3 2 40381860.0 2.24 0.5 4.785170 27.50170 14646.0 118.8915 75.5 3 2975029.0 1.40 0.1 1.804106 25.35542 7383.0 132.8108 72.5 4 21370348.0 1.96 0.1 18.016313 27.56373 41312.0 117.3755 81.5 child_mortality 0 29.5 1 192.0 2 15.4 3 20.0 4 5.2 . # Import necessary modules from sklearn.linear_model import ElasticNet from sklearn.metrics import mean_squared_error from sklearn.model_selection import GridSearchCV, train_test_split # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Create the hyperparameter grid l1_space = np.linspace(0, 1, 30) param_grid = {&#39;l1_ratio&#39;: l1_space} # Instantiate the ElasticNet regressor: elastic_net elastic_net = ElasticNet() # Setup the GridSearchCV object: gm_cv gm_cv = GridSearchCV(elastic_net, param_grid, cv=5) # Fit it to the training data gm_cv.fit(X_train, y_train) # Predict on the test set and compute metrics y_pred = gm_cv.predict(X_test) r2 = gm_cv.score(X_test, y_test) mse = mean_squared_error(y_test, y_pred) print(&quot;Tuned ElasticNet l1 ratio: {}&quot;.format(gm_cv.best_params_)) print(&quot;Tuned ElasticNet R squared: {}&quot;.format(r2)) print(&quot;Tuned ElasticNet MSE: {}&quot;.format(mse)) . Tuned ElasticNet l1 ratio: {&#39;l1_ratio&#39;: 0.20689655172413793} Tuned ElasticNet R squared: 0.8668305372460283 Tuned ElasticNet MSE: 10.05791413339844 . . Preprocessing and pipelines . ### Preprocessing data . #### Exploring categorical features . df.head() population fertility HIV CO2 BMI_male GDP BMI_female life 0 34811059.0 2.73 0.1 3.328945 24.59620 12314.0 129.9049 75.3 1 19842251.0 6.43 2.0 1.474353 22.25083 7103.0 130.1247 58.3 2 40381860.0 2.24 0.5 4.785170 27.50170 14646.0 118.8915 75.5 3 2975029.0 1.40 0.1 1.804106 25.35542 7383.0 132.8108 72.5 4 21370348.0 1.96 0.1 18.016313 27.56373 41312.0 117.3755 81.5 child_mortality Region 0 29.5 Middle East &amp; North Africa 1 192.0 Sub-Saharan Africa 2 15.4 America 3 20.0 Europe &amp; Central Asia 4 5.2 East Asia &amp; Pacific . # Import pandas import pandas as pd # Read &#39;gapminder.csv&#39; into a DataFrame: df df = pd.read_csv(&#39;gapminder.csv&#39;) # Create a boxplot of life expectancy per region df.boxplot(&#39;life&#39;, &#39;Region&#39;, rot=60) # Show the plot plt.show() . . #### Creating dummy variables . # Create dummy variables: df_region df_region = pd.get_dummies(df) # Print the columns of df_region print(df_region.columns) # Create dummy variables with drop_first=True: df_region df_region = pd.get_dummies(df, drop_first=True) # Print the new columns of df_region print(df_region.columns) . Index([&#39;population&#39;, &#39;fertility&#39;, &#39;HIV&#39;, &#39;CO2&#39;, &#39;BMI_male&#39;, &#39;GDP&#39;, &#39;BMI_female&#39;, &#39;life&#39;, &#39;child_mortality&#39;, &#39;Region_America&#39;, &#39;Region_East Asia &amp; Pacific&#39;, &#39;Region_Europe &amp; Central Asia&#39;, &#39;Region_Middle East &amp; North Africa&#39;, &#39;Region_South Asia&#39;, &#39;Region_Sub-Saharan Africa&#39;], dtype=&#39;object&#39;) # Region_America has been dropped Index([&#39;population&#39;, &#39;fertility&#39;, &#39;HIV&#39;, &#39;CO2&#39;, &#39;BMI_male&#39;, &#39;GDP&#39;, &#39;BMI_female&#39;, &#39;life&#39;, &#39;child_mortality&#39;, &#39;Region_East Asia &amp; Pacific&#39;, &#39;Region_Europe &amp; Central Asia&#39;, &#39;Region_Middle East &amp; North Africa&#39;, &#39;Region_South Asia&#39;, &#39;Region_Sub-Saharan Africa&#39;], dtype=&#39;object&#39;) df_region.head() population fertility HIV CO2 BMI_male GDP BMI_female life 0 34811059.0 2.73 0.1 3.328945 24.59620 12314.0 129.9049 75.3 1 19842251.0 6.43 2.0 1.474353 22.25083 7103.0 130.1247 58.3 2 40381860.0 2.24 0.5 4.785170 27.50170 14646.0 118.8915 75.5 child_mortality Region_East Asia &amp; Pacific Region_Europe &amp; Central Asia 0 29.5 0 0 1 192.0 0 0 2 15.4 0 0 Region_Middle East &amp; North Africa Region_South Asia 0 1 0 1 0 0 2 0 0 Region_Sub-Saharan Africa 0 0 1 1 2 0 . #### Regression with categorical features . # Import necessary modules from sklearn.linear_model import Ridge from sklearn.model_selection import cross_val_score # Instantiate a ridge regressor: ridge ridge = Ridge(alpha=0.5, normalize=True) # Perform 5-fold cross-validation: ridge_cv ridge_cv = cross_val_score(ridge, X, y, cv=5) # Print the cross-validated scores print(ridge_cv) [ 0.86808336 0.80623545 0.84004203 0.7754344 0.87503712] . . ### Handling missing data . #### Dropping missing data . Now, it’s time for you to take care of these yourself! . The unprocessed dataset has been loaded into a DataFrame df . Explore it in the IPython Shell with the .head() method. You will see that there are certain data points labeled with a &#39;?&#39; . These denote missing values. As you saw in the video, different datasets encode missing values in different ways. Sometimes it may be a &#39;9999&#39; , other times a 0 – real-world data can be very messy! If you’re lucky, the missing values will already be encoded as NaN . We use NaN because it is an efficient and simplified way of internally representing missing data, and it lets us take advantage of pandas methods such as .dropna() and .fillna() , as well as scikit-learn’s Imputation transformer Imputer() . . In this exercise, your job is to convert the &#39;?&#39; s to NaNs, and then drop the rows that contain them from the DataFrame. . df.head(3) party infants water budget physician salvador religious satellite aid 0 republican 0 1 0 1 1 1 0 0 1 republican 0 1 0 1 1 1 0 0 2 democrat ? 1 1 ? 1 1 0 0 missile immigration synfuels education superfund crime duty_free_exports 0 0 1 ? 1 1 1 0 1 0 0 0 1 1 1 0 2 0 0 1 0 1 1 0 eaa_rsa 0 1 1 ? 2 0 . # Convert &#39;?&#39; to NaN df[df == &#39;?&#39;] = np.nan # Print the number of NaNs print(df.isnull().sum()) # Print shape of original DataFrame print(&quot;Shape of Original DataFrame: {}&quot;.format(df.shape)) # Drop missing values and print shape of new DataFrame df = df.dropna() # Print shape of new DataFrame print(&quot;Shape of DataFrame After Dropping All Rows with Missing Values: {}&quot;.format(df.shape)) . party 0 infants 12 water 48 budget 11 physician 11 salvador 15 religious 11 satellite 14 aid 15 missile 22 immigration 7 synfuels 21 education 31 superfund 25 crime 17 duty_free_exports 28 eaa_rsa 104 dtype: int64 Shape of Original DataFrame: (435, 17) Shape of DataFrame After Dropping All Rows with Missing Values: (232, 17) . When many values in your dataset are missing, if you drop them, you may end up throwing away valuable information along with the missing data. It’s better instead to develop an imputation strategy. This is where domain knowledge is useful, but in the absence of it, you can impute missing values with the mean or the median of the row or column that the missing value is in. . #### Imputing missing data in a ML Pipeline I . As you’ve come to appreciate, there are many steps to building a model, from creating training and test sets, to fitting a classifier or regressor, to tuning its parameters, to evaluating its performance on new data. Imputation can be seen as the first step of this machine learning process, the entirety of which can be viewed within the context of a pipeline. Scikit-learn provides a pipeline constructor that allows you to piece together these steps into one process and thereby simplify your workflow. . You’ll now practice setting up a pipeline with two steps: the imputation step, followed by the instantiation of a classifier. You’ve seen three classifiers in this course so far: k-NN, logistic regression, and the decision tree. You will now be introduced to a fourth one – the Support Vector Machine, or SVM . . # Import the Imputer module from sklearn.preprocessing import Imputer from sklearn.svm import SVC # Setup the Imputation transformer: imp # axis=0 for column imp = Imputer(missing_values=&#39;NaN&#39;, strategy=&#39;most_frequent&#39;, axis=0) # Instantiate the SVC classifier: clf clf = SVC() # Setup the pipeline with the required steps: steps steps = [(&#39;imputation&#39;, imp), (&#39;SVM&#39;, clf)] . #### Imputing missing data in a ML Pipeline II . Having setup the steps of the pipeline in the previous exercise, you will now use it on the voting dataset to classify a Congressman’s party affiliation. . What makes pipelines so incredibly useful is the simple interface that they provide. You can use the .fit() and .predict() methods on pipelines just as you did with your classifiers and regressors! . # Import necessary modules from sklearn.preprocessing import Imputer from sklearn.pipeline import Pipeline from sklearn.svm import SVC # Setup the pipeline steps: steps steps = [(&#39;imputation&#39;, Imputer(missing_values=&#39;NaN&#39;, strategy=&#39;most_frequent&#39;, axis=0)), (&#39;SVM&#39;, SVC())] # Create the pipeline: pipeline pipeline = Pipeline(steps) # Create training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Fit the pipeline to the train set pipeline.fit(X_train, y_train) # Predict the labels of the test set y_pred = pipeline.predict(X_test) # Compute metrics print(classification_report(y_test, y_pred)) . precision recall f1-score support democrat 0.99 0.96 0.98 85 republican 0.94 0.98 0.96 46 avg / total 0.97 0.97 0.97 131 . ### Centering and scaling . #### Centering and scaling your data . You will now explore scaling for yourself on a new dataset – White Wine Quality ! . We have used the &#39;quality&#39; feature of the wine to create a binary target variable: If &#39;quality&#39; is less than 5 , the target variable is 1 , and otherwise, it is 0 . . Notice how some features seem to have different units of measurement. &#39;density&#39; , for instance, takes values between 0.98 and 1.04, while &#39;total sulfur dioxide&#39; ranges from 9 to 440. As a result, it may be worth scaling the features here. Your job in this exercise is to scale the features and compute the mean and standard deviation of the unscaled features compared to the scaled features. . # white wine quality data set df.head(3) fixed acidity volatile acidity citric acid residual sugar chlorides 0 7.0 0.27 0.36 20.7 0.045 1 6.3 0.30 0.34 1.6 0.049 2 8.1 0.28 0.40 6.9 0.050 free sulfur dioxide total sulfur dioxide density pH sulphates 0 45.0 170.0 1.0010 3.00 0.45 1 14.0 132.0 0.9940 3.30 0.49 2 30.0 97.0 0.9951 3.26 0.44 alcohol quality 0 8.8 6 1 9.5 6 2 10.1 6 . # Import scale from sklearn.preprocessing import scale # Scale the features: X_scaled X_scaled = scale(X) # Print the mean and standard deviation of the unscaled features print(&quot;Mean of Unscaled Features: {}&quot;.format(np.mean(X))) print(&quot;Standard Deviation of Unscaled Features: {}&quot;.format(np.std(X))) # Print the mean and standard deviation of the scaled features print(&quot;Mean of Scaled Features: {}&quot;.format(np.mean(X_scaled))) print(&quot;Standard Deviation of Scaled Features: {}&quot;.format(np.std(X_scaled))) . Mean of Unscaled Features: 18.432687072460002 Standard Deviation of Unscaled Features: 41.54494764094571 Mean of Scaled Features: 2.7314972981668206e-15 Standard Deviation of Scaled Features: 0.9999999999999999 . #### Centering and scaling in a pipeline . With regard to whether or not scaling is effective, the proof is in the pudding! See for yourself whether or not scaling the features of the White Wine Quality dataset has any impact on its performance. . You will use a k-NN classifier as part of a pipeline that includes scaling, and for the purposes of comparison, a k-NN classifier trained on the unscaled data has been provided. . # Import the necessary modules from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline # Setup the pipeline steps: steps steps = [(&#39;scaler&#39;, StandardScaler()), (&#39;knn&#39;, KNeighborsClassifier())] # Create the pipeline: pipeline pipeline = Pipeline(steps) # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Fit the pipeline to the training set: knn_scaled knn_scaled = pipeline.fit(X_train, y_train) # Instantiate and fit a k-NN classifier to the unscaled data knn_unscaled = KNeighborsClassifier().fit(X_train, y_train) # Compute and print metrics print(&#39;Accuracy with Scaling: {}&#39;.format(knn_scaled.score(X_test, y_test))) print(&#39;Accuracy without Scaling: {}&#39;.format(knn_unscaled.score(X_test, y_test))) Accuracy with Scaling: 0.7700680272108843 Accuracy without Scaling: 0.6979591836734694 . It looks like scaling has significantly improved model performance! . . #### Bringing it all together I: Pipeline for classification . It is time now to piece together everything you have learned so far into a pipeline for classification! Your job in this exercise is to build a pipeline that includes scaling and hyperparameter tuning to classify wine quality. . You’ll return to using the SVM classifier you were briefly introduced to earlier in this chapter. The hyperparameters you will tune are C and gamma. C controls the regularization strength. It is analogous to the C you tuned for logistic regression in Chapter 3, while gamma controls the kernel coefficient: . # Setup the pipeline steps = [(&#39;scaler&#39;, StandardScaler()), (&#39;SVM&#39;, SVC())] pipeline = Pipeline(steps) # Specify the hyperparameter space parameters = {&#39;SVM__C&#39;:[1, 10, 100], &#39;SVM__gamma&#39;:[0.1, 0.01]} # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21) # Instantiate the GridSearchCV object: cv cv = GridSearchCV(pipeline, parameters) # Fit to the training set cv.fit(X_train, y_train) # Predict the labels of the test set: y_pred y_pred = cv.predict(X_test) # Compute and print metrics print(&quot;Accuracy: {}&quot;.format(cv.score(X_test, y_test))) print(classification_report(y_test, y_pred)) print(&quot;Tuned Model Parameters: {}&quot;.format(cv.best_params_)) . Accuracy: 0.7795918367346939 precision recall f1-score support False 0.83 0.85 0.84 662 True 0.67 0.63 0.65 318 avg / total 0.78 0.78 0.78 980 Tuned Model Parameters: {&#39;SVM__C&#39;: 10, &#39;SVM__gamma&#39;: 0.1} . #### Bringing it all together II: Pipeline for regression . Your job is to build a pipeline that imputes the missing data, scales the features, and fits an ElasticNet to the Gapminder data. You will then tune the l1_ratio of your ElasticNet using GridSearchCV. . # Setup the pipeline steps: steps steps = [(&#39;imputation&#39;, Imputer(missing_values=&#39;NaN&#39;, strategy=&#39;mean&#39;, axis=0)), (&#39;scaler&#39;, StandardScaler()), (&#39;elasticnet&#39;, ElasticNet())] # Create the pipeline: pipeline pipeline = Pipeline(steps) # Specify the hyperparameter space parameters = {&#39;elasticnet__l1_ratio&#39;:np.linspace(0,1,30)} # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Create the GridSearchCV object: gm_cv gm_cv = GridSearchCV(pipeline, parameters) # Fit to the training set gm_cv.fit(X_train, y_train) # Compute and print the metrics r2 = gm_cv.score(X_test, y_test) print(&quot;Tuned ElasticNet Alpha: {}&quot;.format(gm_cv.best_params_)) print(&quot;Tuned ElasticNet R squared: {}&quot;.format(r2)) # Tuned ElasticNet Alpha: {&#39;elasticnet__l1_ratio&#39;: 1.0} # Tuned ElasticNet R squared: 0.8862016570888217 . The End. . Thank you for reading. . . Classification . | ### Machine learning introduction . What is machine learning? Giving computers the ability to learn to make decisions from data without being explicitly programmed . Examples of machine learning: Learning to predict whether an email is spam or not (supervised) . Clustering Wikipedia entries into different categories (unsupervised) . #### Types of Machine Learning . supervised learning | unsupervised learning | reinforcement learning | . Supervised learning: . Predictor variables/ features and a target variable . Aim: Predict the target variable, given the predictor variables . Classification: Target variable consists of categories . Regression: Target variable is continuous . Unsupervised learning: . Uncovering hidden patterns from unlabeled data . Example of unsupervised learning: . Grouping customers into distinct categories (Clustering) . Reinforcement learning: Software agents interact with an environment . Learn how to optimize their behavior . Given a system of rewards and punishments . Applications . Economics . Genetics . Game playing (AlphaGo) . #### Naming conventions . Features = predictor variables = independent variables | Target variable = dependent variable = response variable | . . #### Features of Supervised learning . Automate time-consuming or expensive manual tasks (ex. Doctor’s diagnosis) | Make predictions about the future (ex. Will a customer click on an ad or not) | Need labeled data (Historical data with labels etc.) | . #### Popular libraries . scikit-learning (basic) | TensorFlow | keras | . . ### Exploratory data analysis . #### Numerical EDA . In this chapter, you’ll be working with a dataset obtained from the UCI Machine Learning Repository consisting of votes made by US House of Representatives Congressmen. . Your goal will be to predict their party affiliation (‘Democrat’ or ‘Republican’) based on how they voted on certain key issues. . Here, it’s worth noting that we have preprocessed this dataset to deal with missing values. This is so that your focus can be directed towards understanding how to train and evaluate supervised learning models. . Once you have mastered these fundamentals, you will be introduced to preprocessing techniques in Chapter 4 and have the chance to apply them there yourself – including on this very same dataset! . Before thinking about what supervised learning models you can apply to this, however, you need to perform Exploratory data analysis (EDA) in order to understand the structure of the data. . df.head() party infants water budget physician salvador religious 0 republican 0 1 0 1 1 1 1 republican 0 1 0 1 1 1 2 democrat 0 1 1 0 1 1 3 democrat 0 1 1 0 1 1 4 democrat 1 1 1 0 1 1 satellite aid missile immigration synfuels education superfund 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 1 2 0 0 0 0 1 0 1 3 0 0 0 0 1 0 1 4 0 0 0 0 1 0 1 crime duty_free_exports eaa_rsa 0 1 0 1 1 1 0 1 2 1 0 0 3 0 0 1 4 1 1 1 . df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 435 entries, 0 to 434 Data columns (total 17 columns): party 435 non-null object infants 435 non-null int64 water 435 non-null int64 budget 435 non-null int64 physician 435 non-null int64 salvador 435 non-null int64 religious 435 non-null int64 satellite 435 non-null int64 aid 435 non-null int64 missile 435 non-null int64 immigration 435 non-null int64 synfuels 435 non-null int64 education 435 non-null int64 superfund 435 non-null int64 crime 435 non-null int64 duty_free_exports 435 non-null int64 eaa_rsa 435 non-null int64 dtypes: int64(16), object(1) memory usage: 57.9+ KB . ### Visual EDA . . Above is a countplot of the &#39;education&#39; bill, generated from the following code: . plt.figure() sns.countplot(x=&#39;education&#39;, hue=&#39;party&#39;, data=df, palette=&#39;RdBu&#39;) plt.xticks([0,1], [&#39;No&#39;, &#39;Yes&#39;]) plt.show() . In sns.countplot() , we specify the x-axis data to be &#39;education&#39; , and hue to be &#39;party&#39; . Recall that &#39;party&#39; is also our target variable. So the resulting plot shows the difference in voting behavior between the two parties for the &#39;education&#39; bill, with each party colored differently. We manually specified the color to be &#39;RdBu&#39; , as the Republican party has been traditionally associated with red, and the Democratic party with blue. . It seems like Democrats voted resoundingly against this bill, compared to Republicans. . plt.figure() sns.countplot(x=&#39;missile&#39;, hue=&#39;party&#39;, data=df, palette=&#39;RdBu&#39;) plt.xticks([0,1], [&#39;No&#39;, &#39;Yes&#39;]) plt.show() . . Democrats vote resoundingly in favor of missile, compared to Republicans. . . ### The classification challenge . #### k-Nearest Neighbors: Fit . k-nearest neighbors algorithm . # Import KNeighborsClassifier from sklearn.neighbors from sklearn.neighbors import KNeighborsClassifier # Create arrays for the features and the response variable y = df[&#39;party&#39;].values X = df.drop(&#39;party&#39;, axis=1).values # Create a k-NN classifier with 6 neighbors knn = KNeighborsClassifier(n_neighbors=6) # Fit the classifier to the data knn.fit(X, y) . #### k-Nearest Neighbors: Predict . # Import KNeighborsClassifier from sklearn.neighbors from sklearn.neighbors import KNeighborsClassifier # Create arrays for the features and the response variable y = df[&#39;party&#39;] X = df.drop(&#39;party&#39;, axis=1).values # Create a k-NN classifier with 6 neighbors: knn knn = KNeighborsClassifier(n_neighbors=6) # Fit the classifier to the data knn.fit(X, y) # Predict the labels for the training data X y_pred = knn.predict(X) # Predict and print the label for the new data point X_new new_prediction = knn.predict(X_new) print(&quot;Prediction: {}&quot;.format(new_prediction)) # Prediction: [&#39;democrat&#39;] . How sure can you be of its predictions? In other words, how can you measure its performance? . . ### Measuring model performance . #### The digits recognition dataset: MNIST . In the following exercises, you’ll be working with the MNIST digits recognition dataset, which has 10 classes, the digits 0 through 9! A reduced version of the MNIST dataset is one of scikit-learn’s included datasets, and that is the one we will use in this exercise. . Each sample in this scikit-learn dataset is an 8×8 image representing a handwritten digit. Each pixel is represented by an integer in the range 0 to 16, indicating varying levels of black. . It is a famous dataset in machine learning and computer vision, and frequently used as a benchmark to evaluate the performance of a new model. . # Import necessary modules from sklearn import datasets import matplotlib.pyplot as plt # Load the digits dataset: digits digits = datasets.load_digits() # Print the keys and DESCR of the dataset print(digits.keys()) #dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;target_names&#39;, &#39;images&#39;, &#39;DESCR&#39;]) print(digits.DESCR) /* Optical Recognition of Handwritten Digits Data Set =================================================== Notes -- Data Set Characteristics: :Number of Instances: 5620 :Number of Attributes: 64 :Attribute Information: 8x8 image of integer pixels in the range 0..16. :Missing Attribute Values: None :Creator: E. Alpaydin (alpaydin &#39;@&#39; boun.edu.tr) :Date: July; 1998 ... */ # Print the shape of the images and data keys print(digits.images.shape) (1797, 8, 8) print(digits.data.shape) (1797, 64) # Display digit 1010 plt.imshow(digits.images[1010], cmap=plt.cm.gray_r, interpolation=&#39;nearest&#39;) plt.show() . . #### Train/Test Split + Fit/Predict/Accuracy . # Import necessary modules from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import train_test_split # Create feature and target arrays X = digits.data y = digits.target # Split into training and test set # Stratify the split according to the labels so that they are distributed in the training and test sets as they are in the original dataset. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y) # Create a k-NN classifier with 7 neighbors: knn knn = KNeighborsClassifier(n_neighbors=7) # Fit the classifier to the training data knn.fit(X_train, y_train) # Print the accuracy print(knn.score(X_test, y_test)) # 0.983333333333 . Incredibly, this out of the box k-NN classifier with 7 neighbors has learned from the training data and predicted the labels of the images in the test set with 98% accuracy, and it did so in less than a second! This is one illustration of how incredibly useful machine learning techniques can be. . . #### Overfitting and underfitting . In this exercise, you will compute and plot the training and testing accuracy scores for a variety of different neighbor values. By observing how the accuracy scores differ for the training and testing sets with different values of k, you will develop your intuition for overfitting and underfitting. . # Setup arrays to store train and test accuracies neighbors = np.arange(1, 9) train_accuracy = np.empty(len(neighbors)) test_accuracy = np.empty(len(neighbors)) # Loop over different values of k for i, k in enumerate(neighbors): # Setup a k-NN Classifier with k neighbors: knn knn = KNeighborsClassifier(n_neighbors=k) # Fit the classifier to the training data knn.fit(X_train, y_train) #Compute accuracy on the training set train_accuracy[i] = knn.score(X_train, y_train) #Compute accuracy on the testing set test_accuracy[i] = knn.score(X_test, y_test) # Generate plot plt.title(&#39;k-NN: Varying Number of Neighbors&#39;) plt.plot(neighbors, test_accuracy, label = &#39;Testing Accuracy&#39;) plt.plot(neighbors, train_accuracy, label = &#39;Training Accuracy&#39;) plt.legend() plt.xlabel(&#39;Number of Neighbors&#39;) plt.ylabel(&#39;Accuracy&#39;) plt.show() . . It looks like the test accuracy is highest when using 3 and 5 neighbors. Using 8 neighbors or more seems to result in a simple model that underfits the data. . . 2. Regression . . ### Introduction to regression . #### Importing data for supervised learning . In this chapter, you will work with Gapminder data that we have consolidated into one CSV file available in the workspace as &#39;gapminder.csv&#39; . Specifically, your goal will be to use this data to predict the life expectancy in a given country based on features such as the country’s GDP, fertility rate, and population. . # Import numpy and pandas import numpy as np import pandas as pd # Read the CSV file into a DataFrame: df df = pd.read_csv(&#39;gapminder.csv&#39;) # Create arrays for features and target variable y = df[&#39;life&#39;].values X = df[&#39;fertility&#39;].values # Print the dimensions of X and y before reshaping print(&quot;Dimensions of y before reshaping: {}&quot;.format(y.shape)) print(&quot;Dimensions of X before reshaping: {}&quot;.format(X.shape)) # Dimensions of y before reshaping: (139,) # Dimensions of X before reshaping: (139,) # Reshape X and y y = y.reshape(-1,1) X = X.reshape(-1,1) # Print the dimensions of X and y after reshaping print(&quot;Dimensions of y after reshaping: {}&quot;.format(y.shape)) print(&quot;Dimensions of X after reshaping: {}&quot;.format(X.shape)) # Dimensions of y after reshaping: (139, 1) # Dimensions of X after reshaping: (139, 1) . #### Exploring the Gapminder data . df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 139 entries, 0 to 138 Data columns (total 9 columns): population 139 non-null float64 fertility 139 non-null float64 HIV 139 non-null float64 CO2 139 non-null float64 BMI_male 139 non-null float64 GDP 139 non-null float64 BMI_female 139 non-null float64 life 139 non-null float64 child_mortality 139 non-null float64 dtypes: float64(9) memory usage: 9.9 KB . sns.heatmap(df.corr(), square=True, cmap=&#39;RdYlGn&#39;) plt.show() . . ### The basics of linear regression . We suppose that y and x have a linear relationship that can be model by . y = ax + b . An linear regression is to find a, b that minimize the sum of the squared residual (= Ordinary Least Squares, OLS) . Why squared residual? . Residuals may be positive and negative. . They cancel each other. square residual can solve this problem. . . green lines are residuals . When we have n variables of x, . y = a1x1 + a2x2 + … an*xn + b . we find a1, a2, … an, b that minimize the sum of the squared residual. . #### Fit &amp; predict for regression . In this exercise, you will use the &#39;fertility&#39; feature of the Gapminder dataset. Since the goal is to predict life expectancy, the target variable here is &#39;life&#39; . . You will also compute and print the R2 score using sckit-learn’s .score() method. . # Import LinearRegression from sklearn.linear_model import LinearRegression # Create the regressor: reg reg = LinearRegression() # Create the prediction space prediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1,1) # Fit the model to the data reg.fit(X_fertility, y) # Compute predictions over the prediction space: y_pred y_pred = reg.predict(prediction_space) # Print R^2 print(reg.score(X_fertility, y)) 0.619244216774 # Plot regression line plt.plot(prediction_space, y_pred, color=&#39;black&#39;, linewidth=3) plt.show() . . #### Train/test split for regression . In this exercise, you will split the Gapminder dataset into training and testing sets, and then fit and predict a linear regression over all features. In addition to computing the R2 score, you will also compute the Root Mean Squared Error (RMSE), which is another commonly used metric to evaluate regression models. . # Import necessary modules from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split # Create training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42) # Create the regressor: reg_all reg_all = LinearRegression() # Fit the regressor to the training data reg_all.fit(X_train, y_train) # Predict on the test data: y_pred y_pred = reg_all.predict(X_test) # Compute and print R^2 and RMSE print(&quot;R^2: {}&quot;.format(reg_all.score(X_test, y_test))) # R^2: 0.838046873142936 rmse = np.sqrt(mean_squared_error(y_test, y_pred)) print(&quot;Root Mean Squared Error: {}&quot;.format(rmse)) # Root Mean Squared Error: 3.2476010800377213 . ### Cross-validation . What is cross validation? . https://en.wikipedia.org/wiki/Cross-validation_(statistics) . #### 5-fold cross-validation . In this exercise, you will practice 5-fold cross validation on the Gapminder data. By default, scikit-learn’s cross_val_score() function uses R2R2 as the metric of choice for regression. . # Import the necessary modules from sklearn.linear_model import LinearRegression from sklearn.model_selection import cross_val_score # Create a linear regression object: reg reg = LinearRegression() # Compute 5-fold cross-validation scores: cv_scores cv_scores = cross_val_score(reg, X, y, cv=5) # Print the 5-fold cross-validation scores print(cv_scores) # [ 0.81720569 0.82917058 0.90214134 0.80633989 0.94495637] print(&quot;Average 5-Fold CV Score: {}&quot;.format(np.mean(cv_scores))) # Average 5-Fold CV Score: 0.8599627722793232 . #### K-Fold CV comparison . Cross validation is essential but do not forget that the more folds you use, the more computationally expensive cross-validation becomes. In this exercise, you will explore this for yourself. Your job is to perform 3-fold cross-validation and then 10-fold cross-validation on the Gapminder dataset. . # Import necessary modules from sklearn.linear_model import LinearRegression from sklearn.model_selection import cross_val_score # Create a linear regression object: reg reg = LinearRegression() # Perform 3-fold CV cvscores_3 = cross_val_score(reg, X, y, cv=3) print(np.mean(cvscores_3)) # 0.871871278262 # Perform 10-fold CV cvscores_10 = cross_val_score(reg, X, y, cv=10) print(np.mean(cvscores_10)) # 0.843612862013 . %timeit cross_val_score(reg, X, y, cv=3) 100 loops, best of 3: 8.73 ms per loop %timeit cross_val_score(reg, X, y, cv=10) 10 loops, best of 3: 27.5 ms per loop . . ### Regularized regression . #### Regularization I: Lasso . In this exercise, you will fit a lasso regression to the Gapminder data you have been working with and plot the coefficients. Just as with the Boston data, you will find that the coefficients of some features are shrunk to 0, with only the most important ones remaining. . df.columns Index([&#39;population&#39;, &#39;fertility&#39;, &#39;HIV&#39;, &#39;CO2&#39;, &#39;BMI_male&#39;, &#39;GDP&#39;, &#39;BMI_female&#39;, &#39;life&#39;, &#39;child_mortality&#39;], dtype=&#39;object&#39;) X: [&#39;population&#39;, &#39;fertility&#39;, &#39;HIV&#39;, &#39;CO2&#39;, &#39;BMI_male&#39;, &#39;GDP&#39;, &#39;BMI_female&#39;, &#39;child_mortality&#39;] y: life . # Import Lasso from sklearn.linear_model import Lasso # Instantiate a lasso regressor: lasso lasso = Lasso(alpha=0.4, normalize=True) # Fit the regressor to the data lasso.fit(X, y) # Compute and print the coefficients lasso_coef = lasso.fit(X, y).coef_ print(lasso_coef) # [-0. -0. -0. 0. 0. 0. -0. -0.07087587] # Plot the coefficients plt.plot(range(len(df_columns)), lasso_coef) plt.xticks(range(len(df_columns)), df_columns.values, rotation=60) plt.margins(0.02) plt.show() . . According to the lasso algorithm, it seems like &#39;child_mortality&#39; is the most important feature when predicting life expectancy. . #### Regularization II: Ridge . Lasso is great for feature selection, but when building regression models, Ridge regression should be your first choice. . Recall that lasso performs regularization by adding to the loss function a penalty term of the absolute value of each coefficient multiplied by some alpha. This is also known as L1L1 regularization because the regularization term is the L1L1 norm of the coefficients. This is not the only way to regularize, however. . If instead you took the sum of the squared values of the coefficients multiplied by some alpha – like in Ridge regression – you would be computing the L2L2norm. In this exercise, you will practice fitting ridge regression models over a range of different alphas, and plot cross-validated R2R2 scores for each, using this function that we have defined for you, which plots the R2R2 score as well as standard error for each alpha: . def display_plot(cv_scores, cv_scores_std): fig = plt.figure() ax = fig.add_subplot(1,1,1) ax.plot(alpha_space, cv_scores) std_error = cv_scores_std / np.sqrt(10) ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2) ax.set_ylabel(&#39;CV Score +/- Std Error&#39;) ax.set_xlabel(&#39;Alpha&#39;) ax.axhline(np.max(cv_scores), linestyle=&#39;--&#39;, color=&#39;.5&#39;) ax.set_xlim([alpha_space[0], alpha_space[-1]]) ax.set_xscale(&#39;log&#39;) plt.show() . Don’t worry about the specifics of the above function works. The motivation behind this exercise is for you to see how the R2R2 score varies with different alphas, and to understand the importance of selecting the right value for alpha. You’ll learn how to tune alpha in the next chapter. . # Import necessary modules from sklearn.linear_model import Ridge from sklearn.model_selection import cross_val_score # Setup the array of alphas and lists to store scores alpha_space = np.logspace(-4, 0, 50) ridge_scores = [] ridge_scores_std = [] # Create a ridge regressor: ridge ridge = Ridge(normalize=True) # Compute scores over range of alphas for alpha in alpha_space: # Specify the alpha value to use: ridge.alpha ridge.alpha = alpha # Perform 10-fold CV: ridge_cv_scores ridge_cv_scores = cross_val_score(ridge, X, y, cv=10) # Append the mean of ridge_cv_scores to ridge_scores ridge_scores.append(np.mean(ridge_cv_scores)) # Append the std of ridge_cv_scores to ridge_scores_std ridge_scores_std.append(np.std(ridge_cv_scores)) # Display the plot display_plot(ridge_scores, ridge_scores_std) . . Notice how the cross-validation scores change with different alphas. . . 3. Fine-tuning your model . . ### confusion matrix . What is confusion matrix . https://en.wikipedia.org/wiki/Confusion_matrix . . sensitivity , recall , hit rate , or true positive rate (TPR) . . specificity , selectivity or true negative rate (TNR) . . precision or positive predictive value (PPV) . . accuracy (ACC) . . F1 score is the harmonic mean of precision and sensitivity . #### illustration for TPR, TNR and PPV . . source . #### Metrics for classification . In this exercise, you will dive more deeply into evaluating the performance of binary classifiers by computing a confusion matrix and generating a classification report. . Here, you’ll work with the PIMA Indians dataset obtained from the UCI Machine Learning Repository. The goal is to predict whether or not a given female patient will contract diabetes based on features such as BMI, age, and number of pregnancies. . Therefore, it is a binary classification problem. A target value of 0 indicates that the patient does not have diabetes, while a value of 1 indicates that the patient does have diabetes. . df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 768 entries, 0 to 767 Data columns (total 9 columns): pregnancies 768 non-null int64 glucose 768 non-null int64 diastolic 768 non-null int64 triceps 768 non-null float64 insulin 768 non-null float64 bmi 768 non-null float64 dpf 768 non-null float64 age 768 non-null int64 diabetes 768 non-null int64 dtypes: float64(4), int64(5) memory usage: 54.1 KB . # Import necessary modules from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix # Create training and test set X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Instantiate a k-NN classifier: knn knn = KNeighborsClassifier(n_neighbors=6) # Fit the classifier to the training data knn.fit(X_train, y_train) # Predict the labels of the test data: y_pred y_pred = knn.predict(X_test) # Generate the confusion matrix and classification report print(confusion_matrix(y_test, y_pred)) print(classification_report(y_test, y_pred)) . [[176 30] [ 52 50]] precision recall f1-score support 0 0.77 0.85 0.81 206 1 0.62 0.49 0.55 102 avg / total 0.72 0.73 0.72 308 . . ### Logistic regression and the ROC curve . What is logistic regression? . https://en.wikipedia.org/wiki/Logistic_regression . What is ROC? . Receiver operating characteristic . Further Reading: scikit-learn document . . #### Building a logistic regression model . X.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 768 entries, 0 to 767 Data columns (total 8 columns): pregnancies 768 non-null int64 glucose 768 non-null int64 diastolic 768 non-null int64 triceps 768 non-null float64 insulin 768 non-null float64 bmi 768 non-null float64 dpf 768 non-null float64 age 768 non-null int64 dtypes: float64(4), int64(4) memory usage: 48.1 KB y 0 1 1 0 2 1 .. 765 0 766 1 767 0 Name: diabetes, dtype: int64 . # Import the necessary modules from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import classification_report, confusion_matrix # Create training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42) # Create the classifier: logreg logreg = LogisticRegression() # Fit the classifier to the training data logreg.fit(X_train, y_train) # Predict the labels of the test set: y_pred y_pred = logreg.predict(X_test) # Compute and print the confusion matrix and classification report print(confusion_matrix(y_test, y_pred)) print(classification_report(y_test, y_pred)) . [[176 30] [ 35 67]] precision recall f1-score support 0 0.83 0.85 0.84 206 1 0.69 0.66 0.67 102 avg / total 0.79 0.79 0.79 308 . #### Plotting an ROC curve . .predict_proba() . # Import necessary modules from sklearn.metrics import roc_curve # Compute predicted probabilities: y_pred_prob y_pred_prob = logreg.predict_proba(X_test)[:,1] # Generate ROC curve values: fpr, tpr, thresholds fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob) # Plot ROC curve plt.plot([0, 1], [0, 1], &#39;k--&#39;) plt.plot(fpr, tpr) plt.xlabel(&#39;False Positive Rate&#39;) plt.ylabel(&#39;True Positive Rate&#39;) plt.title(&#39;ROC Curve&#39;) plt.show() . logreg.predict_proba(X_test) # False, True # 0, 1 # Negative, Positive array([[ 0.60409835, 0.39590165], [ 0.76042394, 0.23957606], [ 0.79670177, 0.20329823], ... [ 0.84686912, 0.15313088], [ 0.97617225, 0.02382775], [ 0.40380502, 0.59619498]]) . . #### Precision-recall Curve . There are other ways to visually evaluate model performance. One such way is the precision-recall curve, which is generated by plotting the precision and recall for different thresholds. . Note that here, the class is positive (1) if the individual has diabetes. . . A recall of 1 corresponds to a classifier with a low threshold in which all females who contract diabetes were correctly classified as such, at the expense of many misclassifications of those who did not have diabetes. | Precision is undefined for a classifier which makes no positive predictions, that is, classifies everyone as not having diabetes. | When the threshold is very close to 1, precision is also 1, because the classifier is absolutely certain about its predictions. | . recall or sensitivity, TPR = 1 means all true positive are detected. We can predict all to positive to get a recall of 1. . precision, PPV = 1 means no false positive are detected. We can predict less positive to get a higher precision. . #### Area under the ROC curve . #### AUC( Area Under the Curve ) computation . # diabetes data set df.head() pregnancies glucose diastolic triceps insulin bmi dpf age 0 6 148 72 35.00000 155.548223 33.6 0.627 50 1 1 85 66 29.00000 155.548223 26.6 0.351 31 2 8 183 64 29.15342 155.548223 23.3 0.672 32 3 1 89 66 23.00000 94.000000 28.1 0.167 21 4 0 137 40 35.00000 168.000000 43.1 2.288 33 diabetes 0 1 1 0 2 1 3 0 4 1 . # Import necessary modules from sklearn.metrics import roc_auc_score from sklearn.model_selection import cross_val_score # Compute predicted probabilities: y_pred_prob y_pred_prob = logreg.predict_proba(X_test)[:,1] # Compute and print AUC score print(&quot;AUC: {}&quot;.format(roc_auc_score(y_test, y_pred_prob))) # Compute cross-validated AUC scores: cv_auc cv_auc = cross_val_score(logreg, X, y, cv=5, scoring=&#39;roc_auc&#39;) # Print list of AUC scores print(&quot;AUC scores computed using 5-fold cross-validation: {}&quot;.format(cv_auc)) . AUC: 0.8254806777079764 AUC scores computed using 5-fold cross-validation: [ 0.80148148 0.8062963 0.81481481 0.86245283 0.8554717 ] . . ### Hyperparameter tuning . #### Hyperparameter tuning with GridSearchCV . You will now practice this yourself, but by using logistic regression on the diabetes dataset. . Like the alpha parameter of lasso and ridge regularization that you saw earlier, logistic regression also has a regularization parameter: CC. CC controls the inverse of the regularization strength, and this is what you will tune in this exercise. A large CC can lead to an overfit model, while a small CC can lead to an underfit model. . The hyperparameter space for CC has been setup for you. Your job is to use GridSearchCV and logistic regression to find the optimal CC in this hyperparameter space. . # diabetes data set . # Import necessary modules from sklearn.linear_model import LogisticRegression from sklearn.model_selection import GridSearchCV # Setup the hyperparameter grid c_space = np.logspace(-5, 8, 15) param_grid = {&#39;C&#39;: c_space} # Instantiate a logistic regression classifier: logreg logreg = LogisticRegression() # Instantiate the GridSearchCV object: logreg_cv logreg_cv = GridSearchCV(logreg, param_grid, cv=5) # Fit it to the data logreg_cv.fit(X, y) # Print the tuned parameters and score print(&quot;Tuned Logistic Regression Parameters: {}&quot;.format(logreg_cv.best_params_)) print(&quot;Best score is {}&quot;.format(logreg_cv.best_score_)) . Tuned Logistic Regression Parameters: {&#39;C&#39;: 3.7275937203149381} Best score is 0.7708333333333334 . #### Hyperparameter tuning with RandomizedSearchCV . GridSearchCV can be computationally expensive, especially if you are searching over a large hyperparameter space and dealing with multiple hyperparameters. A solution to this is to use RandomizedSearchCV , in which not all hyperparameter values are tried out. Instead, a fixed number of hyperparameter settings is sampled from specified probability distributions. You’ll practice using RandomizedSearchCV in this exercise and see how this works. . # diabetes data set . # Import necessary modules from scipy.stats import randint from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import RandomizedSearchCV # Setup the parameters and distributions to sample from: param_dist param_dist = {&quot;max_depth&quot;: [3, None], &quot;max_features&quot;: randint(1, 9), &quot;min_samples_leaf&quot;: randint(1, 9), &quot;criterion&quot;: [&quot;gini&quot;, &quot;entropy&quot;]} # Instantiate a Decision Tree classifier: tree tree = DecisionTreeClassifier() # Instantiate the RandomizedSearchCV object: tree_cv tree_cv = RandomizedSearchCV(tree, param_dist, cv=5) # Fit it to the data tree_cv.fit(X, y) # Print the tuned parameters and score print(&quot;Tuned Decision Tree Parameters: {}&quot;.format(tree_cv.best_params_)) print(&quot;Best score is {}&quot;.format(tree_cv.best_score_)) . Tuned Decision Tree Parameters: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 3, &#39;max_features&#39;: 7, &#39;min_samples_leaf&#39;: 1} Best score is 0.7317708333333334 . Note that RandomizedSearchCV will never outperform GridSearchCV . Instead, it is valuable because it saves on computation time. . ### Hold-out set for final evaluation . #### Hold-out set in practice I: Classification . In addition to CC, logistic regression has a &#39;penalty&#39; hyperparameter which specifies whether to use &#39;l1&#39; or &#39;l2&#39; regularization. Your job in this exercise is to create a hold-out set, tune the &#39;C&#39; and &#39;penalty&#39; hyperparameters of a logistic regression classifier using GridSearchCV on the training set. . # diabetes data set . # Import necessary modules from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.model_selection import GridSearchCV # Create the hyperparameter grid c_space = np.logspace(-5, 8, 15) param_grid = {&#39;C&#39;: c_space, &#39;penalty&#39;: [&#39;l1&#39;, &#39;l2&#39;]} # Instantiate the logistic regression classifier: logreg logreg = LogisticRegression() # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Instantiate the GridSearchCV object: logreg_cv logreg_cv = GridSearchCV(logreg, param_grid, cv=5) # Fit it to the training data logreg_cv.fit(X_train, y_train) # Print the optimal parameters and best score print(&quot;Tuned Logistic Regression Parameter: {}&quot;.format(logreg_cv.best_params_)) print(&quot;Tuned Logistic Regression Accuracy: {}&quot;.format(logreg_cv.best_score_)) . Tuned Logistic Regression Parameter: {&#39;C&#39;: 0.43939705607607948, &#39;penalty&#39;: &#39;l1&#39;} Tuned Logistic Regression Accuracy: 0.7652173913043478 . #### Hold-out set in practice II: Regression . Remember lasso and ridge regression from the previous chapter? Lasso used the L1 penalty to regularize, while ridge used the L2 penalty. There is another type of regularized regression known as the elastic net. In elastic net regularization, the penalty term is a linear combination of the L1 and L2 penalties: . a∗L1+b∗L2 . In scikit-learn, this term is represented by the &#39;l1_ratio&#39; parameter: An &#39;l1_ratio&#39; of 1 corresponds to an L1L1 penalty, and anything lower is a combination of L1L1 and L2L2. . In this exercise, you will GridSearchCV to tune the &#39;l1_ratio&#39; of an elastic net model trained on the Gapminder data. As in the previous exercise, use a hold-out set to evaluate your model’s performance. . df.head() population fertility HIV CO2 BMI_male GDP BMI_female life 0 34811059.0 2.73 0.1 3.328945 24.59620 12314.0 129.9049 75.3 1 19842251.0 6.43 2.0 1.474353 22.25083 7103.0 130.1247 58.3 2 40381860.0 2.24 0.5 4.785170 27.50170 14646.0 118.8915 75.5 3 2975029.0 1.40 0.1 1.804106 25.35542 7383.0 132.8108 72.5 4 21370348.0 1.96 0.1 18.016313 27.56373 41312.0 117.3755 81.5 child_mortality 0 29.5 1 192.0 2 15.4 3 20.0 4 5.2 . # Import necessary modules from sklearn.linear_model import ElasticNet from sklearn.metrics import mean_squared_error from sklearn.model_selection import GridSearchCV, train_test_split # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Create the hyperparameter grid l1_space = np.linspace(0, 1, 30) param_grid = {&#39;l1_ratio&#39;: l1_space} # Instantiate the ElasticNet regressor: elastic_net elastic_net = ElasticNet() # Setup the GridSearchCV object: gm_cv gm_cv = GridSearchCV(elastic_net, param_grid, cv=5) # Fit it to the training data gm_cv.fit(X_train, y_train) # Predict on the test set and compute metrics y_pred = gm_cv.predict(X_test) r2 = gm_cv.score(X_test, y_test) mse = mean_squared_error(y_test, y_pred) print(&quot;Tuned ElasticNet l1 ratio: {}&quot;.format(gm_cv.best_params_)) print(&quot;Tuned ElasticNet R squared: {}&quot;.format(r2)) print(&quot;Tuned ElasticNet MSE: {}&quot;.format(mse)) . Tuned ElasticNet l1 ratio: {&#39;l1_ratio&#39;: 0.20689655172413793} Tuned ElasticNet R squared: 0.8668305372460283 Tuned ElasticNet MSE: 10.05791413339844 . . Preprocessing and pipelines . ### Preprocessing data . #### Exploring categorical features . df.head() population fertility HIV CO2 BMI_male GDP BMI_female life 0 34811059.0 2.73 0.1 3.328945 24.59620 12314.0 129.9049 75.3 1 19842251.0 6.43 2.0 1.474353 22.25083 7103.0 130.1247 58.3 2 40381860.0 2.24 0.5 4.785170 27.50170 14646.0 118.8915 75.5 3 2975029.0 1.40 0.1 1.804106 25.35542 7383.0 132.8108 72.5 4 21370348.0 1.96 0.1 18.016313 27.56373 41312.0 117.3755 81.5 child_mortality Region 0 29.5 Middle East &amp; North Africa 1 192.0 Sub-Saharan Africa 2 15.4 America 3 20.0 Europe &amp; Central Asia 4 5.2 East Asia &amp; Pacific . # Import pandas import pandas as pd # Read &#39;gapminder.csv&#39; into a DataFrame: df df = pd.read_csv(&#39;gapminder.csv&#39;) # Create a boxplot of life expectancy per region df.boxplot(&#39;life&#39;, &#39;Region&#39;, rot=60) # Show the plot plt.show() . . #### Creating dummy variables . # Create dummy variables: df_region df_region = pd.get_dummies(df) # Print the columns of df_region print(df_region.columns) # Create dummy variables with drop_first=True: df_region df_region = pd.get_dummies(df, drop_first=True) # Print the new columns of df_region print(df_region.columns) . Index([&#39;population&#39;, &#39;fertility&#39;, &#39;HIV&#39;, &#39;CO2&#39;, &#39;BMI_male&#39;, &#39;GDP&#39;, &#39;BMI_female&#39;, &#39;life&#39;, &#39;child_mortality&#39;, &#39;Region_America&#39;, &#39;Region_East Asia &amp; Pacific&#39;, &#39;Region_Europe &amp; Central Asia&#39;, &#39;Region_Middle East &amp; North Africa&#39;, &#39;Region_South Asia&#39;, &#39;Region_Sub-Saharan Africa&#39;], dtype=&#39;object&#39;) # Region_America has been dropped Index([&#39;population&#39;, &#39;fertility&#39;, &#39;HIV&#39;, &#39;CO2&#39;, &#39;BMI_male&#39;, &#39;GDP&#39;, &#39;BMI_female&#39;, &#39;life&#39;, &#39;child_mortality&#39;, &#39;Region_East Asia &amp; Pacific&#39;, &#39;Region_Europe &amp; Central Asia&#39;, &#39;Region_Middle East &amp; North Africa&#39;, &#39;Region_South Asia&#39;, &#39;Region_Sub-Saharan Africa&#39;], dtype=&#39;object&#39;) df_region.head() population fertility HIV CO2 BMI_male GDP BMI_female life 0 34811059.0 2.73 0.1 3.328945 24.59620 12314.0 129.9049 75.3 1 19842251.0 6.43 2.0 1.474353 22.25083 7103.0 130.1247 58.3 2 40381860.0 2.24 0.5 4.785170 27.50170 14646.0 118.8915 75.5 child_mortality Region_East Asia &amp; Pacific Region_Europe &amp; Central Asia 0 29.5 0 0 1 192.0 0 0 2 15.4 0 0 Region_Middle East &amp; North Africa Region_South Asia 0 1 0 1 0 0 2 0 0 Region_Sub-Saharan Africa 0 0 1 1 2 0 . #### Regression with categorical features . # Import necessary modules from sklearn.linear_model import Ridge from sklearn.model_selection import cross_val_score # Instantiate a ridge regressor: ridge ridge = Ridge(alpha=0.5, normalize=True) # Perform 5-fold cross-validation: ridge_cv ridge_cv = cross_val_score(ridge, X, y, cv=5) # Print the cross-validated scores print(ridge_cv) [ 0.86808336 0.80623545 0.84004203 0.7754344 0.87503712] . . ### Handling missing data . #### Dropping missing data . Now, it’s time for you to take care of these yourself! . The unprocessed dataset has been loaded into a DataFrame df . Explore it in the IPython Shell with the .head() method. You will see that there are certain data points labeled with a &#39;?&#39; . These denote missing values. As you saw in the video, different datasets encode missing values in different ways. Sometimes it may be a &#39;9999&#39; , other times a 0 – real-world data can be very messy! If you’re lucky, the missing values will already be encoded as NaN . We use NaN because it is an efficient and simplified way of internally representing missing data, and it lets us take advantage of pandas methods such as .dropna() and .fillna() , as well as scikit-learn’s Imputation transformer Imputer() . . In this exercise, your job is to convert the &#39;?&#39; s to NaNs, and then drop the rows that contain them from the DataFrame. . df.head(3) party infants water budget physician salvador religious satellite aid 0 republican 0 1 0 1 1 1 0 0 1 republican 0 1 0 1 1 1 0 0 2 democrat ? 1 1 ? 1 1 0 0 missile immigration synfuels education superfund crime duty_free_exports 0 0 1 ? 1 1 1 0 1 0 0 0 1 1 1 0 2 0 0 1 0 1 1 0 eaa_rsa 0 1 1 ? 2 0 . # Convert &#39;?&#39; to NaN df[df == &#39;?&#39;] = np.nan # Print the number of NaNs print(df.isnull().sum()) # Print shape of original DataFrame print(&quot;Shape of Original DataFrame: {}&quot;.format(df.shape)) # Drop missing values and print shape of new DataFrame df = df.dropna() # Print shape of new DataFrame print(&quot;Shape of DataFrame After Dropping All Rows with Missing Values: {}&quot;.format(df.shape)) . party 0 infants 12 water 48 budget 11 physician 11 salvador 15 religious 11 satellite 14 aid 15 missile 22 immigration 7 synfuels 21 education 31 superfund 25 crime 17 duty_free_exports 28 eaa_rsa 104 dtype: int64 Shape of Original DataFrame: (435, 17) Shape of DataFrame After Dropping All Rows with Missing Values: (232, 17) . When many values in your dataset are missing, if you drop them, you may end up throwing away valuable information along with the missing data. It’s better instead to develop an imputation strategy. This is where domain knowledge is useful, but in the absence of it, you can impute missing values with the mean or the median of the row or column that the missing value is in. . #### Imputing missing data in a ML Pipeline I . As you’ve come to appreciate, there are many steps to building a model, from creating training and test sets, to fitting a classifier or regressor, to tuning its parameters, to evaluating its performance on new data. Imputation can be seen as the first step of this machine learning process, the entirety of which can be viewed within the context of a pipeline. Scikit-learn provides a pipeline constructor that allows you to piece together these steps into one process and thereby simplify your workflow. . You’ll now practice setting up a pipeline with two steps: the imputation step, followed by the instantiation of a classifier. You’ve seen three classifiers in this course so far: k-NN, logistic regression, and the decision tree. You will now be introduced to a fourth one – the Support Vector Machine, or SVM . . # Import the Imputer module from sklearn.preprocessing import Imputer from sklearn.svm import SVC # Setup the Imputation transformer: imp # axis=0 for column imp = Imputer(missing_values=&#39;NaN&#39;, strategy=&#39;most_frequent&#39;, axis=0) # Instantiate the SVC classifier: clf clf = SVC() # Setup the pipeline with the required steps: steps steps = [(&#39;imputation&#39;, imp), (&#39;SVM&#39;, clf)] . #### Imputing missing data in a ML Pipeline II . Having setup the steps of the pipeline in the previous exercise, you will now use it on the voting dataset to classify a Congressman’s party affiliation. . What makes pipelines so incredibly useful is the simple interface that they provide. You can use the .fit() and .predict() methods on pipelines just as you did with your classifiers and regressors! . # Import necessary modules from sklearn.preprocessing import Imputer from sklearn.pipeline import Pipeline from sklearn.svm import SVC # Setup the pipeline steps: steps steps = [(&#39;imputation&#39;, Imputer(missing_values=&#39;NaN&#39;, strategy=&#39;most_frequent&#39;, axis=0)), (&#39;SVM&#39;, SVC())] # Create the pipeline: pipeline pipeline = Pipeline(steps) # Create training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Fit the pipeline to the train set pipeline.fit(X_train, y_train) # Predict the labels of the test set y_pred = pipeline.predict(X_test) # Compute metrics print(classification_report(y_test, y_pred)) . precision recall f1-score support democrat 0.99 0.96 0.98 85 republican 0.94 0.98 0.96 46 avg / total 0.97 0.97 0.97 131 . ### Centering and scaling . #### Centering and scaling your data . You will now explore scaling for yourself on a new dataset – White Wine Quality ! . We have used the &#39;quality&#39; feature of the wine to create a binary target variable: If &#39;quality&#39; is less than 5 , the target variable is 1 , and otherwise, it is 0 . . Notice how some features seem to have different units of measurement. &#39;density&#39; , for instance, takes values between 0.98 and 1.04, while &#39;total sulfur dioxide&#39; ranges from 9 to 440. As a result, it may be worth scaling the features here. Your job in this exercise is to scale the features and compute the mean and standard deviation of the unscaled features compared to the scaled features. . # white wine quality data set df.head(3) fixed acidity volatile acidity citric acid residual sugar chlorides 0 7.0 0.27 0.36 20.7 0.045 1 6.3 0.30 0.34 1.6 0.049 2 8.1 0.28 0.40 6.9 0.050 free sulfur dioxide total sulfur dioxide density pH sulphates 0 45.0 170.0 1.0010 3.00 0.45 1 14.0 132.0 0.9940 3.30 0.49 2 30.0 97.0 0.9951 3.26 0.44 alcohol quality 0 8.8 6 1 9.5 6 2 10.1 6 . # Import scale from sklearn.preprocessing import scale # Scale the features: X_scaled X_scaled = scale(X) # Print the mean and standard deviation of the unscaled features print(&quot;Mean of Unscaled Features: {}&quot;.format(np.mean(X))) print(&quot;Standard Deviation of Unscaled Features: {}&quot;.format(np.std(X))) # Print the mean and standard deviation of the scaled features print(&quot;Mean of Scaled Features: {}&quot;.format(np.mean(X_scaled))) print(&quot;Standard Deviation of Scaled Features: {}&quot;.format(np.std(X_scaled))) . Mean of Unscaled Features: 18.432687072460002 Standard Deviation of Unscaled Features: 41.54494764094571 Mean of Scaled Features: 2.7314972981668206e-15 Standard Deviation of Scaled Features: 0.9999999999999999 . #### Centering and scaling in a pipeline . With regard to whether or not scaling is effective, the proof is in the pudding! See for yourself whether or not scaling the features of the White Wine Quality dataset has any impact on its performance. . You will use a k-NN classifier as part of a pipeline that includes scaling, and for the purposes of comparison, a k-NN classifier trained on the unscaled data has been provided. . # Import the necessary modules from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline # Setup the pipeline steps: steps steps = [(&#39;scaler&#39;, StandardScaler()), (&#39;knn&#39;, KNeighborsClassifier())] # Create the pipeline: pipeline pipeline = Pipeline(steps) # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Fit the pipeline to the training set: knn_scaled knn_scaled = pipeline.fit(X_train, y_train) # Instantiate and fit a k-NN classifier to the unscaled data knn_unscaled = KNeighborsClassifier().fit(X_train, y_train) # Compute and print metrics print(&#39;Accuracy with Scaling: {}&#39;.format(knn_scaled.score(X_test, y_test))) print(&#39;Accuracy without Scaling: {}&#39;.format(knn_unscaled.score(X_test, y_test))) Accuracy with Scaling: 0.7700680272108843 Accuracy without Scaling: 0.6979591836734694 . It looks like scaling has significantly improved model performance! . . #### Bringing it all together I: Pipeline for classification . It is time now to piece together everything you have learned so far into a pipeline for classification! Your job in this exercise is to build a pipeline that includes scaling and hyperparameter tuning to classify wine quality. . You’ll return to using the SVM classifier you were briefly introduced to earlier in this chapter. The hyperparameters you will tune are C and gamma. C controls the regularization strength. It is analogous to the C you tuned for logistic regression in Chapter 3, while gamma controls the kernel coefficient: . # Setup the pipeline steps = [(&#39;scaler&#39;, StandardScaler()), (&#39;SVM&#39;, SVC())] pipeline = Pipeline(steps) # Specify the hyperparameter space parameters = {&#39;SVM__C&#39;:[1, 10, 100], &#39;SVM__gamma&#39;:[0.1, 0.01]} # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21) # Instantiate the GridSearchCV object: cv cv = GridSearchCV(pipeline, parameters) # Fit to the training set cv.fit(X_train, y_train) # Predict the labels of the test set: y_pred y_pred = cv.predict(X_test) # Compute and print metrics print(&quot;Accuracy: {}&quot;.format(cv.score(X_test, y_test))) print(classification_report(y_test, y_pred)) print(&quot;Tuned Model Parameters: {}&quot;.format(cv.best_params_)) . Accuracy: 0.7795918367346939 precision recall f1-score support False 0.83 0.85 0.84 662 True 0.67 0.63 0.65 318 avg / total 0.78 0.78 0.78 980 Tuned Model Parameters: {&#39;SVM__C&#39;: 10, &#39;SVM__gamma&#39;: 0.1} . #### Bringing it all together II: Pipeline for regression . Your job is to build a pipeline that imputes the missing data, scales the features, and fits an ElasticNet to the Gapminder data. You will then tune the l1_ratio of your ElasticNet using GridSearchCV. . # Setup the pipeline steps: steps steps = [(&#39;imputation&#39;, Imputer(missing_values=&#39;NaN&#39;, strategy=&#39;mean&#39;, axis=0)), (&#39;scaler&#39;, StandardScaler()), (&#39;elasticnet&#39;, ElasticNet())] # Create the pipeline: pipeline pipeline = Pipeline(steps) # Specify the hyperparameter space parameters = {&#39;elasticnet__l1_ratio&#39;:np.linspace(0,1,30)} # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Create the GridSearchCV object: gm_cv gm_cv = GridSearchCV(pipeline, parameters) # Fit to the training set gm_cv.fit(X_train, y_train) # Compute and print the metrics r2 = gm_cv.score(X_test, y_test) print(&quot;Tuned ElasticNet Alpha: {}&quot;.format(gm_cv.best_params_)) print(&quot;Tuned ElasticNet R squared: {}&quot;.format(r2)) # Tuned ElasticNet Alpha: {&#39;elasticnet__l1_ratio&#39;: 1.0} # Tuned ElasticNet R squared: 0.8862016570888217 . The End. . Thank you for reading. . . Classification . | ### Machine learning introduction . What is machine learning? Giving computers the ability to learn to make decisions from data without being explicitly programmed . Examples of machine learning: Learning to predict whether an email is spam or not (supervised) . Clustering Wikipedia entries into different categories (unsupervised) . #### Types of Machine Learning . supervised learning | unsupervised learning | reinforcement learning | . Supervised learning: . Predictor variables/ features and a target variable . Aim: Predict the target variable, given the predictor variables . Classification: Target variable consists of categories . Regression: Target variable is continuous . Unsupervised learning: . Uncovering hidden patterns from unlabeled data . Example of unsupervised learning: . Grouping customers into distinct categories (Clustering) . Reinforcement learning: Software agents interact with an environment . Learn how to optimize their behavior . Given a system of rewards and punishments . Applications . Economics . Genetics . Game playing (AlphaGo) . #### Naming conventions . Features = predictor variables = independent variables | Target variable = dependent variable = response variable | . . #### Features of Supervised learning . Automate time-consuming or expensive manual tasks (ex. Doctor’s diagnosis) | Make predictions about the future (ex. Will a customer click on an ad or not) | Need labeled data (Historical data with labels etc.) | . #### Popular libraries . scikit-learning (basic) | TensorFlow | keras | . . ### Exploratory data analysis . #### Numerical EDA . In this chapter, you’ll be working with a dataset obtained from the UCI Machine Learning Repository consisting of votes made by US House of Representatives Congressmen. . Your goal will be to predict their party affiliation (‘Democrat’ or ‘Republican’) based on how they voted on certain key issues. . Here, it’s worth noting that we have preprocessed this dataset to deal with missing values. This is so that your focus can be directed towards understanding how to train and evaluate supervised learning models. . Once you have mastered these fundamentals, you will be introduced to preprocessing techniques in Chapter 4 and have the chance to apply them there yourself – including on this very same dataset! . Before thinking about what supervised learning models you can apply to this, however, you need to perform Exploratory data analysis (EDA) in order to understand the structure of the data. . df.head() party infants water budget physician salvador religious 0 republican 0 1 0 1 1 1 1 republican 0 1 0 1 1 1 2 democrat 0 1 1 0 1 1 3 democrat 0 1 1 0 1 1 4 democrat 1 1 1 0 1 1 satellite aid missile immigration synfuels education superfund 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 1 2 0 0 0 0 1 0 1 3 0 0 0 0 1 0 1 4 0 0 0 0 1 0 1 crime duty_free_exports eaa_rsa 0 1 0 1 1 1 0 1 2 1 0 0 3 0 0 1 4 1 1 1 . df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 435 entries, 0 to 434 Data columns (total 17 columns): party 435 non-null object infants 435 non-null int64 water 435 non-null int64 budget 435 non-null int64 physician 435 non-null int64 salvador 435 non-null int64 religious 435 non-null int64 satellite 435 non-null int64 aid 435 non-null int64 missile 435 non-null int64 immigration 435 non-null int64 synfuels 435 non-null int64 education 435 non-null int64 superfund 435 non-null int64 crime 435 non-null int64 duty_free_exports 435 non-null int64 eaa_rsa 435 non-null int64 dtypes: int64(16), object(1) memory usage: 57.9+ KB . ### Visual EDA . . Above is a countplot of the &#39;education&#39; bill, generated from the following code: . plt.figure() sns.countplot(x=&#39;education&#39;, hue=&#39;party&#39;, data=df, palette=&#39;RdBu&#39;) plt.xticks([0,1], [&#39;No&#39;, &#39;Yes&#39;]) plt.show() . In sns.countplot() , we specify the x-axis data to be &#39;education&#39; , and hue to be &#39;party&#39; . Recall that &#39;party&#39; is also our target variable. So the resulting plot shows the difference in voting behavior between the two parties for the &#39;education&#39; bill, with each party colored differently. We manually specified the color to be &#39;RdBu&#39; , as the Republican party has been traditionally associated with red, and the Democratic party with blue. . It seems like Democrats voted resoundingly against this bill, compared to Republicans. . plt.figure() sns.countplot(x=&#39;missile&#39;, hue=&#39;party&#39;, data=df, palette=&#39;RdBu&#39;) plt.xticks([0,1], [&#39;No&#39;, &#39;Yes&#39;]) plt.show() . . Democrats vote resoundingly in favor of missile, compared to Republicans. . . ### The classification challenge . #### k-Nearest Neighbors: Fit . k-nearest neighbors algorithm . # Import KNeighborsClassifier from sklearn.neighbors from sklearn.neighbors import KNeighborsClassifier # Create arrays for the features and the response variable y = df[&#39;party&#39;].values X = df.drop(&#39;party&#39;, axis=1).values # Create a k-NN classifier with 6 neighbors knn = KNeighborsClassifier(n_neighbors=6) # Fit the classifier to the data knn.fit(X, y) . #### k-Nearest Neighbors: Predict . # Import KNeighborsClassifier from sklearn.neighbors from sklearn.neighbors import KNeighborsClassifier # Create arrays for the features and the response variable y = df[&#39;party&#39;] X = df.drop(&#39;party&#39;, axis=1).values # Create a k-NN classifier with 6 neighbors: knn knn = KNeighborsClassifier(n_neighbors=6) # Fit the classifier to the data knn.fit(X, y) # Predict the labels for the training data X y_pred = knn.predict(X) # Predict and print the label for the new data point X_new new_prediction = knn.predict(X_new) print(&quot;Prediction: {}&quot;.format(new_prediction)) # Prediction: [&#39;democrat&#39;] . How sure can you be of its predictions? In other words, how can you measure its performance? . . ### Measuring model performance . #### The digits recognition dataset: MNIST . In the following exercises, you’ll be working with the MNIST digits recognition dataset, which has 10 classes, the digits 0 through 9! A reduced version of the MNIST dataset is one of scikit-learn’s included datasets, and that is the one we will use in this exercise. . Each sample in this scikit-learn dataset is an 8×8 image representing a handwritten digit. Each pixel is represented by an integer in the range 0 to 16, indicating varying levels of black. . It is a famous dataset in machine learning and computer vision, and frequently used as a benchmark to evaluate the performance of a new model. . # Import necessary modules from sklearn import datasets import matplotlib.pyplot as plt # Load the digits dataset: digits digits = datasets.load_digits() # Print the keys and DESCR of the dataset print(digits.keys()) #dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;target_names&#39;, &#39;images&#39;, &#39;DESCR&#39;]) print(digits.DESCR) /* Optical Recognition of Handwritten Digits Data Set =================================================== Notes -- Data Set Characteristics: :Number of Instances: 5620 :Number of Attributes: 64 :Attribute Information: 8x8 image of integer pixels in the range 0..16. :Missing Attribute Values: None :Creator: E. Alpaydin (alpaydin &#39;@&#39; boun.edu.tr) :Date: July; 1998 ... */ # Print the shape of the images and data keys print(digits.images.shape) (1797, 8, 8) print(digits.data.shape) (1797, 64) # Display digit 1010 plt.imshow(digits.images[1010], cmap=plt.cm.gray_r, interpolation=&#39;nearest&#39;) plt.show() . . #### Train/Test Split + Fit/Predict/Accuracy . # Import necessary modules from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import train_test_split # Create feature and target arrays X = digits.data y = digits.target # Split into training and test set # Stratify the split according to the labels so that they are distributed in the training and test sets as they are in the original dataset. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y) # Create a k-NN classifier with 7 neighbors: knn knn = KNeighborsClassifier(n_neighbors=7) # Fit the classifier to the training data knn.fit(X_train, y_train) # Print the accuracy print(knn.score(X_test, y_test)) # 0.983333333333 . Incredibly, this out of the box k-NN classifier with 7 neighbors has learned from the training data and predicted the labels of the images in the test set with 98% accuracy, and it did so in less than a second! This is one illustration of how incredibly useful machine learning techniques can be. . . #### Overfitting and underfitting . In this exercise, you will compute and plot the training and testing accuracy scores for a variety of different neighbor values. By observing how the accuracy scores differ for the training and testing sets with different values of k, you will develop your intuition for overfitting and underfitting. . # Setup arrays to store train and test accuracies neighbors = np.arange(1, 9) train_accuracy = np.empty(len(neighbors)) test_accuracy = np.empty(len(neighbors)) # Loop over different values of k for i, k in enumerate(neighbors): # Setup a k-NN Classifier with k neighbors: knn knn = KNeighborsClassifier(n_neighbors=k) # Fit the classifier to the training data knn.fit(X_train, y_train) #Compute accuracy on the training set train_accuracy[i] = knn.score(X_train, y_train) #Compute accuracy on the testing set test_accuracy[i] = knn.score(X_test, y_test) # Generate plot plt.title(&#39;k-NN: Varying Number of Neighbors&#39;) plt.plot(neighbors, test_accuracy, label = &#39;Testing Accuracy&#39;) plt.plot(neighbors, train_accuracy, label = &#39;Training Accuracy&#39;) plt.legend() plt.xlabel(&#39;Number of Neighbors&#39;) plt.ylabel(&#39;Accuracy&#39;) plt.show() . . It looks like the test accuracy is highest when using 3 and 5 neighbors. Using 8 neighbors or more seems to result in a simple model that underfits the data. . . 2. Regression . . ### Introduction to regression . #### Importing data for supervised learning . In this chapter, you will work with Gapminder data that we have consolidated into one CSV file available in the workspace as &#39;gapminder.csv&#39; . Specifically, your goal will be to use this data to predict the life expectancy in a given country based on features such as the country’s GDP, fertility rate, and population. . # Import numpy and pandas import numpy as np import pandas as pd # Read the CSV file into a DataFrame: df df = pd.read_csv(&#39;gapminder.csv&#39;) # Create arrays for features and target variable y = df[&#39;life&#39;].values X = df[&#39;fertility&#39;].values # Print the dimensions of X and y before reshaping print(&quot;Dimensions of y before reshaping: {}&quot;.format(y.shape)) print(&quot;Dimensions of X before reshaping: {}&quot;.format(X.shape)) # Dimensions of y before reshaping: (139,) # Dimensions of X before reshaping: (139,) # Reshape X and y y = y.reshape(-1,1) X = X.reshape(-1,1) # Print the dimensions of X and y after reshaping print(&quot;Dimensions of y after reshaping: {}&quot;.format(y.shape)) print(&quot;Dimensions of X after reshaping: {}&quot;.format(X.shape)) # Dimensions of y after reshaping: (139, 1) # Dimensions of X after reshaping: (139, 1) . #### Exploring the Gapminder data . df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 139 entries, 0 to 138 Data columns (total 9 columns): population 139 non-null float64 fertility 139 non-null float64 HIV 139 non-null float64 CO2 139 non-null float64 BMI_male 139 non-null float64 GDP 139 non-null float64 BMI_female 139 non-null float64 life 139 non-null float64 child_mortality 139 non-null float64 dtypes: float64(9) memory usage: 9.9 KB . sns.heatmap(df.corr(), square=True, cmap=&#39;RdYlGn&#39;) plt.show() . . ### The basics of linear regression . We suppose that y and x have a linear relationship that can be model by . y = ax + b . An linear regression is to find a, b that minimize the sum of the squared residual (= Ordinary Least Squares, OLS) . Why squared residual? . Residuals may be positive and negative. . They cancel each other. square residual can solve this problem. . . green lines are residuals . When we have n variables of x, . y = a1x1 + a2x2 + … an*xn + b . we find a1, a2, … an, b that minimize the sum of the squared residual. . #### Fit &amp; predict for regression . In this exercise, you will use the &#39;fertility&#39; feature of the Gapminder dataset. Since the goal is to predict life expectancy, the target variable here is &#39;life&#39; . . You will also compute and print the R2 score using sckit-learn’s .score() method. . # Import LinearRegression from sklearn.linear_model import LinearRegression # Create the regressor: reg reg = LinearRegression() # Create the prediction space prediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1,1) # Fit the model to the data reg.fit(X_fertility, y) # Compute predictions over the prediction space: y_pred y_pred = reg.predict(prediction_space) # Print R^2 print(reg.score(X_fertility, y)) 0.619244216774 # Plot regression line plt.plot(prediction_space, y_pred, color=&#39;black&#39;, linewidth=3) plt.show() . . #### Train/test split for regression . In this exercise, you will split the Gapminder dataset into training and testing sets, and then fit and predict a linear regression over all features. In addition to computing the R2 score, you will also compute the Root Mean Squared Error (RMSE), which is another commonly used metric to evaluate regression models. . # Import necessary modules from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split # Create training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42) # Create the regressor: reg_all reg_all = LinearRegression() # Fit the regressor to the training data reg_all.fit(X_train, y_train) # Predict on the test data: y_pred y_pred = reg_all.predict(X_test) # Compute and print R^2 and RMSE print(&quot;R^2: {}&quot;.format(reg_all.score(X_test, y_test))) # R^2: 0.838046873142936 rmse = np.sqrt(mean_squared_error(y_test, y_pred)) print(&quot;Root Mean Squared Error: {}&quot;.format(rmse)) # Root Mean Squared Error: 3.2476010800377213 . ### Cross-validation . What is cross validation? . https://en.wikipedia.org/wiki/Cross-validation_(statistics) . #### 5-fold cross-validation . In this exercise, you will practice 5-fold cross validation on the Gapminder data. By default, scikit-learn’s cross_val_score() function uses R2R2 as the metric of choice for regression. . # Import the necessary modules from sklearn.linear_model import LinearRegression from sklearn.model_selection import cross_val_score # Create a linear regression object: reg reg = LinearRegression() # Compute 5-fold cross-validation scores: cv_scores cv_scores = cross_val_score(reg, X, y, cv=5) # Print the 5-fold cross-validation scores print(cv_scores) # [ 0.81720569 0.82917058 0.90214134 0.80633989 0.94495637] print(&quot;Average 5-Fold CV Score: {}&quot;.format(np.mean(cv_scores))) # Average 5-Fold CV Score: 0.8599627722793232 . #### K-Fold CV comparison . Cross validation is essential but do not forget that the more folds you use, the more computationally expensive cross-validation becomes. In this exercise, you will explore this for yourself. Your job is to perform 3-fold cross-validation and then 10-fold cross-validation on the Gapminder dataset. . # Import necessary modules from sklearn.linear_model import LinearRegression from sklearn.model_selection import cross_val_score # Create a linear regression object: reg reg = LinearRegression() # Perform 3-fold CV cvscores_3 = cross_val_score(reg, X, y, cv=3) print(np.mean(cvscores_3)) # 0.871871278262 # Perform 10-fold CV cvscores_10 = cross_val_score(reg, X, y, cv=10) print(np.mean(cvscores_10)) # 0.843612862013 . %timeit cross_val_score(reg, X, y, cv=3) 100 loops, best of 3: 8.73 ms per loop %timeit cross_val_score(reg, X, y, cv=10) 10 loops, best of 3: 27.5 ms per loop . . ### Regularized regression . #### Regularization I: Lasso . In this exercise, you will fit a lasso regression to the Gapminder data you have been working with and plot the coefficients. Just as with the Boston data, you will find that the coefficients of some features are shrunk to 0, with only the most important ones remaining. . df.columns Index([&#39;population&#39;, &#39;fertility&#39;, &#39;HIV&#39;, &#39;CO2&#39;, &#39;BMI_male&#39;, &#39;GDP&#39;, &#39;BMI_female&#39;, &#39;life&#39;, &#39;child_mortality&#39;], dtype=&#39;object&#39;) X: [&#39;population&#39;, &#39;fertility&#39;, &#39;HIV&#39;, &#39;CO2&#39;, &#39;BMI_male&#39;, &#39;GDP&#39;, &#39;BMI_female&#39;, &#39;child_mortality&#39;] y: life . # Import Lasso from sklearn.linear_model import Lasso # Instantiate a lasso regressor: lasso lasso = Lasso(alpha=0.4, normalize=True) # Fit the regressor to the data lasso.fit(X, y) # Compute and print the coefficients lasso_coef = lasso.fit(X, y).coef_ print(lasso_coef) # [-0. -0. -0. 0. 0. 0. -0. -0.07087587] # Plot the coefficients plt.plot(range(len(df_columns)), lasso_coef) plt.xticks(range(len(df_columns)), df_columns.values, rotation=60) plt.margins(0.02) plt.show() . . According to the lasso algorithm, it seems like &#39;child_mortality&#39; is the most important feature when predicting life expectancy. . #### Regularization II: Ridge . Lasso is great for feature selection, but when building regression models, Ridge regression should be your first choice. . Recall that lasso performs regularization by adding to the loss function a penalty term of the absolute value of each coefficient multiplied by some alpha. This is also known as L1L1 regularization because the regularization term is the L1L1 norm of the coefficients. This is not the only way to regularize, however. . If instead you took the sum of the squared values of the coefficients multiplied by some alpha – like in Ridge regression – you would be computing the L2L2norm. In this exercise, you will practice fitting ridge regression models over a range of different alphas, and plot cross-validated R2R2 scores for each, using this function that we have defined for you, which plots the R2R2 score as well as standard error for each alpha: . def display_plot(cv_scores, cv_scores_std): fig = plt.figure() ax = fig.add_subplot(1,1,1) ax.plot(alpha_space, cv_scores) std_error = cv_scores_std / np.sqrt(10) ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2) ax.set_ylabel(&#39;CV Score +/- Std Error&#39;) ax.set_xlabel(&#39;Alpha&#39;) ax.axhline(np.max(cv_scores), linestyle=&#39;--&#39;, color=&#39;.5&#39;) ax.set_xlim([alpha_space[0], alpha_space[-1]]) ax.set_xscale(&#39;log&#39;) plt.show() . Don’t worry about the specifics of the above function works. The motivation behind this exercise is for you to see how the R2R2 score varies with different alphas, and to understand the importance of selecting the right value for alpha. You’ll learn how to tune alpha in the next chapter. . # Import necessary modules from sklearn.linear_model import Ridge from sklearn.model_selection import cross_val_score # Setup the array of alphas and lists to store scores alpha_space = np.logspace(-4, 0, 50) ridge_scores = [] ridge_scores_std = [] # Create a ridge regressor: ridge ridge = Ridge(normalize=True) # Compute scores over range of alphas for alpha in alpha_space: # Specify the alpha value to use: ridge.alpha ridge.alpha = alpha # Perform 10-fold CV: ridge_cv_scores ridge_cv_scores = cross_val_score(ridge, X, y, cv=10) # Append the mean of ridge_cv_scores to ridge_scores ridge_scores.append(np.mean(ridge_cv_scores)) # Append the std of ridge_cv_scores to ridge_scores_std ridge_scores_std.append(np.std(ridge_cv_scores)) # Display the plot display_plot(ridge_scores, ridge_scores_std) . . Notice how the cross-validation scores change with different alphas. . . 3. Fine-tuning your model . . ### confusion matrix . What is confusion matrix . https://en.wikipedia.org/wiki/Confusion_matrix . . sensitivity , recall , hit rate , or true positive rate (TPR) . . specificity , selectivity or true negative rate (TNR) . . precision or positive predictive value (PPV) . . accuracy (ACC) . . F1 score is the harmonic mean of precision and sensitivity . #### illustration for TPR, TNR and PPV . . source . #### Metrics for classification . In this exercise, you will dive more deeply into evaluating the performance of binary classifiers by computing a confusion matrix and generating a classification report. . Here, you’ll work with the PIMA Indians dataset obtained from the UCI Machine Learning Repository. The goal is to predict whether or not a given female patient will contract diabetes based on features such as BMI, age, and number of pregnancies. . Therefore, it is a binary classification problem. A target value of 0 indicates that the patient does not have diabetes, while a value of 1 indicates that the patient does have diabetes. . df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 768 entries, 0 to 767 Data columns (total 9 columns): pregnancies 768 non-null int64 glucose 768 non-null int64 diastolic 768 non-null int64 triceps 768 non-null float64 insulin 768 non-null float64 bmi 768 non-null float64 dpf 768 non-null float64 age 768 non-null int64 diabetes 768 non-null int64 dtypes: float64(4), int64(5) memory usage: 54.1 KB . # Import necessary modules from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix # Create training and test set X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Instantiate a k-NN classifier: knn knn = KNeighborsClassifier(n_neighbors=6) # Fit the classifier to the training data knn.fit(X_train, y_train) # Predict the labels of the test data: y_pred y_pred = knn.predict(X_test) # Generate the confusion matrix and classification report print(confusion_matrix(y_test, y_pred)) print(classification_report(y_test, y_pred)) . [[176 30] [ 52 50]] precision recall f1-score support 0 0.77 0.85 0.81 206 1 0.62 0.49 0.55 102 avg / total 0.72 0.73 0.72 308 . . ### Logistic regression and the ROC curve . What is logistic regression? . https://en.wikipedia.org/wiki/Logistic_regression . What is ROC? . Receiver operating characteristic . Further Reading: scikit-learn document . . #### Building a logistic regression model . X.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 768 entries, 0 to 767 Data columns (total 8 columns): pregnancies 768 non-null int64 glucose 768 non-null int64 diastolic 768 non-null int64 triceps 768 non-null float64 insulin 768 non-null float64 bmi 768 non-null float64 dpf 768 non-null float64 age 768 non-null int64 dtypes: float64(4), int64(4) memory usage: 48.1 KB y 0 1 1 0 2 1 .. 765 0 766 1 767 0 Name: diabetes, dtype: int64 . # Import the necessary modules from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import classification_report, confusion_matrix # Create training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42) # Create the classifier: logreg logreg = LogisticRegression() # Fit the classifier to the training data logreg.fit(X_train, y_train) # Predict the labels of the test set: y_pred y_pred = logreg.predict(X_test) # Compute and print the confusion matrix and classification report print(confusion_matrix(y_test, y_pred)) print(classification_report(y_test, y_pred)) . [[176 30] [ 35 67]] precision recall f1-score support 0 0.83 0.85 0.84 206 1 0.69 0.66 0.67 102 avg / total 0.79 0.79 0.79 308 . #### Plotting an ROC curve . .predict_proba() . # Import necessary modules from sklearn.metrics import roc_curve # Compute predicted probabilities: y_pred_prob y_pred_prob = logreg.predict_proba(X_test)[:,1] # Generate ROC curve values: fpr, tpr, thresholds fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob) # Plot ROC curve plt.plot([0, 1], [0, 1], &#39;k--&#39;) plt.plot(fpr, tpr) plt.xlabel(&#39;False Positive Rate&#39;) plt.ylabel(&#39;True Positive Rate&#39;) plt.title(&#39;ROC Curve&#39;) plt.show() . logreg.predict_proba(X_test) # False, True # 0, 1 # Negative, Positive array([[ 0.60409835, 0.39590165], [ 0.76042394, 0.23957606], [ 0.79670177, 0.20329823], ... [ 0.84686912, 0.15313088], [ 0.97617225, 0.02382775], [ 0.40380502, 0.59619498]]) . . #### Precision-recall Curve . There are other ways to visually evaluate model performance. One such way is the precision-recall curve, which is generated by plotting the precision and recall for different thresholds. . Note that here, the class is positive (1) if the individual has diabetes. . . A recall of 1 corresponds to a classifier with a low threshold in which all females who contract diabetes were correctly classified as such, at the expense of many misclassifications of those who did not have diabetes. | Precision is undefined for a classifier which makes no positive predictions, that is, classifies everyone as not having diabetes. | When the threshold is very close to 1, precision is also 1, because the classifier is absolutely certain about its predictions. | . recall or sensitivity, TPR = 1 means all true positive are detected. We can predict all to positive to get a recall of 1. . precision, PPV = 1 means no false positive are detected. We can predict less positive to get a higher precision. . #### Area under the ROC curve . #### AUC( Area Under the Curve ) computation . # diabetes data set df.head() pregnancies glucose diastolic triceps insulin bmi dpf age 0 6 148 72 35.00000 155.548223 33.6 0.627 50 1 1 85 66 29.00000 155.548223 26.6 0.351 31 2 8 183 64 29.15342 155.548223 23.3 0.672 32 3 1 89 66 23.00000 94.000000 28.1 0.167 21 4 0 137 40 35.00000 168.000000 43.1 2.288 33 diabetes 0 1 1 0 2 1 3 0 4 1 . # Import necessary modules from sklearn.metrics import roc_auc_score from sklearn.model_selection import cross_val_score # Compute predicted probabilities: y_pred_prob y_pred_prob = logreg.predict_proba(X_test)[:,1] # Compute and print AUC score print(&quot;AUC: {}&quot;.format(roc_auc_score(y_test, y_pred_prob))) # Compute cross-validated AUC scores: cv_auc cv_auc = cross_val_score(logreg, X, y, cv=5, scoring=&#39;roc_auc&#39;) # Print list of AUC scores print(&quot;AUC scores computed using 5-fold cross-validation: {}&quot;.format(cv_auc)) . AUC: 0.8254806777079764 AUC scores computed using 5-fold cross-validation: [ 0.80148148 0.8062963 0.81481481 0.86245283 0.8554717 ] . . ### Hyperparameter tuning . #### Hyperparameter tuning with GridSearchCV . You will now practice this yourself, but by using logistic regression on the diabetes dataset. . Like the alpha parameter of lasso and ridge regularization that you saw earlier, logistic regression also has a regularization parameter: CC. CC controls the inverse of the regularization strength, and this is what you will tune in this exercise. A large CC can lead to an overfit model, while a small CC can lead to an underfit model. . The hyperparameter space for CC has been setup for you. Your job is to use GridSearchCV and logistic regression to find the optimal CC in this hyperparameter space. . # diabetes data set . # Import necessary modules from sklearn.linear_model import LogisticRegression from sklearn.model_selection import GridSearchCV # Setup the hyperparameter grid c_space = np.logspace(-5, 8, 15) param_grid = {&#39;C&#39;: c_space} # Instantiate a logistic regression classifier: logreg logreg = LogisticRegression() # Instantiate the GridSearchCV object: logreg_cv logreg_cv = GridSearchCV(logreg, param_grid, cv=5) # Fit it to the data logreg_cv.fit(X, y) # Print the tuned parameters and score print(&quot;Tuned Logistic Regression Parameters: {}&quot;.format(logreg_cv.best_params_)) print(&quot;Best score is {}&quot;.format(logreg_cv.best_score_)) . Tuned Logistic Regression Parameters: {&#39;C&#39;: 3.7275937203149381} Best score is 0.7708333333333334 . #### Hyperparameter tuning with RandomizedSearchCV . GridSearchCV can be computationally expensive, especially if you are searching over a large hyperparameter space and dealing with multiple hyperparameters. A solution to this is to use RandomizedSearchCV , in which not all hyperparameter values are tried out. Instead, a fixed number of hyperparameter settings is sampled from specified probability distributions. You’ll practice using RandomizedSearchCV in this exercise and see how this works. . # diabetes data set . # Import necessary modules from scipy.stats import randint from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import RandomizedSearchCV # Setup the parameters and distributions to sample from: param_dist param_dist = {&quot;max_depth&quot;: [3, None], &quot;max_features&quot;: randint(1, 9), &quot;min_samples_leaf&quot;: randint(1, 9), &quot;criterion&quot;: [&quot;gini&quot;, &quot;entropy&quot;]} # Instantiate a Decision Tree classifier: tree tree = DecisionTreeClassifier() # Instantiate the RandomizedSearchCV object: tree_cv tree_cv = RandomizedSearchCV(tree, param_dist, cv=5) # Fit it to the data tree_cv.fit(X, y) # Print the tuned parameters and score print(&quot;Tuned Decision Tree Parameters: {}&quot;.format(tree_cv.best_params_)) print(&quot;Best score is {}&quot;.format(tree_cv.best_score_)) . Tuned Decision Tree Parameters: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 3, &#39;max_features&#39;: 7, &#39;min_samples_leaf&#39;: 1} Best score is 0.7317708333333334 . Note that RandomizedSearchCV will never outperform GridSearchCV . Instead, it is valuable because it saves on computation time. . ### Hold-out set for final evaluation . #### Hold-out set in practice I: Classification . In addition to CC, logistic regression has a &#39;penalty&#39; hyperparameter which specifies whether to use &#39;l1&#39; or &#39;l2&#39; regularization. Your job in this exercise is to create a hold-out set, tune the &#39;C&#39; and &#39;penalty&#39; hyperparameters of a logistic regression classifier using GridSearchCV on the training set. . # diabetes data set . # Import necessary modules from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.model_selection import GridSearchCV # Create the hyperparameter grid c_space = np.logspace(-5, 8, 15) param_grid = {&#39;C&#39;: c_space, &#39;penalty&#39;: [&#39;l1&#39;, &#39;l2&#39;]} # Instantiate the logistic regression classifier: logreg logreg = LogisticRegression() # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Instantiate the GridSearchCV object: logreg_cv logreg_cv = GridSearchCV(logreg, param_grid, cv=5) # Fit it to the training data logreg_cv.fit(X_train, y_train) # Print the optimal parameters and best score print(&quot;Tuned Logistic Regression Parameter: {}&quot;.format(logreg_cv.best_params_)) print(&quot;Tuned Logistic Regression Accuracy: {}&quot;.format(logreg_cv.best_score_)) . Tuned Logistic Regression Parameter: {&#39;C&#39;: 0.43939705607607948, &#39;penalty&#39;: &#39;l1&#39;} Tuned Logistic Regression Accuracy: 0.7652173913043478 . #### Hold-out set in practice II: Regression . Remember lasso and ridge regression from the previous chapter? Lasso used the L1 penalty to regularize, while ridge used the L2 penalty. There is another type of regularized regression known as the elastic net. In elastic net regularization, the penalty term is a linear combination of the L1 and L2 penalties: . a∗L1+b∗L2 . In scikit-learn, this term is represented by the &#39;l1_ratio&#39; parameter: An &#39;l1_ratio&#39; of 1 corresponds to an L1L1 penalty, and anything lower is a combination of L1L1 and L2L2. . In this exercise, you will GridSearchCV to tune the &#39;l1_ratio&#39; of an elastic net model trained on the Gapminder data. As in the previous exercise, use a hold-out set to evaluate your model’s performance. . df.head() population fertility HIV CO2 BMI_male GDP BMI_female life 0 34811059.0 2.73 0.1 3.328945 24.59620 12314.0 129.9049 75.3 1 19842251.0 6.43 2.0 1.474353 22.25083 7103.0 130.1247 58.3 2 40381860.0 2.24 0.5 4.785170 27.50170 14646.0 118.8915 75.5 3 2975029.0 1.40 0.1 1.804106 25.35542 7383.0 132.8108 72.5 4 21370348.0 1.96 0.1 18.016313 27.56373 41312.0 117.3755 81.5 child_mortality 0 29.5 1 192.0 2 15.4 3 20.0 4 5.2 . # Import necessary modules from sklearn.linear_model import ElasticNet from sklearn.metrics import mean_squared_error from sklearn.model_selection import GridSearchCV, train_test_split # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Create the hyperparameter grid l1_space = np.linspace(0, 1, 30) param_grid = {&#39;l1_ratio&#39;: l1_space} # Instantiate the ElasticNet regressor: elastic_net elastic_net = ElasticNet() # Setup the GridSearchCV object: gm_cv gm_cv = GridSearchCV(elastic_net, param_grid, cv=5) # Fit it to the training data gm_cv.fit(X_train, y_train) # Predict on the test set and compute metrics y_pred = gm_cv.predict(X_test) r2 = gm_cv.score(X_test, y_test) mse = mean_squared_error(y_test, y_pred) print(&quot;Tuned ElasticNet l1 ratio: {}&quot;.format(gm_cv.best_params_)) print(&quot;Tuned ElasticNet R squared: {}&quot;.format(r2)) print(&quot;Tuned ElasticNet MSE: {}&quot;.format(mse)) . Tuned ElasticNet l1 ratio: {&#39;l1_ratio&#39;: 0.20689655172413793} Tuned ElasticNet R squared: 0.8668305372460283 Tuned ElasticNet MSE: 10.05791413339844 . . Preprocessing and pipelines . ### Preprocessing data . #### Exploring categorical features . df.head() population fertility HIV CO2 BMI_male GDP BMI_female life 0 34811059.0 2.73 0.1 3.328945 24.59620 12314.0 129.9049 75.3 1 19842251.0 6.43 2.0 1.474353 22.25083 7103.0 130.1247 58.3 2 40381860.0 2.24 0.5 4.785170 27.50170 14646.0 118.8915 75.5 3 2975029.0 1.40 0.1 1.804106 25.35542 7383.0 132.8108 72.5 4 21370348.0 1.96 0.1 18.016313 27.56373 41312.0 117.3755 81.5 child_mortality Region 0 29.5 Middle East &amp; North Africa 1 192.0 Sub-Saharan Africa 2 15.4 America 3 20.0 Europe &amp; Central Asia 4 5.2 East Asia &amp; Pacific . # Import pandas import pandas as pd # Read &#39;gapminder.csv&#39; into a DataFrame: df df = pd.read_csv(&#39;gapminder.csv&#39;) # Create a boxplot of life expectancy per region df.boxplot(&#39;life&#39;, &#39;Region&#39;, rot=60) # Show the plot plt.show() . . #### Creating dummy variables . # Create dummy variables: df_region df_region = pd.get_dummies(df) # Print the columns of df_region print(df_region.columns) # Create dummy variables with drop_first=True: df_region df_region = pd.get_dummies(df, drop_first=True) # Print the new columns of df_region print(df_region.columns) . Index([&#39;population&#39;, &#39;fertility&#39;, &#39;HIV&#39;, &#39;CO2&#39;, &#39;BMI_male&#39;, &#39;GDP&#39;, &#39;BMI_female&#39;, &#39;life&#39;, &#39;child_mortality&#39;, &#39;Region_America&#39;, &#39;Region_East Asia &amp; Pacific&#39;, &#39;Region_Europe &amp; Central Asia&#39;, &#39;Region_Middle East &amp; North Africa&#39;, &#39;Region_South Asia&#39;, &#39;Region_Sub-Saharan Africa&#39;], dtype=&#39;object&#39;) # Region_America has been dropped Index([&#39;population&#39;, &#39;fertility&#39;, &#39;HIV&#39;, &#39;CO2&#39;, &#39;BMI_male&#39;, &#39;GDP&#39;, &#39;BMI_female&#39;, &#39;life&#39;, &#39;child_mortality&#39;, &#39;Region_East Asia &amp; Pacific&#39;, &#39;Region_Europe &amp; Central Asia&#39;, &#39;Region_Middle East &amp; North Africa&#39;, &#39;Region_South Asia&#39;, &#39;Region_Sub-Saharan Africa&#39;], dtype=&#39;object&#39;) df_region.head() population fertility HIV CO2 BMI_male GDP BMI_female life 0 34811059.0 2.73 0.1 3.328945 24.59620 12314.0 129.9049 75.3 1 19842251.0 6.43 2.0 1.474353 22.25083 7103.0 130.1247 58.3 2 40381860.0 2.24 0.5 4.785170 27.50170 14646.0 118.8915 75.5 child_mortality Region_East Asia &amp; Pacific Region_Europe &amp; Central Asia 0 29.5 0 0 1 192.0 0 0 2 15.4 0 0 Region_Middle East &amp; North Africa Region_South Asia 0 1 0 1 0 0 2 0 0 Region_Sub-Saharan Africa 0 0 1 1 2 0 . #### Regression with categorical features . # Import necessary modules from sklearn.linear_model import Ridge from sklearn.model_selection import cross_val_score # Instantiate a ridge regressor: ridge ridge = Ridge(alpha=0.5, normalize=True) # Perform 5-fold cross-validation: ridge_cv ridge_cv = cross_val_score(ridge, X, y, cv=5) # Print the cross-validated scores print(ridge_cv) [ 0.86808336 0.80623545 0.84004203 0.7754344 0.87503712] . . ### Handling missing data . #### Dropping missing data . Now, it’s time for you to take care of these yourself! . The unprocessed dataset has been loaded into a DataFrame df . Explore it in the IPython Shell with the .head() method. You will see that there are certain data points labeled with a &#39;?&#39; . These denote missing values. As you saw in the video, different datasets encode missing values in different ways. Sometimes it may be a &#39;9999&#39; , other times a 0 – real-world data can be very messy! If you’re lucky, the missing values will already be encoded as NaN . We use NaN because it is an efficient and simplified way of internally representing missing data, and it lets us take advantage of pandas methods such as .dropna() and .fillna() , as well as scikit-learn’s Imputation transformer Imputer() . . In this exercise, your job is to convert the &#39;?&#39; s to NaNs, and then drop the rows that contain them from the DataFrame. . df.head(3) party infants water budget physician salvador religious satellite aid 0 republican 0 1 0 1 1 1 0 0 1 republican 0 1 0 1 1 1 0 0 2 democrat ? 1 1 ? 1 1 0 0 missile immigration synfuels education superfund crime duty_free_exports 0 0 1 ? 1 1 1 0 1 0 0 0 1 1 1 0 2 0 0 1 0 1 1 0 eaa_rsa 0 1 1 ? 2 0 . # Convert &#39;?&#39; to NaN df[df == &#39;?&#39;] = np.nan # Print the number of NaNs print(df.isnull().sum()) # Print shape of original DataFrame print(&quot;Shape of Original DataFrame: {}&quot;.format(df.shape)) # Drop missing values and print shape of new DataFrame df = df.dropna() # Print shape of new DataFrame print(&quot;Shape of DataFrame After Dropping All Rows with Missing Values: {}&quot;.format(df.shape)) . party 0 infants 12 water 48 budget 11 physician 11 salvador 15 religious 11 satellite 14 aid 15 missile 22 immigration 7 synfuels 21 education 31 superfund 25 crime 17 duty_free_exports 28 eaa_rsa 104 dtype: int64 Shape of Original DataFrame: (435, 17) Shape of DataFrame After Dropping All Rows with Missing Values: (232, 17) . When many values in your dataset are missing, if you drop them, you may end up throwing away valuable information along with the missing data. It’s better instead to develop an imputation strategy. This is where domain knowledge is useful, but in the absence of it, you can impute missing values with the mean or the median of the row or column that the missing value is in. . #### Imputing missing data in a ML Pipeline I . As you’ve come to appreciate, there are many steps to building a model, from creating training and test sets, to fitting a classifier or regressor, to tuning its parameters, to evaluating its performance on new data. Imputation can be seen as the first step of this machine learning process, the entirety of which can be viewed within the context of a pipeline. Scikit-learn provides a pipeline constructor that allows you to piece together these steps into one process and thereby simplify your workflow. . You’ll now practice setting up a pipeline with two steps: the imputation step, followed by the instantiation of a classifier. You’ve seen three classifiers in this course so far: k-NN, logistic regression, and the decision tree. You will now be introduced to a fourth one – the Support Vector Machine, or SVM . . # Import the Imputer module from sklearn.preprocessing import Imputer from sklearn.svm import SVC # Setup the Imputation transformer: imp # axis=0 for column imp = Imputer(missing_values=&#39;NaN&#39;, strategy=&#39;most_frequent&#39;, axis=0) # Instantiate the SVC classifier: clf clf = SVC() # Setup the pipeline with the required steps: steps steps = [(&#39;imputation&#39;, imp), (&#39;SVM&#39;, clf)] . #### Imputing missing data in a ML Pipeline II . Having setup the steps of the pipeline in the previous exercise, you will now use it on the voting dataset to classify a Congressman’s party affiliation. . What makes pipelines so incredibly useful is the simple interface that they provide. You can use the .fit() and .predict() methods on pipelines just as you did with your classifiers and regressors! . # Import necessary modules from sklearn.preprocessing import Imputer from sklearn.pipeline import Pipeline from sklearn.svm import SVC # Setup the pipeline steps: steps steps = [(&#39;imputation&#39;, Imputer(missing_values=&#39;NaN&#39;, strategy=&#39;most_frequent&#39;, axis=0)), (&#39;SVM&#39;, SVC())] # Create the pipeline: pipeline pipeline = Pipeline(steps) # Create training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Fit the pipeline to the train set pipeline.fit(X_train, y_train) # Predict the labels of the test set y_pred = pipeline.predict(X_test) # Compute metrics print(classification_report(y_test, y_pred)) . precision recall f1-score support democrat 0.99 0.96 0.98 85 republican 0.94 0.98 0.96 46 avg / total 0.97 0.97 0.97 131 . ### Centering and scaling . #### Centering and scaling your data . You will now explore scaling for yourself on a new dataset – White Wine Quality ! . We have used the &#39;quality&#39; feature of the wine to create a binary target variable: If &#39;quality&#39; is less than 5 , the target variable is 1 , and otherwise, it is 0 . . Notice how some features seem to have different units of measurement. &#39;density&#39; , for instance, takes values between 0.98 and 1.04, while &#39;total sulfur dioxide&#39; ranges from 9 to 440. As a result, it may be worth scaling the features here. Your job in this exercise is to scale the features and compute the mean and standard deviation of the unscaled features compared to the scaled features. . # white wine quality data set df.head(3) fixed acidity volatile acidity citric acid residual sugar chlorides 0 7.0 0.27 0.36 20.7 0.045 1 6.3 0.30 0.34 1.6 0.049 2 8.1 0.28 0.40 6.9 0.050 free sulfur dioxide total sulfur dioxide density pH sulphates 0 45.0 170.0 1.0010 3.00 0.45 1 14.0 132.0 0.9940 3.30 0.49 2 30.0 97.0 0.9951 3.26 0.44 alcohol quality 0 8.8 6 1 9.5 6 2 10.1 6 . # Import scale from sklearn.preprocessing import scale # Scale the features: X_scaled X_scaled = scale(X) # Print the mean and standard deviation of the unscaled features print(&quot;Mean of Unscaled Features: {}&quot;.format(np.mean(X))) print(&quot;Standard Deviation of Unscaled Features: {}&quot;.format(np.std(X))) # Print the mean and standard deviation of the scaled features print(&quot;Mean of Scaled Features: {}&quot;.format(np.mean(X_scaled))) print(&quot;Standard Deviation of Scaled Features: {}&quot;.format(np.std(X_scaled))) . Mean of Unscaled Features: 18.432687072460002 Standard Deviation of Unscaled Features: 41.54494764094571 Mean of Scaled Features: 2.7314972981668206e-15 Standard Deviation of Scaled Features: 0.9999999999999999 . #### Centering and scaling in a pipeline . With regard to whether or not scaling is effective, the proof is in the pudding! See for yourself whether or not scaling the features of the White Wine Quality dataset has any impact on its performance. . You will use a k-NN classifier as part of a pipeline that includes scaling, and for the purposes of comparison, a k-NN classifier trained on the unscaled data has been provided. . # Import the necessary modules from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline # Setup the pipeline steps: steps steps = [(&#39;scaler&#39;, StandardScaler()), (&#39;knn&#39;, KNeighborsClassifier())] # Create the pipeline: pipeline pipeline = Pipeline(steps) # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Fit the pipeline to the training set: knn_scaled knn_scaled = pipeline.fit(X_train, y_train) # Instantiate and fit a k-NN classifier to the unscaled data knn_unscaled = KNeighborsClassifier().fit(X_train, y_train) # Compute and print metrics print(&#39;Accuracy with Scaling: {}&#39;.format(knn_scaled.score(X_test, y_test))) print(&#39;Accuracy without Scaling: {}&#39;.format(knn_unscaled.score(X_test, y_test))) Accuracy with Scaling: 0.7700680272108843 Accuracy without Scaling: 0.6979591836734694 . It looks like scaling has significantly improved model performance! . . #### Bringing it all together I: Pipeline for classification . It is time now to piece together everything you have learned so far into a pipeline for classification! Your job in this exercise is to build a pipeline that includes scaling and hyperparameter tuning to classify wine quality. . You’ll return to using the SVM classifier you were briefly introduced to earlier in this chapter. The hyperparameters you will tune are C and gamma. C controls the regularization strength. It is analogous to the C you tuned for logistic regression in Chapter 3, while gamma controls the kernel coefficient: . # Setup the pipeline steps = [(&#39;scaler&#39;, StandardScaler()), (&#39;SVM&#39;, SVC())] pipeline = Pipeline(steps) # Specify the hyperparameter space parameters = {&#39;SVM__C&#39;:[1, 10, 100], &#39;SVM__gamma&#39;:[0.1, 0.01]} # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21) # Instantiate the GridSearchCV object: cv cv = GridSearchCV(pipeline, parameters) # Fit to the training set cv.fit(X_train, y_train) # Predict the labels of the test set: y_pred y_pred = cv.predict(X_test) # Compute and print metrics print(&quot;Accuracy: {}&quot;.format(cv.score(X_test, y_test))) print(classification_report(y_test, y_pred)) print(&quot;Tuned Model Parameters: {}&quot;.format(cv.best_params_)) . Accuracy: 0.7795918367346939 precision recall f1-score support False 0.83 0.85 0.84 662 True 0.67 0.63 0.65 318 avg / total 0.78 0.78 0.78 980 Tuned Model Parameters: {&#39;SVM__C&#39;: 10, &#39;SVM__gamma&#39;: 0.1} . #### Bringing it all together II: Pipeline for regression . Your job is to build a pipeline that imputes the missing data, scales the features, and fits an ElasticNet to the Gapminder data. You will then tune the l1_ratio of your ElasticNet using GridSearchCV. . # Setup the pipeline steps: steps steps = [(&#39;imputation&#39;, Imputer(missing_values=&#39;NaN&#39;, strategy=&#39;mean&#39;, axis=0)), (&#39;scaler&#39;, StandardScaler()), (&#39;elasticnet&#39;, ElasticNet())] # Create the pipeline: pipeline pipeline = Pipeline(steps) # Specify the hyperparameter space parameters = {&#39;elasticnet__l1_ratio&#39;:np.linspace(0,1,30)} # Create train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Create the GridSearchCV object: gm_cv gm_cv = GridSearchCV(pipeline, parameters) # Fit to the training set gm_cv.fit(X_train, y_train) # Compute and print the metrics r2 = gm_cv.score(X_test, y_test) print(&quot;Tuned ElasticNet Alpha: {}&quot;.format(gm_cv.best_params_)) print(&quot;Tuned ElasticNet R squared: {}&quot;.format(r2)) # Tuned ElasticNet Alpha: {&#39;elasticnet__l1_ratio&#39;: 1.0} # Tuned ElasticNet R squared: 0.8862016570888217 . The End. . Thank you for reading. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/supervised-learning-with-scikit-learn.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/supervised-learning-with-scikit-learn.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post37": {
            "title": "Machine Learning with the Experts School Budgets from Datacamp",
            "content": "Machine Learning with the Experts: School Budgets from Datacamp . This is the memo of the 22th course of ‘Data Scientist with Python’ track. . You can find the original course HERE . . . 1. Exploring the raw data . . You’re going to be working with school district budget data. This data can be classified in many ways according to certain labels, e.g. Function: Career &amp; Academic Counseling , or Position_Type: Librarian . . Your goal is to develop a model that predicts the probability for each possible label by relying on some correctly labeled examples. . This is a supervised Learning, because the model will be trained using labeled examples. . #### What is the goal of the algorithm? . Your goal is to correctly label budget line items by training a supervised model to predict the probability of each possible label, taking most probable label as the correct label. . It’s a classification problem, because predicted probabilities will be used to select a label class. . Specifically, we have ourselves a multi-class-multi-label classification problem (quite a mouthful!), because there are 9 broad categories that each take on many possible sub-label instances. . ### Exploring the data . #### Loading the data . df = pd.read_csv(&#39;TrainingData.csv&#39;, index_col=0) df.head(3) Function Use Sharing Reporting 198 NO_LABEL NO_LABEL NO_LABEL NO_LABEL 209 Student Transportation NO_LABEL Shared Services Non-School 750 Teacher Compensation Instruction School Reported School Student_Type Position_Type Object_Type Pre_K 198 NO_LABEL NO_LABEL NO_LABEL NO_LABEL 209 NO_LABEL NO_LABEL Other Non-Compensation NO_LABEL 750 Unspecified Teacher Base Salary/Compensation Non PreK Operating_Status Object_Description ... 198 Non-Operating Supplemental * ... 209 PreK-12 Operating REPAIR AND MAINTENANCE SERVICES ... 750 PreK-12 Operating Personal Services - Teachers ... Sub_Object_Description Location_Description FTE 198 Non-Certificated Salaries And Wages NaN NaN 209 NaN ADMIN. SERVICES NaN 750 NaN NaN 1.0 Function_Description Facility_or_Department 198 Care and Upkeep of Building Services NaN 209 STUDENT TRANSPORT SERVICE NaN 750 NaN NaN Position_Extra Total Program_Description 198 NaN -8291.86 NaN 209 NaN 618.29 PUPIL TRANSPORTATION 750 TEACHER 49768.82 Instruction - Regular Fund_Description Text_1 198 Title I - Disadvantaged Children/Targeted Assi... TITLE I CARRYOVER 209 General Fund NaN 750 General Purpose School NaN [3 rows x 25 columns] . df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 1560 entries, 198 to 101861 Data columns (total 25 columns): Function 1560 non-null object Use 1560 non-null object Sharing 1560 non-null object Reporting 1560 non-null object Student_Type 1560 non-null object Position_Type 1560 non-null object Object_Type 1560 non-null object Pre_K 1560 non-null object Operating_Status 1560 non-null object Object_Description 1461 non-null object Text_2 382 non-null object SubFund_Description 1183 non-null object Job_Title_Description 1131 non-null object Text_3 677 non-null object Text_4 193 non-null object Sub_Object_Description 364 non-null object Location_Description 874 non-null object FTE 449 non-null float64 Function_Description 1340 non-null object Facility_or_Department 252 non-null object Position_Extra 1026 non-null object Total 1542 non-null float64 Program_Description 1192 non-null object Fund_Description 819 non-null object Text_1 1132 non-null object dtypes: float64(2), object(23) memory usage: 316.9+ KB . #### Summarizing the data . There are two numeric columns, called FTE and Total . . FTE Stands for “full-time equivalent”. If the budget item is associated to an employee, this number tells us the percentage of full-time that the employee works. A value of 1 means the associated employee works for the school full-time. A value close to 0 means the item is associated to a part-time or contracted employee. | Total Stands for the total cost of the expenditure. This number tells us how much the budget item cost. | . After printing summary statistics for the numeric data, your job is to plot a histogram of the non-null FTE column to see the distribution of part-time and full-time employees in the dataset. . # Print the summary statistics print(df.describe()) # Import matplotlib.pyplot as plt import matplotlib.pyplot as plt # Create the histogram plt.hist(df.FTE.dropna()) # Add title and labels plt.title(&#39;Distribution of %full-time n employee works&#39;) plt.xlabel(&#39;% of full-time&#39;) plt.ylabel(&#39;num employees&#39;) # Display the histogram plt.show() . FTE Total count 449.000000 1.542000e+03 mean 0.493532 1.446867e+04 std 0.452844 7.916752e+04 min -0.002369 -1.044084e+06 25% NaN NaN 50% NaN NaN 75% NaN NaN max 1.047222 1.367500e+06 . . The high variance in expenditures makes sense (some purchases are cheap some are expensive). Also, it looks like the FTE column is bimodal. That is, there are some part-time and some full-time employees. . ### Covert object data to category . #### Count the data type and value . df.dtypes.value_counts() object 23 float64 2 dtype: int64 . #### Encode the labels as categorical variables . There are 9 columns of labels in the dataset. Each of these columns is a category that has many possible values it can take . The 9 labels have been loaded into a list called LABELS . . You will notice that every label is encoded as an object datatype. Because category datatypes are much more efficient your task is to convert the labels to category types using the .astype() method. . LABELS [&#39;Function&#39;, &#39;Use&#39;, &#39;Sharing&#39;, &#39;Reporting&#39;, &#39;Student_Type&#39;, &#39;Position_Type&#39;, &#39;Object_Type&#39;, &#39;Pre_K&#39;, &#39;Operating_Status&#39;] df[LABELS].dtypes Function object Use object Sharing object Reporting object Student_Type object Position_Type object Object_Type object Pre_K object Operating_Status object dtype: object . # Define the lambda function: categorize_label categorize_label = lambda x: x.astype(&#39;category&#39;) # Convert df[LABELS] to a categorical type df[LABELS] = df[LABELS].apply(categorize_label, axis=0) # Print the converted dtypes print(df[LABELS].dtypes) . Function category Use category Sharing category Reporting category Student_Type category Position_Type category Object_Type category Pre_K category Operating_Status category dtype: object . &#39;&#39;&#39; https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html apply axis : {0 or ‘index’, 1 or ‘columns’}, default 0 Axis along which the function is applied: 0 or ‘index’: apply function to each column. 1 or ‘columns’: apply function to each row. &#39;&#39;&#39; . #### **Counting unique labels . .apply(pd.Series.nunique)** . # Import matplotlib.pyplot import matplotlib.pyplot as plt # Calculate number of unique values for each label: num_unique_labels num_unique_labels = df[LABELS].apply(pd.Series.nunique) # Plot number of unique values for each label num_unique_labels.plot(kind=&#39;bar&#39;) # Label the axes plt.xlabel(&#39;Labels&#39;) plt.ylabel(&#39;Number of unique values&#39;) # Display the plot plt.show() . . . ### How do we measure success? . We can use a loss function. . What is Log Loss? . . #### Penalizing highly confident wrong answers . Log loss provides a steep penalty for predictions that are both wrong and confident, i.e., a high probability is assigned to the incorrect class. . Suppose you have the following 3 examples: . A:y=1,p=0.85 | B:y=0,p=0.99 | C:y=0,p=0.51 | . Select the ordering of the examples which corresponds to the lowest to highest log loss scores. y is an indicator of whether the example was classified correctly. You shouldn’t need to crunch any numbers! . Of the two incorrect predictions, B will have a higher log loss because it is confident and wrong. . Lowest: A, Middle: C, Highest: B . The lower loss score is ,the better the prediction is. . #### Computing log loss with NumPy . To see how the log loss metric handles the trade-off between accuracy and confidence, we will use some sample data generated with NumPy and compute the log loss using the provided function compute_log_loss() . . Your job is to compute the log loss for each sample set provided using the compute_log_loss(predicted_values, actual_values) . It takes the predicted values as the first argument and the actual values as the second argument. . actual_labels # array([1., 1., 1., 1., 1., 0., 0., 0., 0., 0.]) correct_confident # array([0.95, 0.95, 0.95, 0.95, 0.95, 0.05, 0.05, 0.05, 0.05, 0.05]) correct_not_confident # array([0.65, 0.65, 0.65, 0.65, 0.65, 0.35, 0.35, 0.35, 0.35, 0.35]) wrong_not_confident # array([0.35, 0.35, 0.35, 0.35, 0.35, 0.65, 0.65, 0.65, 0.65, 0.65]) wrong_confident # array([0.05, 0.05, 0.05, 0.05, 0.05, 0.95, 0.95, 0.95, 0.95, 0.95]) . # Compute and print log loss for 1st case correct_confident_loss = compute_log_loss(correct_confident, actual_labels) print(&quot;Log loss, correct and confident: {}&quot;.format(correct_confident_loss)) # Compute log loss for 2nd case correct_not_confident_loss = compute_log_loss(correct_not_confident, actual_labels) print(&quot;Log loss, correct and not confident: {}&quot;.format(correct_not_confident_loss)) # Compute and print log loss for 3rd case wrong_not_confident_loss = compute_log_loss(wrong_not_confident, actual_labels) print(&quot;Log loss, wrong and not confident: {}&quot;.format(wrong_not_confident_loss)) # Compute and print log loss for 4th case wrong_confident_loss = compute_log_loss(wrong_confident, actual_labels) print(&quot;Log loss, wrong and confident: {}&quot;.format(wrong_confident_loss)) # Compute and print log loss for actual labels actual_labels_loss = compute_log_loss(actual_labels, actual_labels) print(&quot;Log loss, actual labels: {}&quot;.format(actual_labels_loss)) . Log loss, correct and confident: 0.05129329438755058 Log loss, correct and not confident: 0.4307829160924542 Log loss, wrong and not confident: 1.049822124498678 Log loss, wrong and confident: 2.9957322735539904 Log loss, actual labels: 9.99200722162646e-15 . Log loss penalizes highly confident wrong answers much more than any other type. This will be a good metric to use on your models. . . 2. Creating a simple first model . . ### It’s time to build a model . #### Setting up a train-test split in scikit-learn . It’s finally time to start training models! . The first step is to split the data into a training set and a test set. Some labels don’t occur very often, but we want to make sure that they appear in both the training and the test sets. We provide a function that will make sure at least min_count examples of each label appear in each split: multilabel_train_test_split . . Feel free to check out the full code for multilabel_train_test_split here . . You’ll start with a simple model that uses just the numeric columns of your DataFrame when calling multilabel_train_test_split . The data has been read into a DataFrame df and a list consisting of just the numeric columns is available as NUMERIC_COLUMNS . . # X df[NUMERIC_COLUMNS].head(3) FTE Total 198 NaN -8291.86 209 NaN 618.29 750 1.0 49768.82 df[NUMERIC_COLUMNS].shape (1560, 2) # y label_dummies.columns Out[16]: Index([&#39;Function_Aides Compensation&#39;, &#39;Function_Career &amp; Academic Counseling&#39;, &#39;Function_Communications&#39;, &#39;Function_Curriculum Development&#39;, &#39;Function_Data Processing &amp; Information Services&#39;, ... &#39;Operating_Status_Non-Operating&#39;, &#39;Operating_Status_Operating, Not PreK-12&#39;, &#39;Operating_Status_PreK-12 Operating&#39;], dtype=&#39;object&#39;, length=104) label_dummies.shape (1560, 104) . # Create the new DataFrame: numeric_data_only numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000) # Get labels and convert to dummy variables: label_dummies label_dummies = pd.get_dummies(df[LABELS]) # Create training and test sets X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only, label_dummies, size=0.2, seed=123) # Print the info print(&quot;X_train info:&quot;) print(X_train.info()) print(&quot; nX_test info:&quot;) print(X_test.info()) print(&quot; ny_train info:&quot;) print(y_train.info()) print(&quot; ny_test info:&quot;) print(y_test.info()) . X_train info: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 1040 entries, 198 to 101861 Data columns (total 2 columns): FTE 1040 non-null float64 Total 1040 non-null float64 dtypes: float64(2) memory usage: 24.4 KB None X_test info: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 520 entries, 209 to 448628 Data columns (total 2 columns): FTE 520 non-null float64 Total 520 non-null float64 dtypes: float64(2) memory usage: 12.2 KB None y_train info: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 1040 entries, 198 to 101861 Columns: 104 entries, Function_Aides Compensation to Operating_Status_PreK-12 Operating dtypes: float64(104) memory usage: 853.1 KB None y_test info: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 520 entries, 209 to 448628 Columns: 104 entries, Function_Aides Compensation to Operating_Status_PreK-12 Operating dtypes: float64(104) memory usage: 426.6 KB None . #### Training a model . # Import classifiers from sklearn.linear_model import LogisticRegression from sklearn.multiclass import OneVsRestClassifier # Create the DataFrame: numeric_data_only numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000) # Get labels and convert to dummy variables: label_dummies label_dummies = pd.get_dummies(df[LABELS]) # Create training and test sets X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only, label_dummies, size=0.2, seed=123) # Instantiate the classifier: clf clf = OneVsRestClassifier(LogisticRegression()) # Fit the classifier to the training data clf.fit(X_train, y_train) # Print the accuracy print(&quot;Accuracy: {}&quot;.format(clf.score(X_test, y_test))) . Accuracy: 0.0 . The good news is that your workflow didn’t cause any errors. The bad news is that your model scored the lowest possible accuracy: 0.0 ! . But hey, you just threw away ALL of the text data in the budget. Later, you won’t. Before you add the text data, let’s see how the model does when scored by log loss. . ### Making predictions . #### Use your model to predict values on holdout data . You’re ready to make some predictions! Remember, the train-test-split you’ve carried out so far is for model development. The original competition provides an additional test set, for which you’ll never actually see the correct labels. This is called the “holdout data.” . Remember that the original goal is to predict the probability of each label . In this exercise you’ll do just that by using the .predict_proba() method on your trained model. . # Instantiate the classifier: clf clf = OneVsRestClassifier(LogisticRegression()) # Fit it to the training data clf.fit(X_train, y_train) # Load the holdout data: holdout holdout = pd.read_csv(&#39;HoldoutData.csv&#39;, index_col=0) holdout = holdout[NUMERIC_COLUMNS].fillna(-1000) # Generate predictions: predictions predictions = clf.predict_proba(holdout) . #### Writing out your results to a csv for submission . At last, you’re ready to submit some predictions for scoring. In this exercise, you’ll write your predictions to a .csv using the .to_csv() method on a pandas DataFrame. Then you’ll evaluate your performance according to the LogLoss metric! . You’ll need to make sure your submission obeys the correct format . . Interpreting LogLoss &amp; Beating the Benchmark: . When interpreting your log loss score, keep in mind that the score will change based on the number of samples tested. To get a sense of how this very basic model performs, compare your score to the DrivenData benchmark model performance: 2.0455 , which merely submitted uniform probabilities for each class. . Remember, the lower the log loss the better. Is your model’s log loss lower than 2.0455? . # Generate predictions: predictions predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000)) # Format predictions in DataFrame: prediction_df prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS]).columns, index=holdout.index, data=predictions) # Save prediction_df to csv prediction_df.to_csv(&#39;predictions.csv&#39;) # Submit the predictions for scoring: score score = score_submission(pred_path=&#39;predictions.csv&#39;) # Print score print(&#39;Your model, trained with numeric data only, yields logloss score: {}&#39;.format(score)) . predictions array([[0.108789 , 0.04790553, 0.02505001, ..., 0.13005936, 0.03531605, 0.87679861], ..., [0.10169308, 0.04809784, 0.02423019, ..., 0.12078767, 0.03686144, 0.88204561]]) predictions.shape (2000, 104) Your model, trained with numeric data only, yields logloss score: 1.9067227623381413 . Even though your basic model scored 0.0 accuracy, it nevertheless performs better than the benchmark score of 2.0455. You’ve now got the basics down and have made a first pass at this complicated supervised learning problem. It’s time to step up your game and incorporate the text data. . . ### A very brief introduction to NLP(Natural Language Processing) . #### Tokenizing text . Tokenization is the process of chopping up a character sequence into pieces called tokens . . How do we determine what constitutes a token? Often, tokens are separated by whitespace. But we can specify other delimiters as well. For example, if we decided to tokenize on punctuation, then any punctuation mark would be treated like a whitespace. How we tokenize text in our DataFrame can affect the statistics we use in our model. . A particular cell in our budget DataFrame may have the string content Title I - Disadvantaged Children/Targeted Assistance . The number of n-grams generated by this text data is sensitive to whether or not we tokenize on punctuation. . How many tokens (1-grams) are in the string . Title I – Disadvantaged Children/Targeted Assistance . if we tokenize on punctuation? . 6 tokens(1-grams) . Title | I | Disadvantaged | Children | Targeted | Assistance | . #### n_grams token . one_grams = [‘petro’, ‘vend’, ‘fuel’, ‘and’, ‘fluids’] | two _grams = [‘petro vend’, ‘vend fuel’, ‘fuel and’, ‘and fluids’] | three _grams = [‘petro vend fuel’, ‘vend fuel and’, ‘ fuel and fluids’] | . ### Representing text numerically . #### Creating a bag-of-words in scikit-learn . In this exercise, you’ll study the effects of tokenizing in different ways by comparing the bag-of-words representations resulting from different token patterns. . You will focus on one feature only, the Position_Extra column, which describes any additional information not captured by the Position_Type label. . Your task is to turn the raw text in this column into a bag-of-words representation by creating tokens that contain only alphanumeric characters. . For comparison purposes, the first 15 tokens of vec_basic , which splits df.Position_Extra into tokens when it encounters only whitespace characters, have been printed along with the length of the representation. . # Import CountVectorizer from sklearn.feature_extraction.text import CountVectorizer # Create the token pattern: TOKENS_ALPHANUMERIC TOKENS_ALPHANUMERIC = &#39;[A-Za-z0-9]+(?= s+)&#39; # Fill missing values in df.Position_Extra df.Position_Extra.fillna(&#39;&#39;, inplace=True) # Instantiate the CountVectorizer: vec_alphanumeric vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC) # Fit to the data vec_alphanumeric.fit(df.Position_Extra) # Print the number of tokens and first 15 tokens msg = &quot;There are {} tokens in Position_Extra if we split on non-alpha numeric&quot; print(msg.format(len(vec_alphanumeric.get_feature_names()))) print(vec_alphanumeric.get_feature_names()[:15]) . There are 123 tokens in Position_Extra if we split on non-alpha numeric [&#39;1st&#39;, &#39;2nd&#39;, &#39;3rd&#39;, &#39;a&#39;, &#39;ab&#39;, &#39;additional&#39;, &#39;adm&#39;, &#39;administrative&#39;, &#39;and&#39;, &#39;any&#39;, &#39;art&#39;, &#39;assessment&#39;, &#39;assistant&#39;, &#39;asst&#39;, &#39;athletic&#39;] . Treating only alpha-numeric characters as tokens gives you a smaller number of more meaningful tokens. You’ve got bag-of-words in the bag! . #### Combining text columns for tokenization . In order to get a bag-of-words representation for all of the text data in our DataFrame, you must first convert the text data in each row of the DataFrame into a single string. . In the previous exercise, this wasn’t necessary because you only looked at one column of data, so each row was already just a single string. CountVectorizer expects each row to just be a single string, so in order to use all of the text columns, you’ll need a method to turn a list of strings into a single string. . In this exercise, you’ll complete the function definition combine_text_columns() . When completed, this function will convert all training text data in your DataFrame to a single string per row that can be passed to the vectorizer object and made into a bag-of-words using the .fit_transform() method. . Note that the function uses NUMERIC_COLUMNS and LABELS to determine which columns to drop. These lists have been loaded into the workspace. . # Define combine_text_columns() def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS): &quot;&quot;&quot; converts all text in each row of data_frame to single vector &quot;&quot;&quot; # Drop non-text columns that are in the df to_drop = set(to_drop) &amp; set(data_frame.columns.tolist()) text_data = data_frame.drop(to_drop, axis=1) # Replace nans with blanks text_data.fillna(&#39;&#39;, inplace=True) # Join all text items in a row that have a space in between return text_data.apply(lambda x: &quot; &quot;.join(x), axis=1) . #### What’s in a token? . Now you will use combine_text_columns to convert all training text data in your DataFrame to a single vector that can be passed to the vectorizer object and made into a bag-of-words using the .fit_transform() method. . You’ll compare the effect of tokenizing using any non-whitespace characters as a token and using only alphanumeric characters as a token. . # Import the CountVectorizer from sklearn.feature_extraction.text import CountVectorizer # Create the basic token pattern TOKENS_BASIC = &#39; S+(?= s+)&#39; # Create the alphanumeric token pattern TOKENS_ALPHANUMERIC = &#39;[A-Za-z0-9]+(?= s+)&#39; # Instantiate basic CountVectorizer: vec_basic vec_basic = CountVectorizer(token_pattern=TOKENS_BASIC) # Instantiate alphanumeric CountVectorizer: vec_alphanumeric vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC) # Create the text vector text_vector = combine_text_columns(df) # Fit and transform vec_basic vec_basic.fit_transform(text_vector) # Print number of tokens of vec_basic print(&quot;There are {} tokens in the dataset&quot;.format(len(vec_basic.get_feature_names()))) # Fit and transform vec_alphanumeric vec_alphanumeric.fit_transform(text_vector) # Print number of tokens of vec_alphanumeric print(&quot;There are {} alpha-numeric tokens in the dataset&quot;.format(len(vec_alphanumeric.get_feature_names()))) . There are 1405 tokens in the dataset There are 1117 alpha-numeric tokens in the dataset . text_vector 198 Supplemental * Operation and Maintenance of P... 209 REPAIR AND MAINTENANCE SERVICES PUPIL TRANSPO... ... 305347 Extra Duty Pay/Overtime For Support Personnel ... 101861 SALARIES OF REGULAR EMPLOYEES FEDERAL GDPG FU... dtype: object . Notice that tokenizing on alpha-numeric tokens reduced the number of tokens. We’ll keep this in mind when building a better model with the Pipeline object next. . . 3. Improving your model . . ### Pipelines, feature &amp; text preprocessing . What is pipeline? . You can look up at document here . And here is an example. . Simply speaking, pipeline just put several machine learning steps(like scaling data, parameter tuning) together. . Why should you use pipeline? . Your code will be much easier to read | You can easily modify the parameters | . #### Instantiate pipeline . For the next few exercises, you’ll reacquaint yourself with pipelines and train a classifier on some synthetic (sample) data of multiple datatypes before using the same techniques on the main dataset. . In this exercise, your job is to instantiate a pipeline that trains using the numeric column of the sample data. . numeric text with_missing label 0 -10.856306 4.433240 b 1 9.973454 foo 4.310229 b 2 2.829785 foo bar 2.469828 a 3 -15.062947 2.852981 b 4 -5.786003 foo bar 1.826475 a pd.get_dummies(sample_df[&#39;label&#39;]) a b 0 0.0 1.0 1 0.0 1.0 2 1.0 0.0 3 0.0 1.0 . # Import Pipeline from sklearn.pipeline import Pipeline # Import other necessary modules from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.multiclass import OneVsRestClassifier # Split and select numeric data only, no nans X_train, X_test, y_train, y_test = train_test_split(sample_df[[&#39;numeric&#39;]], pd.get_dummies(sample_df[&#39;label&#39;]), random_state=22) # Instantiate Pipeline object: pl pl = Pipeline([ (&#39;clf&#39;, OneVsRestClassifier(LogisticRegression())) ]) # Fit the pipeline to the training data pl.fit(X_train, y_train) # Compute and print accuracy accuracy = pl.score(X_test, y_test) print(&quot; nAccuracy on sample data - numeric, no nans: &quot;, accuracy) # Accuracy on sample data - numeric, no nans: 0.62 . Now it’s time to incorporate numeric data with missing values by adding a preprocessing step! . #### Preprocessing numeric features . In this exercise you’ll improve your pipeline a bit by using the Imputer() imputation transformer from scikit-learn to fill in missing values in your sample data. . By default, the imputer transformer replaces NaNs with the mean value of the column. That’s a good enough imputation strategy for the sample data, so you won’t need to pass anything extra to the imputer. . After importing the transformer, you will edit the steps list used in the previous exercise by inserting a (name, transform) tuple. Recall that steps are processed sequentially, so make sure the new tuple encoding your preprocessing step is put in the right place. . # Import the Imputer object from sklearn.preprocessing import Imputer # Create training and test sets using only numeric data X_train, X_test, y_train, y_test = train_test_split(sample_df[[&#39;numeric&#39;, &#39;with_missing&#39;]], pd.get_dummies(sample_df[&#39;label&#39;]), random_state=456) # Insantiate Pipeline object: pl pl = Pipeline([ (&#39;imp&#39;, Imputer()), (&#39;clf&#39;, OneVsRestClassifier(LogisticRegression())) ]) # Fit the pipeline to the training data pl.fit(X_train, y_train) # Compute and print accuracy accuracy = pl.score(X_test, y_test) print(&quot; nAccuracy on sample data - all numeric, incl nans: &quot;, accuracy) # Accuracy on sample data - all numeric, incl nans: 0.636 . Now you know how to use preprocessing in pipelines with numeric data, and it looks like the accuracy has improved because of it! Text data preprocessing is next! . . ### Text features and feature unions . #### Preprocessing text features . sample_df[&#39;text&#39;] 0 1 foo 2 foo bar 3 4 foo bar 5 6 foo bar 7 foo . # Import the CountVectorizer from sklearn.feature_extraction.text import CountVectorizer # Split out only the text data X_train, X_test, y_train, y_test = train_test_split(sample_df[&#39;text&#39;], pd.get_dummies(sample_df[&#39;label&#39;]), random_state=456) # Instantiate Pipeline object: pl pl = Pipeline([ (&#39;vec&#39;, CountVectorizer()), (&#39;clf&#39;, OneVsRestClassifier(LogisticRegression())) ]) # Fit to the training data pl.fit(X_train, y_train) # Compute and print accuracy accuracy = pl.score(X_test, y_test) print(&quot; nAccuracy on sample data - just text data: &quot;, accuracy) # Accuracy on sample data - just text data: 0.808 . #### Multiple types of processing: FunctionTransformer . The next two exercises will introduce new topics you’ll need to make your pipeline truly excel. . Any step in the pipeline must be an object that implements the fit and transform methods. The FunctionTransformer creates an object with these methods out of any Python function that you pass to it. We’ll use it to help select subsets of data in a way that plays nicely with pipelines. . You are working with numeric data that needs imputation, and text data that needs to be converted into a bag-of-words. You’ll create functions that separate the text from the numeric variables and see how the .fit() and .transform() methods work. . # Import FunctionTransformer from sklearn.preprocessing import FunctionTransformer # Obtain the text data: get_text_data get_text_data = FunctionTransformer(lambda x: x[&#39;text&#39;], validate=False) # Obtain the numeric data: get_numeric_data get_numeric_data = FunctionTransformer(lambda x: x[[&#39;numeric&#39;, &#39;with_missing&#39;]], validate=False) # Fit and transform the text data: just_text_data just_text_data = get_text_data.fit_transform(sample_df) # Fit and transform the numeric data: just_numeric_data just_numeric_data = get_numeric_data.fit_transform(sample_df) # Print head to check results print(&#39;Text Data&#39;) print(just_text_data.head()) print(&#39; nNumeric Data&#39;) print(just_numeric_data.head()) . Text Data 0 1 foo 2 foo bar 3 4 foo bar Name: text, dtype: object Numeric Data numeric with_missing 0 -10.856306 4.433240 1 9.973454 4.310229 2 2.829785 2.469828 3 -15.062947 2.852981 4 -5.786003 1.826475 . #### Multiple types of processing: FeatureUnion . Now that you can separate text and numeric data in your pipeline, you’re ready to perform separate steps on each by nesting pipelines and using FeatureUnion() . . These tools will allow you to streamline all preprocessing steps for your model, even when multiple datatypes are involved. Here, for example, you don’t want to impute our text data, and you don’t want to create a bag-of-words with our numeric data. Instead, you want to deal with these separately and then join the results together using FeatureUnion() . . In the end, you’ll still have only two high-level steps in your pipeline: preprocessing and model instantiation. The difference is that the first preprocessing step actually consists of a pipeline for numeric data and a pipeline for text data. The results of those pipelines are joined using FeatureUnion() . . # Import FeatureUnion from sklearn.pipeline import FeatureUnion # Split using ALL data in sample_df X_train, X_test, y_train, y_test = train_test_split(sample_df[[&#39;numeric&#39;, &#39;with_missing&#39;, &#39;text&#39;]], pd.get_dummies(sample_df[&#39;label&#39;]), random_state=22) # Create a FeatureUnion with nested pipeline: process_and_join_features process_and_join_features = FeatureUnion( transformer_list = [ (&#39;numeric_features&#39;, Pipeline([ (&#39;selector&#39;, get_numeric_data), (&#39;imputer&#39;, Imputer()) ])), (&#39;text_features&#39;, Pipeline([ (&#39;selector&#39;, get_text_data), (&#39;vectorizer&#39;, CountVectorizer()) ])) ] ) # Instantiate nested pipeline: pl pl = Pipeline([ (&#39;union&#39;, process_and_join_features), (&#39;clf&#39;, OneVsRestClassifier(LogisticRegression())) ]) # Fit pl to the training data pl.fit(X_train, y_train) # Compute and print accuracy accuracy = pl.score(X_test, y_test) print(&quot; nAccuracy on sample data - all data: &quot;, accuracy) # Accuracy on sample data - all data: 0.928 . . ### Choosing a classification model . #### Using FunctionTransformer on the main dataset . In this exercise you’re going to use FunctionTransformer on the primary budget data, before instantiating a multiple-datatype pipeline in the next exercise. . # Import FunctionTransformer from sklearn.preprocessing import FunctionTransformer # Get the dummy encoding of the labels dummy_labels = pd.get_dummies(df[LABELS]) # Get the columns that are features in the original df NON_LABELS = [c for c in df.columns if c not in LABELS] # Split into training and test sets X_train, X_test, y_train, y_test = multilabel_train_test_split(df[NON_LABELS], dummy_labels, 0.2, seed=123) # Preprocess the text data: get_text_data get_text_data = FunctionTransformer(combine_text_columns, validate=False) # Preprocess the numeric data: get_numeric_data get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False) . #### Add a model to the pipeline . You’re about to take everything you’ve learned so far and implement it in a Pipeline that works with the real, DrivenData budget line item data you’ve been exploring. . Surprise! The structure of the pipeline is exactly the same as earlier in this chapter: . the preprocessing step uses FeatureUnion to join the results of nested pipelines that each rely on FunctionTransformer to select multiple datatypes | the model step stores the model object | . You can then call familiar methods like .fit() and .score() on the Pipeline object pl . . # Complete the pipeline: pl pl = Pipeline([ (&#39;union&#39;, FeatureUnion( transformer_list = [ (&#39;numeric_features&#39;, Pipeline([ (&#39;selector&#39;, get_numeric_data), (&#39;imputer&#39;, Imputer()) ])), (&#39;text_features&#39;, Pipeline([ (&#39;selector&#39;, get_text_data), (&#39;vectorizer&#39;, CountVectorizer()) ])) ] )), (&#39;clf&#39;, OneVsRestClassifier(LogisticRegression())) ]) # Fit to the training data pl.fit(X_train, y_train) # Compute and print accuracy accuracy = pl.score(X_test, y_test) print(&quot; nAccuracy on budget dataset: &quot;, accuracy) # Accuracy on budget dataset: 0.20384615384615384 . #### Try a different class of model . Now you’re cruising. One of the great strengths of pipelines is how easy they make the process of testing different models. . Until now, you’ve been using the model step (&#39;clf&#39;, OneVsRestClassifier(LogisticRegression())) in your pipeline. . But what if you want to try a different model? Do you need to build an entirely new pipeline? New nests? New FeatureUnions? Nope! You just have a simple one-line change, as you’ll see in this exercise. . In particular, you’ll swap out the logistic-regression model and replace it with a random forest classifier, which uses the statistics of an ensemble of decision trees to generate predictions. . # Import random forest classifer from sklearn.ensemble import RandomForestClassifier # Edit model step in pipeline pl = Pipeline([ (&#39;union&#39;, FeatureUnion( transformer_list = [ (&#39;numeric_features&#39;, Pipeline([ (&#39;selector&#39;, get_numeric_data), (&#39;imputer&#39;, Imputer()) ])), (&#39;text_features&#39;, Pipeline([ (&#39;selector&#39;, get_text_data), (&#39;vectorizer&#39;, CountVectorizer()) ])) ] )), (&#39;clf&#39;, RandomForestClassifier()) ]) # Fit to the training data pl.fit(X_train, y_train) # Compute and print accuracy accuracy = pl.score(X_test, y_test) print(&quot; nAccuracy on budget dataset: &quot;, accuracy) # Accuracy on budget dataset: 0.2826923076923077 . An accuracy improvement- amazing! All your work building the pipeline is paying off. It’s now very simple to test different models! . #### Can you adjust the model or parameters to improve accuracy? . You just saw a substantial improvement in accuracy by swapping out the model. Pipelines are amazing! . Can you make it better? Try changing the parameter n_estimators of RandomForestClassifier() , whose default value is 10 , to 15 . . # Import RandomForestClassifier from sklearn.ensemble import RandomForestClassifier # Add model step to pipeline: pl pl = Pipeline([ (&#39;union&#39;, FeatureUnion( transformer_list = [ (&#39;numeric_features&#39;, Pipeline([ (&#39;selector&#39;, get_numeric_data), (&#39;imputer&#39;, Imputer()) ])), (&#39;text_features&#39;, Pipeline([ (&#39;selector&#39;, get_text_data), (&#39;vectorizer&#39;, CountVectorizer()) ])) ] )), (&#39;clf&#39;, RandomForestClassifier(n_estimators=15)) ]) # Fit to the training data pl.fit(X_train, y_train) # Compute and print accuracy accuracy = pl.score(X_test, y_test) print(&quot; nAccuracy on budget dataset: &quot;, accuracy) # Accuracy on budget dataset: 0.3211538461538462 . Wow, you’re becoming a master! It’s time to get serious and work with the log loss metric. You’ll learn expert techniques in the next chapter to take the model to the next level. . . 4. Learning from the experts . . ### Learning from the expert: processing . #### Deciding what’s a word . Before you build up to the winning pipeline, it will be useful to look a little deeper into how the text features will be processed. . In this exercise, you will use CountVectorizer on the training data X_train (preloaded into the workspace) to see the effect of tokenization on punctuation. . Remember, since CountVectorizer expects a vector, you’ll need to use the preloaded function, combine_text_columns before fitting to the training data. . # Import the CountVectorizer from sklearn.feature_extraction.text import CountVectorizer # Create the text vector text_vector = combine_text_columns(X_train) # Create the token pattern: TOKENS_ALPHANUMERIC TOKENS_ALPHANUMERIC = &#39;[A-Za-z0-9]+(?= s+)&#39; # Instantiate the CountVectorizer: text_features text_features = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC) # Fit text_features to the text vector text_features.fit(text_vector) # Print the first 10 tokens print(text_features.get_feature_names()[:10]) # [&#39;00a&#39;, &#39;12&#39;, &#39;1st&#39;, &#39;2nd&#39;, &#39;3rd&#39;, &#39;5th&#39;, &#39;70&#39;, &#39;70h&#39;, &#39;8&#39;, &#39;a&#39;] . text_features.get_feature_names() [&#39;00a&#39;, &#39;12&#39;, &#39;1st&#39;, &#39;2nd&#39;, &#39;3rd&#39;, &#39;5th&#39;, &#39;70&#39;, &#39;70h&#39;, &#39;8&#39;, &#39;a&#39;, &#39;aaps&#39;, &#39;ab&#39;, &#39;acad&#39;, &#39;academ&#39;, &#39;academic&#39;, &#39;accelerated&#39;, &#39;access&#39;, ... . . #### N-gram range in scikit-learn . In this exercise you’ll insert a CountVectorizer instance into your pipeline for the main dataset, and compute multiple n-gram features to be used in the model. . In order to look for ngram relationships at multiple scales, you will use the ngram_range parameter as Peter discussed in the video. . Special functions: You’ll notice a couple of new steps provided in the pipeline in this and many of the remaining exercises. Specifically, the dim_red step following the vectorizer step , and the scale step preceeding the clf (classification) step. . These have been added in order to account for the fact that you’re using a reduced-size sample of the full dataset in this course. To make sure the models perform as the expert competition winner intended, we have to apply a dimensionality reduction technique, which is what the dim_red step does, and we have to scale the features to lie between -1 and 1, which is what the scale step does. . The dim_red step uses a scikit-learn function called SelectKBest() , applying something called the chi-squared test to select the K “best” features. The scale step uses a scikit-learn function called MaxAbsScaler() in order to squash the relevant features into the interval -1 to 1. . You won’t need to do anything extra with these functions here, just complete the vectorizing pipeline steps below. However, notice how easy it was to add more processing steps to our pipeline! . # Import pipeline from sklearn.pipeline import Pipeline # Import classifiers from sklearn.linear_model import LogisticRegression from sklearn.multiclass import OneVsRestClassifier # Import CountVectorizer from sklearn.feature_extraction.text import CountVectorizer # Import other preprocessing modules from sklearn.preprocessing import Imputer from sklearn.feature_selection import chi2, SelectKBest # Select 300 best features chi_k = 300 # Import functional utilities from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler from sklearn.pipeline import FeatureUnion # Perform preprocessing get_text_data = FunctionTransformer(combine_text_columns, validate=False) get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False) # Create the token pattern: TOKENS_ALPHANUMERIC TOKENS_ALPHANUMERIC = &#39;[A-Za-z0-9]+(?= s+)&#39; # Instantiate pipeline: pl pl = Pipeline([ (&#39;union&#39;, FeatureUnion( transformer_list = [ (&#39;numeric_features&#39;, Pipeline([ (&#39;selector&#39;, get_numeric_data), (&#39;imputer&#39;, Imputer()) ])), (&#39;text_features&#39;, Pipeline([ (&#39;selector&#39;, get_text_data), (&#39;vectorizer&#39;, CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC, ngram_range=(1,2))), (&#39;dim_red&#39;, SelectKBest(chi2, chi_k)) ])) ] )), (&#39;scale&#39;, MaxAbsScaler()), (&#39;clf&#39;, OneVsRestClassifier(LogisticRegression())) ]) . Log loss score: 1.2681. Great work! You’ll now add some additional tricks to make the pipeline even better. . . ### Learning from the expert: a stats trick . #### Implement interaction modeling in scikit-learn . It’s time to add interaction features to your model. The PolynomialFeatures object in scikit-learn does just that, but here you’re going to use a custom interaction object, SparseInteractions . Interaction terms are a statistical tool that lets your model express what happens if two features appear together in the same row. . SparseInteractions does the same thing as PolynomialFeatures , but it uses sparse matrices to do so. You can get the code for SparseInteractions at this GitHub Gist . . PolynomialFeatures and SparseInteractions both take the argument degree , which tells them what polynomial degree of interactions to compute. . You’re going to consider interaction terms of degree=2 in your pipeline. You will insert these steps after the preprocessing steps you’ve built out so far, but before the classifier steps. . Pipelines with interaction terms take a while to train (since you’re making n features into n-squared features!), so as long as you set it up right, we’ll do the heavy lifting and tell you what your score is! . # Instantiate pipeline: pl pl = Pipeline([ (&#39;union&#39;, FeatureUnion( transformer_list = [ (&#39;numeric_features&#39;, Pipeline([ (&#39;selector&#39;, get_numeric_data), (&#39;imputer&#39;, Imputer()) ])), (&#39;text_features&#39;, Pipeline([ (&#39;selector&#39;, get_text_data), (&#39;vectorizer&#39;, CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC, ngram_range=(1, 2))), (&#39;dim_red&#39;, SelectKBest(chi2, chi_k)) ])) ] )), (&#39;int&#39;, SparseInteractions(degree=2)), (&#39;scale&#39;, MaxAbsScaler()), (&#39;clf&#39;, OneVsRestClassifier(LogisticRegression())) ]) . Log loss score: 1.2256. Nice improvement from 1.2681! The student is becoming the master! . ### Learning from the expert: a computational trick and the winning model . #### Why is hashing a useful trick? . A hash function takes an input, in your case a token, and outputs a hash value. For example, the input may be a string and the hash value may be an integer. . We’ve loaded a familiar python datatype, a dictionary called hash_dict , that makes this mapping concept a bit more explicit. In fact, python dictionaries ARE hash tables ! . Print hash_dict in the IPython Shell to get a sense of how strings can be mapped to integers. . By explicitly stating how many possible outputs the hashing function may have, we limit the size of the objects that need to be processed. With these limits known, computation can be made more efficient and we can get results faster, even on large datasets. . Some problems are memory-bound and not easily parallelizable, and hashing enforces a fixed length computation instead of using a mutable datatype (like a dictionary). . hash_dict {&#39;and&#39;: 780, &#39;fluids&#39;: 354, &#39;fuel&#39;: 895, &#39;petro&#39;: 354, &#39;vend&#39;: 785} . #### Implementing the hashing trick in scikit-learn . In this exercise you will check out the scikit-learn implementation of HashingVectorizer before adding it to your pipeline later. . As you saw in the video, HashingVectorizer acts just like CountVectorizer in that it can accept token_pattern and ngram_range parameters. The important difference is that it creates hash values from the text, so that we get all the computational advantages of hashing! . # Import HashingVectorizer from sklearn.feature_extraction.text import HashingVectorizer # Get text data: text_data text_data = combine_text_columns(X_train) # Create the token pattern: TOKENS_ALPHANUMERIC TOKENS_ALPHANUMERIC = &#39;[A-Za-z0-9]+(?= s+)&#39; # Instantiate the HashingVectorizer: hashing_vec hashing_vec = HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC) # Fit and transform the Hashing Vectorizer hashed_text = hashing_vec.fit_transform(text_data) # Create DataFrame and print the head hashed_df = pd.DataFrame(hashed_text.data) print(hashed_df.head()) . As you can see, some text is hashed to the same value, but this doesn’t neccessarily hurt performance. . #### Build the winning model . You have arrived! This is where all of your hard work pays off. It’s time to build the model that won DrivenData’s competition. . You’ve constructed a robust, powerful pipeline capable of processing training and testing data. Now that you understand the data and know all of the tools you need, you can essentially solve the whole problem in a relatively small number of lines of code. Wow! . All you need to do is add the HashingVectorizer step to the pipeline to replace the CountVectorizer step. . The parameters non_negative=True , norm=None , and binary=False make the HashingVectorizer perform similarly to the default settings on the CountVectorizer so you can just replace one with the other. . # Import the hashing vectorizer from sklearn.feature_extraction.text import HashingVectorizer # Instantiate the winning model pipeline: pl pl = Pipeline([ (&#39;union&#39;, FeatureUnion( transformer_list = [ (&#39;numeric_features&#39;, Pipeline([ (&#39;selector&#39;, get_numeric_data), (&#39;imputer&#39;, Imputer()) ])), (&#39;text_features&#39;, Pipeline([ (&#39;selector&#39;, get_text_data), (&#39;vectorizer&#39;, HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC, non_negative=True, norm=None, binary=False, ngram_range=(1,2))), (&#39;dim_red&#39;, SelectKBest(chi2, chi_k)) ])) ] )), (&#39;int&#39;, SparseInteractions(degree=2)), (&#39;scale&#39;, MaxAbsScaler()), (&#39;clf&#39;, OneVsRestClassifier(LogisticRegression())) ]) . Log loss: 1.2258. Looks like the performance is about the same, but this is expected since the HashingVectorizer should work the same as the CountVectorizer. Try this pipeline out on the whole dataset on your local machine to see its full power! . #### What tactics got the winner the best score? . The winner used skillful NLP, efficient computation, and simple but powerful stats tricks to master the budget data. . Often times simpler is better, and understanding the problem in depth leads to simpler solutions! . Full solution code url . The End. . Thank you for reading. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/machine-learning-with-the-experts-school-budgets-from-datacamp.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/machine-learning-with-the-experts-school-budgets-from-datacamp.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post38": {
            "title": "Introduction to Shell for Data Science",
            "content": "Introduction to Shell for Data Science . This is the memo of the 19th course of ‘Data Scientist with Python’ track. . You can find the original course HERE . . . 1. Manipulating files and directories . . #### How does the shell compare to a desktop interface? . They are both interfaces for issuing commands to the operating system. . #### Some Basic commands: pwd, ls . pwd — p rint w orking d irectory . ls — l i s ting . #### Absolute Path vs. Relative Path . The shell decides if a path is absolute or relative by looking at its first character: if it begins with / , it is absolute, and if it doesn’t, it is relative. . #### Some Basic commands: cd . cd — change directory . #### Directory Basic . . — the current directory . .. — the directory above the one I’m currently in . ~ — your home directory . #### Some Basic commands: cp, mv . cp — copy . cp fileA fileB # copy fileA to fileB . cp fileA fileB fileC NewDirectory #copy fileA fileB fileC to NewDirectory . mv — move (when a new destination directory is specified) . mv – rename (when no new destination directory is specified) . same syntax as copy . mv fileA fileB # rename fileA to fileB . mv fileA fileB fileC NewDirectory #move fileA fileB fileC to NewDirectory . # caution: both cp and mv will overwrite the existing file . #### Some Basic commands: rm, rmdir, mkdir . rm — remove files . rm fileA fileB . # caution: if a file is removed, it’s removed forever . rmdir — remove an empty directory . rmdir directoryA . mkdir — make a directory . mkdir directoryA . . 2. Manipulating data . . #### Some Basic commands: cat . cat — concatenate, show file contents . cat fileA . #### Some Basic commands: less . less — is more, show file contents . less fileA fileB . when less fileA . space — page down . :n — to move to the next file . :p — to go back to the previous one . :q — to quit. . #### Some Basic commands: head . head — display first 10 lines of a file . head fileA . #### Tips: tab completion . If you start typing the name of a file and then press the tab key, the shell will do its best to auto-complete the path. . #### command-line flag . head -n 3 fileA # display first n(n = 3 here) lines of a file . -n — n umber of lines . #### ls -RF . ls -RF directoryA . ls -FR directoryA . ls -F -R directoryA . ls -R -F directoryA . are all the same command . -R — recursive, list everything below a directory . -F — prints a / after the name of every directory and a * after the name of every runnable program. . #### Some Basic commands: man . man — manual . man head . . man automatically invokes less , so you may need to press spacebar to page through the information and :q to quit. . The one-line description under NAME tells you briefly what the command does, and the summary under SYNOPSIS lists all the flags it understands. Anything that is optional is shown in square brackets [...] , either/or alternatives are separated by | , and things that can be repeated are shown by ... , so head ‘s manual page is telling you that you can either give a line count with -n or a byte count with -c , and that you can give it any number of filenames. . #### Some Basic commands: tail . tail — display last 10 lines of a file . tail -n 1 fileA # display last 1 lines of a fileA . tail -n +2 fileA # display from the 2nd lines to the end of a fileA . #### Some Basic commands: cut . cut -f 2-5,8 -d , fileA.csv . -f 2-5, 8 — fields, select columns 2 through 5 and columns 8, using comma as the separator . -d , — delimiter, use ‘,’ as delimiter . cut -f2 -d , fileA.csv . cut -f 2 -d , fileA.csv . are the same. . Space is optional between -f and 2 . #### repeat commands . history — print a list of commands you have run recently . !some_command — run the last some_command again(ex. !cut) . !2 — run the 2nd command listed in history . #### Some Basic commands: grep . grep patternA fileA . -c print a count of matching lines rather than the lines themselves | -h do not print the names of files when searching multiple files | -i ignore case (e.g., treat “Regression” and “regression” as matches) | -l print the names of files that contain matches, not the matches | -n print line numbers for matching lines | -v invert the match, i.e., only show lines that don’t match | . $ grep molar seasonal/autumn.csv 2017-02-01,molar 2017-05-25,molar $ grep -nv molar seasonal/spring.csv 1:Date,Tooth 2:2017-01-25,wisdom 3:2017-02-19,canine ... 8:2017-03-14,incisor 10:2017-04-29,wisdom 11:2017-05-08,canine ... 22:2017-08-13,incisor 23:2017-08-13,wisdom $ grep -c incisor seasonal/autumn.csv seasonal/winter.csv seasonal/autumn.csv:3 seasonal/winter.csv:6 . #### Some Basic commands: paste . paste — to combine data files . $ paste -d , seasonal/autumn.csv seasonal/winter.csv Date,Tooth,Date,Tooth2017-01-05,canine,2017-01-03,bicuspid 2017-01-17,wisdom,2017-01-05,incisor 2017-01-18,canine,2017-01-21,wisdom ... 2017-08-16,canine,2017-07-01,incisor ,2017-07-17,canine ,2017-08-10,incisor ... # The last few rows have the wrong number of columns. . . 3. Combining tools . . #### Store a command’s output in a file . some_command &gt; new_file . ex. tail -n 5 seasonal/winter.csv &gt; last.csv . #### combine commands . command_A | command_B | … | . $ cut -f 2 -d , seasonal/summer.csv | grep -v Tooth canine wisdom bicuspid ... . #### Some Basic commands: wc . wc — word count, prints the number of characters, words, and lines in a file. You can make it print only one of these using -c , -w , or -l respectively. . $ grep 2017-07 seasonal/spring.csv | wc -l | . #### *wildcards: ** . * , which means “match zero or more characters”. . $ head -n 3 seasonal/s*.csv ==&gt; seasonal/spring.csv &lt;== Date,Tooth 2017-01-25,wisdom 2017-02-19,canine ==&gt; seasonal/summer.csv &lt;== Date,Tooth 2017-01-11,canine 2017-01-18,wisdom . #### wildcards: ?, [], {} . ? matches a single character, so 201?.txt will match 2017.txt or 2018.txt , but not 2017-01.txt . | [...] matches any one of the characters inside the square brackets, so 201[78].txt matches 2017.txt or 2018.txt , but not 2016.txt . | {...} matches any of the comma-separated patterns inside the curly brackets, so {*.txt, *.csv} matches any file whose name ends with .txt or .csv , but not files whose names end with .pdf . | . #### Some Basic commands: sort . sort — By default it does this in ascending alphabetical order . -n and -r can be used to sort numerically and reverse the order of its output . -b tells it to ignore leading blanks . -f tells it to f old case (i.e., be case-insensitive) . $ sort -r seasonal/summer.csv Date,Tooth 2017-08-04,canine 2017-08-03,bicuspid 2017-08-02,canine ... . #### Some Basic commands: uniq . uniq — remove duplicated lines . If a file contains: . 2017-07-03 2017-07-03 2017-08-03 2017-08-03 . then uniq will produce: . 2017-07-03 2017-08-03 . but if it contains: . 2017-07-03 2017-08-03 2017-07-03 2017-08-03 . then uniq will print all four lines. . The reason is that uniq is built to work with very large files. In order to remove non-adjacent lines from a file, it would have to keep the whole file in memory (or at least, all the unique lines seen so far). By only removing adjacent duplicates, it only has to keep the most recent unique line in memory. . get the second column from seasonal/winter.csv , | remove the word “Tooth” from the output so that only tooth names are displayed | sort the output so that all occurrences of a particular tooth name are adjacent | display each tooth name once along with a count of how often it occurs | . $ cut -d , -f 2 seasonal/winter.csv | grep -v Tooth | sort | uniq -c 4 bicuspid 7 canine 6 incisor 4 molar 4 wisdom . #### stop a running program . Ctrl + C . #### Wrapping up . $ wc -l seasonal/*.csv 21 seasonal/autumn.csv 24 seasonal/spring.csv 25 seasonal/summer.csv 26 seasonal/winter.csv 96 total $ wc -l seasonal/*.csv | grep -v total 21 seasonal/autumn.csv 24 seasonal/spring.csv 25 seasonal/summer.csv 26 seasonal/winter.csv $ wc -l seasonal/*.csv | grep -v total | sort -n | head -n 1 21 seasonal/autumn.csv . . 4. Batch processing . . #### environment variables . | Variable | Purpose | Value | | — | — | — | | HOME | User’s home directory | /home/repl | | PWD | Present working directory | Same as pwd command | | SHELL | Which shell program is being used | /bin/bash | | USER | User’s ID | repl | . To get a complete list (which is quite long), you can type set in the shell. . ex. HISTFILESIZE determines how many old commands are stored in your command history. . $ set | grep HISTFILESIZE HISTFILESIZE=2000 . #### **print variable . echo $variable_name** . echo — prints its arguments. . To get the variable’s value, you must put a dollar sign $ in front of it. . This is true everywhere: to get the value of a variable called X , you must write $X . (This is so that the shell can tell whether you mean “a file named X” or “the value of a variable named X”.) . $ echo $OSTYPE linux-gnu . #### **shell variable . variable_name=value** . To create a shell variable, you simply assign a value to a name without any spaces before or after the = sign. . $ testing=seasonal/winter.csv $ head -n 1 $testing Date,Tooth . #### loops . for filetype in gif jpg png; do echo $filetype; done . it produces: . gif jpg png . Notice these things about the loop: . The structure is for …variable… in …list… ; do …body… ; done | The list of things the loop is to process (in our case, the words gif , jpg , and png ). | The variable that keeps track of which thing the loop is currently processing (in our case, filetype ). | The body of the loop that does the processing (in our case, echo $filetype ). | Notice that the body uses $filetype to get the variable’s value instead of just filetype , just like it does with any other shell variable. Also notice where the semi-colons go: the first one comes between the list and the keyword do , and the second comes between the body and the keyword done . . #### *loops with wildcard ** . $ for filename in seasonal/*.csv; do echo $filename; doneseasonal/autumn.csv seasonal/spring.csv seasonal/summer.csv seasonal/winter.csv . #### loops with variable $ . $ files=seasonal/*.csv $ for f in $files; do echo $f; done seasonal/autumn.csv seasonal/spring.csv seasonal/summer.csv seasonal/winter.csv . #### loops with pipe | . $ for file in seasonal/*.csv; do head -n 2 $file | tail -n 1; done 2017-01-05,canine 2017-01-25,wisdom 2017-01-11,canine 2017-01-03,bicuspid $ for file in seasonal/*.csv; do grep -h 2017-07 $file; done 2017-07-10,incisor 2017-07-10,wisdom 2017-07-20,incisor ... . #### Avoiding use space in file_name . use ‘ or ” if there is a space in file_name . mv &#39;July 2017.csv&#39; &#39;2017 July data.csv&#39; . #### loops with several commands . seperate commands with ; . $ for f in seasonal/*.csv; do echo $f head -n 2 $f | tail -n 1; done seasonal/autumn.csv head -n 2 seasonal/autumn.csv seasonal/spring.csv head -n 2 seasonal/spring.csv seasonal/summer.csv head -n 2 seasonal/summer.csv seasonal/winter.csv head -n 2 seasonal/winter.csv $ for f in seasonal/*.csv; do echo $f; head -n 2 $f | tail -n 1; done seasonal/autumn.csv 2017-01-05,canine seasonal/spring.csv 2017-01-25,wisdom seasonal/summer.csv 2017-01-11,canine seasonal/winter.csv 2017-01-03,bicuspid . . 5. Creating new tools . . #### Edit file with nano . Unix has a bewildering variety of text editors. For this course, we will use a simple one called Nano. If you type nano filename , it will open filename for editing (or create it if it doesn’t already exist). You can move around with the arrow keys, delete characters using backspace, and do other operations with control-key combinations: . Ctrl + K delete a line. | Ctrl + U un-delete a line. | Ctrl + O save the file (‘O’ stands for ‘output’). | Ctrl + X exit the editor. | . #### Save history commands for future use . $ cp seasonal/s*.csv ~/ $ grep -hv Tooth s*.csv &gt; temp.csv $ history | tail -n 3 &gt; steps.txt $ cat steps.txt 9 cp seasonal/s*.csv ~/ 10 grep -hv Tooth s*.csv &gt; temp.csv 11 history | tail -n 3 &gt; steps.txt . #### run shell script . run shell script using . bash script.sh . $ nano dates.sh $ cat dates.sh cut -d , -f 1 seasonal/*.csv $ bash dates.sh Date 2017-01-05 2017-01-17 2017-01-18 ... . #### save a script output into a file . $ nano teeth.sh $ cat teeth.sh cut -d , -f 2 seasonal/*.csv | grep -v Tooth | sort | uniq -c $ bash teeth.sh &gt; teeth.out $ cat teeth.out 15 bicuspid 31 canine 18 incisor 11 molar 17 wisdom . #### pass filenames to scripts $@ . if unique-lines.sh contains this: . sort $@ | uniq . then when you run: . bash unique-lines.sh seasonal/summer.csv . the shell replaces $@ with seasonal/summer.csv and processes one file. If you run this: . bash unique-lines.sh seasonal/summer.csv seasonal/autumn.csv . it processes two data files, and so on. . $ nano count-records.sh $ cat count-records.sh tail -q -n +2 $@ | wc -l $ bash count-records.sh seasonal/*.csv &gt; num-records.out $ head num-records.out 92 . #### command-line parameters . As well as $@ , the shell lets you use $1 , $2 , and so on to refer to specific command-line parameters. . The script get-field.sh is supposed to take a filename, the number of the row to select, the number of the column to select, and print just that field from a CSV file. . bash get-field.sh seasonal/summer.csv 4 2 . should select the second field from line 4 of seasonal/summer.csv . . head -n $2 $1 | tail -n 1 | cut -d , -f $3 bash get-field.sh seasonal/summer.csv 4 2 . #### scripts with 2 or more lines . $ cat range.sh wc -l $@ | grep -v total | sort -n | head -n 1 wc -l $@ | grep -v total | sort -nr | head -n 1 $ bash range.sh seasonal/*.csv &gt; range.out $ head range.out 21 seasonal/autumn.csv 26 seasonal/winter.csv . #### scripts with loops . $ cat date-range.sh # Print the first and last date from each data file. for filename in $@ do cut -d , -f 1 $filename | grep -v Date | sort | head -n 1 cut -d , -f 1 $filename | grep -v Date | sort | tail -n 1 done $ bash date-range.sh seasonal/*.csv 2017-01-05 2017-08-16 2017-01-25 ... $ bash date-range.sh seasonal/*.csv | sort 2017-01-03 2017-01-05 2017-01-11 ... . The End. . Thank you for reading. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/introduction-to-shell-for-data-science.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/introduction-to-shell-for-data-science.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post39": {
            "title": "Interactive Data Visualization with Bokeh",
            "content": "Interactive Data Visualization with Bokeh . Basic plotting with Bokeh . ### Plotting with glyphs . #### What are glyphs? . In Bokeh, visual properties of shapes are called glyphs. . Multiple glyphs can be drawn by setting glyph properties to ordered sequences of values. . #### A simple scatter plot . # Import figure from bokeh.plotting from bokeh.plotting import figure # Import output_file and show from bokeh.io from bokeh.io import output_file, show # Create the figure: p p = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add a circle glyph to the figure p p.circle(fertility, female_literacy) # Call the output_file() function and specify the name of the file output_file(&#39;fert_lit.html&#39;) # Display the plot show(p) . . #### A scatter plot with different shapes . # Create the figure: p p = figure(x_axis_label=&#39;fertility&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add a circle glyph to the figure p p.circle(fertility_latinamerica, female_literacy_latinamerica) # Add an x glyph to the figure p p.x(fertility_africa, female_literacy_africa) # Specify the name of the file output_file(&#39;fert_lit_separate.html&#39;) # Display the plot show(p) . . #### Customizing your scatter plots . # Create the figure: p p = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add a blue circle glyph to the figure p p.circle(fertility_latinamerica, female_literacy_latinamerica, color=&#39;blue&#39;, size=10, alpha=0.8) # Add a red circle glyph to the figure p p.circle(fertility_africa, female_literacy_africa, color=&#39;red&#39;, size=10, alpha=0.8) # Specify the name of the file output_file(&#39;fert_lit_separate_colors.html&#39;) # Display the plot show(p) . . CSS color names . ### Additional glyphs . #### Lines . # Import figure from bokeh.plotting from bokeh.plotting import figure # Create a figure with x_axis_type=&quot;datetime&quot;: p p = figure(x_axis_type=&quot;datetime&quot;, x_axis_label=&#39;Date&#39;, y_axis_label=&#39;US Dollars&#39;) # Plot date along the x axis and price along the y axis p.line(date,price) # Specify the name of the output file and show the result output_file(&#39;line.html&#39;) show(p) . . #### Lines and markers . # Import figure from bokeh.plotting from bokeh.plotting import figure # Create a figure with x_axis_type=&#39;datetime&#39;: p p = figure(x_axis_type=&#39;datetime&#39;, x_axis_label=&#39;Date&#39;, y_axis_label=&#39;US Dollars&#39;) # Plot date along the x-axis and price along the y-axis p.line(date, price) # With date on the x-axis and price on the y-axis, add a white circle glyph of size 4 p.circle(date, price, fill_color=&#39;white&#39;, size=4) # Specify the name of the output file and show the result output_file(&#39;line.html&#39;) show(p) . . #### Patches . # Create a list of az_lons, co_lons, nm_lons and ut_lons: x x = [az_lons, co_lons, nm_lons, ut_lons] # Create a list of az_lats, co_lats, nm_lats and ut_lats: y y = [az_lats, co_lats, nm_lats, ut_lats] # Add patches to figure p with line_color=white for x and y p.patches(x, y, line_color = &#39;white&#39;) # Specify the name of the output file and show the result output_file(&#39;four_corners.html&#39;) show(p) . . ### Data formats . #### Plotting data from NumPy arrays . # Import numpy as np import numpy as np # Create array using np.linspace: x x = np.linspace(0,5,100) # Create array using np.cos: y y = np.cos(x) # Add circles at x and y p.circle(x,y) # Specify the name of the output file and show the result output_file(&#39;numpy.html&#39;) show(p) . . #### Plotting data from Pandas DataFrames . # Import pandas as pd import pandas as pd # Read in the CSV file: df df = pd.read_csv(&#39;auto.csv&#39;) # Import figure from bokeh.plotting from bokeh.plotting import figure # Create the figure: p p = figure(x_axis_label=&#39;HP&#39;, y_axis_label=&#39;MPG&#39;) # Plot mpg vs hp by color p.circle(df[&#39;hp&#39;], df[&#39;mpg&#39;], color=df[&#39;color&#39;], size=10) # Specify the name of the output file and show the result output_file(&#39;auto-df.html&#39;) show(p) . . #### The Bokeh ColumnDataSource . The ColumnDataSource is a table-like data object that maps string column names to sequences (columns) of data. It is the central and most common data structure in Bokeh. . All columns in a ColumnDataSource must have the same length.press . #### The Bokeh ColumnDataSource (continued) . df.head() Name Country Medal Time Year color 0 Usain Bolt JAM GOLD 9.63 2012 goldenrod 1 Yohan Blake JAM SILVER 9.75 2012 silver 2 Justin Gatlin USA BRONZE 9.79 2012 saddlebrown 3 Usain Bolt JAM GOLD 9.69 2008 goldenrod 4 Richard Thompson TRI SILVER 9.89 2008 silver . # Import the ColumnDataSource class from bokeh.plotting from bokeh.plotting import ColumnDataSource # Create a ColumnDataSource from df: source source = ColumnDataSource(df) # Add circle glyphs to the figure p p.circle(&#39;Year&#39;, &#39;Time&#39;, source=source, color=&#39;color&#39;, size=8) # Specify the name of the output file and show the result output_file(&#39;sprint.html&#39;) show(p) . . #### Selection and non-selection glyphs . # Create a figure with the &quot;box_select&quot; tool: p p = figure(x_axis_label=&#39;Year&#39;, y_axis_label=&#39;Time&#39;, tools=&#39;box_select&#39;) # Add circle glyphs to the figure p with the selected and non-selected properties p.circle(&#39;Year&#39;, &#39;Time&#39;, source=source, selection_color = &#39;red&#39;, nonselection_alpha = 0.1) # Specify the name of the output file and show the result output_file(&#39;selection_glyph.html&#39;) show(p) . . #### Hover glyphs . # import the HoverTool from bokeh.models import HoverTool # Add circle glyphs to figure p p.circle(x, y, size=10, fill_color=&#39;grey&#39;, alpha=0.1, line_color=None, hover_fill_color=&#39;firebrick&#39;, hover_alpha=0.5, hover_line_color=&#39;white&#39;) # Create a HoverTool: hover hover = HoverTool(tooltips=None, mode=&#39;vline&#39;) # Add the hover tool to the figure p p.add_tools(hover) # Specify the name of the output file and show the result output_file(&#39;hover_glyph.html&#39;) show(p) . . #### Colormapping . #Import CategoricalColorMapper from bokeh.models from bokeh.models import CategoricalColorMapper # Convert df to a ColumnDataSource: source source = ColumnDataSource(df) # Make a CategoricalColorMapper object: color_mapper color_mapper = CategoricalColorMapper(factors=[&#39;Europe&#39;, &#39;Asia&#39;, &#39;US&#39;], palette=[&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;]) # Add a circle glyph to the figure p p.circle(&#39;weight&#39;, &#39;mpg&#39;, source=source, color=dict(field=&#39;origin&#39;, transform=color_mapper), legend=&#39;origin&#39;) # Specify the name of the output file and show the result output_file(&#39;colormap.html&#39;) show(p) . . Layouts, Interactions, and Annotations . ### Introduction to layouts . #### Creating rows of plots . # Import row from bokeh.layouts from bokeh.layouts import row # Create the first figure: p1 p1 = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add a circle glyph to p1 p1.circle(&#39;fertility&#39;, &#39;female_literacy&#39;, source=source) # Create the second figure: p2 p2 = figure(x_axis_label=&#39;population&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add a circle glyph to p2 p2.circle(&#39;population&#39;, &#39;female_literacy&#39;, source=source) # Put p1 and p2 into a horizontal row: layout layout = row(p1, p2) # Specify the name of the output_file and show the result output_file(&#39;fert_row.html&#39;) show(layout) . . #### Creating columns of plots . # Import column from the bokeh.layouts module from bokeh.layouts import column # Create a blank figure: p1 p1 = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add circle scatter to the figure p1 p1.circle(&#39;fertility&#39;, &#39;female_literacy&#39;, source=source) # Create a new blank figure: p2 p2 = figure(x_axis_label=&#39;population&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add circle scatter to the figure p2 p2.circle(&#39;population&#39;, &#39;female_literacy&#39;, source=source) # Put plots p1 and p2 in a column: layout layout = column(p1, p2) # Specify the name of the output_file and show the result output_file(&#39;fert_column.html&#39;) show(layout) . . #### Nesting rows and columns of plots . # Import column and row from bokeh.layouts from bokeh.layouts import row, column # Make a column layout that will be used as the second row: row2 row2 = column([mpg_hp, mpg_weight], sizing_mode=&#39;scale_width&#39;) # Make a row layout that includes the above column layout: layout layout = row([avg_mpg, row2], sizing_mode=&#39;scale_width&#39;) # Specify the name of the output_file and show the result output_file(&#39;layout_custom.html&#39;) show(layout) . . ### Advanced layouts . #### Creating gridded layouts . # Import gridplot from bokeh.layouts from bokeh.layouts import gridplot # Create a list containing plots p1 and p2: row1 row1 = [p1, p2] # Create a list containing plots p3 and p4: row2 row2 = [p3, p4] # Create a gridplot using row1 and row2: layout layout = gridplot([row1, row2]) # Specify the name of the output_file and show the result output_file(&#39;grid.html&#39;) show(layout) . . #### Starting tabbed layouts . # Import Panel from bokeh.models.widgets from bokeh.models.widgets import Panel # Create tab1 from plot p1: tab1 tab1 = Panel(child=p1, title=&#39;Latin America&#39;) # Create tab2 from plot p2: tab2 tab2 = Panel(child=p2, title=&#39;Africa&#39;) # Create tab3 from plot p3: tab3 tab3 = Panel(child=p3, title=&#39;Asia&#39;) # Create tab4 from plot p4: tab4 tab4 = Panel(child=p4, title=&#39;Europe&#39;) . #### Displaying tabbed layouts . # Import Tabs from bokeh.models.widgets from bokeh.models.widgets import Tabs # Create a Tabs layout: layout layout = Tabs(tabs=[tab1, tab2, tab3, tab4]) # Specify the name of the output_file and show the result output_file(&#39;tabs.html&#39;) show(layout) . . ### Linking plots together . #### Linked axes . # Link the x_range of p2 to p1: p2.x_range p2.x_range = p1.x_range # Link the y_range of p2 to p1: p2.y_range p2.y_range = p1.y_range # Link the x_range of p3 to p1: p3.x_range p3.x_range = p1.x_range # Link the y_range of p4 to p1: p4.y_range p4.y_range = p1.y_range # Specify the name of the output_file and show the result output_file(&#39;linked_range.html&#39;) show(layout) . . #### Linked brushing . Country Continent female literacy fertility population 0 Chine ASI 90.5 1.769 1324.655000 1 Inde ASI 50.8 2.682 1139.964932 2 USA NAM 99 2.077 304.060000 3 Indonésie ASI 88.8 2.132 227.345082 4 Brésil LAT 90.2 1.827 191.971506 . # Create ColumnDataSource: source source = ColumnDataSource(data) # Create the first figure: p1 p1 = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;female literacy (% population)&#39;, tools=&#39;box_select,lasso_select&#39;) # Add a circle glyph to p1 p1.circle(&#39;fertility&#39;, &#39;female literacy&#39;, source=source) # Create the second figure: p2 p2 = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;population (millions)&#39;, tools=&#39;box_select,lasso_select&#39;) # Add a circle glyph to p2 p2.circle(&#39;fertility&#39;, &#39;population&#39;, source=source) # Create row layout of figures p1 and p2: layout layout = row(p1, p2) # Specify the name of the output_file and show the result output_file(&#39;linked_brush.html&#39;) show(layout) . . ### Annotations and guides . #### How to create legends . # Add the first circle glyph to the figure p p.circle(&#39;fertility&#39;, &#39;female_literacy&#39;, source=latin_america, size=10, color=&#39;red&#39;, legend=&#39;Latin America&#39;) # Add the second circle glyph to the figure p p.circle(&#39;fertility&#39;, &#39;female_literacy&#39;, source=africa, size=10, color=&#39;blue&#39;, legend=&#39;Africa&#39;) # Specify the name of the output_file and show the result output_file(&#39;fert_lit_groups.html&#39;) show(p) . . #### Positioning and styling legends . # Assign the legend to the bottom left: p.legend.location p.legend.location = &#39;bottom_left&#39; # Fill the legend background with the color &#39;lightgray&#39;: p.legend.background_fill_color p.legend.background_fill_color = &#39;lightgray&#39; # Specify the name of the output_file and show the result output_file(&#39;fert_lit_groups.html&#39;) show(p) . . #### Adding a hover tooltip . # Import HoverTool from bokeh.models from bokeh.models import HoverTool # Create a HoverTool object: hover hover = HoverTool(tooltips = [(&#39;Country&#39;,&#39;@Country&#39;)]) # Add the HoverTool object to figure p p.add_tools(hover) # Specify the name of the output_file and show the result output_file(&#39;hover.html&#39;) show(p) . . Building interactive apps with Bokeh . ### Introducing the Bokeh Server . #### Understanding Bokeh apps . The main purpose of the Bokeh server is to synchronize python objects with web applications in a browser, so that rich, interactive data applications can be connected to powerful PyData libraries such as NumPy, SciPy, Pandas, and scikit-learn. . The Bokeh server can automatically keep in sync any property of any Bokeh object. . bokeh serve myapp.py . #### . Using the current document . # Perform necessary imports from bokeh.io import curdoc from bokeh.plotting import figure # Create a new plot: plot plot = figure() # Add a line to the plot plot.line(x = [1,2,3,4,5], y = [2,5,4,6,7]) # Add the plot to the current document curdoc().add_root(plot) . . #### Add a single slider . # Perform the necessary imports from bokeh.io import curdoc from bokeh.layouts import widgetbox from bokeh.models import Slider # Create a slider: slider slider = Slider(title=&#39;my slider&#39;, start=0, end=10, step=0.1, value=2) # Create a widgetbox layout: layout layout = widgetbox(slider) # Add the layout to the current document curdoc().add_root(layout) . . #### Multiple sliders in one document . # Perform necessary imports from bokeh.io import curdoc from bokeh.layouts import widgetbox from bokeh.models import Slider # Create first slider: slider1 slider1 = Slider(title = &#39;slider1&#39;, start = 0, end = 10, step = 0.1, value = 2) # Create second slider: slider2 slider2 = Slider(title = &#39;slider2&#39;, start = 10, end = 100, step = 1, value = 20) # Add slider1 and slider2 to a widgetbox layout = widgetbox(slider1, slider2) # Add the layout to the current document curdoc().add_root(layout) . . ### Connecting sliders to plots . #### Adding callbacks to sliders . Callbacks are functions that a user can define, like def callback(attr, old, new) , that can be called automatically when some property of a Bokeh object (e.g., the value of a Slider ) changes. . For the value property of Slider objects, callbacks are added by passing a callback function to the on_change method. . myslider.on_change(&#39;value&#39;, callback) . #### How to combine Bokeh models into layouts . # Create ColumnDataSource: source source = ColumnDataSource(data = {&#39;x&#39;: x, &#39;y&#39;: y}) # Add a line to the plot plot.line(&#39;x&#39;, &#39;y&#39;, source=source) # Create a column layout: layout layout = column(widgetbox(slider), plot) # Add the layout to the current document curdoc().add_root(layout) . . #### Learn about widget callbacks . # Define a callback function: callback def callback(attr, old, new): # Read the current value of the slider: scale scale = slider.value # Compute the updated y using np.sin(scale/x): new_y new_y = np.sin(scale/x) # Update source with the new data values source.data = {&#39;x&#39;: x, &#39;y&#39;: new_y} # Attach the callback to the &#39;value&#39; property of slider slider.on_change(&#39;value&#39;, callback) # Create layout and add to current document layout = column(widgetbox(slider), plot) curdoc().add_root(layout) . . . ### Updating plots from dropdowns . #### Updating data sources from dropdown callbacks . # Perform necessary imports from bokeh.models import ColumnDataSource, Select # Create ColumnDataSource: source source = ColumnDataSource(data={ &#39;x&#39; : fertility, &#39;y&#39; : female_literacy }) # Create a new plot: plot plot = figure() # Add circles to the plot plot.circle(&#39;x&#39;, &#39;y&#39;, source=source) # Define a callback function: update_plot def update_plot(attr, old, new): # If the new Selection is &#39;female_literacy&#39;, update &#39;y&#39; to female_literacy if new == &#39;female_literacy&#39;: source.data = { &#39;x&#39; : fertility, &#39;y&#39; : female_literacy } # Else, update &#39;y&#39; to population else: source.data = { &#39;x&#39; : fertility, &#39;y&#39; : population } # Create a dropdown Select widget: select select = Select(title=&quot;distribution&quot;, options=[&#39;female_literacy&#39;, &#39;population&#39;], value=&#39;female_literacy&#39;) # Attach the update_plot callback to the &#39;value&#39; property of select select.on_change(&#39;value&#39;, update_plot) # Create layout and add to current document layout = row(select, plot) curdoc().add_root(layout) . . #### Synchronize two dropdowns . # Create two dropdown Select widgets: select1, select2 select1 = Select(title=&#39;First&#39;, options=[&#39;A&#39;, &#39;B&#39;], value=&#39;A&#39;) select2 = Select(title=&#39;Second&#39;, options=[&#39;1&#39;, &#39;2&#39;, &#39;3&#39;], value=&#39;1&#39;) # Define a callback function: callback def callback(attr, old, new): # If select1 is &#39;A&#39; if select1.value == &#39;A&#39;: # Set select2 options to [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;] select2.options = [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;] # Set select2 value to &#39;1&#39; select2.value = &#39;1&#39; else: # Set select2 options to [&#39;100&#39;, &#39;200&#39;, &#39;300&#39;] select2.options = [&#39;100&#39;, &#39;200&#39;, &#39;300&#39;] # Set select2 value to &#39;100&#39; select2.value = &#39;100&#39; # Attach the callback to the &#39;value&#39; property of select1 select1.on_change(&#39;value&#39;, callback) # Create layout and add to current document layout = widgetbox(select1, select2) curdoc().add_root(layout) . . . ### Buttons . #### Button widgets . # Create a Button with label &#39;Update Data&#39; button = Button(label=&#39;Update Data&#39;) # Define an update callback with no arguments: update def update(): # Compute new y values: y y = np.sin(x) + np.random.random(N) # Update the ColumnDataSource data dictionary source.data = {&#39;x&#39;:x,&#39;y&#39;:y} # Add the update callback to the button button.on_click(update) # Create layout and add to current document layout = column(widgetbox(button), plot) curdoc().add_root(layout) . . #### Button styles . # Import CheckboxGroup, RadioGroup, Toggle from bokeh.models from bokeh.models import CheckboxGroup, RadioGroup, Toggle # Add a Toggle: toggle toggle = Toggle(button_type = &#39;success&#39;, label = &#39;Toggle button&#39;) # Add a CheckboxGroup: checkbox checkbox = CheckboxGroup(labels=[&#39;Option 1&#39;, &#39;Option 2&#39;, &#39;Option 3&#39;]) # Add a RadioGroup: radio radio = RadioGroup(labels=[&#39;Option 1&#39;, &#39;Option 2&#39;, &#39;Option 3&#39;]) # Add widgetbox(toggle, checkbox, radio) to the current document curdoc().add_root(widgetbox(toggle, checkbox, radio)) . . Putting It All Together! A Case Study . ### Time to put it all together! . #### Introducing the project dataset . data.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 10111 entries, 1964 to 2006 Data columns (total 7 columns): Country 10111 non-null object fertility 10100 non-null float64 life 10111 non-null float64 population 10108 non-null float64 child_mortality 9210 non-null float64 gdp 9000 non-null float64 region 10111 non-null object dtypes: float64(5), object(2) memory usage: 631.9+ KB data.head() Country fertility life population child_mortality gdp Year 1964 Afghanistan 7.671 33.639 10474903.0 339.7 1182.0 1965 Afghanistan 7.671 34.152 10697983.0 334.1 1182.0 1966 Afghanistan 7.671 34.662 10927724.0 328.7 1168.0 1967 Afghanistan 7.671 35.170 11163656.0 323.3 1173.0 1968 Afghanistan 7.671 35.674 11411022.0 318.1 1187.0 region Year 1964 South Asia 1965 South Asia 1966 South Asia 1967 South Asia 1968 South Asia . #### Some exploratory plots of the data . # Perform necessary imports from bokeh.io import output_file, show from bokeh.plotting import figure from bokeh.models import HoverTool, ColumnDataSource # Make the ColumnDataSource: source source = ColumnDataSource(data={ &#39;x&#39; : data.loc[1970].fertility, &#39;y&#39; : data.loc[1970].life, &#39;country&#39; : data.loc[1970].Country, }) # Create the figure: p p = figure(title=&#39;1970&#39;, x_axis_label=&#39;Fertility (children per woman)&#39;, y_axis_label=&#39;Life Expectancy (years)&#39;, plot_height=400, plot_width=700, tools=[HoverTool(tooltips=&#39;@country&#39;)]) # Add a circle glyph to the figure p p.circle(x=&#39;x&#39;, y=&#39;y&#39;, source=source) # Output the file and show the figure output_file(&#39;gapminder.html&#39;) show(p) . . ### Starting the app . . #### Beginning with just a plot . # Import the necessary modules from bokeh.io import curdoc from bokeh.models import ColumnDataSource from bokeh.plotting import figure # Make the ColumnDataSource: source source = ColumnDataSource(data={ &#39;x&#39; : data.loc[1970].fertility, &#39;y&#39; : data.loc[1970].life, &#39;country&#39; : data.loc[1970].Country, &#39;pop&#39; : (data.loc[1970].population / 20000000) + 2, &#39;region&#39; : data.loc[1970].region, }) # Save the minimum and maximum values of the fertility column: xmin, xmax xmin, xmax = min(data.fertility), max(data.fertility) # Save the minimum and maximum values of the life expectancy column: ymin, ymax ymin, ymax = min(data.life), max(data.life) # Create the figure: plot plot = figure(title=&#39;Gapminder Data for 1970&#39;, plot_height=400, plot_width=700, x_range=(xmin, xmax), y_range=(ymin, ymax)) # Add circle glyphs to the plot plot.circle(x=&#39;x&#39;, y=&#39;y&#39;, fill_alpha=0.8, source=source) # Set the x-axis label plot.xaxis.axis_label =&#39;Fertility (children per woman)&#39; # Set the y-axis label plot.yaxis.axis_label = &#39;Life Expectancy (years)&#39; # Add the plot to the current document and add a title curdoc().add_root(plot) curdoc().title = &#39;Gapminder&#39; . . #### Enhancing the plot with some shading . # Make a list of the unique values from the region column: regions_list regions_list = data.region.unique().tolist() # Import CategoricalColorMapper from bokeh.models and the Spectral6 palette from bokeh.palettes from bokeh.models import CategoricalColorMapper from bokeh.palettes import Spectral6 # Make a color mapper: color_mapper color_mapper = CategoricalColorMapper(factors=regions_list, palette=Spectral6) # Add the color mapper to the circle glyph plot.circle(x=&#39;x&#39;, y=&#39;y&#39;, fill_alpha=0.8, source=source, color=dict(field=&#39;region&#39;, transform=color_mapper), legend=&#39;region&#39;) # Set the legend.location attribute of the plot to &#39;top_right&#39; plot.legend.location = &#39;top_right&#39; # Add the plot to the current document and add the title curdoc().add_root(plot) curdoc().title = &#39;Gapminder&#39; . . #### Adding a slider to vary the year . # Import the necessary modules from bokeh.layouts import row, widgetbox from bokeh.models import Slider # Define the callback function: update_plot def update_plot(attr, old, new): # Set the yr name to slider.value and new_data to source.data yr = slider.value new_data = { &#39;x&#39; : data.loc[yr].fertility, &#39;y&#39; : data.loc[yr].life, &#39;country&#39; : data.loc[yr].Country, &#39;pop&#39; : (data.loc[yr].population / 20000000) + 2, &#39;region&#39; : data.loc[yr].region, } source.data = new_data # Make a slider object: slider slider = Slider(start = 1970, end = 2010, step = 1, value = 1970, title = &#39;Year&#39;) # Attach the callback to the &#39;value&#39; property of slider slider.on_change(&#39;value&#39;, update_plot) # Make a row layout of widgetbox(slider) and plot and add it to the current document layout = row(widgetbox(slider), plot) curdoc().add_root(layout) . . ### Customizing based on user input . # Define the callback function: update_plot def update_plot(attr, old, new): # Assign the value of the slider: yr yr = slider.value # Set new_data new_data = { &#39;x&#39; : data.loc[yr].fertility, &#39;y&#39; : data.loc[yr].life, &#39;country&#39; : data.loc[yr].Country, &#39;pop&#39; : (data.loc[yr].population / 20000000) + 2, &#39;region&#39; : data.loc[yr].region, } # Assign new_data to: source.data source.data = new_data # Add title to figure: plot.title.text plot.title.text = &#39;Gapminder data for %d&#39; % yr # Make a slider object: slider slider = Slider(start = 1970, end = 2010, step = 1, value = 1970, title = &#39;Year&#39;) # Attach the callback to the &#39;value&#39; property of slider slider.on_change(&#39;value&#39;, update_plot) # Make a row layout of widgetbox(slider) and plot and add it to the current document layout = row(widgetbox(slider), plot) curdoc().add_root(layout) . . ### Adding more interactivity to the app . #### Adding a hover tool . . # Import HoverTool from bokeh.models from bokeh.models import HoverTool # Create a HoverTool: hover hover = HoverTool(tooltips=[(&#39;Country&#39;, &#39;@country&#39;)]) # Add the HoverTool to the plot plot.add_tools(hover) # Create layout: layout layout = row(widgetbox(slider), plot) # Add layout to current document curdoc().add_root(layout) . . #### Adding dropdowns to the app . . # Define the callback: update_plot def update_plot(attr, old, new): # Read the current value off the slider and 2 dropdowns: yr, x, y yr = slider.value x = x_select.value y = y_select.value # Label axes of plot plot.xaxis.axis_label = x plot.yaxis.axis_label = y # Set new_data new_data = { &#39;x&#39; : data.loc[yr][x], &#39;y&#39; : data.loc[yr][y], &#39;country&#39; : data.loc[yr].Country, &#39;pop&#39; : (data.loc[yr].population / 20000000) + 2, &#39;region&#39; : data.loc[yr].region, } # Assign new_data to source.data source.data = new_data # Set the range of all axes plot.x_range.start = min(data[x]) plot.x_range.end = max(data[x]) plot.y_range.start = min(data[y]) plot.y_range.end = max(data[y]) # Add title to plot plot.title.text = &#39;Gapminder data for %d&#39; % yr # Create a dropdown slider widget: slider slider = Slider(start=1970, end=2010, step=1, value=1970, title=&#39;Year&#39;) # Attach the callback to the &#39;value&#39; property of slider slider.on_change(&#39;value&#39;, update_plot) # Create a dropdown Select widget for the x data: x_select x_select = Select( options=[&#39;fertility&#39;, &#39;life&#39;, &#39;child_mortality&#39;, &#39;gdp&#39;], value=&#39;fertility&#39;, title=&#39;x-axis data&#39; ) # Attach the update_plot callback to the &#39;value&#39; property of x_select x_select.on_change(&#39;value&#39;, update_plot) # Create a dropdown Select widget for the y data: y_select y_select = Select( options=[&#39;fertility&#39;, &#39;life&#39;, &#39;child_mortality&#39;, &#39;gdp&#39;], value=&#39;life&#39;, title=&#39;y-axis data&#39; ) # Attach the update_plot callback to the &#39;value&#39; property of y_select y_select.on_change(&#39;value&#39;, update_plot) # Create layout and add to current document layout = row(widgetbox(slider, x_select, y_select), plot) curdoc().add_root(layout) . Basic plotting with Bokeh . ### Plotting with glyphs . #### What are glyphs? . In Bokeh, visual properties of shapes are called glyphs. . Multiple glyphs can be drawn by setting glyph properties to ordered sequences of values. . #### A simple scatter plot . # Import figure from bokeh.plotting from bokeh.plotting import figure # Import output_file and show from bokeh.io from bokeh.io import output_file, show # Create the figure: p p = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add a circle glyph to the figure p p.circle(fertility, female_literacy) # Call the output_file() function and specify the name of the file output_file(&#39;fert_lit.html&#39;) # Display the plot show(p) . . #### A scatter plot with different shapes . # Create the figure: p p = figure(x_axis_label=&#39;fertility&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add a circle glyph to the figure p p.circle(fertility_latinamerica, female_literacy_latinamerica) # Add an x glyph to the figure p p.x(fertility_africa, female_literacy_africa) # Specify the name of the file output_file(&#39;fert_lit_separate.html&#39;) # Display the plot show(p) . . #### Customizing your scatter plots . # Create the figure: p p = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add a blue circle glyph to the figure p p.circle(fertility_latinamerica, female_literacy_latinamerica, color=&#39;blue&#39;, size=10, alpha=0.8) # Add a red circle glyph to the figure p p.circle(fertility_africa, female_literacy_africa, color=&#39;red&#39;, size=10, alpha=0.8) # Specify the name of the file output_file(&#39;fert_lit_separate_colors.html&#39;) # Display the plot show(p) . . CSS color names . ### Additional glyphs . #### Lines . # Import figure from bokeh.plotting from bokeh.plotting import figure # Create a figure with x_axis_type=&quot;datetime&quot;: p p = figure(x_axis_type=&quot;datetime&quot;, x_axis_label=&#39;Date&#39;, y_axis_label=&#39;US Dollars&#39;) # Plot date along the x axis and price along the y axis p.line(date,price) # Specify the name of the output file and show the result output_file(&#39;line.html&#39;) show(p) . . #### Lines and markers . # Import figure from bokeh.plotting from bokeh.plotting import figure # Create a figure with x_axis_type=&#39;datetime&#39;: p p = figure(x_axis_type=&#39;datetime&#39;, x_axis_label=&#39;Date&#39;, y_axis_label=&#39;US Dollars&#39;) # Plot date along the x-axis and price along the y-axis p.line(date, price) # With date on the x-axis and price on the y-axis, add a white circle glyph of size 4 p.circle(date, price, fill_color=&#39;white&#39;, size=4) # Specify the name of the output file and show the result output_file(&#39;line.html&#39;) show(p) . . #### Patches . # Create a list of az_lons, co_lons, nm_lons and ut_lons: x x = [az_lons, co_lons, nm_lons, ut_lons] # Create a list of az_lats, co_lats, nm_lats and ut_lats: y y = [az_lats, co_lats, nm_lats, ut_lats] # Add patches to figure p with line_color=white for x and y p.patches(x, y, line_color = &#39;white&#39;) # Specify the name of the output file and show the result output_file(&#39;four_corners.html&#39;) show(p) . . ### Data formats . #### Plotting data from NumPy arrays . # Import numpy as np import numpy as np # Create array using np.linspace: x x = np.linspace(0,5,100) # Create array using np.cos: y y = np.cos(x) # Add circles at x and y p.circle(x,y) # Specify the name of the output file and show the result output_file(&#39;numpy.html&#39;) show(p) . . #### Plotting data from Pandas DataFrames . # Import pandas as pd import pandas as pd # Read in the CSV file: df df = pd.read_csv(&#39;auto.csv&#39;) # Import figure from bokeh.plotting from bokeh.plotting import figure # Create the figure: p p = figure(x_axis_label=&#39;HP&#39;, y_axis_label=&#39;MPG&#39;) # Plot mpg vs hp by color p.circle(df[&#39;hp&#39;], df[&#39;mpg&#39;], color=df[&#39;color&#39;], size=10) # Specify the name of the output file and show the result output_file(&#39;auto-df.html&#39;) show(p) . . #### The Bokeh ColumnDataSource . The ColumnDataSource is a table-like data object that maps string column names to sequences (columns) of data. It is the central and most common data structure in Bokeh. . All columns in a ColumnDataSource must have the same length.press . #### The Bokeh ColumnDataSource (continued) . df.head() Name Country Medal Time Year color 0 Usain Bolt JAM GOLD 9.63 2012 goldenrod 1 Yohan Blake JAM SILVER 9.75 2012 silver 2 Justin Gatlin USA BRONZE 9.79 2012 saddlebrown 3 Usain Bolt JAM GOLD 9.69 2008 goldenrod 4 Richard Thompson TRI SILVER 9.89 2008 silver . # Import the ColumnDataSource class from bokeh.plotting from bokeh.plotting import ColumnDataSource # Create a ColumnDataSource from df: source source = ColumnDataSource(df) # Add circle glyphs to the figure p p.circle(&#39;Year&#39;, &#39;Time&#39;, source=source, color=&#39;color&#39;, size=8) # Specify the name of the output file and show the result output_file(&#39;sprint.html&#39;) show(p) . . #### Selection and non-selection glyphs . # Create a figure with the &quot;box_select&quot; tool: p p = figure(x_axis_label=&#39;Year&#39;, y_axis_label=&#39;Time&#39;, tools=&#39;box_select&#39;) # Add circle glyphs to the figure p with the selected and non-selected properties p.circle(&#39;Year&#39;, &#39;Time&#39;, source=source, selection_color = &#39;red&#39;, nonselection_alpha = 0.1) # Specify the name of the output file and show the result output_file(&#39;selection_glyph.html&#39;) show(p) . . #### Hover glyphs . # import the HoverTool from bokeh.models import HoverTool # Add circle glyphs to figure p p.circle(x, y, size=10, fill_color=&#39;grey&#39;, alpha=0.1, line_color=None, hover_fill_color=&#39;firebrick&#39;, hover_alpha=0.5, hover_line_color=&#39;white&#39;) # Create a HoverTool: hover hover = HoverTool(tooltips=None, mode=&#39;vline&#39;) # Add the hover tool to the figure p p.add_tools(hover) # Specify the name of the output file and show the result output_file(&#39;hover_glyph.html&#39;) show(p) . . #### Colormapping . #Import CategoricalColorMapper from bokeh.models from bokeh.models import CategoricalColorMapper # Convert df to a ColumnDataSource: source source = ColumnDataSource(df) # Make a CategoricalColorMapper object: color_mapper color_mapper = CategoricalColorMapper(factors=[&#39;Europe&#39;, &#39;Asia&#39;, &#39;US&#39;], palette=[&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;]) # Add a circle glyph to the figure p p.circle(&#39;weight&#39;, &#39;mpg&#39;, source=source, color=dict(field=&#39;origin&#39;, transform=color_mapper), legend=&#39;origin&#39;) # Specify the name of the output file and show the result output_file(&#39;colormap.html&#39;) show(p) . . Layouts, Interactions, and Annotations . ### Introduction to layouts . #### Creating rows of plots . # Import row from bokeh.layouts from bokeh.layouts import row # Create the first figure: p1 p1 = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add a circle glyph to p1 p1.circle(&#39;fertility&#39;, &#39;female_literacy&#39;, source=source) # Create the second figure: p2 p2 = figure(x_axis_label=&#39;population&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add a circle glyph to p2 p2.circle(&#39;population&#39;, &#39;female_literacy&#39;, source=source) # Put p1 and p2 into a horizontal row: layout layout = row(p1, p2) # Specify the name of the output_file and show the result output_file(&#39;fert_row.html&#39;) show(layout) . . #### Creating columns of plots . # Import column from the bokeh.layouts module from bokeh.layouts import column # Create a blank figure: p1 p1 = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add circle scatter to the figure p1 p1.circle(&#39;fertility&#39;, &#39;female_literacy&#39;, source=source) # Create a new blank figure: p2 p2 = figure(x_axis_label=&#39;population&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add circle scatter to the figure p2 p2.circle(&#39;population&#39;, &#39;female_literacy&#39;, source=source) # Put plots p1 and p2 in a column: layout layout = column(p1, p2) # Specify the name of the output_file and show the result output_file(&#39;fert_column.html&#39;) show(layout) . . #### Nesting rows and columns of plots . # Import column and row from bokeh.layouts from bokeh.layouts import row, column # Make a column layout that will be used as the second row: row2 row2 = column([mpg_hp, mpg_weight], sizing_mode=&#39;scale_width&#39;) # Make a row layout that includes the above column layout: layout layout = row([avg_mpg, row2], sizing_mode=&#39;scale_width&#39;) # Specify the name of the output_file and show the result output_file(&#39;layout_custom.html&#39;) show(layout) . . ### Advanced layouts . #### Creating gridded layouts . # Import gridplot from bokeh.layouts from bokeh.layouts import gridplot # Create a list containing plots p1 and p2: row1 row1 = [p1, p2] # Create a list containing plots p3 and p4: row2 row2 = [p3, p4] # Create a gridplot using row1 and row2: layout layout = gridplot([row1, row2]) # Specify the name of the output_file and show the result output_file(&#39;grid.html&#39;) show(layout) . . #### Starting tabbed layouts . # Import Panel from bokeh.models.widgets from bokeh.models.widgets import Panel # Create tab1 from plot p1: tab1 tab1 = Panel(child=p1, title=&#39;Latin America&#39;) # Create tab2 from plot p2: tab2 tab2 = Panel(child=p2, title=&#39;Africa&#39;) # Create tab3 from plot p3: tab3 tab3 = Panel(child=p3, title=&#39;Asia&#39;) # Create tab4 from plot p4: tab4 tab4 = Panel(child=p4, title=&#39;Europe&#39;) . #### Displaying tabbed layouts . # Import Tabs from bokeh.models.widgets from bokeh.models.widgets import Tabs # Create a Tabs layout: layout layout = Tabs(tabs=[tab1, tab2, tab3, tab4]) # Specify the name of the output_file and show the result output_file(&#39;tabs.html&#39;) show(layout) . . ### Linking plots together . #### Linked axes . # Link the x_range of p2 to p1: p2.x_range p2.x_range = p1.x_range # Link the y_range of p2 to p1: p2.y_range p2.y_range = p1.y_range # Link the x_range of p3 to p1: p3.x_range p3.x_range = p1.x_range # Link the y_range of p4 to p1: p4.y_range p4.y_range = p1.y_range # Specify the name of the output_file and show the result output_file(&#39;linked_range.html&#39;) show(layout) . . #### Linked brushing . Country Continent female literacy fertility population 0 Chine ASI 90.5 1.769 1324.655000 1 Inde ASI 50.8 2.682 1139.964932 2 USA NAM 99 2.077 304.060000 3 Indonésie ASI 88.8 2.132 227.345082 4 Brésil LAT 90.2 1.827 191.971506 . # Create ColumnDataSource: source source = ColumnDataSource(data) # Create the first figure: p1 p1 = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;female literacy (% population)&#39;, tools=&#39;box_select,lasso_select&#39;) # Add a circle glyph to p1 p1.circle(&#39;fertility&#39;, &#39;female literacy&#39;, source=source) # Create the second figure: p2 p2 = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;population (millions)&#39;, tools=&#39;box_select,lasso_select&#39;) # Add a circle glyph to p2 p2.circle(&#39;fertility&#39;, &#39;population&#39;, source=source) # Create row layout of figures p1 and p2: layout layout = row(p1, p2) # Specify the name of the output_file and show the result output_file(&#39;linked_brush.html&#39;) show(layout) . . ### Annotations and guides . #### How to create legends . # Add the first circle glyph to the figure p p.circle(&#39;fertility&#39;, &#39;female_literacy&#39;, source=latin_america, size=10, color=&#39;red&#39;, legend=&#39;Latin America&#39;) # Add the second circle glyph to the figure p p.circle(&#39;fertility&#39;, &#39;female_literacy&#39;, source=africa, size=10, color=&#39;blue&#39;, legend=&#39;Africa&#39;) # Specify the name of the output_file and show the result output_file(&#39;fert_lit_groups.html&#39;) show(p) . . #### Positioning and styling legends . # Assign the legend to the bottom left: p.legend.location p.legend.location = &#39;bottom_left&#39; # Fill the legend background with the color &#39;lightgray&#39;: p.legend.background_fill_color p.legend.background_fill_color = &#39;lightgray&#39; # Specify the name of the output_file and show the result output_file(&#39;fert_lit_groups.html&#39;) show(p) . . #### Adding a hover tooltip . # Import HoverTool from bokeh.models from bokeh.models import HoverTool # Create a HoverTool object: hover hover = HoverTool(tooltips = [(&#39;Country&#39;,&#39;@Country&#39;)]) # Add the HoverTool object to figure p p.add_tools(hover) # Specify the name of the output_file and show the result output_file(&#39;hover.html&#39;) show(p) . . Building interactive apps with Bokeh . ### Introducing the Bokeh Server . #### Understanding Bokeh apps . The main purpose of the Bokeh server is to synchronize python objects with web applications in a browser, so that rich, interactive data applications can be connected to powerful PyData libraries such as NumPy, SciPy, Pandas, and scikit-learn. . The Bokeh server can automatically keep in sync any property of any Bokeh object. . bokeh serve myapp.py . #### . Using the current document . # Perform necessary imports from bokeh.io import curdoc from bokeh.plotting import figure # Create a new plot: plot plot = figure() # Add a line to the plot plot.line(x = [1,2,3,4,5], y = [2,5,4,6,7]) # Add the plot to the current document curdoc().add_root(plot) . . #### Add a single slider . # Perform the necessary imports from bokeh.io import curdoc from bokeh.layouts import widgetbox from bokeh.models import Slider # Create a slider: slider slider = Slider(title=&#39;my slider&#39;, start=0, end=10, step=0.1, value=2) # Create a widgetbox layout: layout layout = widgetbox(slider) # Add the layout to the current document curdoc().add_root(layout) . . #### Multiple sliders in one document . # Perform necessary imports from bokeh.io import curdoc from bokeh.layouts import widgetbox from bokeh.models import Slider # Create first slider: slider1 slider1 = Slider(title = &#39;slider1&#39;, start = 0, end = 10, step = 0.1, value = 2) # Create second slider: slider2 slider2 = Slider(title = &#39;slider2&#39;, start = 10, end = 100, step = 1, value = 20) # Add slider1 and slider2 to a widgetbox layout = widgetbox(slider1, slider2) # Add the layout to the current document curdoc().add_root(layout) . . ### Connecting sliders to plots . #### Adding callbacks to sliders . Callbacks are functions that a user can define, like def callback(attr, old, new) , that can be called automatically when some property of a Bokeh object (e.g., the value of a Slider ) changes. . For the value property of Slider objects, callbacks are added by passing a callback function to the on_change method. . myslider.on_change(&#39;value&#39;, callback) . #### How to combine Bokeh models into layouts . # Create ColumnDataSource: source source = ColumnDataSource(data = {&#39;x&#39;: x, &#39;y&#39;: y}) # Add a line to the plot plot.line(&#39;x&#39;, &#39;y&#39;, source=source) # Create a column layout: layout layout = column(widgetbox(slider), plot) # Add the layout to the current document curdoc().add_root(layout) . . #### Learn about widget callbacks . # Define a callback function: callback def callback(attr, old, new): # Read the current value of the slider: scale scale = slider.value # Compute the updated y using np.sin(scale/x): new_y new_y = np.sin(scale/x) # Update source with the new data values source.data = {&#39;x&#39;: x, &#39;y&#39;: new_y} # Attach the callback to the &#39;value&#39; property of slider slider.on_change(&#39;value&#39;, callback) # Create layout and add to current document layout = column(widgetbox(slider), plot) curdoc().add_root(layout) . . . ### Updating plots from dropdowns . #### Updating data sources from dropdown callbacks . # Perform necessary imports from bokeh.models import ColumnDataSource, Select # Create ColumnDataSource: source source = ColumnDataSource(data={ &#39;x&#39; : fertility, &#39;y&#39; : female_literacy }) # Create a new plot: plot plot = figure() # Add circles to the plot plot.circle(&#39;x&#39;, &#39;y&#39;, source=source) # Define a callback function: update_plot def update_plot(attr, old, new): # If the new Selection is &#39;female_literacy&#39;, update &#39;y&#39; to female_literacy if new == &#39;female_literacy&#39;: source.data = { &#39;x&#39; : fertility, &#39;y&#39; : female_literacy } # Else, update &#39;y&#39; to population else: source.data = { &#39;x&#39; : fertility, &#39;y&#39; : population } # Create a dropdown Select widget: select select = Select(title=&quot;distribution&quot;, options=[&#39;female_literacy&#39;, &#39;population&#39;], value=&#39;female_literacy&#39;) # Attach the update_plot callback to the &#39;value&#39; property of select select.on_change(&#39;value&#39;, update_plot) # Create layout and add to current document layout = row(select, plot) curdoc().add_root(layout) . . #### Synchronize two dropdowns . # Create two dropdown Select widgets: select1, select2 select1 = Select(title=&#39;First&#39;, options=[&#39;A&#39;, &#39;B&#39;], value=&#39;A&#39;) select2 = Select(title=&#39;Second&#39;, options=[&#39;1&#39;, &#39;2&#39;, &#39;3&#39;], value=&#39;1&#39;) # Define a callback function: callback def callback(attr, old, new): # If select1 is &#39;A&#39; if select1.value == &#39;A&#39;: # Set select2 options to [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;] select2.options = [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;] # Set select2 value to &#39;1&#39; select2.value = &#39;1&#39; else: # Set select2 options to [&#39;100&#39;, &#39;200&#39;, &#39;300&#39;] select2.options = [&#39;100&#39;, &#39;200&#39;, &#39;300&#39;] # Set select2 value to &#39;100&#39; select2.value = &#39;100&#39; # Attach the callback to the &#39;value&#39; property of select1 select1.on_change(&#39;value&#39;, callback) # Create layout and add to current document layout = widgetbox(select1, select2) curdoc().add_root(layout) . . . ### Buttons . #### Button widgets . # Create a Button with label &#39;Update Data&#39; button = Button(label=&#39;Update Data&#39;) # Define an update callback with no arguments: update def update(): # Compute new y values: y y = np.sin(x) + np.random.random(N) # Update the ColumnDataSource data dictionary source.data = {&#39;x&#39;:x,&#39;y&#39;:y} # Add the update callback to the button button.on_click(update) # Create layout and add to current document layout = column(widgetbox(button), plot) curdoc().add_root(layout) . . #### Button styles . # Import CheckboxGroup, RadioGroup, Toggle from bokeh.models from bokeh.models import CheckboxGroup, RadioGroup, Toggle # Add a Toggle: toggle toggle = Toggle(button_type = &#39;success&#39;, label = &#39;Toggle button&#39;) # Add a CheckboxGroup: checkbox checkbox = CheckboxGroup(labels=[&#39;Option 1&#39;, &#39;Option 2&#39;, &#39;Option 3&#39;]) # Add a RadioGroup: radio radio = RadioGroup(labels=[&#39;Option 1&#39;, &#39;Option 2&#39;, &#39;Option 3&#39;]) # Add widgetbox(toggle, checkbox, radio) to the current document curdoc().add_root(widgetbox(toggle, checkbox, radio)) . . Putting It All Together! A Case Study . ### Time to put it all together! . #### Introducing the project dataset . data.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 10111 entries, 1964 to 2006 Data columns (total 7 columns): Country 10111 non-null object fertility 10100 non-null float64 life 10111 non-null float64 population 10108 non-null float64 child_mortality 9210 non-null float64 gdp 9000 non-null float64 region 10111 non-null object dtypes: float64(5), object(2) memory usage: 631.9+ KB data.head() Country fertility life population child_mortality gdp Year 1964 Afghanistan 7.671 33.639 10474903.0 339.7 1182.0 1965 Afghanistan 7.671 34.152 10697983.0 334.1 1182.0 1966 Afghanistan 7.671 34.662 10927724.0 328.7 1168.0 1967 Afghanistan 7.671 35.170 11163656.0 323.3 1173.0 1968 Afghanistan 7.671 35.674 11411022.0 318.1 1187.0 region Year 1964 South Asia 1965 South Asia 1966 South Asia 1967 South Asia 1968 South Asia . #### Some exploratory plots of the data . # Perform necessary imports from bokeh.io import output_file, show from bokeh.plotting import figure from bokeh.models import HoverTool, ColumnDataSource # Make the ColumnDataSource: source source = ColumnDataSource(data={ &#39;x&#39; : data.loc[1970].fertility, &#39;y&#39; : data.loc[1970].life, &#39;country&#39; : data.loc[1970].Country, }) # Create the figure: p p = figure(title=&#39;1970&#39;, x_axis_label=&#39;Fertility (children per woman)&#39;, y_axis_label=&#39;Life Expectancy (years)&#39;, plot_height=400, plot_width=700, tools=[HoverTool(tooltips=&#39;@country&#39;)]) # Add a circle glyph to the figure p p.circle(x=&#39;x&#39;, y=&#39;y&#39;, source=source) # Output the file and show the figure output_file(&#39;gapminder.html&#39;) show(p) . . ### Starting the app . . #### Beginning with just a plot . # Import the necessary modules from bokeh.io import curdoc from bokeh.models import ColumnDataSource from bokeh.plotting import figure # Make the ColumnDataSource: source source = ColumnDataSource(data={ &#39;x&#39; : data.loc[1970].fertility, &#39;y&#39; : data.loc[1970].life, &#39;country&#39; : data.loc[1970].Country, &#39;pop&#39; : (data.loc[1970].population / 20000000) + 2, &#39;region&#39; : data.loc[1970].region, }) # Save the minimum and maximum values of the fertility column: xmin, xmax xmin, xmax = min(data.fertility), max(data.fertility) # Save the minimum and maximum values of the life expectancy column: ymin, ymax ymin, ymax = min(data.life), max(data.life) # Create the figure: plot plot = figure(title=&#39;Gapminder Data for 1970&#39;, plot_height=400, plot_width=700, x_range=(xmin, xmax), y_range=(ymin, ymax)) # Add circle glyphs to the plot plot.circle(x=&#39;x&#39;, y=&#39;y&#39;, fill_alpha=0.8, source=source) # Set the x-axis label plot.xaxis.axis_label =&#39;Fertility (children per woman)&#39; # Set the y-axis label plot.yaxis.axis_label = &#39;Life Expectancy (years)&#39; # Add the plot to the current document and add a title curdoc().add_root(plot) curdoc().title = &#39;Gapminder&#39; . . #### Enhancing the plot with some shading . # Make a list of the unique values from the region column: regions_list regions_list = data.region.unique().tolist() # Import CategoricalColorMapper from bokeh.models and the Spectral6 palette from bokeh.palettes from bokeh.models import CategoricalColorMapper from bokeh.palettes import Spectral6 # Make a color mapper: color_mapper color_mapper = CategoricalColorMapper(factors=regions_list, palette=Spectral6) # Add the color mapper to the circle glyph plot.circle(x=&#39;x&#39;, y=&#39;y&#39;, fill_alpha=0.8, source=source, color=dict(field=&#39;region&#39;, transform=color_mapper), legend=&#39;region&#39;) # Set the legend.location attribute of the plot to &#39;top_right&#39; plot.legend.location = &#39;top_right&#39; # Add the plot to the current document and add the title curdoc().add_root(plot) curdoc().title = &#39;Gapminder&#39; . . #### Adding a slider to vary the year . # Import the necessary modules from bokeh.layouts import row, widgetbox from bokeh.models import Slider # Define the callback function: update_plot def update_plot(attr, old, new): # Set the yr name to slider.value and new_data to source.data yr = slider.value new_data = { &#39;x&#39; : data.loc[yr].fertility, &#39;y&#39; : data.loc[yr].life, &#39;country&#39; : data.loc[yr].Country, &#39;pop&#39; : (data.loc[yr].population / 20000000) + 2, &#39;region&#39; : data.loc[yr].region, } source.data = new_data # Make a slider object: slider slider = Slider(start = 1970, end = 2010, step = 1, value = 1970, title = &#39;Year&#39;) # Attach the callback to the &#39;value&#39; property of slider slider.on_change(&#39;value&#39;, update_plot) # Make a row layout of widgetbox(slider) and plot and add it to the current document layout = row(widgetbox(slider), plot) curdoc().add_root(layout) . . ### Customizing based on user input . # Define the callback function: update_plot def update_plot(attr, old, new): # Assign the value of the slider: yr yr = slider.value # Set new_data new_data = { &#39;x&#39; : data.loc[yr].fertility, &#39;y&#39; : data.loc[yr].life, &#39;country&#39; : data.loc[yr].Country, &#39;pop&#39; : (data.loc[yr].population / 20000000) + 2, &#39;region&#39; : data.loc[yr].region, } # Assign new_data to: source.data source.data = new_data # Add title to figure: plot.title.text plot.title.text = &#39;Gapminder data for %d&#39; % yr # Make a slider object: slider slider = Slider(start = 1970, end = 2010, step = 1, value = 1970, title = &#39;Year&#39;) # Attach the callback to the &#39;value&#39; property of slider slider.on_change(&#39;value&#39;, update_plot) # Make a row layout of widgetbox(slider) and plot and add it to the current document layout = row(widgetbox(slider), plot) curdoc().add_root(layout) . . ### Adding more interactivity to the app . #### Adding a hover tool . . # Import HoverTool from bokeh.models from bokeh.models import HoverTool # Create a HoverTool: hover hover = HoverTool(tooltips=[(&#39;Country&#39;, &#39;@country&#39;)]) # Add the HoverTool to the plot plot.add_tools(hover) # Create layout: layout layout = row(widgetbox(slider), plot) # Add layout to current document curdoc().add_root(layout) . . #### Adding dropdowns to the app . . # Define the callback: update_plot def update_plot(attr, old, new): # Read the current value off the slider and 2 dropdowns: yr, x, y yr = slider.value x = x_select.value y = y_select.value # Label axes of plot plot.xaxis.axis_label = x plot.yaxis.axis_label = y # Set new_data new_data = { &#39;x&#39; : data.loc[yr][x], &#39;y&#39; : data.loc[yr][y], &#39;country&#39; : data.loc[yr].Country, &#39;pop&#39; : (data.loc[yr].population / 20000000) + 2, &#39;region&#39; : data.loc[yr].region, } # Assign new_data to source.data source.data = new_data # Set the range of all axes plot.x_range.start = min(data[x]) plot.x_range.end = max(data[x]) plot.y_range.start = min(data[y]) plot.y_range.end = max(data[y]) # Add title to plot plot.title.text = &#39;Gapminder data for %d&#39; % yr # Create a dropdown slider widget: slider slider = Slider(start=1970, end=2010, step=1, value=1970, title=&#39;Year&#39;) # Attach the callback to the &#39;value&#39; property of slider slider.on_change(&#39;value&#39;, update_plot) # Create a dropdown Select widget for the x data: x_select x_select = Select( options=[&#39;fertility&#39;, &#39;life&#39;, &#39;child_mortality&#39;, &#39;gdp&#39;], value=&#39;fertility&#39;, title=&#39;x-axis data&#39; ) # Attach the update_plot callback to the &#39;value&#39; property of x_select x_select.on_change(&#39;value&#39;, update_plot) # Create a dropdown Select widget for the y data: y_select y_select = Select( options=[&#39;fertility&#39;, &#39;life&#39;, &#39;child_mortality&#39;, &#39;gdp&#39;], value=&#39;life&#39;, title=&#39;y-axis data&#39; ) # Attach the update_plot callback to the &#39;value&#39; property of y_select y_select.on_change(&#39;value&#39;, update_plot) # Create layout and add to current document layout = row(widgetbox(slider, x_select, y_select), plot) curdoc().add_root(layout) . Basic plotting with Bokeh . ### Plotting with glyphs . #### What are glyphs? . In Bokeh, visual properties of shapes are called glyphs. . Multiple glyphs can be drawn by setting glyph properties to ordered sequences of values. . #### A simple scatter plot . # Import figure from bokeh.plotting from bokeh.plotting import figure # Import output_file and show from bokeh.io from bokeh.io import output_file, show # Create the figure: p p = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add a circle glyph to the figure p p.circle(fertility, female_literacy) # Call the output_file() function and specify the name of the file output_file(&#39;fert_lit.html&#39;) # Display the plot show(p) . . #### A scatter plot with different shapes . # Create the figure: p p = figure(x_axis_label=&#39;fertility&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add a circle glyph to the figure p p.circle(fertility_latinamerica, female_literacy_latinamerica) # Add an x glyph to the figure p p.x(fertility_africa, female_literacy_africa) # Specify the name of the file output_file(&#39;fert_lit_separate.html&#39;) # Display the plot show(p) . . #### Customizing your scatter plots . # Create the figure: p p = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add a blue circle glyph to the figure p p.circle(fertility_latinamerica, female_literacy_latinamerica, color=&#39;blue&#39;, size=10, alpha=0.8) # Add a red circle glyph to the figure p p.circle(fertility_africa, female_literacy_africa, color=&#39;red&#39;, size=10, alpha=0.8) # Specify the name of the file output_file(&#39;fert_lit_separate_colors.html&#39;) # Display the plot show(p) . . CSS color names . ### Additional glyphs . #### Lines . # Import figure from bokeh.plotting from bokeh.plotting import figure # Create a figure with x_axis_type=&quot;datetime&quot;: p p = figure(x_axis_type=&quot;datetime&quot;, x_axis_label=&#39;Date&#39;, y_axis_label=&#39;US Dollars&#39;) # Plot date along the x axis and price along the y axis p.line(date,price) # Specify the name of the output file and show the result output_file(&#39;line.html&#39;) show(p) . . #### Lines and markers . # Import figure from bokeh.plotting from bokeh.plotting import figure # Create a figure with x_axis_type=&#39;datetime&#39;: p p = figure(x_axis_type=&#39;datetime&#39;, x_axis_label=&#39;Date&#39;, y_axis_label=&#39;US Dollars&#39;) # Plot date along the x-axis and price along the y-axis p.line(date, price) # With date on the x-axis and price on the y-axis, add a white circle glyph of size 4 p.circle(date, price, fill_color=&#39;white&#39;, size=4) # Specify the name of the output file and show the result output_file(&#39;line.html&#39;) show(p) . . #### Patches . # Create a list of az_lons, co_lons, nm_lons and ut_lons: x x = [az_lons, co_lons, nm_lons, ut_lons] # Create a list of az_lats, co_lats, nm_lats and ut_lats: y y = [az_lats, co_lats, nm_lats, ut_lats] # Add patches to figure p with line_color=white for x and y p.patches(x, y, line_color = &#39;white&#39;) # Specify the name of the output file and show the result output_file(&#39;four_corners.html&#39;) show(p) . . ### Data formats . #### Plotting data from NumPy arrays . # Import numpy as np import numpy as np # Create array using np.linspace: x x = np.linspace(0,5,100) # Create array using np.cos: y y = np.cos(x) # Add circles at x and y p.circle(x,y) # Specify the name of the output file and show the result output_file(&#39;numpy.html&#39;) show(p) . . #### Plotting data from Pandas DataFrames . # Import pandas as pd import pandas as pd # Read in the CSV file: df df = pd.read_csv(&#39;auto.csv&#39;) # Import figure from bokeh.plotting from bokeh.plotting import figure # Create the figure: p p = figure(x_axis_label=&#39;HP&#39;, y_axis_label=&#39;MPG&#39;) # Plot mpg vs hp by color p.circle(df[&#39;hp&#39;], df[&#39;mpg&#39;], color=df[&#39;color&#39;], size=10) # Specify the name of the output file and show the result output_file(&#39;auto-df.html&#39;) show(p) . . #### The Bokeh ColumnDataSource . The ColumnDataSource is a table-like data object that maps string column names to sequences (columns) of data. It is the central and most common data structure in Bokeh. . All columns in a ColumnDataSource must have the same length.press . #### The Bokeh ColumnDataSource (continued) . df.head() Name Country Medal Time Year color 0 Usain Bolt JAM GOLD 9.63 2012 goldenrod 1 Yohan Blake JAM SILVER 9.75 2012 silver 2 Justin Gatlin USA BRONZE 9.79 2012 saddlebrown 3 Usain Bolt JAM GOLD 9.69 2008 goldenrod 4 Richard Thompson TRI SILVER 9.89 2008 silver . # Import the ColumnDataSource class from bokeh.plotting from bokeh.plotting import ColumnDataSource # Create a ColumnDataSource from df: source source = ColumnDataSource(df) # Add circle glyphs to the figure p p.circle(&#39;Year&#39;, &#39;Time&#39;, source=source, color=&#39;color&#39;, size=8) # Specify the name of the output file and show the result output_file(&#39;sprint.html&#39;) show(p) . . #### Selection and non-selection glyphs . # Create a figure with the &quot;box_select&quot; tool: p p = figure(x_axis_label=&#39;Year&#39;, y_axis_label=&#39;Time&#39;, tools=&#39;box_select&#39;) # Add circle glyphs to the figure p with the selected and non-selected properties p.circle(&#39;Year&#39;, &#39;Time&#39;, source=source, selection_color = &#39;red&#39;, nonselection_alpha = 0.1) # Specify the name of the output file and show the result output_file(&#39;selection_glyph.html&#39;) show(p) . . #### Hover glyphs . # import the HoverTool from bokeh.models import HoverTool # Add circle glyphs to figure p p.circle(x, y, size=10, fill_color=&#39;grey&#39;, alpha=0.1, line_color=None, hover_fill_color=&#39;firebrick&#39;, hover_alpha=0.5, hover_line_color=&#39;white&#39;) # Create a HoverTool: hover hover = HoverTool(tooltips=None, mode=&#39;vline&#39;) # Add the hover tool to the figure p p.add_tools(hover) # Specify the name of the output file and show the result output_file(&#39;hover_glyph.html&#39;) show(p) . . #### Colormapping . #Import CategoricalColorMapper from bokeh.models from bokeh.models import CategoricalColorMapper # Convert df to a ColumnDataSource: source source = ColumnDataSource(df) # Make a CategoricalColorMapper object: color_mapper color_mapper = CategoricalColorMapper(factors=[&#39;Europe&#39;, &#39;Asia&#39;, &#39;US&#39;], palette=[&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;]) # Add a circle glyph to the figure p p.circle(&#39;weight&#39;, &#39;mpg&#39;, source=source, color=dict(field=&#39;origin&#39;, transform=color_mapper), legend=&#39;origin&#39;) # Specify the name of the output file and show the result output_file(&#39;colormap.html&#39;) show(p) . . Layouts, Interactions, and Annotations . ### Introduction to layouts . #### Creating rows of plots . # Import row from bokeh.layouts from bokeh.layouts import row # Create the first figure: p1 p1 = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add a circle glyph to p1 p1.circle(&#39;fertility&#39;, &#39;female_literacy&#39;, source=source) # Create the second figure: p2 p2 = figure(x_axis_label=&#39;population&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add a circle glyph to p2 p2.circle(&#39;population&#39;, &#39;female_literacy&#39;, source=source) # Put p1 and p2 into a horizontal row: layout layout = row(p1, p2) # Specify the name of the output_file and show the result output_file(&#39;fert_row.html&#39;) show(layout) . . #### Creating columns of plots . # Import column from the bokeh.layouts module from bokeh.layouts import column # Create a blank figure: p1 p1 = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add circle scatter to the figure p1 p1.circle(&#39;fertility&#39;, &#39;female_literacy&#39;, source=source) # Create a new blank figure: p2 p2 = figure(x_axis_label=&#39;population&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add circle scatter to the figure p2 p2.circle(&#39;population&#39;, &#39;female_literacy&#39;, source=source) # Put plots p1 and p2 in a column: layout layout = column(p1, p2) # Specify the name of the output_file and show the result output_file(&#39;fert_column.html&#39;) show(layout) . . #### Nesting rows and columns of plots . # Import column and row from bokeh.layouts from bokeh.layouts import row, column # Make a column layout that will be used as the second row: row2 row2 = column([mpg_hp, mpg_weight], sizing_mode=&#39;scale_width&#39;) # Make a row layout that includes the above column layout: layout layout = row([avg_mpg, row2], sizing_mode=&#39;scale_width&#39;) # Specify the name of the output_file and show the result output_file(&#39;layout_custom.html&#39;) show(layout) . . ### Advanced layouts . #### Creating gridded layouts . # Import gridplot from bokeh.layouts from bokeh.layouts import gridplot # Create a list containing plots p1 and p2: row1 row1 = [p1, p2] # Create a list containing plots p3 and p4: row2 row2 = [p3, p4] # Create a gridplot using row1 and row2: layout layout = gridplot([row1, row2]) # Specify the name of the output_file and show the result output_file(&#39;grid.html&#39;) show(layout) . . #### Starting tabbed layouts . # Import Panel from bokeh.models.widgets from bokeh.models.widgets import Panel # Create tab1 from plot p1: tab1 tab1 = Panel(child=p1, title=&#39;Latin America&#39;) # Create tab2 from plot p2: tab2 tab2 = Panel(child=p2, title=&#39;Africa&#39;) # Create tab3 from plot p3: tab3 tab3 = Panel(child=p3, title=&#39;Asia&#39;) # Create tab4 from plot p4: tab4 tab4 = Panel(child=p4, title=&#39;Europe&#39;) . #### Displaying tabbed layouts . # Import Tabs from bokeh.models.widgets from bokeh.models.widgets import Tabs # Create a Tabs layout: layout layout = Tabs(tabs=[tab1, tab2, tab3, tab4]) # Specify the name of the output_file and show the result output_file(&#39;tabs.html&#39;) show(layout) . . ### Linking plots together . #### Linked axes . # Link the x_range of p2 to p1: p2.x_range p2.x_range = p1.x_range # Link the y_range of p2 to p1: p2.y_range p2.y_range = p1.y_range # Link the x_range of p3 to p1: p3.x_range p3.x_range = p1.x_range # Link the y_range of p4 to p1: p4.y_range p4.y_range = p1.y_range # Specify the name of the output_file and show the result output_file(&#39;linked_range.html&#39;) show(layout) . . #### Linked brushing . Country Continent female literacy fertility population 0 Chine ASI 90.5 1.769 1324.655000 1 Inde ASI 50.8 2.682 1139.964932 2 USA NAM 99 2.077 304.060000 3 Indonésie ASI 88.8 2.132 227.345082 4 Brésil LAT 90.2 1.827 191.971506 . # Create ColumnDataSource: source source = ColumnDataSource(data) # Create the first figure: p1 p1 = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;female literacy (% population)&#39;, tools=&#39;box_select,lasso_select&#39;) # Add a circle glyph to p1 p1.circle(&#39;fertility&#39;, &#39;female literacy&#39;, source=source) # Create the second figure: p2 p2 = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;population (millions)&#39;, tools=&#39;box_select,lasso_select&#39;) # Add a circle glyph to p2 p2.circle(&#39;fertility&#39;, &#39;population&#39;, source=source) # Create row layout of figures p1 and p2: layout layout = row(p1, p2) # Specify the name of the output_file and show the result output_file(&#39;linked_brush.html&#39;) show(layout) . . ### Annotations and guides . #### How to create legends . # Add the first circle glyph to the figure p p.circle(&#39;fertility&#39;, &#39;female_literacy&#39;, source=latin_america, size=10, color=&#39;red&#39;, legend=&#39;Latin America&#39;) # Add the second circle glyph to the figure p p.circle(&#39;fertility&#39;, &#39;female_literacy&#39;, source=africa, size=10, color=&#39;blue&#39;, legend=&#39;Africa&#39;) # Specify the name of the output_file and show the result output_file(&#39;fert_lit_groups.html&#39;) show(p) . . #### Positioning and styling legends . # Assign the legend to the bottom left: p.legend.location p.legend.location = &#39;bottom_left&#39; # Fill the legend background with the color &#39;lightgray&#39;: p.legend.background_fill_color p.legend.background_fill_color = &#39;lightgray&#39; # Specify the name of the output_file and show the result output_file(&#39;fert_lit_groups.html&#39;) show(p) . . #### Adding a hover tooltip . # Import HoverTool from bokeh.models from bokeh.models import HoverTool # Create a HoverTool object: hover hover = HoverTool(tooltips = [(&#39;Country&#39;,&#39;@Country&#39;)]) # Add the HoverTool object to figure p p.add_tools(hover) # Specify the name of the output_file and show the result output_file(&#39;hover.html&#39;) show(p) . . Building interactive apps with Bokeh . ### Introducing the Bokeh Server . #### Understanding Bokeh apps . The main purpose of the Bokeh server is to synchronize python objects with web applications in a browser, so that rich, interactive data applications can be connected to powerful PyData libraries such as NumPy, SciPy, Pandas, and scikit-learn. . The Bokeh server can automatically keep in sync any property of any Bokeh object. . bokeh serve myapp.py . #### . Using the current document . # Perform necessary imports from bokeh.io import curdoc from bokeh.plotting import figure # Create a new plot: plot plot = figure() # Add a line to the plot plot.line(x = [1,2,3,4,5], y = [2,5,4,6,7]) # Add the plot to the current document curdoc().add_root(plot) . . #### Add a single slider . # Perform the necessary imports from bokeh.io import curdoc from bokeh.layouts import widgetbox from bokeh.models import Slider # Create a slider: slider slider = Slider(title=&#39;my slider&#39;, start=0, end=10, step=0.1, value=2) # Create a widgetbox layout: layout layout = widgetbox(slider) # Add the layout to the current document curdoc().add_root(layout) . . #### Multiple sliders in one document . # Perform necessary imports from bokeh.io import curdoc from bokeh.layouts import widgetbox from bokeh.models import Slider # Create first slider: slider1 slider1 = Slider(title = &#39;slider1&#39;, start = 0, end = 10, step = 0.1, value = 2) # Create second slider: slider2 slider2 = Slider(title = &#39;slider2&#39;, start = 10, end = 100, step = 1, value = 20) # Add slider1 and slider2 to a widgetbox layout = widgetbox(slider1, slider2) # Add the layout to the current document curdoc().add_root(layout) . . ### Connecting sliders to plots . #### Adding callbacks to sliders . Callbacks are functions that a user can define, like def callback(attr, old, new) , that can be called automatically when some property of a Bokeh object (e.g., the value of a Slider ) changes. . For the value property of Slider objects, callbacks are added by passing a callback function to the on_change method. . myslider.on_change(&#39;value&#39;, callback) . #### How to combine Bokeh models into layouts . # Create ColumnDataSource: source source = ColumnDataSource(data = {&#39;x&#39;: x, &#39;y&#39;: y}) # Add a line to the plot plot.line(&#39;x&#39;, &#39;y&#39;, source=source) # Create a column layout: layout layout = column(widgetbox(slider), plot) # Add the layout to the current document curdoc().add_root(layout) . . #### Learn about widget callbacks . # Define a callback function: callback def callback(attr, old, new): # Read the current value of the slider: scale scale = slider.value # Compute the updated y using np.sin(scale/x): new_y new_y = np.sin(scale/x) # Update source with the new data values source.data = {&#39;x&#39;: x, &#39;y&#39;: new_y} # Attach the callback to the &#39;value&#39; property of slider slider.on_change(&#39;value&#39;, callback) # Create layout and add to current document layout = column(widgetbox(slider), plot) curdoc().add_root(layout) . . . ### Updating plots from dropdowns . #### Updating data sources from dropdown callbacks . # Perform necessary imports from bokeh.models import ColumnDataSource, Select # Create ColumnDataSource: source source = ColumnDataSource(data={ &#39;x&#39; : fertility, &#39;y&#39; : female_literacy }) # Create a new plot: plot plot = figure() # Add circles to the plot plot.circle(&#39;x&#39;, &#39;y&#39;, source=source) # Define a callback function: update_plot def update_plot(attr, old, new): # If the new Selection is &#39;female_literacy&#39;, update &#39;y&#39; to female_literacy if new == &#39;female_literacy&#39;: source.data = { &#39;x&#39; : fertility, &#39;y&#39; : female_literacy } # Else, update &#39;y&#39; to population else: source.data = { &#39;x&#39; : fertility, &#39;y&#39; : population } # Create a dropdown Select widget: select select = Select(title=&quot;distribution&quot;, options=[&#39;female_literacy&#39;, &#39;population&#39;], value=&#39;female_literacy&#39;) # Attach the update_plot callback to the &#39;value&#39; property of select select.on_change(&#39;value&#39;, update_plot) # Create layout and add to current document layout = row(select, plot) curdoc().add_root(layout) . . #### Synchronize two dropdowns . # Create two dropdown Select widgets: select1, select2 select1 = Select(title=&#39;First&#39;, options=[&#39;A&#39;, &#39;B&#39;], value=&#39;A&#39;) select2 = Select(title=&#39;Second&#39;, options=[&#39;1&#39;, &#39;2&#39;, &#39;3&#39;], value=&#39;1&#39;) # Define a callback function: callback def callback(attr, old, new): # If select1 is &#39;A&#39; if select1.value == &#39;A&#39;: # Set select2 options to [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;] select2.options = [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;] # Set select2 value to &#39;1&#39; select2.value = &#39;1&#39; else: # Set select2 options to [&#39;100&#39;, &#39;200&#39;, &#39;300&#39;] select2.options = [&#39;100&#39;, &#39;200&#39;, &#39;300&#39;] # Set select2 value to &#39;100&#39; select2.value = &#39;100&#39; # Attach the callback to the &#39;value&#39; property of select1 select1.on_change(&#39;value&#39;, callback) # Create layout and add to current document layout = widgetbox(select1, select2) curdoc().add_root(layout) . . . ### Buttons . #### Button widgets . # Create a Button with label &#39;Update Data&#39; button = Button(label=&#39;Update Data&#39;) # Define an update callback with no arguments: update def update(): # Compute new y values: y y = np.sin(x) + np.random.random(N) # Update the ColumnDataSource data dictionary source.data = {&#39;x&#39;:x,&#39;y&#39;:y} # Add the update callback to the button button.on_click(update) # Create layout and add to current document layout = column(widgetbox(button), plot) curdoc().add_root(layout) . . #### Button styles . # Import CheckboxGroup, RadioGroup, Toggle from bokeh.models from bokeh.models import CheckboxGroup, RadioGroup, Toggle # Add a Toggle: toggle toggle = Toggle(button_type = &#39;success&#39;, label = &#39;Toggle button&#39;) # Add a CheckboxGroup: checkbox checkbox = CheckboxGroup(labels=[&#39;Option 1&#39;, &#39;Option 2&#39;, &#39;Option 3&#39;]) # Add a RadioGroup: radio radio = RadioGroup(labels=[&#39;Option 1&#39;, &#39;Option 2&#39;, &#39;Option 3&#39;]) # Add widgetbox(toggle, checkbox, radio) to the current document curdoc().add_root(widgetbox(toggle, checkbox, radio)) . . Putting It All Together! A Case Study . ### Time to put it all together! . #### Introducing the project dataset . data.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 10111 entries, 1964 to 2006 Data columns (total 7 columns): Country 10111 non-null object fertility 10100 non-null float64 life 10111 non-null float64 population 10108 non-null float64 child_mortality 9210 non-null float64 gdp 9000 non-null float64 region 10111 non-null object dtypes: float64(5), object(2) memory usage: 631.9+ KB data.head() Country fertility life population child_mortality gdp Year 1964 Afghanistan 7.671 33.639 10474903.0 339.7 1182.0 1965 Afghanistan 7.671 34.152 10697983.0 334.1 1182.0 1966 Afghanistan 7.671 34.662 10927724.0 328.7 1168.0 1967 Afghanistan 7.671 35.170 11163656.0 323.3 1173.0 1968 Afghanistan 7.671 35.674 11411022.0 318.1 1187.0 region Year 1964 South Asia 1965 South Asia 1966 South Asia 1967 South Asia 1968 South Asia . #### Some exploratory plots of the data . # Perform necessary imports from bokeh.io import output_file, show from bokeh.plotting import figure from bokeh.models import HoverTool, ColumnDataSource # Make the ColumnDataSource: source source = ColumnDataSource(data={ &#39;x&#39; : data.loc[1970].fertility, &#39;y&#39; : data.loc[1970].life, &#39;country&#39; : data.loc[1970].Country, }) # Create the figure: p p = figure(title=&#39;1970&#39;, x_axis_label=&#39;Fertility (children per woman)&#39;, y_axis_label=&#39;Life Expectancy (years)&#39;, plot_height=400, plot_width=700, tools=[HoverTool(tooltips=&#39;@country&#39;)]) # Add a circle glyph to the figure p p.circle(x=&#39;x&#39;, y=&#39;y&#39;, source=source) # Output the file and show the figure output_file(&#39;gapminder.html&#39;) show(p) . . ### Starting the app . . #### Beginning with just a plot . # Import the necessary modules from bokeh.io import curdoc from bokeh.models import ColumnDataSource from bokeh.plotting import figure # Make the ColumnDataSource: source source = ColumnDataSource(data={ &#39;x&#39; : data.loc[1970].fertility, &#39;y&#39; : data.loc[1970].life, &#39;country&#39; : data.loc[1970].Country, &#39;pop&#39; : (data.loc[1970].population / 20000000) + 2, &#39;region&#39; : data.loc[1970].region, }) # Save the minimum and maximum values of the fertility column: xmin, xmax xmin, xmax = min(data.fertility), max(data.fertility) # Save the minimum and maximum values of the life expectancy column: ymin, ymax ymin, ymax = min(data.life), max(data.life) # Create the figure: plot plot = figure(title=&#39;Gapminder Data for 1970&#39;, plot_height=400, plot_width=700, x_range=(xmin, xmax), y_range=(ymin, ymax)) # Add circle glyphs to the plot plot.circle(x=&#39;x&#39;, y=&#39;y&#39;, fill_alpha=0.8, source=source) # Set the x-axis label plot.xaxis.axis_label =&#39;Fertility (children per woman)&#39; # Set the y-axis label plot.yaxis.axis_label = &#39;Life Expectancy (years)&#39; # Add the plot to the current document and add a title curdoc().add_root(plot) curdoc().title = &#39;Gapminder&#39; . . #### Enhancing the plot with some shading . # Make a list of the unique values from the region column: regions_list regions_list = data.region.unique().tolist() # Import CategoricalColorMapper from bokeh.models and the Spectral6 palette from bokeh.palettes from bokeh.models import CategoricalColorMapper from bokeh.palettes import Spectral6 # Make a color mapper: color_mapper color_mapper = CategoricalColorMapper(factors=regions_list, palette=Spectral6) # Add the color mapper to the circle glyph plot.circle(x=&#39;x&#39;, y=&#39;y&#39;, fill_alpha=0.8, source=source, color=dict(field=&#39;region&#39;, transform=color_mapper), legend=&#39;region&#39;) # Set the legend.location attribute of the plot to &#39;top_right&#39; plot.legend.location = &#39;top_right&#39; # Add the plot to the current document and add the title curdoc().add_root(plot) curdoc().title = &#39;Gapminder&#39; . . #### Adding a slider to vary the year . # Import the necessary modules from bokeh.layouts import row, widgetbox from bokeh.models import Slider # Define the callback function: update_plot def update_plot(attr, old, new): # Set the yr name to slider.value and new_data to source.data yr = slider.value new_data = { &#39;x&#39; : data.loc[yr].fertility, &#39;y&#39; : data.loc[yr].life, &#39;country&#39; : data.loc[yr].Country, &#39;pop&#39; : (data.loc[yr].population / 20000000) + 2, &#39;region&#39; : data.loc[yr].region, } source.data = new_data # Make a slider object: slider slider = Slider(start = 1970, end = 2010, step = 1, value = 1970, title = &#39;Year&#39;) # Attach the callback to the &#39;value&#39; property of slider slider.on_change(&#39;value&#39;, update_plot) # Make a row layout of widgetbox(slider) and plot and add it to the current document layout = row(widgetbox(slider), plot) curdoc().add_root(layout) . . ### Customizing based on user input . # Define the callback function: update_plot def update_plot(attr, old, new): # Assign the value of the slider: yr yr = slider.value # Set new_data new_data = { &#39;x&#39; : data.loc[yr].fertility, &#39;y&#39; : data.loc[yr].life, &#39;country&#39; : data.loc[yr].Country, &#39;pop&#39; : (data.loc[yr].population / 20000000) + 2, &#39;region&#39; : data.loc[yr].region, } # Assign new_data to: source.data source.data = new_data # Add title to figure: plot.title.text plot.title.text = &#39;Gapminder data for %d&#39; % yr # Make a slider object: slider slider = Slider(start = 1970, end = 2010, step = 1, value = 1970, title = &#39;Year&#39;) # Attach the callback to the &#39;value&#39; property of slider slider.on_change(&#39;value&#39;, update_plot) # Make a row layout of widgetbox(slider) and plot and add it to the current document layout = row(widgetbox(slider), plot) curdoc().add_root(layout) . . ### Adding more interactivity to the app . #### Adding a hover tool . . # Import HoverTool from bokeh.models from bokeh.models import HoverTool # Create a HoverTool: hover hover = HoverTool(tooltips=[(&#39;Country&#39;, &#39;@country&#39;)]) # Add the HoverTool to the plot plot.add_tools(hover) # Create layout: layout layout = row(widgetbox(slider), plot) # Add layout to current document curdoc().add_root(layout) . . #### Adding dropdowns to the app . . # Define the callback: update_plot def update_plot(attr, old, new): # Read the current value off the slider and 2 dropdowns: yr, x, y yr = slider.value x = x_select.value y = y_select.value # Label axes of plot plot.xaxis.axis_label = x plot.yaxis.axis_label = y # Set new_data new_data = { &#39;x&#39; : data.loc[yr][x], &#39;y&#39; : data.loc[yr][y], &#39;country&#39; : data.loc[yr].Country, &#39;pop&#39; : (data.loc[yr].population / 20000000) + 2, &#39;region&#39; : data.loc[yr].region, } # Assign new_data to source.data source.data = new_data # Set the range of all axes plot.x_range.start = min(data[x]) plot.x_range.end = max(data[x]) plot.y_range.start = min(data[y]) plot.y_range.end = max(data[y]) # Add title to plot plot.title.text = &#39;Gapminder data for %d&#39; % yr # Create a dropdown slider widget: slider slider = Slider(start=1970, end=2010, step=1, value=1970, title=&#39;Year&#39;) # Attach the callback to the &#39;value&#39; property of slider slider.on_change(&#39;value&#39;, update_plot) # Create a dropdown Select widget for the x data: x_select x_select = Select( options=[&#39;fertility&#39;, &#39;life&#39;, &#39;child_mortality&#39;, &#39;gdp&#39;], value=&#39;fertility&#39;, title=&#39;x-axis data&#39; ) # Attach the update_plot callback to the &#39;value&#39; property of x_select x_select.on_change(&#39;value&#39;, update_plot) # Create a dropdown Select widget for the y data: y_select y_select = Select( options=[&#39;fertility&#39;, &#39;life&#39;, &#39;child_mortality&#39;, &#39;gdp&#39;], value=&#39;life&#39;, title=&#39;y-axis data&#39; ) # Attach the update_plot callback to the &#39;value&#39; property of y_select y_select.on_change(&#39;value&#39;, update_plot) # Create layout and add to current document layout = row(widgetbox(slider, x_select, y_select), plot) curdoc().add_root(layout) . Basic plotting with Bokeh . ### Plotting with glyphs . #### What are glyphs? . In Bokeh, visual properties of shapes are called glyphs. . Multiple glyphs can be drawn by setting glyph properties to ordered sequences of values. . #### A simple scatter plot . # Import figure from bokeh.plotting from bokeh.plotting import figure # Import output_file and show from bokeh.io from bokeh.io import output_file, show # Create the figure: p p = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add a circle glyph to the figure p p.circle(fertility, female_literacy) # Call the output_file() function and specify the name of the file output_file(&#39;fert_lit.html&#39;) # Display the plot show(p) . . #### A scatter plot with different shapes . # Create the figure: p p = figure(x_axis_label=&#39;fertility&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add a circle glyph to the figure p p.circle(fertility_latinamerica, female_literacy_latinamerica) # Add an x glyph to the figure p p.x(fertility_africa, female_literacy_africa) # Specify the name of the file output_file(&#39;fert_lit_separate.html&#39;) # Display the plot show(p) . . #### Customizing your scatter plots . # Create the figure: p p = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add a blue circle glyph to the figure p p.circle(fertility_latinamerica, female_literacy_latinamerica, color=&#39;blue&#39;, size=10, alpha=0.8) # Add a red circle glyph to the figure p p.circle(fertility_africa, female_literacy_africa, color=&#39;red&#39;, size=10, alpha=0.8) # Specify the name of the file output_file(&#39;fert_lit_separate_colors.html&#39;) # Display the plot show(p) . . CSS color names . ### Additional glyphs . #### Lines . # Import figure from bokeh.plotting from bokeh.plotting import figure # Create a figure with x_axis_type=&quot;datetime&quot;: p p = figure(x_axis_type=&quot;datetime&quot;, x_axis_label=&#39;Date&#39;, y_axis_label=&#39;US Dollars&#39;) # Plot date along the x axis and price along the y axis p.line(date,price) # Specify the name of the output file and show the result output_file(&#39;line.html&#39;) show(p) . . #### Lines and markers . # Import figure from bokeh.plotting from bokeh.plotting import figure # Create a figure with x_axis_type=&#39;datetime&#39;: p p = figure(x_axis_type=&#39;datetime&#39;, x_axis_label=&#39;Date&#39;, y_axis_label=&#39;US Dollars&#39;) # Plot date along the x-axis and price along the y-axis p.line(date, price) # With date on the x-axis and price on the y-axis, add a white circle glyph of size 4 p.circle(date, price, fill_color=&#39;white&#39;, size=4) # Specify the name of the output file and show the result output_file(&#39;line.html&#39;) show(p) . . #### Patches . # Create a list of az_lons, co_lons, nm_lons and ut_lons: x x = [az_lons, co_lons, nm_lons, ut_lons] # Create a list of az_lats, co_lats, nm_lats and ut_lats: y y = [az_lats, co_lats, nm_lats, ut_lats] # Add patches to figure p with line_color=white for x and y p.patches(x, y, line_color = &#39;white&#39;) # Specify the name of the output file and show the result output_file(&#39;four_corners.html&#39;) show(p) . . ### Data formats . #### Plotting data from NumPy arrays . # Import numpy as np import numpy as np # Create array using np.linspace: x x = np.linspace(0,5,100) # Create array using np.cos: y y = np.cos(x) # Add circles at x and y p.circle(x,y) # Specify the name of the output file and show the result output_file(&#39;numpy.html&#39;) show(p) . . #### Plotting data from Pandas DataFrames . # Import pandas as pd import pandas as pd # Read in the CSV file: df df = pd.read_csv(&#39;auto.csv&#39;) # Import figure from bokeh.plotting from bokeh.plotting import figure # Create the figure: p p = figure(x_axis_label=&#39;HP&#39;, y_axis_label=&#39;MPG&#39;) # Plot mpg vs hp by color p.circle(df[&#39;hp&#39;], df[&#39;mpg&#39;], color=df[&#39;color&#39;], size=10) # Specify the name of the output file and show the result output_file(&#39;auto-df.html&#39;) show(p) . . #### The Bokeh ColumnDataSource . The ColumnDataSource is a table-like data object that maps string column names to sequences (columns) of data. It is the central and most common data structure in Bokeh. . All columns in a ColumnDataSource must have the same length.press . #### The Bokeh ColumnDataSource (continued) . df.head() Name Country Medal Time Year color 0 Usain Bolt JAM GOLD 9.63 2012 goldenrod 1 Yohan Blake JAM SILVER 9.75 2012 silver 2 Justin Gatlin USA BRONZE 9.79 2012 saddlebrown 3 Usain Bolt JAM GOLD 9.69 2008 goldenrod 4 Richard Thompson TRI SILVER 9.89 2008 silver . # Import the ColumnDataSource class from bokeh.plotting from bokeh.plotting import ColumnDataSource # Create a ColumnDataSource from df: source source = ColumnDataSource(df) # Add circle glyphs to the figure p p.circle(&#39;Year&#39;, &#39;Time&#39;, source=source, color=&#39;color&#39;, size=8) # Specify the name of the output file and show the result output_file(&#39;sprint.html&#39;) show(p) . . #### Selection and non-selection glyphs . # Create a figure with the &quot;box_select&quot; tool: p p = figure(x_axis_label=&#39;Year&#39;, y_axis_label=&#39;Time&#39;, tools=&#39;box_select&#39;) # Add circle glyphs to the figure p with the selected and non-selected properties p.circle(&#39;Year&#39;, &#39;Time&#39;, source=source, selection_color = &#39;red&#39;, nonselection_alpha = 0.1) # Specify the name of the output file and show the result output_file(&#39;selection_glyph.html&#39;) show(p) . . #### Hover glyphs . # import the HoverTool from bokeh.models import HoverTool # Add circle glyphs to figure p p.circle(x, y, size=10, fill_color=&#39;grey&#39;, alpha=0.1, line_color=None, hover_fill_color=&#39;firebrick&#39;, hover_alpha=0.5, hover_line_color=&#39;white&#39;) # Create a HoverTool: hover hover = HoverTool(tooltips=None, mode=&#39;vline&#39;) # Add the hover tool to the figure p p.add_tools(hover) # Specify the name of the output file and show the result output_file(&#39;hover_glyph.html&#39;) show(p) . . #### Colormapping . #Import CategoricalColorMapper from bokeh.models from bokeh.models import CategoricalColorMapper # Convert df to a ColumnDataSource: source source = ColumnDataSource(df) # Make a CategoricalColorMapper object: color_mapper color_mapper = CategoricalColorMapper(factors=[&#39;Europe&#39;, &#39;Asia&#39;, &#39;US&#39;], palette=[&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;]) # Add a circle glyph to the figure p p.circle(&#39;weight&#39;, &#39;mpg&#39;, source=source, color=dict(field=&#39;origin&#39;, transform=color_mapper), legend=&#39;origin&#39;) # Specify the name of the output file and show the result output_file(&#39;colormap.html&#39;) show(p) . . Layouts, Interactions, and Annotations . ### Introduction to layouts . #### Creating rows of plots . # Import row from bokeh.layouts from bokeh.layouts import row # Create the first figure: p1 p1 = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add a circle glyph to p1 p1.circle(&#39;fertility&#39;, &#39;female_literacy&#39;, source=source) # Create the second figure: p2 p2 = figure(x_axis_label=&#39;population&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add a circle glyph to p2 p2.circle(&#39;population&#39;, &#39;female_literacy&#39;, source=source) # Put p1 and p2 into a horizontal row: layout layout = row(p1, p2) # Specify the name of the output_file and show the result output_file(&#39;fert_row.html&#39;) show(layout) . . #### Creating columns of plots . # Import column from the bokeh.layouts module from bokeh.layouts import column # Create a blank figure: p1 p1 = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add circle scatter to the figure p1 p1.circle(&#39;fertility&#39;, &#39;female_literacy&#39;, source=source) # Create a new blank figure: p2 p2 = figure(x_axis_label=&#39;population&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add circle scatter to the figure p2 p2.circle(&#39;population&#39;, &#39;female_literacy&#39;, source=source) # Put plots p1 and p2 in a column: layout layout = column(p1, p2) # Specify the name of the output_file and show the result output_file(&#39;fert_column.html&#39;) show(layout) . . #### Nesting rows and columns of plots . # Import column and row from bokeh.layouts from bokeh.layouts import row, column # Make a column layout that will be used as the second row: row2 row2 = column([mpg_hp, mpg_weight], sizing_mode=&#39;scale_width&#39;) # Make a row layout that includes the above column layout: layout layout = row([avg_mpg, row2], sizing_mode=&#39;scale_width&#39;) # Specify the name of the output_file and show the result output_file(&#39;layout_custom.html&#39;) show(layout) . . ### Advanced layouts . #### Creating gridded layouts . # Import gridplot from bokeh.layouts from bokeh.layouts import gridplot # Create a list containing plots p1 and p2: row1 row1 = [p1, p2] # Create a list containing plots p3 and p4: row2 row2 = [p3, p4] # Create a gridplot using row1 and row2: layout layout = gridplot([row1, row2]) # Specify the name of the output_file and show the result output_file(&#39;grid.html&#39;) show(layout) . . #### Starting tabbed layouts . # Import Panel from bokeh.models.widgets from bokeh.models.widgets import Panel # Create tab1 from plot p1: tab1 tab1 = Panel(child=p1, title=&#39;Latin America&#39;) # Create tab2 from plot p2: tab2 tab2 = Panel(child=p2, title=&#39;Africa&#39;) # Create tab3 from plot p3: tab3 tab3 = Panel(child=p3, title=&#39;Asia&#39;) # Create tab4 from plot p4: tab4 tab4 = Panel(child=p4, title=&#39;Europe&#39;) . #### Displaying tabbed layouts . # Import Tabs from bokeh.models.widgets from bokeh.models.widgets import Tabs # Create a Tabs layout: layout layout = Tabs(tabs=[tab1, tab2, tab3, tab4]) # Specify the name of the output_file and show the result output_file(&#39;tabs.html&#39;) show(layout) . . ### Linking plots together . #### Linked axes . # Link the x_range of p2 to p1: p2.x_range p2.x_range = p1.x_range # Link the y_range of p2 to p1: p2.y_range p2.y_range = p1.y_range # Link the x_range of p3 to p1: p3.x_range p3.x_range = p1.x_range # Link the y_range of p4 to p1: p4.y_range p4.y_range = p1.y_range # Specify the name of the output_file and show the result output_file(&#39;linked_range.html&#39;) show(layout) . . #### Linked brushing . Country Continent female literacy fertility population 0 Chine ASI 90.5 1.769 1324.655000 1 Inde ASI 50.8 2.682 1139.964932 2 USA NAM 99 2.077 304.060000 3 Indonésie ASI 88.8 2.132 227.345082 4 Brésil LAT 90.2 1.827 191.971506 . # Create ColumnDataSource: source source = ColumnDataSource(data) # Create the first figure: p1 p1 = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;female literacy (% population)&#39;, tools=&#39;box_select,lasso_select&#39;) # Add a circle glyph to p1 p1.circle(&#39;fertility&#39;, &#39;female literacy&#39;, source=source) # Create the second figure: p2 p2 = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;population (millions)&#39;, tools=&#39;box_select,lasso_select&#39;) # Add a circle glyph to p2 p2.circle(&#39;fertility&#39;, &#39;population&#39;, source=source) # Create row layout of figures p1 and p2: layout layout = row(p1, p2) # Specify the name of the output_file and show the result output_file(&#39;linked_brush.html&#39;) show(layout) . . ### Annotations and guides . #### How to create legends . # Add the first circle glyph to the figure p p.circle(&#39;fertility&#39;, &#39;female_literacy&#39;, source=latin_america, size=10, color=&#39;red&#39;, legend=&#39;Latin America&#39;) # Add the second circle glyph to the figure p p.circle(&#39;fertility&#39;, &#39;female_literacy&#39;, source=africa, size=10, color=&#39;blue&#39;, legend=&#39;Africa&#39;) # Specify the name of the output_file and show the result output_file(&#39;fert_lit_groups.html&#39;) show(p) . . #### Positioning and styling legends . # Assign the legend to the bottom left: p.legend.location p.legend.location = &#39;bottom_left&#39; # Fill the legend background with the color &#39;lightgray&#39;: p.legend.background_fill_color p.legend.background_fill_color = &#39;lightgray&#39; # Specify the name of the output_file and show the result output_file(&#39;fert_lit_groups.html&#39;) show(p) . . #### Adding a hover tooltip . # Import HoverTool from bokeh.models from bokeh.models import HoverTool # Create a HoverTool object: hover hover = HoverTool(tooltips = [(&#39;Country&#39;,&#39;@Country&#39;)]) # Add the HoverTool object to figure p p.add_tools(hover) # Specify the name of the output_file and show the result output_file(&#39;hover.html&#39;) show(p) . . Building interactive apps with Bokeh . ### Introducing the Bokeh Server . #### Understanding Bokeh apps . The main purpose of the Bokeh server is to synchronize python objects with web applications in a browser, so that rich, interactive data applications can be connected to powerful PyData libraries such as NumPy, SciPy, Pandas, and scikit-learn. . The Bokeh server can automatically keep in sync any property of any Bokeh object. . bokeh serve myapp.py . #### . Using the current document . # Perform necessary imports from bokeh.io import curdoc from bokeh.plotting import figure # Create a new plot: plot plot = figure() # Add a line to the plot plot.line(x = [1,2,3,4,5], y = [2,5,4,6,7]) # Add the plot to the current document curdoc().add_root(plot) . . #### Add a single slider . # Perform the necessary imports from bokeh.io import curdoc from bokeh.layouts import widgetbox from bokeh.models import Slider # Create a slider: slider slider = Slider(title=&#39;my slider&#39;, start=0, end=10, step=0.1, value=2) # Create a widgetbox layout: layout layout = widgetbox(slider) # Add the layout to the current document curdoc().add_root(layout) . . #### Multiple sliders in one document . # Perform necessary imports from bokeh.io import curdoc from bokeh.layouts import widgetbox from bokeh.models import Slider # Create first slider: slider1 slider1 = Slider(title = &#39;slider1&#39;, start = 0, end = 10, step = 0.1, value = 2) # Create second slider: slider2 slider2 = Slider(title = &#39;slider2&#39;, start = 10, end = 100, step = 1, value = 20) # Add slider1 and slider2 to a widgetbox layout = widgetbox(slider1, slider2) # Add the layout to the current document curdoc().add_root(layout) . . ### Connecting sliders to plots . #### Adding callbacks to sliders . Callbacks are functions that a user can define, like def callback(attr, old, new) , that can be called automatically when some property of a Bokeh object (e.g., the value of a Slider ) changes. . For the value property of Slider objects, callbacks are added by passing a callback function to the on_change method. . myslider.on_change(&#39;value&#39;, callback) . #### How to combine Bokeh models into layouts . # Create ColumnDataSource: source source = ColumnDataSource(data = {&#39;x&#39;: x, &#39;y&#39;: y}) # Add a line to the plot plot.line(&#39;x&#39;, &#39;y&#39;, source=source) # Create a column layout: layout layout = column(widgetbox(slider), plot) # Add the layout to the current document curdoc().add_root(layout) . . #### Learn about widget callbacks . # Define a callback function: callback def callback(attr, old, new): # Read the current value of the slider: scale scale = slider.value # Compute the updated y using np.sin(scale/x): new_y new_y = np.sin(scale/x) # Update source with the new data values source.data = {&#39;x&#39;: x, &#39;y&#39;: new_y} # Attach the callback to the &#39;value&#39; property of slider slider.on_change(&#39;value&#39;, callback) # Create layout and add to current document layout = column(widgetbox(slider), plot) curdoc().add_root(layout) . . . ### Updating plots from dropdowns . #### Updating data sources from dropdown callbacks . # Perform necessary imports from bokeh.models import ColumnDataSource, Select # Create ColumnDataSource: source source = ColumnDataSource(data={ &#39;x&#39; : fertility, &#39;y&#39; : female_literacy }) # Create a new plot: plot plot = figure() # Add circles to the plot plot.circle(&#39;x&#39;, &#39;y&#39;, source=source) # Define a callback function: update_plot def update_plot(attr, old, new): # If the new Selection is &#39;female_literacy&#39;, update &#39;y&#39; to female_literacy if new == &#39;female_literacy&#39;: source.data = { &#39;x&#39; : fertility, &#39;y&#39; : female_literacy } # Else, update &#39;y&#39; to population else: source.data = { &#39;x&#39; : fertility, &#39;y&#39; : population } # Create a dropdown Select widget: select select = Select(title=&quot;distribution&quot;, options=[&#39;female_literacy&#39;, &#39;population&#39;], value=&#39;female_literacy&#39;) # Attach the update_plot callback to the &#39;value&#39; property of select select.on_change(&#39;value&#39;, update_plot) # Create layout and add to current document layout = row(select, plot) curdoc().add_root(layout) . . #### Synchronize two dropdowns . # Create two dropdown Select widgets: select1, select2 select1 = Select(title=&#39;First&#39;, options=[&#39;A&#39;, &#39;B&#39;], value=&#39;A&#39;) select2 = Select(title=&#39;Second&#39;, options=[&#39;1&#39;, &#39;2&#39;, &#39;3&#39;], value=&#39;1&#39;) # Define a callback function: callback def callback(attr, old, new): # If select1 is &#39;A&#39; if select1.value == &#39;A&#39;: # Set select2 options to [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;] select2.options = [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;] # Set select2 value to &#39;1&#39; select2.value = &#39;1&#39; else: # Set select2 options to [&#39;100&#39;, &#39;200&#39;, &#39;300&#39;] select2.options = [&#39;100&#39;, &#39;200&#39;, &#39;300&#39;] # Set select2 value to &#39;100&#39; select2.value = &#39;100&#39; # Attach the callback to the &#39;value&#39; property of select1 select1.on_change(&#39;value&#39;, callback) # Create layout and add to current document layout = widgetbox(select1, select2) curdoc().add_root(layout) . . . ### Buttons . #### Button widgets . # Create a Button with label &#39;Update Data&#39; button = Button(label=&#39;Update Data&#39;) # Define an update callback with no arguments: update def update(): # Compute new y values: y y = np.sin(x) + np.random.random(N) # Update the ColumnDataSource data dictionary source.data = {&#39;x&#39;:x,&#39;y&#39;:y} # Add the update callback to the button button.on_click(update) # Create layout and add to current document layout = column(widgetbox(button), plot) curdoc().add_root(layout) . . #### Button styles . # Import CheckboxGroup, RadioGroup, Toggle from bokeh.models from bokeh.models import CheckboxGroup, RadioGroup, Toggle # Add a Toggle: toggle toggle = Toggle(button_type = &#39;success&#39;, label = &#39;Toggle button&#39;) # Add a CheckboxGroup: checkbox checkbox = CheckboxGroup(labels=[&#39;Option 1&#39;, &#39;Option 2&#39;, &#39;Option 3&#39;]) # Add a RadioGroup: radio radio = RadioGroup(labels=[&#39;Option 1&#39;, &#39;Option 2&#39;, &#39;Option 3&#39;]) # Add widgetbox(toggle, checkbox, radio) to the current document curdoc().add_root(widgetbox(toggle, checkbox, radio)) . . Putting It All Together! A Case Study . ### Time to put it all together! . #### Introducing the project dataset . data.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 10111 entries, 1964 to 2006 Data columns (total 7 columns): Country 10111 non-null object fertility 10100 non-null float64 life 10111 non-null float64 population 10108 non-null float64 child_mortality 9210 non-null float64 gdp 9000 non-null float64 region 10111 non-null object dtypes: float64(5), object(2) memory usage: 631.9+ KB data.head() Country fertility life population child_mortality gdp Year 1964 Afghanistan 7.671 33.639 10474903.0 339.7 1182.0 1965 Afghanistan 7.671 34.152 10697983.0 334.1 1182.0 1966 Afghanistan 7.671 34.662 10927724.0 328.7 1168.0 1967 Afghanistan 7.671 35.170 11163656.0 323.3 1173.0 1968 Afghanistan 7.671 35.674 11411022.0 318.1 1187.0 region Year 1964 South Asia 1965 South Asia 1966 South Asia 1967 South Asia 1968 South Asia . #### Some exploratory plots of the data . # Perform necessary imports from bokeh.io import output_file, show from bokeh.plotting import figure from bokeh.models import HoverTool, ColumnDataSource # Make the ColumnDataSource: source source = ColumnDataSource(data={ &#39;x&#39; : data.loc[1970].fertility, &#39;y&#39; : data.loc[1970].life, &#39;country&#39; : data.loc[1970].Country, }) # Create the figure: p p = figure(title=&#39;1970&#39;, x_axis_label=&#39;Fertility (children per woman)&#39;, y_axis_label=&#39;Life Expectancy (years)&#39;, plot_height=400, plot_width=700, tools=[HoverTool(tooltips=&#39;@country&#39;)]) # Add a circle glyph to the figure p p.circle(x=&#39;x&#39;, y=&#39;y&#39;, source=source) # Output the file and show the figure output_file(&#39;gapminder.html&#39;) show(p) . . ### Starting the app . . #### Beginning with just a plot . # Import the necessary modules from bokeh.io import curdoc from bokeh.models import ColumnDataSource from bokeh.plotting import figure # Make the ColumnDataSource: source source = ColumnDataSource(data={ &#39;x&#39; : data.loc[1970].fertility, &#39;y&#39; : data.loc[1970].life, &#39;country&#39; : data.loc[1970].Country, &#39;pop&#39; : (data.loc[1970].population / 20000000) + 2, &#39;region&#39; : data.loc[1970].region, }) # Save the minimum and maximum values of the fertility column: xmin, xmax xmin, xmax = min(data.fertility), max(data.fertility) # Save the minimum and maximum values of the life expectancy column: ymin, ymax ymin, ymax = min(data.life), max(data.life) # Create the figure: plot plot = figure(title=&#39;Gapminder Data for 1970&#39;, plot_height=400, plot_width=700, x_range=(xmin, xmax), y_range=(ymin, ymax)) # Add circle glyphs to the plot plot.circle(x=&#39;x&#39;, y=&#39;y&#39;, fill_alpha=0.8, source=source) # Set the x-axis label plot.xaxis.axis_label =&#39;Fertility (children per woman)&#39; # Set the y-axis label plot.yaxis.axis_label = &#39;Life Expectancy (years)&#39; # Add the plot to the current document and add a title curdoc().add_root(plot) curdoc().title = &#39;Gapminder&#39; . . #### Enhancing the plot with some shading . # Make a list of the unique values from the region column: regions_list regions_list = data.region.unique().tolist() # Import CategoricalColorMapper from bokeh.models and the Spectral6 palette from bokeh.palettes from bokeh.models import CategoricalColorMapper from bokeh.palettes import Spectral6 # Make a color mapper: color_mapper color_mapper = CategoricalColorMapper(factors=regions_list, palette=Spectral6) # Add the color mapper to the circle glyph plot.circle(x=&#39;x&#39;, y=&#39;y&#39;, fill_alpha=0.8, source=source, color=dict(field=&#39;region&#39;, transform=color_mapper), legend=&#39;region&#39;) # Set the legend.location attribute of the plot to &#39;top_right&#39; plot.legend.location = &#39;top_right&#39; # Add the plot to the current document and add the title curdoc().add_root(plot) curdoc().title = &#39;Gapminder&#39; . . #### Adding a slider to vary the year . # Import the necessary modules from bokeh.layouts import row, widgetbox from bokeh.models import Slider # Define the callback function: update_plot def update_plot(attr, old, new): # Set the yr name to slider.value and new_data to source.data yr = slider.value new_data = { &#39;x&#39; : data.loc[yr].fertility, &#39;y&#39; : data.loc[yr].life, &#39;country&#39; : data.loc[yr].Country, &#39;pop&#39; : (data.loc[yr].population / 20000000) + 2, &#39;region&#39; : data.loc[yr].region, } source.data = new_data # Make a slider object: slider slider = Slider(start = 1970, end = 2010, step = 1, value = 1970, title = &#39;Year&#39;) # Attach the callback to the &#39;value&#39; property of slider slider.on_change(&#39;value&#39;, update_plot) # Make a row layout of widgetbox(slider) and plot and add it to the current document layout = row(widgetbox(slider), plot) curdoc().add_root(layout) . . ### Customizing based on user input . # Define the callback function: update_plot def update_plot(attr, old, new): # Assign the value of the slider: yr yr = slider.value # Set new_data new_data = { &#39;x&#39; : data.loc[yr].fertility, &#39;y&#39; : data.loc[yr].life, &#39;country&#39; : data.loc[yr].Country, &#39;pop&#39; : (data.loc[yr].population / 20000000) + 2, &#39;region&#39; : data.loc[yr].region, } # Assign new_data to: source.data source.data = new_data # Add title to figure: plot.title.text plot.title.text = &#39;Gapminder data for %d&#39; % yr # Make a slider object: slider slider = Slider(start = 1970, end = 2010, step = 1, value = 1970, title = &#39;Year&#39;) # Attach the callback to the &#39;value&#39; property of slider slider.on_change(&#39;value&#39;, update_plot) # Make a row layout of widgetbox(slider) and plot and add it to the current document layout = row(widgetbox(slider), plot) curdoc().add_root(layout) . . ### Adding more interactivity to the app . #### Adding a hover tool . . # Import HoverTool from bokeh.models from bokeh.models import HoverTool # Create a HoverTool: hover hover = HoverTool(tooltips=[(&#39;Country&#39;, &#39;@country&#39;)]) # Add the HoverTool to the plot plot.add_tools(hover) # Create layout: layout layout = row(widgetbox(slider), plot) # Add layout to current document curdoc().add_root(layout) . . #### Adding dropdowns to the app . . # Define the callback: update_plot def update_plot(attr, old, new): # Read the current value off the slider and 2 dropdowns: yr, x, y yr = slider.value x = x_select.value y = y_select.value # Label axes of plot plot.xaxis.axis_label = x plot.yaxis.axis_label = y # Set new_data new_data = { &#39;x&#39; : data.loc[yr][x], &#39;y&#39; : data.loc[yr][y], &#39;country&#39; : data.loc[yr].Country, &#39;pop&#39; : (data.loc[yr].population / 20000000) + 2, &#39;region&#39; : data.loc[yr].region, } # Assign new_data to source.data source.data = new_data # Set the range of all axes plot.x_range.start = min(data[x]) plot.x_range.end = max(data[x]) plot.y_range.start = min(data[y]) plot.y_range.end = max(data[y]) # Add title to plot plot.title.text = &#39;Gapminder data for %d&#39; % yr # Create a dropdown slider widget: slider slider = Slider(start=1970, end=2010, step=1, value=1970, title=&#39;Year&#39;) # Attach the callback to the &#39;value&#39; property of slider slider.on_change(&#39;value&#39;, update_plot) # Create a dropdown Select widget for the x data: x_select x_select = Select( options=[&#39;fertility&#39;, &#39;life&#39;, &#39;child_mortality&#39;, &#39;gdp&#39;], value=&#39;fertility&#39;, title=&#39;x-axis data&#39; ) # Attach the update_plot callback to the &#39;value&#39; property of x_select x_select.on_change(&#39;value&#39;, update_plot) # Create a dropdown Select widget for the y data: y_select y_select = Select( options=[&#39;fertility&#39;, &#39;life&#39;, &#39;child_mortality&#39;, &#39;gdp&#39;], value=&#39;life&#39;, title=&#39;y-axis data&#39; ) # Attach the update_plot callback to the &#39;value&#39; property of y_select y_select.on_change(&#39;value&#39;, update_plot) # Create layout and add to current document layout = row(widgetbox(slider, x_select, y_select), plot) curdoc().add_root(layout) . Basic plotting with Bokeh . ### Plotting with glyphs . #### What are glyphs? . In Bokeh, visual properties of shapes are called glyphs. . Multiple glyphs can be drawn by setting glyph properties to ordered sequences of values. . #### A simple scatter plot . # Import figure from bokeh.plotting from bokeh.plotting import figure # Import output_file and show from bokeh.io from bokeh.io import output_file, show # Create the figure: p p = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add a circle glyph to the figure p p.circle(fertility, female_literacy) # Call the output_file() function and specify the name of the file output_file(&#39;fert_lit.html&#39;) # Display the plot show(p) . . #### A scatter plot with different shapes . # Create the figure: p p = figure(x_axis_label=&#39;fertility&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add a circle glyph to the figure p p.circle(fertility_latinamerica, female_literacy_latinamerica) # Add an x glyph to the figure p p.x(fertility_africa, female_literacy_africa) # Specify the name of the file output_file(&#39;fert_lit_separate.html&#39;) # Display the plot show(p) . . #### Customizing your scatter plots . # Create the figure: p p = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add a blue circle glyph to the figure p p.circle(fertility_latinamerica, female_literacy_latinamerica, color=&#39;blue&#39;, size=10, alpha=0.8) # Add a red circle glyph to the figure p p.circle(fertility_africa, female_literacy_africa, color=&#39;red&#39;, size=10, alpha=0.8) # Specify the name of the file output_file(&#39;fert_lit_separate_colors.html&#39;) # Display the plot show(p) . . CSS color names . ### Additional glyphs . #### Lines . # Import figure from bokeh.plotting from bokeh.plotting import figure # Create a figure with x_axis_type=&quot;datetime&quot;: p p = figure(x_axis_type=&quot;datetime&quot;, x_axis_label=&#39;Date&#39;, y_axis_label=&#39;US Dollars&#39;) # Plot date along the x axis and price along the y axis p.line(date,price) # Specify the name of the output file and show the result output_file(&#39;line.html&#39;) show(p) . . #### Lines and markers . # Import figure from bokeh.plotting from bokeh.plotting import figure # Create a figure with x_axis_type=&#39;datetime&#39;: p p = figure(x_axis_type=&#39;datetime&#39;, x_axis_label=&#39;Date&#39;, y_axis_label=&#39;US Dollars&#39;) # Plot date along the x-axis and price along the y-axis p.line(date, price) # With date on the x-axis and price on the y-axis, add a white circle glyph of size 4 p.circle(date, price, fill_color=&#39;white&#39;, size=4) # Specify the name of the output file and show the result output_file(&#39;line.html&#39;) show(p) . . #### Patches . # Create a list of az_lons, co_lons, nm_lons and ut_lons: x x = [az_lons, co_lons, nm_lons, ut_lons] # Create a list of az_lats, co_lats, nm_lats and ut_lats: y y = [az_lats, co_lats, nm_lats, ut_lats] # Add patches to figure p with line_color=white for x and y p.patches(x, y, line_color = &#39;white&#39;) # Specify the name of the output file and show the result output_file(&#39;four_corners.html&#39;) show(p) . . ### Data formats . #### Plotting data from NumPy arrays . # Import numpy as np import numpy as np # Create array using np.linspace: x x = np.linspace(0,5,100) # Create array using np.cos: y y = np.cos(x) # Add circles at x and y p.circle(x,y) # Specify the name of the output file and show the result output_file(&#39;numpy.html&#39;) show(p) . . #### Plotting data from Pandas DataFrames . # Import pandas as pd import pandas as pd # Read in the CSV file: df df = pd.read_csv(&#39;auto.csv&#39;) # Import figure from bokeh.plotting from bokeh.plotting import figure # Create the figure: p p = figure(x_axis_label=&#39;HP&#39;, y_axis_label=&#39;MPG&#39;) # Plot mpg vs hp by color p.circle(df[&#39;hp&#39;], df[&#39;mpg&#39;], color=df[&#39;color&#39;], size=10) # Specify the name of the output file and show the result output_file(&#39;auto-df.html&#39;) show(p) . . #### The Bokeh ColumnDataSource . The ColumnDataSource is a table-like data object that maps string column names to sequences (columns) of data. It is the central and most common data structure in Bokeh. . All columns in a ColumnDataSource must have the same length.press . #### The Bokeh ColumnDataSource (continued) . df.head() Name Country Medal Time Year color 0 Usain Bolt JAM GOLD 9.63 2012 goldenrod 1 Yohan Blake JAM SILVER 9.75 2012 silver 2 Justin Gatlin USA BRONZE 9.79 2012 saddlebrown 3 Usain Bolt JAM GOLD 9.69 2008 goldenrod 4 Richard Thompson TRI SILVER 9.89 2008 silver . # Import the ColumnDataSource class from bokeh.plotting from bokeh.plotting import ColumnDataSource # Create a ColumnDataSource from df: source source = ColumnDataSource(df) # Add circle glyphs to the figure p p.circle(&#39;Year&#39;, &#39;Time&#39;, source=source, color=&#39;color&#39;, size=8) # Specify the name of the output file and show the result output_file(&#39;sprint.html&#39;) show(p) . . #### Selection and non-selection glyphs . # Create a figure with the &quot;box_select&quot; tool: p p = figure(x_axis_label=&#39;Year&#39;, y_axis_label=&#39;Time&#39;, tools=&#39;box_select&#39;) # Add circle glyphs to the figure p with the selected and non-selected properties p.circle(&#39;Year&#39;, &#39;Time&#39;, source=source, selection_color = &#39;red&#39;, nonselection_alpha = 0.1) # Specify the name of the output file and show the result output_file(&#39;selection_glyph.html&#39;) show(p) . . #### Hover glyphs . # import the HoverTool from bokeh.models import HoverTool # Add circle glyphs to figure p p.circle(x, y, size=10, fill_color=&#39;grey&#39;, alpha=0.1, line_color=None, hover_fill_color=&#39;firebrick&#39;, hover_alpha=0.5, hover_line_color=&#39;white&#39;) # Create a HoverTool: hover hover = HoverTool(tooltips=None, mode=&#39;vline&#39;) # Add the hover tool to the figure p p.add_tools(hover) # Specify the name of the output file and show the result output_file(&#39;hover_glyph.html&#39;) show(p) . . #### Colormapping . #Import CategoricalColorMapper from bokeh.models from bokeh.models import CategoricalColorMapper # Convert df to a ColumnDataSource: source source = ColumnDataSource(df) # Make a CategoricalColorMapper object: color_mapper color_mapper = CategoricalColorMapper(factors=[&#39;Europe&#39;, &#39;Asia&#39;, &#39;US&#39;], palette=[&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;]) # Add a circle glyph to the figure p p.circle(&#39;weight&#39;, &#39;mpg&#39;, source=source, color=dict(field=&#39;origin&#39;, transform=color_mapper), legend=&#39;origin&#39;) # Specify the name of the output file and show the result output_file(&#39;colormap.html&#39;) show(p) . . Layouts, Interactions, and Annotations . ### Introduction to layouts . #### Creating rows of plots . # Import row from bokeh.layouts from bokeh.layouts import row # Create the first figure: p1 p1 = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add a circle glyph to p1 p1.circle(&#39;fertility&#39;, &#39;female_literacy&#39;, source=source) # Create the second figure: p2 p2 = figure(x_axis_label=&#39;population&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add a circle glyph to p2 p2.circle(&#39;population&#39;, &#39;female_literacy&#39;, source=source) # Put p1 and p2 into a horizontal row: layout layout = row(p1, p2) # Specify the name of the output_file and show the result output_file(&#39;fert_row.html&#39;) show(layout) . . #### Creating columns of plots . # Import column from the bokeh.layouts module from bokeh.layouts import column # Create a blank figure: p1 p1 = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add circle scatter to the figure p1 p1.circle(&#39;fertility&#39;, &#39;female_literacy&#39;, source=source) # Create a new blank figure: p2 p2 = figure(x_axis_label=&#39;population&#39;, y_axis_label=&#39;female_literacy (% population)&#39;) # Add circle scatter to the figure p2 p2.circle(&#39;population&#39;, &#39;female_literacy&#39;, source=source) # Put plots p1 and p2 in a column: layout layout = column(p1, p2) # Specify the name of the output_file and show the result output_file(&#39;fert_column.html&#39;) show(layout) . . #### Nesting rows and columns of plots . # Import column and row from bokeh.layouts from bokeh.layouts import row, column # Make a column layout that will be used as the second row: row2 row2 = column([mpg_hp, mpg_weight], sizing_mode=&#39;scale_width&#39;) # Make a row layout that includes the above column layout: layout layout = row([avg_mpg, row2], sizing_mode=&#39;scale_width&#39;) # Specify the name of the output_file and show the result output_file(&#39;layout_custom.html&#39;) show(layout) . . ### Advanced layouts . #### Creating gridded layouts . # Import gridplot from bokeh.layouts from bokeh.layouts import gridplot # Create a list containing plots p1 and p2: row1 row1 = [p1, p2] # Create a list containing plots p3 and p4: row2 row2 = [p3, p4] # Create a gridplot using row1 and row2: layout layout = gridplot([row1, row2]) # Specify the name of the output_file and show the result output_file(&#39;grid.html&#39;) show(layout) . . #### Starting tabbed layouts . # Import Panel from bokeh.models.widgets from bokeh.models.widgets import Panel # Create tab1 from plot p1: tab1 tab1 = Panel(child=p1, title=&#39;Latin America&#39;) # Create tab2 from plot p2: tab2 tab2 = Panel(child=p2, title=&#39;Africa&#39;) # Create tab3 from plot p3: tab3 tab3 = Panel(child=p3, title=&#39;Asia&#39;) # Create tab4 from plot p4: tab4 tab4 = Panel(child=p4, title=&#39;Europe&#39;) . #### Displaying tabbed layouts . # Import Tabs from bokeh.models.widgets from bokeh.models.widgets import Tabs # Create a Tabs layout: layout layout = Tabs(tabs=[tab1, tab2, tab3, tab4]) # Specify the name of the output_file and show the result output_file(&#39;tabs.html&#39;) show(layout) . . ### Linking plots together . #### Linked axes . # Link the x_range of p2 to p1: p2.x_range p2.x_range = p1.x_range # Link the y_range of p2 to p1: p2.y_range p2.y_range = p1.y_range # Link the x_range of p3 to p1: p3.x_range p3.x_range = p1.x_range # Link the y_range of p4 to p1: p4.y_range p4.y_range = p1.y_range # Specify the name of the output_file and show the result output_file(&#39;linked_range.html&#39;) show(layout) . . #### Linked brushing . Country Continent female literacy fertility population 0 Chine ASI 90.5 1.769 1324.655000 1 Inde ASI 50.8 2.682 1139.964932 2 USA NAM 99 2.077 304.060000 3 Indonésie ASI 88.8 2.132 227.345082 4 Brésil LAT 90.2 1.827 191.971506 . # Create ColumnDataSource: source source = ColumnDataSource(data) # Create the first figure: p1 p1 = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;female literacy (% population)&#39;, tools=&#39;box_select,lasso_select&#39;) # Add a circle glyph to p1 p1.circle(&#39;fertility&#39;, &#39;female literacy&#39;, source=source) # Create the second figure: p2 p2 = figure(x_axis_label=&#39;fertility (children per woman)&#39;, y_axis_label=&#39;population (millions)&#39;, tools=&#39;box_select,lasso_select&#39;) # Add a circle glyph to p2 p2.circle(&#39;fertility&#39;, &#39;population&#39;, source=source) # Create row layout of figures p1 and p2: layout layout = row(p1, p2) # Specify the name of the output_file and show the result output_file(&#39;linked_brush.html&#39;) show(layout) . . ### Annotations and guides . #### How to create legends . # Add the first circle glyph to the figure p p.circle(&#39;fertility&#39;, &#39;female_literacy&#39;, source=latin_america, size=10, color=&#39;red&#39;, legend=&#39;Latin America&#39;) # Add the second circle glyph to the figure p p.circle(&#39;fertility&#39;, &#39;female_literacy&#39;, source=africa, size=10, color=&#39;blue&#39;, legend=&#39;Africa&#39;) # Specify the name of the output_file and show the result output_file(&#39;fert_lit_groups.html&#39;) show(p) . . #### Positioning and styling legends . # Assign the legend to the bottom left: p.legend.location p.legend.location = &#39;bottom_left&#39; # Fill the legend background with the color &#39;lightgray&#39;: p.legend.background_fill_color p.legend.background_fill_color = &#39;lightgray&#39; # Specify the name of the output_file and show the result output_file(&#39;fert_lit_groups.html&#39;) show(p) . . #### Adding a hover tooltip . # Import HoverTool from bokeh.models from bokeh.models import HoverTool # Create a HoverTool object: hover hover = HoverTool(tooltips = [(&#39;Country&#39;,&#39;@Country&#39;)]) # Add the HoverTool object to figure p p.add_tools(hover) # Specify the name of the output_file and show the result output_file(&#39;hover.html&#39;) show(p) . . Building interactive apps with Bokeh . ### Introducing the Bokeh Server . #### Understanding Bokeh apps . The main purpose of the Bokeh server is to synchronize python objects with web applications in a browser, so that rich, interactive data applications can be connected to powerful PyData libraries such as NumPy, SciPy, Pandas, and scikit-learn. . The Bokeh server can automatically keep in sync any property of any Bokeh object. . bokeh serve myapp.py . #### . Using the current document . # Perform necessary imports from bokeh.io import curdoc from bokeh.plotting import figure # Create a new plot: plot plot = figure() # Add a line to the plot plot.line(x = [1,2,3,4,5], y = [2,5,4,6,7]) # Add the plot to the current document curdoc().add_root(plot) . . #### Add a single slider . # Perform the necessary imports from bokeh.io import curdoc from bokeh.layouts import widgetbox from bokeh.models import Slider # Create a slider: slider slider = Slider(title=&#39;my slider&#39;, start=0, end=10, step=0.1, value=2) # Create a widgetbox layout: layout layout = widgetbox(slider) # Add the layout to the current document curdoc().add_root(layout) . . #### Multiple sliders in one document . # Perform necessary imports from bokeh.io import curdoc from bokeh.layouts import widgetbox from bokeh.models import Slider # Create first slider: slider1 slider1 = Slider(title = &#39;slider1&#39;, start = 0, end = 10, step = 0.1, value = 2) # Create second slider: slider2 slider2 = Slider(title = &#39;slider2&#39;, start = 10, end = 100, step = 1, value = 20) # Add slider1 and slider2 to a widgetbox layout = widgetbox(slider1, slider2) # Add the layout to the current document curdoc().add_root(layout) . . ### Connecting sliders to plots . #### Adding callbacks to sliders . Callbacks are functions that a user can define, like def callback(attr, old, new) , that can be called automatically when some property of a Bokeh object (e.g., the value of a Slider ) changes. . For the value property of Slider objects, callbacks are added by passing a callback function to the on_change method. . myslider.on_change(&#39;value&#39;, callback) . #### How to combine Bokeh models into layouts . # Create ColumnDataSource: source source = ColumnDataSource(data = {&#39;x&#39;: x, &#39;y&#39;: y}) # Add a line to the plot plot.line(&#39;x&#39;, &#39;y&#39;, source=source) # Create a column layout: layout layout = column(widgetbox(slider), plot) # Add the layout to the current document curdoc().add_root(layout) . . #### Learn about widget callbacks . # Define a callback function: callback def callback(attr, old, new): # Read the current value of the slider: scale scale = slider.value # Compute the updated y using np.sin(scale/x): new_y new_y = np.sin(scale/x) # Update source with the new data values source.data = {&#39;x&#39;: x, &#39;y&#39;: new_y} # Attach the callback to the &#39;value&#39; property of slider slider.on_change(&#39;value&#39;, callback) # Create layout and add to current document layout = column(widgetbox(slider), plot) curdoc().add_root(layout) . . . ### Updating plots from dropdowns . #### Updating data sources from dropdown callbacks . # Perform necessary imports from bokeh.models import ColumnDataSource, Select # Create ColumnDataSource: source source = ColumnDataSource(data={ &#39;x&#39; : fertility, &#39;y&#39; : female_literacy }) # Create a new plot: plot plot = figure() # Add circles to the plot plot.circle(&#39;x&#39;, &#39;y&#39;, source=source) # Define a callback function: update_plot def update_plot(attr, old, new): # If the new Selection is &#39;female_literacy&#39;, update &#39;y&#39; to female_literacy if new == &#39;female_literacy&#39;: source.data = { &#39;x&#39; : fertility, &#39;y&#39; : female_literacy } # Else, update &#39;y&#39; to population else: source.data = { &#39;x&#39; : fertility, &#39;y&#39; : population } # Create a dropdown Select widget: select select = Select(title=&quot;distribution&quot;, options=[&#39;female_literacy&#39;, &#39;population&#39;], value=&#39;female_literacy&#39;) # Attach the update_plot callback to the &#39;value&#39; property of select select.on_change(&#39;value&#39;, update_plot) # Create layout and add to current document layout = row(select, plot) curdoc().add_root(layout) . . #### Synchronize two dropdowns . # Create two dropdown Select widgets: select1, select2 select1 = Select(title=&#39;First&#39;, options=[&#39;A&#39;, &#39;B&#39;], value=&#39;A&#39;) select2 = Select(title=&#39;Second&#39;, options=[&#39;1&#39;, &#39;2&#39;, &#39;3&#39;], value=&#39;1&#39;) # Define a callback function: callback def callback(attr, old, new): # If select1 is &#39;A&#39; if select1.value == &#39;A&#39;: # Set select2 options to [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;] select2.options = [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;] # Set select2 value to &#39;1&#39; select2.value = &#39;1&#39; else: # Set select2 options to [&#39;100&#39;, &#39;200&#39;, &#39;300&#39;] select2.options = [&#39;100&#39;, &#39;200&#39;, &#39;300&#39;] # Set select2 value to &#39;100&#39; select2.value = &#39;100&#39; # Attach the callback to the &#39;value&#39; property of select1 select1.on_change(&#39;value&#39;, callback) # Create layout and add to current document layout = widgetbox(select1, select2) curdoc().add_root(layout) . . . ### Buttons . #### Button widgets . # Create a Button with label &#39;Update Data&#39; button = Button(label=&#39;Update Data&#39;) # Define an update callback with no arguments: update def update(): # Compute new y values: y y = np.sin(x) + np.random.random(N) # Update the ColumnDataSource data dictionary source.data = {&#39;x&#39;:x,&#39;y&#39;:y} # Add the update callback to the button button.on_click(update) # Create layout and add to current document layout = column(widgetbox(button), plot) curdoc().add_root(layout) . . #### Button styles . # Import CheckboxGroup, RadioGroup, Toggle from bokeh.models from bokeh.models import CheckboxGroup, RadioGroup, Toggle # Add a Toggle: toggle toggle = Toggle(button_type = &#39;success&#39;, label = &#39;Toggle button&#39;) # Add a CheckboxGroup: checkbox checkbox = CheckboxGroup(labels=[&#39;Option 1&#39;, &#39;Option 2&#39;, &#39;Option 3&#39;]) # Add a RadioGroup: radio radio = RadioGroup(labels=[&#39;Option 1&#39;, &#39;Option 2&#39;, &#39;Option 3&#39;]) # Add widgetbox(toggle, checkbox, radio) to the current document curdoc().add_root(widgetbox(toggle, checkbox, radio)) . . Putting It All Together! A Case Study . ### Time to put it all together! . #### Introducing the project dataset . data.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 10111 entries, 1964 to 2006 Data columns (total 7 columns): Country 10111 non-null object fertility 10100 non-null float64 life 10111 non-null float64 population 10108 non-null float64 child_mortality 9210 non-null float64 gdp 9000 non-null float64 region 10111 non-null object dtypes: float64(5), object(2) memory usage: 631.9+ KB data.head() Country fertility life population child_mortality gdp Year 1964 Afghanistan 7.671 33.639 10474903.0 339.7 1182.0 1965 Afghanistan 7.671 34.152 10697983.0 334.1 1182.0 1966 Afghanistan 7.671 34.662 10927724.0 328.7 1168.0 1967 Afghanistan 7.671 35.170 11163656.0 323.3 1173.0 1968 Afghanistan 7.671 35.674 11411022.0 318.1 1187.0 region Year 1964 South Asia 1965 South Asia 1966 South Asia 1967 South Asia 1968 South Asia . #### Some exploratory plots of the data . # Perform necessary imports from bokeh.io import output_file, show from bokeh.plotting import figure from bokeh.models import HoverTool, ColumnDataSource # Make the ColumnDataSource: source source = ColumnDataSource(data={ &#39;x&#39; : data.loc[1970].fertility, &#39;y&#39; : data.loc[1970].life, &#39;country&#39; : data.loc[1970].Country, }) # Create the figure: p p = figure(title=&#39;1970&#39;, x_axis_label=&#39;Fertility (children per woman)&#39;, y_axis_label=&#39;Life Expectancy (years)&#39;, plot_height=400, plot_width=700, tools=[HoverTool(tooltips=&#39;@country&#39;)]) # Add a circle glyph to the figure p p.circle(x=&#39;x&#39;, y=&#39;y&#39;, source=source) # Output the file and show the figure output_file(&#39;gapminder.html&#39;) show(p) . . ### Starting the app . . #### Beginning with just a plot . # Import the necessary modules from bokeh.io import curdoc from bokeh.models import ColumnDataSource from bokeh.plotting import figure # Make the ColumnDataSource: source source = ColumnDataSource(data={ &#39;x&#39; : data.loc[1970].fertility, &#39;y&#39; : data.loc[1970].life, &#39;country&#39; : data.loc[1970].Country, &#39;pop&#39; : (data.loc[1970].population / 20000000) + 2, &#39;region&#39; : data.loc[1970].region, }) # Save the minimum and maximum values of the fertility column: xmin, xmax xmin, xmax = min(data.fertility), max(data.fertility) # Save the minimum and maximum values of the life expectancy column: ymin, ymax ymin, ymax = min(data.life), max(data.life) # Create the figure: plot plot = figure(title=&#39;Gapminder Data for 1970&#39;, plot_height=400, plot_width=700, x_range=(xmin, xmax), y_range=(ymin, ymax)) # Add circle glyphs to the plot plot.circle(x=&#39;x&#39;, y=&#39;y&#39;, fill_alpha=0.8, source=source) # Set the x-axis label plot.xaxis.axis_label =&#39;Fertility (children per woman)&#39; # Set the y-axis label plot.yaxis.axis_label = &#39;Life Expectancy (years)&#39; # Add the plot to the current document and add a title curdoc().add_root(plot) curdoc().title = &#39;Gapminder&#39; . . #### Enhancing the plot with some shading . # Make a list of the unique values from the region column: regions_list regions_list = data.region.unique().tolist() # Import CategoricalColorMapper from bokeh.models and the Spectral6 palette from bokeh.palettes from bokeh.models import CategoricalColorMapper from bokeh.palettes import Spectral6 # Make a color mapper: color_mapper color_mapper = CategoricalColorMapper(factors=regions_list, palette=Spectral6) # Add the color mapper to the circle glyph plot.circle(x=&#39;x&#39;, y=&#39;y&#39;, fill_alpha=0.8, source=source, color=dict(field=&#39;region&#39;, transform=color_mapper), legend=&#39;region&#39;) # Set the legend.location attribute of the plot to &#39;top_right&#39; plot.legend.location = &#39;top_right&#39; # Add the plot to the current document and add the title curdoc().add_root(plot) curdoc().title = &#39;Gapminder&#39; . . #### Adding a slider to vary the year . # Import the necessary modules from bokeh.layouts import row, widgetbox from bokeh.models import Slider # Define the callback function: update_plot def update_plot(attr, old, new): # Set the yr name to slider.value and new_data to source.data yr = slider.value new_data = { &#39;x&#39; : data.loc[yr].fertility, &#39;y&#39; : data.loc[yr].life, &#39;country&#39; : data.loc[yr].Country, &#39;pop&#39; : (data.loc[yr].population / 20000000) + 2, &#39;region&#39; : data.loc[yr].region, } source.data = new_data # Make a slider object: slider slider = Slider(start = 1970, end = 2010, step = 1, value = 1970, title = &#39;Year&#39;) # Attach the callback to the &#39;value&#39; property of slider slider.on_change(&#39;value&#39;, update_plot) # Make a row layout of widgetbox(slider) and plot and add it to the current document layout = row(widgetbox(slider), plot) curdoc().add_root(layout) . . ### Customizing based on user input . # Define the callback function: update_plot def update_plot(attr, old, new): # Assign the value of the slider: yr yr = slider.value # Set new_data new_data = { &#39;x&#39; : data.loc[yr].fertility, &#39;y&#39; : data.loc[yr].life, &#39;country&#39; : data.loc[yr].Country, &#39;pop&#39; : (data.loc[yr].population / 20000000) + 2, &#39;region&#39; : data.loc[yr].region, } # Assign new_data to: source.data source.data = new_data # Add title to figure: plot.title.text plot.title.text = &#39;Gapminder data for %d&#39; % yr # Make a slider object: slider slider = Slider(start = 1970, end = 2010, step = 1, value = 1970, title = &#39;Year&#39;) # Attach the callback to the &#39;value&#39; property of slider slider.on_change(&#39;value&#39;, update_plot) # Make a row layout of widgetbox(slider) and plot and add it to the current document layout = row(widgetbox(slider), plot) curdoc().add_root(layout) . . ### Adding more interactivity to the app . #### Adding a hover tool . . # Import HoverTool from bokeh.models from bokeh.models import HoverTool # Create a HoverTool: hover hover = HoverTool(tooltips=[(&#39;Country&#39;, &#39;@country&#39;)]) # Add the HoverTool to the plot plot.add_tools(hover) # Create layout: layout layout = row(widgetbox(slider), plot) # Add layout to current document curdoc().add_root(layout) . . #### Adding dropdowns to the app . . # Define the callback: update_plot def update_plot(attr, old, new): # Read the current value off the slider and 2 dropdowns: yr, x, y yr = slider.value x = x_select.value y = y_select.value # Label axes of plot plot.xaxis.axis_label = x plot.yaxis.axis_label = y # Set new_data new_data = { &#39;x&#39; : data.loc[yr][x], &#39;y&#39; : data.loc[yr][y], &#39;country&#39; : data.loc[yr].Country, &#39;pop&#39; : (data.loc[yr].population / 20000000) + 2, &#39;region&#39; : data.loc[yr].region, } # Assign new_data to source.data source.data = new_data # Set the range of all axes plot.x_range.start = min(data[x]) plot.x_range.end = max(data[x]) plot.y_range.start = min(data[y]) plot.y_range.end = max(data[y]) # Add title to plot plot.title.text = &#39;Gapminder data for %d&#39; % yr # Create a dropdown slider widget: slider slider = Slider(start=1970, end=2010, step=1, value=1970, title=&#39;Year&#39;) # Attach the callback to the &#39;value&#39; property of slider slider.on_change(&#39;value&#39;, update_plot) # Create a dropdown Select widget for the x data: x_select x_select = Select( options=[&#39;fertility&#39;, &#39;life&#39;, &#39;child_mortality&#39;, &#39;gdp&#39;], value=&#39;fertility&#39;, title=&#39;x-axis data&#39; ) # Attach the update_plot callback to the &#39;value&#39; property of x_select x_select.on_change(&#39;value&#39;, update_plot) # Create a dropdown Select widget for the y data: y_select y_select = Select( options=[&#39;fertility&#39;, &#39;life&#39;, &#39;child_mortality&#39;, &#39;gdp&#39;], value=&#39;life&#39;, title=&#39;y-axis data&#39; ) # Attach the update_plot callback to the &#39;value&#39; property of y_select y_select.on_change(&#39;value&#39;, update_plot) # Create layout and add to current document layout = row(widgetbox(slider, x_select, y_select), plot) curdoc().add_root(layout) . .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/interactive-data-visualization-with-bokeh.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/interactive-data-visualization-with-bokeh.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post40": {
            "title": "Conda Essentials",
            "content": "Conda Essentials . This is the memo of the 20th course of ‘Data Scientist with Python’ track. . You can find the original course HERE . . . 1. Installing Packages . . #### What are packages and why are they needed? . Conda packages are files containing a bundle of resources: usually libraries and executables, but not always. In principle, Conda packages can include data, images, notebooks, or other assets. The command-line tool conda is used to install, remove and examine packages; other tools such as the GUI Anaconda Navigator also expose the same capabilities. This course focuses on the conda tool itself (you’ll see use cases other than package management in later chapters). . Conda packages are most widely used with Python, but that’s not all. Nothing about the Conda package format or the conda tool itself assumes any specific programming language. Conda packages can also be used for bundling libraries in other languages (like R, Scala, Julia, etc.) or simply for distributing pure binary executables generated from any programming language. . One of the powerful aspects of conda —both the tool and the package format—is that dependencies are taken care of. That is, when you install any Conda package, any other packages needed get installed automatically. Tracking and determining software dependencies is a hard problem that package managers like Conda are designed to solve. . A Conda package, then, is a file containing all files needed to make a given program execute correctly on a given system. Moreover, a Conda package can contain binary artifacts specific to a particular platform or operating system. Most packages (and their dependencies) are available for Windows ( win-32 or win-64 ), for OSX ( osx-64 ), and for Linux ( linux-32 or linux-64 ). A small number of Conda packages are available for more specialized platforms (e.g., Raspberry Pi 2 or POWER8 LE). As a user, you do not need to specify the platform since Conda will simply choose the Conda package appropriate for the platform you are using. . Conda packages’ features . The Conda package format is programming-language and asset-type independent. | Packages contain a description of all dependencies, all of which are installed together. | The tool conda can be used to install, examine, or remove packages from a working system. | Other GUI or web-based tools can be used as a wrapper for the tool conda for package management. | . #### determine version of conda . (base) $ conda --version conda 4.7.5 . #### Install a conda package (I) . (base) $ conda install --help | grep package_spec [package_spec [package_spec ...]] package_spec Packages to install or update in the conda environment. . #### Install a conda package (II) . (base) $ conda install cytoolz . #### semantic versioning . Most Conda packages use a system called semantic versioning to identify distinct versions of a software package unambiguously. . Under semantic versioning , software is labeled with a three-part version identifier of the form MAJOR.MINOR.PATCH ; the label components are non-negative integers separated by periods. Assuming all software starts at version 0.0.0 . #### package version: conda list . (base) $ conda list # packages in environment at /home/repl/miniconda: ## Name Version Build Channel _libgcc_mutex 0.1 main anaconda-client 1.7.2 py36_0 anaconda-project 0.8.3 py_0 ... . #### Install a specific version of a package (I) . conda install foo-lib=13 # only sepecify MAJOR version conda install foo-lib=12.3 # sepecify MAJOR and MINOR version conda install foo-lib=14.3.2 # sepecify MAJOR, MINOR and PATCH . #### Install a specific version of a package (II) . # install 1.0, 1.4 or 1.4.1b2 conda install &#39;bar-lib=1.0|1.4*&#39; # install later than version 1.3.4, or earlier than version 1.1 conda install &#39;bar-lib&gt;=1.3.4,&lt;1.1&#39; . (base) $ conda install &#39;attrs&gt;16,&lt;17.3&#39; Collecting package metadata (current_repodata.json): done Solving environment: failedCollecting package metadata (repodata.json): done Solving environment: done ## Package Plan ## environment location: /home/repl/miniconda added / updated specs: - attrs[version=&#39;&gt;16,&lt;17.3&#39;] The following packages will be downloaded: package | build |-- attrs-17.2.0 | py36h8d46266_0 34 KB Total: 34 KB . #### Update a conda package . conda update PKGNAME conda update foo bar blob (base) $ conda update pandas . #### Remove a conda package . conda remove PKGNAME . #### Search for available package versions . conda search PKGNAME . #### Find dependencies for a package version . conda search &#39;PKGNAME=1.13.1=py36*&#39; --info (base) $ conda search &#39;numpy=1.13.1=py36*&#39; --info ... ... dependencies: - libgcc-ng &gt;=7.2.0 - libgfortran-ng &gt;=7.2.0,&lt;8.0a0 - python &gt;=3.6,&lt;3.7.0a0 - mkl &gt;=2018.0.0,&lt;2019.0a0 - blas * mkl . . 2. Utilizing Channels . . #### Channels and why are they needed? . All Conda packages we’ve seen so far were published on the main or default channel of Anaconda Cloud. A Conda channel is an identifier of a path (e.g., as in a web address) from which Conda packages can be obtained. . Channels are a means for a user to publish packages independently. . #### Searching within channels . conda search -c channel_name –platform linux-64 package_name . -c, –channel: search in channel . –override-channels: used to prevent searching on default channels . –platform: is used to select a platform . (base) $ conda search --channel davidmertz --override-channels --platform linux-64 Loading channels: done # Name Version Build Channel accelerate 2.2.0 np110py27_2 davidmertz accelerate 2.2.0 np110py35_2 davidmertz accelerate-dldist 0.1 np110py27_1 davidmertz ... textadapter 2.0.0 py36_0 davidmertz (base) $ conda search -c conda-forge -c sseefeld -c gbrener --platform win-64 textadapter Loading channels: done # Name Version Build Channel textadapter 2.0.0 py27_0 conda-forge textadapter 2.0.0 py27_0 sseefeld textadapter 2.0.0 py27h0ff66c2_1000 conda-forge ... textadapter 2.0.0 py36_0 sseefeld . #### Searching package: anaconda search pck_name . anaconda, not conda . (base) $ anaconda search textadapter Using Anaconda Cloud api site https://api.anaconda.orgRun &#39;anaconda show &lt;USER/PACKAGE&gt;&#39; to get more details: Packages: Name | Version | Package Types | Platforms - | | | DavidMertz/textadapter | 2.0.0 | conda | linux-64, osx-64 conda-forge/textadapter | 2.0.0 | conda | linux-64, win-32, osx-64, win-64 gbrener/textadapter | 2.0.0 | conda | linux-64, osx-64 : python interface Amazon S3, and large data files sseefeld/textadapter | 2.0.0 | conda | win-64 : python interface Amazon S3, and large data files stuarteberg/textadapter | 2.0.0 | conda | osx-64 Found 5 packages . #### conda-forge channel . The default channel on Anaconda Cloud is curated by Anaconda Inc., but another channel called conda-forge also has a special status. This channel does not operate any differently than other channels, whether those others are associated with an individual or organization, but it acts as a kind of “community curation” of relatively well-vetted packages. . (base) $ conda search -c conda-forge | grep conda-forge | wc -l 87113 . About 90,000 packages in conda-forge channel. . #### Installing from a channel . conda install --channel my-organization the-package . . 3. Working with Environments . . #### Environments and why are they needed? . Conda environments allow multiple incompatible versions of the same (software) package to coexist on your system. An environment is simply a file path containing a collection of mutually compatible packages. By isolating distinct versions of a given package (and their dependencies) in distinct environments, those versions are all available to work on particular projects or tasks. . Conda environments allow for flexible version management of packages. . #### Which environment am I using? . (course-project) $ conda env list # conda environments: # _tmp /.conda/envs/_tmp course-env /.conda/envs/course-env course-project * /.conda/envs/course-project pd-2015 /.conda/envs/pd-2015 py1.0 /.conda/envs/py1.0 test-env /.conda/envs/test-env base /home/repl/miniconda . #### What packages are installed in an environment? (I) . (base) $ conda list &#39;numpy|pandas&#39; # packages in environment at /home/repl/miniconda: ## Name Version Build Channel numpy 1.16.0 py36h7e9f1db_1 numpy-base 1.16.0 py36hde5b4d6_1 pandas 0.22.0 py36hf484d3e_0 . #### What packages are installed in an environment? (II) . conda list . -n, –name: env_name . (base) $ conda list -n pd-2015 &#39;numpy|pandas&#39; # packages in environment at /.conda/envs/pd-2015: ## Name Version Build Channel numpy 1.16.4 py36h7e9f1db_0 numpy-base 1.16.4 py36hde5b4d6_0 pandas 0.22.0 py36hf484d3e_0 . #### Switch between environments . To activate an environment, you simply use conda activate ENVNAME . To deactivate an environment, you use conda deactivate , which returns you to the root/base environment. . (base) $ conda activate course-env (course-env) $ conda activate pd-2015 (pd-2015) $ conda deactivate (course-env) $ conda env list # conda environments: # _tmp /.conda/envs/_tmp course-env * /.conda/envs/course-env course-project /.conda/envs/course-project pd-2015 /.conda/envs/pd-2015 py1.0 /.conda/envs/py1.0 test-env /.conda/envs/test-env base /home/repl/miniconda . #### Remove an environment . conda env remove –name ENVNAME . -n, –name . (base) $ conda env remove -n deprecated Remove all packages in environment /.conda/envs/deprecated: . #### Create a new environment . conda create –name recent-pd python=3.6 pandas=0.22 scipy statsmodels . (base) $ conda create -n conda-essentials attrs=19.1.0 cytoolz Collecting package metadata (current_repodata.json): done Solving environment: done ... . #### Export an environment . conda env export . -n, –name: export an environment other than the active one . -f, –file: output the environment specification to a file . By convention, the name environment.yml is used for environment, but any name can be used (but the extension .yml is strongly encouraged). . (base) $ conda env export -n course-env -f course-env.yml (base) $ head course-env.yml name: course-env channels: - defaults dependencies: - _libgcc_mutex=0.1=main - blas=1.0=mkl - ca-certificates=2019.5.15=0 - certifi=2019.6.16=py36_0 - intel-openmp=2019.4=243 - libedit=3.1.20181209=hc058e9b_0 . #### Create an environment from a shared specification . conda env create -n env_name -f file-name.yml . (base) $ conda env create --file environment.yml Collecting package metadata (repodata.json): done Solving environment: done . (base) $ cat shared-config.yml name: functional-data channels: - defaults dependencies: - python=3 - cytoolz - attrs (base) $ conda env create -f shared-config.yml . . 4. Case Study on Using Environments . . #### Compatibility with different versions . A common case for using environments is in developing scripts or Jupyter notebooks that rely on particular software versions for their functionality. Over time, the underlying tools might change, making updating the scripts worthwhile. Being able to switch between environments with different versions of the underlying packages installed makes this development process much easier. . (base) $ cat weekly_humidity.py# weekly_humidity.py # rolling mean of humidity import pandas as pd df = pd.read_csv(&#39;pittsburgh2015_celsius.csv&#39;) humidity = df[&#39;Mean Humidity&#39;] print(pd.rolling_mean(humidity, 7).tail(5)) (base) $ python weekly_humidity.py weekly_humidity.py:6: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with Series.rolling(window=7,center=False).mean() print(pd.rolling_mean(humidity, 7).tail(5)) 360 77.000000361 80.428571362 78.857143 363 78.285714 364 78.714286Name: Mean Humidity, dtype: float64 (base) $ conda activate pd-2015 (pd-2015) $ python weekly_humidity.py 360 77.000000 361 80.428571 362 78.857143 363 78.285714 364 78.714286 Name: Mean Humidity, dtype: float64 . FutureWarning is not present in pd-2015 environment. . #### Updating a script . Update the script so the FutureWarning is gone. . (base) $ nano weekly_humidity.py (base) $ cat weekly_humidity.py # weekly_humidity.py # rolling mean of humidity import pandas as pd df = pd.read_csv(&#39;pittsburgh2015_celsius.csv&#39;) humidity = df[&#39;Mean Humidity&#39;] print(humidity.rolling(7).mean().tail(5)) (base) $ python weekly_humidity.py 360 77.000000 361 80.428571 362 78.857143 363 78.285714 364 78.714286 Name: Mean Humidity, dtype: float64 (base) $ conda activate pd-2015 (pd-2015) $ python weekly_humidity.py 360 77.000000 361 80.428571 362 78.857143 363 78.285714 364 78.714286 Name: Mean Humidity, dtype: float64 . The End. . Thank you for reading. .",
            "url": "https://islamalam.github.io/blog/machine%20learning/deep%20learning/datacamp/2021/12/07/conda-essentials.html",
            "relUrl": "/machine%20learning/deep%20learning/datacamp/2021/12/07/conda-essentials.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post41": {
            "title": "Feature Engineering for Machine Learning in Python",
            "content": "Feature Engineering for Machine Learning in Python . This is the memo of the 10th course (23 courses in all) of ‘Machine Learning Scientist with Python’ skill track. You can find the original course HERE. . Course Description . Every day you read about the amazing breakthroughs in how the newest applications of machine learning are changing the world. Often this reporting glosses over the fact that a huge amount of data munging and feature engineering must be done before any of these fancy models can be used. In this course, you will learn how to do just that. You will work with Stack Overflow Developers survey, and historic US presidential inauguration addresses, to understand how best to preprocess and engineer features from categorical, continuous, and unstructured data. This course will give you hands-on experience on how to prepare any data for your own machine learning models. . . Creating Features | Dealing with Messy Data | Conforming to Statistical Assumptions | Dealing with Text Data | 1. Creating Features . 1.1 Why generate features? . | | | . 1.1.1 Getting to know your data . Pandas is one the most popular packages used to work with tabular data in Python. It is generally imported using the alias pd and can be used to load a CSV (or other delimited files) using read_csv(). . You will be working with a modified subset of the Stackoverflow survey response data in the first three chapters of this course. This data set records the details, and preferences of thousands of users of the StackOverflow website. . # Import pandas import pandas as pd # Import so_survey_csv into so_survey_df so_survey_df = pd.read_csv(so_survey_csv) # Print the first five rows of the DataFrame print(so_survey_df.head()) # Print the data type of each column print(so_survey_df.dtypes) . SurveyDate FormalEducation ConvertedSalary Hobby Country ... VersionControl Age Years Experience Gender RawSalary 0 2/28/18 20:20 Bachelor&#39;s degree (BA. BS. B.Eng.. etc.) NaN Yes South Africa ... Git 21 13 Male NaN 1 6/28/18 13:26 Bachelor&#39;s degree (BA. BS. B.Eng.. etc.) 70841.0 Yes Sweeden ... Git;Subversion 38 9 Male 70,841.00 2 6/6/18 3:37 Bachelor&#39;s degree (BA. BS. B.Eng.. etc.) NaN No Sweeden ... Git 45 11 NaN NaN 3 5/9/18 1:06 Some college/university study without earning ... 21426.0 Yes Sweeden ... Zip file back-ups 46 12 Male 21,426.00 4 4/12/18 22:41 Bachelor&#39;s degree (BA. BS. B.Eng.. etc.) 41671.0 Yes UK ... Git 39 7 Male £41,671.00 [5 rows x 11 columns] SurveyDate object FormalEducation object ConvertedSalary float64 Hobby object Country object StackOverflowJobsRecommend float64 VersionControl object Age int64 Years Experience int64 Gender object RawSalary object dtype: object . 1.1.2 Selecting specific data types . Often a data set will contain columns with several different data types (like the one you are working with). The majority of machine learning models require you to have a consistent data type across features. Similarly, most feature engineering techniques are applicable to only one type of data at a time. For these reasons among others, you will often want to be able to access just the columns of certain types when working with a DataFrame. . # Create subset of only the numeric columns so_numeric_df = so_survey_df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]) # Print the column names contained in so_survey_df_num print(so_numeric_df.columns) # Index([&#39;ConvertedSalary&#39;, &#39;StackOverflowJobsRecommend&#39;, &#39;Age&#39;, &#39;Years Experience&#39;], dtype=&#39;object&#39;) . 1.2 Dealing with categorical features . | | | | | | | . 1.2.1 One-hot encoding and dummy variables . To use categorical variables in a machine learning model, you first need to represent them in a quantitative way. The two most common approaches are to one-hot encode the variables using or to use dummy variables. In this exercise, you will create both types of encoding, and compare the created column sets. We will continue using the same DataFrame from previous lesson loaded as so_survey_df and focusing on its Country column. . # Convert the Country column to a one hot encoded Data Frame one_hot_encoded = pd.get_dummies(so_survey_df, columns=[&#39;Country&#39;], prefix=&#39;OH&#39;) # Print the columns names print(one_hot_encoded.columns) . Index([&#39;SurveyDate&#39;, &#39;FormalEducation&#39;, &#39;ConvertedSalary&#39;, &#39;Hobby&#39;, &#39;StackOverflowJobsRecommend&#39;, &#39;VersionControl&#39;, &#39;Age&#39;, &#39;Years Experience&#39;, &#39;Gender&#39;, &#39;RawSalary&#39;, &#39;OH_France&#39;, &#39;OH_India&#39;, &#39;OH_Ireland&#39;, &#39;OH_Russia&#39;, &#39;OH_South Africa&#39;, &#39;OH_Spain&#39;, &#39;OH_Sweeden&#39;, &#39;OH_UK&#39;, &#39;OH_USA&#39;, &#39;OH_Ukraine&#39;], dtype=&#39;object&#39;) . # Create dummy variables for the Country column dummy = pd.get_dummies(so_survey_df, columns=[&#39;Country&#39;], drop_first=True, prefix=&#39;DM&#39;) # Print the columns names print(dummy.columns) . Index([&#39;SurveyDate&#39;, &#39;FormalEducation&#39;, &#39;ConvertedSalary&#39;, &#39;Hobby&#39;, &#39;StackOverflowJobsRecommend&#39;, &#39;VersionControl&#39;, &#39;Age&#39;, &#39;Years Experience&#39;, &#39;Gender&#39;, &#39;RawSalary&#39;, &#39;DM_India&#39;, &#39;DM_Ireland&#39;, &#39;DM_Russia&#39;, &#39;DM_South Africa&#39;, &#39;DM_Spain&#39;, &#39;DM_Sweeden&#39;, &#39;DM_UK&#39;, &#39;DM_USA&#39;, &#39;DM_Ukraine&#39;], dtype=&#39;object&#39;) . Did you notice that the column for France was missing when you created dummy variables? Now you can choose to use one-hot encoding or dummy variables where appropriate. . 1.2.2 Dealing with uncommon categories . Some features can have many different categories but a very uneven distribution of their occurrences. Take for example Data Science’s favorite languages to code in, some common choices are Python, R, and Julia, but there can be individuals with bespoke choices, like FORTRAN, C etc. In these cases, you may not want to create a feature for each value, but only the more common occurrences. . countries.value_counts() South Africa 166 USA 164 Spain 134 Sweeden 119 France 115 Russia 97 India 95 UK 95 Ukraine 9 Ireland 5 Name: Country, dtype: int64 . # Create a series out of the Country column countries = so_survey_df[&#39;Country&#39;] # Get the counts of each category country_counts = countries.value_counts() # Create a mask for only categories that occur less than 10 times mask = countries.isin(country_counts[country_counts &lt; 10].index) # Label all other categories as Other countries[mask] = &#39;Other&#39; # Print the updated category counts print(countries.value_counts()) . South Africa 166 USA 164 Spain 134 Sweeden 119 France 115 Russia 97 India 95 UK 95 Other 14 Name: Country, dtype: int64 . Good work, now you can work with large data sets while grouping low frequency categories. . 1.3 Numeric variables . | | | | | . 1.3.1 Binarizing columns . While numeric values can often be used without any feature engineering, there will be cases when some form of manipulation can be useful. For example on some occasions, you might not care about the magnitude of a value but only care about its direction, or if it exists at all. In these situations, you will want to binarize a column. In the so_survey_df data, you have a large number of survey respondents that are working voluntarily (without pay). You will create a new column titled Paid_Job indicating whether each person is paid (their salary is greater than zero). . # Create the Paid_Job column filled with zeros so_survey_df[&#39;Paid_Job&#39;] = 0 # Replace all the Paid_Job values where ConvertedSalary is &gt; 0 so_survey_df.loc[so_survey_df[&#39;ConvertedSalary&#39;]&gt;0, &#39;Paid_Job&#39;] = 1 # Print the first five rows of the columns print(so_survey_df[[&#39;Paid_Job&#39;, &#39;ConvertedSalary&#39;]].head()) . Paid_Job ConvertedSalary 0 0 0.0 1 1 70841.0 2 0 0.0 3 1 21426.0 4 1 41671.0 . Good work, binarizing columns can also be useful for your target variables. . 1.3.2 Binning values . For many continuous values you will care less about the exact value of a numeric column, but instead care about the bucket it falls into. This can be useful when plotting values, or simplifying your machine learning models. It is mostly used on continuous variables where accuracy is not the biggest concern e.g. age, height, wages. . Bins are created using pd.cut(df[&#39;column_name&#39;], bins) where bins can be an integer specifying the number of evenly spaced bins, or a list of bin boundaries. . # Bin the continuous variable ConvertedSalary into 5 bins so_survey_df[&#39;equal_binned&#39;] = pd.cut(so_survey_df[&#39;ConvertedSalary&#39;], bins=5) # Print the first 5 rows of the equal_binned column print(so_survey_df[[&#39;equal_binned&#39;, &#39;ConvertedSalary&#39;]].head()) . equal_binned ConvertedSalary 0 (-2000.0, 400000.0] 0.0 1 (-2000.0, 400000.0] 70841.0 2 (-2000.0, 400000.0] 0.0 3 (-2000.0, 400000.0] 21426.0 4 (-2000.0, 400000.0] 41671.0 . # Import numpy import numpy as np # Specify the boundaries of the bins bins = [-np.inf, 10000, 50000, 100000, 150000, np.inf] # Bin labels labels = [&#39;Very low&#39;, &#39;Low&#39;, &#39;Medium&#39;, &#39;High&#39;, &#39;Very high&#39;] # Bin the continuous variable ConvertedSalary using these boundaries so_survey_df[&#39;boundary_binned&#39;] = pd.cut(so_survey_df[&#39;ConvertedSalary&#39;], bins=bins, labels=labels) # Print the first 5 rows of the boundary_binned column print(so_survey_df[[&#39;boundary_binned&#39;, &#39;ConvertedSalary&#39;]].head()) . boundary_binned ConvertedSalary 0 Very low 0.0 1 Medium 70841.0 2 Very low 0.0 3 Low 21426.0 4 Low 41671.0 . Now you can bin columns with equal spacing and predefined boundaries. . 2. Dealing with Messy Data . 2.1 Why do missing values exist? . | | | | | | . 2.1.1 How sparse is my data? . Most data sets contain missing values, often represented as NaN (Not a Number). If you are working with Pandas you can easily check how many missing values exist in each column. . Let’s find out how many of the developers taking the survey chose to enter their age (found in the Age column of so_survey_df) and their gender (Gender column of so_survey_df). . # Subset the DataFrame sub_df = so_survey_df[[&#39;Age&#39;,&#39;Gender&#39;]] # Print the number of non-missing values print(sub_df.notnull().sum()) Age 999 Gender 693 dtype: int64 . 2.1.2 Finding the missing values . While having a summary of how much of your data is missing can be useful, often you will need to find the exact locations of these missing values. Using the same subset of the StackOverflow data from the last exercise (sub_df), you will show how a value can be flagged as missing. . # Print the locations of the missing values print(sub_df.head(10).isnull()) Age Gender 0 False False 1 False False 2 False True 3 False False 4 False False 5 False False 6 False False 7 False False 8 False False 9 False True # Print the locations of the non-missing values print(sub_df.head(10).notnull()) Age Gender 0 True True 1 True True 2 True False 3 True True 4 True True 5 True True 6 True True 7 True True 8 True True 9 True False . 2.2 Dealing with missing values (I) . | | | | | | | | . 2.2.1 Listwise deletion . The simplest way to deal with missing values in your dataset when they are occurring entirely at random is to remove those rows, also called ‘listwise deletion’. . Depending on the use case, you will sometimes want to remove all missing values in your data while other times you may want to only remove a particular column if too many values are missing in that column. . # Print the number of rows and columns print(so_survey_df.shape) # (999, 11) # Create a new DataFrame dropping all incomplete rows no_missing_values_rows = so_survey_df.dropna() # Print the shape of the new DataFrame print(no_missing_values_rows.shape) # (264, 11) # Create a new DataFrame dropping all columns with incomplete rows no_missing_values_cols = so_survey_df.dropna(how=&#39;any&#39;, axis=1) # Print the shape of the new DataFrame print(no_missing_values_cols.shape) # (999, 7) # Drop all rows where Gender is missing no_gender = so_survey_df.dropna(subset=[&#39;Gender&#39;]) # Print the shape of the new DataFrame print(no_gender.shape) # (693, 11) . Correct, as you can see dropping all rows that contain any missing values may greatly reduce the size of your dataset. So you need to think carefully and consider several trade-offs when deleting missing values. . 2.2.2 Replacing missing values with constants . While removing missing data entirely maybe a correct approach in many situations, this may result in a lot of information being omitted from your models. . You may find categorical columns where the missing value is a valid piece of information in itself, such as someone refusing to answer a question in a survey. In these cases, you can fill all missing values with a new category entirely, for example ‘No response given’. . # Print the count of occurrences print(so_survey_df[&#39;Gender&#39;].value_counts()) . Male 632 Female 53 Transgender 2 Female;Male 2 Female;Transgender 1 Male;Non-binary. genderqueer. or gender non-conforming 1 Female;Male;Transgender;Non-binary. genderqueer. or gender non-conforming 1 Non-binary. genderqueer. or gender non-conforming 1 Name: Gender, dtype: int64 . # Replace missing values so_survey_df[&#39;Gender&#39;].fillna(&#39;Not Given&#39;, inplace=True) # Print the count of each value print(so_survey_df[&#39;Gender&#39;].value_counts()) . Male 632 Not Given 306 Female 53 Transgender 2 Female;Male 2 Female;Transgender 1 Male;Non-binary. genderqueer. or gender non-conforming 1 Female;Male;Transgender;Non-binary. genderqueer. or gender non-conforming 1 Non-binary. genderqueer. or gender non-conforming 1 Name: Gender, dtype: int64 . 2.3 Dealing with missing values (II) . | | | | | | . 2.3.1 Filling continuous missing values . In the last lesson, you dealt with different methods of removing data missing values and filling in missing values with a fixed string. These approaches are valid in many cases, particularly when dealing with categorical columns but have limited use when working with continuous values. In these cases, it may be most valid to fill the missing values in the column with a value calculated from the entries present in the column. . # Print the first five rows of StackOverflowJobsRecommend column print(so_survey_df[&#39;StackOverflowJobsRecommend&#39;].head(5)) # Fill missing values with the mean so_survey_df[&#39;StackOverflowJobsRecommend&#39;].fillna(so_survey_df[&#39;StackOverflowJobsRecommend&#39;].mean(), inplace=True) # Round the StackOverflowJobsRecommend values so_survey_df[&#39;StackOverflowJobsRecommend&#39;] = round(so_survey_df[&#39;StackOverflowJobsRecommend&#39;]) # Print the top 5 rows print(so_survey_df[&#39;StackOverflowJobsRecommend&#39;].head()) 0 7.0 1 7.0 2 8.0 3 7.0 4 8.0 Name: StackOverflowJobsRecommend, dtype: float64 . Nicely done, remember you should only round your values if you are certain it is applicable. . 2.3.2 Imputing values in predictive models . When working with predictive models you will often have a separate train and test DataFrames. In these cases you want to ensure no information from your test set leaks into your train set. When filling missing values in data to be used in these situations how should approach the two data sets? . Apply the measures of central tendency (mean/median etc.) calculated on the train set to both the train and test sets. . Values calculated on the train test should be applied to both DataFrames. . 2.4 Dealing with other data issues . | | | | . 2.4.1 Dealing with stray characters (I) . In this exercise, you will work with the RawSalary column of so_survey_df which contains the wages of the respondents along with the currency symbols and commas, such as $42,000. When importing data from Microsoft Excel, more often that not you will come across data in this form. . so_survey_df[&#39;RawSalary&#39;] 0 NaN 1 70,841.00 2 NaN 3 21,426.00 4 £41,671.00 . # Remove the commas in the column so_survey_df[&#39;RawSalary&#39;] = so_survey_df[&#39;RawSalary&#39;].str.replace(&#39;,&#39;, &#39;&#39;) # Remove the dollar signs in the column so_survey_df[&#39;RawSalary&#39;] = so_survey_df[&#39;RawSalary&#39;].str.replace(&#39;$&#39;,&#39;&#39;) . 2.4.2 Dealing with stray characters (II) . In the last exercise, you could tell quickly based off of the df.head() call which characters were causing an issue. In many cases this will not be so apparent. There will often be values deep within a column that are preventing you from casting a column as a numeric type so that it can be used in a model or further feature engineering. . One approach to finding these values is to force the column to the data type desired using pd.to_numeric(), coercing any values causing issues to NaN, Then filtering the DataFrame by just the rows containing the NaN values. . Try to cast the RawSalary column as a float and it will fail as an additional character can now be found in it. Find the character and remove it so the column can be cast as a float. . # Attempt to convert the column to numeric values numeric_vals = pd.to_numeric(so_survey_df[&#39;RawSalary&#39;], errors=&#39;coerce&#39;) # Find the indexes of missing values idx = numeric_vals.isna() # Print the relevant rows print(so_survey_df[&#39;RawSalary&#39;][idx]) 0 NaN 2 NaN 4 £41671.00 6 NaN 8 NaN ... . # Replace the offending characters so_survey_df[&#39;RawSalary&#39;] = so_survey_df[&#39;RawSalary&#39;].str.replace(&#39;£&#39;,&#39;&#39;) # Convert the column to float so_survey_df[&#39;RawSalary&#39;] = so_survey_df[&#39;RawSalary&#39;].astype(&#39;float&#39;) # Print the column print(so_survey_df[&#39;RawSalary&#39;]) . Remember that even after removing all the relevant characters, you still need to change the type of the column to numeric if you want to plot these continuous values. . 2.4.3 Method chaining . When applying multiple operations on the same column (like in the previous exercises), you made the changes in several steps, assigning the results back in each step. However, when applying multiple successive operations on the same column, you can “chain” these operations together for clarity and ease of management. This can be achieved by calling multiple methods sequentially: . # Method chaining df[&#39;column&#39;] = df[&#39;column&#39;].method1().method2().method3() # Same as df[&#39;column&#39;] = df[&#39;column&#39;].method1() df[&#39;column&#39;] = df[&#39;column&#39;].method2() df[&#39;column&#39;] = df[&#39;column&#39;].method3() . In this exercise you will repeat the steps you performed in the last two exercises, but do so using method chaining. . # Use method chaining so_survey_df[&#39;RawSalary&#39;] = so_survey_df[&#39;RawSalary&#39;] .str.replace(&#39;,&#39;,&#39;&#39;) .str.replace(&#39;$&#39;,&#39;&#39;) .str.replace(&#39;£&#39;,&#39;&#39;) .astype(&#39;float&#39;) # Print the RawSalary column print(so_survey_df[&#39;RawSalary&#39;]) . Custom functions can be also used when method chaining using the .apply() method. . 3. Conforming to Statistical Assumptions . 3.1 Data distributions . | | | | | | . 3.1.1 What does your data look like? (I) . Up until now you have focused on creating new features and dealing with issues in your data. Feature engineering can also be used to make the most out of the data that you already have and use it more effectively when creating machine learning models. Many algorithms may assume that your data is normally distributed, or at least that all your columns are on the same scale. This will often not be the case, e.g. one feature may be measured in thousands of dollars while another would be number of years. In this exercise, you will create plots to examine the distributions of some numeric columns in the so_survey_df DataFrame, stored in so_numeric_df. . # Create a histogram so_numeric_df.hist() plt.show() . . # Create a boxplot of two columns so_numeric_df[[&#39;Age&#39;, &#39;Years Experience&#39;]].boxplot() plt.show() . . # Create a boxplot of ConvertedSalary so_numeric_df[[&#39;ConvertedSalary&#39;]].boxplot() plt.show() . . 3.1.2 What does your data look like? (II) . In the previous exercise you looked at the distribution of individual columns. While this is a good start, a more detailed view of how different features interact with each other may be useful as this can impact your decision on what to transform and how. . # Import packages from matplotlib import pyplot as plt import seaborn as sns # Plot pairwise relationships sns.pairplot(so_numeric_df) # Show plot plt.show() . . # Print summary statistics print(so_numeric_df.describe()) . ConvertedSalary Age Years Experience count 9.990000e+02 999.000000 999.000000 mean 6.161746e+04 36.003003 9.961962 std 1.760924e+05 13.255127 4.878129 min 0.000000e+00 18.000000 0.000000 25% 0.000000e+00 25.000000 7.000000 50% 2.712000e+04 35.000000 10.000000 75% 7.000000e+04 45.000000 13.000000 max 2.000000e+06 83.000000 27.000000 . Good work, understanding these summary statistics of a column can be very valuable when deciding what transformations are necessary. . 3.1.3 When don’t you have to transform your data? . While making sure that all of your data is on the same scale is advisable for most analyses, for which of the following machine learning models is normalizing data not always necessary? . Decision Trees . As decision trees split along a singular point, they do not require all the columns to be on the same scale. . 3.2 Scaling and transformations . | | | | | | | | . 3.2.1 Normalization . As discussed in the video, in normalization you linearly scale the entire column between 0 and 1, with 0 corresponding with the lowest value in the column, and 1 with the largest. When using scikit-learn (the most commonly used machine learning library in Python) you can use a MinMaxScaler to apply normalization. (It is called this as it scales your values between a minimum and maximum value.) . # Import MinMaxScaler from sklearn.preprocessing import MinMaxScaler # Instantiate MinMaxScaler MM_scaler = MinMaxScaler() # Fit MM_scaler to the data MM_scaler.fit(so_numeric_df[[&#39;Age&#39;]]) # Transform the data using the fitted scaler so_numeric_df[&#39;Age_MM&#39;] = MM_scaler.transform(so_numeric_df[[&#39;Age&#39;]]) # Compare the origional and transformed column print(so_numeric_df[[&#39;Age_MM&#39;, &#39;Age&#39;]].head()) . Age_MM Age 0 0.046154 21 1 0.307692 38 2 0.415385 45 3 0.430769 46 4 0.323077 39 . Did you notice that all values have been scaled between 0 and 1? . 3.2.2 Standardization . While normalization can be useful for scaling a column between two data points, it is hard to compare two scaled columns if even one of them is overly affected by outliers. One commonly used solution to this is called standardization, where instead of having a strict upper and lower bound, you center the data around its mean, and calculate the number of standard deviations away from mean each data point is. . # Import StandardScaler from sklearn.preprocessing import StandardScaler # Instantiate StandardScaler SS_scaler = StandardScaler() # Fit SS_scaler to the data SS_scaler.fit(so_numeric_df[[&#39;Age&#39;]]) # Transform the data using the fitted scaler so_numeric_df[&#39;Age_SS&#39;] = SS_scaler.transform(so_numeric_df[[&#39;Age&#39;]]) # Compare the origional and transformed column print(so_numeric_df[[&#39;Age_SS&#39;, &#39;Age&#39;]].head()) . Age_SS Age 0 -1.132431 21 1 0.150734 38 2 0.679096 45 3 0.754576 46 4 0.226214 39 . you can see that the values have been scaled linearly, but not between set values. . 3.2.3 Log transformation . In the previous exercises you scaled the data linearly, which will not affect the data’s shape. This works great if your data is normally distributed (or closely normally distributed), an assumption that a lot of machine learning models make. Sometimes you will work with data that closely conforms to normality, e.g the height or weight of a population. On the other hand, many variables in the real world do not follow this pattern e.g, wages or age of a population. In this exercise you will use a log transform on the ConvertedSalary column in the so_numeric_df DataFrame as it has a large amount of its data centered around the lower values, but contains very high values also. These distributions are said to have a long right tail. . # Import PowerTransformer from sklearn.preprocessing import PowerTransformer # Instantiate PowerTransformer pow_trans = PowerTransformer() # Train the transform on the data pow_trans.fit(so_numeric_df[[&#39;ConvertedSalary&#39;]]) # Apply the power transform to the data so_numeric_df[&#39;ConvertedSalary_LG&#39;] = pow_trans.transform(so_numeric_df[[&#39;ConvertedSalary&#39;]]) # Plot the data before and after the transformation so_numeric_df[[&#39;ConvertedSalary&#39;, &#39;ConvertedSalary_LG&#39;]].hist() plt.show() . so_numeric_df.head() ConvertedSalary Age Years Experience ConvertedSalary_LG 0 NaN 21 13 NaN 1 70841.0 38 9 0.312939 2 NaN 45 11 NaN 3 21426.0 46 12 -0.652182 4 41671.0 39 7 -0.135589 . . Did you notice the change in the shape of the distribution? ConvertedSalary_LG column looks much more normal than the original ConvertedSalary column. . 3.2.4 When can you use normalization? . When could you use normalization (MinMaxScaler) when working with a dataset? . When you know the the data has a strict upper and lower bound. . Normalization scales all points linearly between the upper and lower bound. . 3.3 Removing outliers . | | | | | . 3.3.1 Percentage based outlier removal . One way to ensure a small portion of data is not having an overly adverse effect is by removing a certain percentage of the largest and/or smallest values in the column. This can be achieved by finding the relevant quantile and trimming the data using it with a mask. This approach is particularly useful if you are concerned that the highest values in your dataset should be avoided. When using this approach, you must remember that even if there are no outliers, this will still remove the same top N percentage from the dataset. . # Find the 95th quantile quantile = so_numeric_df[&#39;ConvertedSalary&#39;].quantile(0.95) # Trim the outliers trimmed_df = so_numeric_df[so_numeric_df[&#39;ConvertedSalary&#39;] &lt; quantile] # The original histogram so_numeric_df[[&#39;ConvertedSalary&#39;]].hist() plt.show() plt.clf() # The trimmed histogram trimmed_df[[&#39;ConvertedSalary&#39;]].hist() plt.show() . | | . In the next exercise, you will work with a more statistically sound approach in removing outliers. . 3.3.2 Statistical outlier removal . While removing the top N% of your data is useful for ensuring that very spurious points are removed, it does have the disadvantage of always removing the same proportion of points, even if the data is correct. A commonly used alternative approach is to remove data that sits further than three standard deviations from the mean. You can implement this by first calculating the mean and standard deviation of the relevant column to find upper and lower bounds, and applying these bounds as a mask to the DataFrame. This method ensures that only data that is genuinely different from the rest is removed, and will remove fewer points if the data is close together. . # Find the mean and standard dev std = so_numeric_df[&#39;ConvertedSalary&#39;].std() mean = so_numeric_df[&#39;ConvertedSalary&#39;].mean() # Calculate the cutoff cut_off = std * 3 lower, upper = mean - cut_off, mean + cut_off # Trim the outliers trimmed_df = so_numeric_df[(so_numeric_df[&#39;ConvertedSalary&#39;] &lt; upper) &amp; (so_numeric_df[&#39;ConvertedSalary&#39;] &gt; lower)] # The trimmed box plot trimmed_df[[&#39;ConvertedSalary&#39;]].boxplot() plt.show() . | | . Did you notice the scale change on the y-axis? . 3.4 Scaling and transforming new data . | | | . 3.4.1 Train and testing transformations (I) . So far you have created scalers based on a column, and then applied the scaler to the same data that it was trained on. When creating machine learning models you will generally build your models on historic data (train set) and apply your model to new unseen data (test set). In these cases you will need to ensure that the same scaling is being applied to both the training and test data. To do this in practice you train the scaler on the train set, and keep the trained scaler to apply it to the test set. You should never retrain a scaler on the test set. . For this exercise and the next, we split the so_numeric_df DataFrame into train (so_train_numeric) and test (so_test_numeric) sets. . # Import StandardScaler from sklearn.preprocessing import StandardScaler # Apply a standard scaler to the data SS_scaler = StandardScaler() # Fit the standard scaler to the data SS_scaler.fit(so_train_numeric[[&#39;Age&#39;]]) # Transform the test data using the fitted scaler so_test_numeric[&#39;Age_ss&#39;] = SS_scaler.transform(so_test_numeric[[&#39;Age&#39;]]) print(so_test_numeric[[&#39;Age&#39;, &#39;Age_ss&#39;]].head()) . Age Age_ss 700 35 -0.069265 701 18 -1.343218 702 47 0.829997 703 57 1.579381 704 41 0.380366 . Data leakage is one of the most common mistakes data scientists tend to make, and I hope that you won’t! . 3.4.2 Train and testing transformations (II) . Similar to applying the same scaler to both your training and test sets, if you have removed outliers from the train set, you probably want to do the same on the test set as well. Once again you should ensure that you use the thresholds calculated only from the train set to remove outliers from the test set. . Similar to the last exercise, we split the so_numeric_df DataFrame into train (so_train_numeric) and test (so_test_numeric) sets. . train_std = so_train_numeric[&#39;ConvertedSalary&#39;].std() train_mean = so_train_numeric[&#39;ConvertedSalary&#39;].mean() cut_off = train_std * 3 train_lower, train_upper = train_mean - cut_off, train_mean + cut_off # Trim the test DataFrame trimmed_df = so_test_numeric[(so_test_numeric[&#39;ConvertedSalary&#39;] &lt; train_upper) &amp; (so_test_numeric[&#39;ConvertedSalary&#39;] &gt; train_lower)] . Very well done. In the next chapter, you will deal with unstructured (text) data. . 4. Dealing with Text Data . 4.1 Encoding text . | | | | | | | | | | . 4.1.1 Cleaning up your text . Unstructured text data cannot be directly used in most analyses. Multiple steps need to be taken to go from a long free form string to a set of numeric columns in the right format that can be ingested by a machine learning model. The first step of this process is to standardize the data and eliminate any characters that could cause problems later on in your analytic pipeline. . In this chapter you will be working with a new dataset containing the inaugural speeches of the presidents of the United States loaded as speech_df, with the speeches stored in the text column. . # Print the first 5 rows of the text column print(speech_df.text.head()) . 0 Fellow-Citizens of the Senate and of the House... 1 Fellow Citizens: I AM again called upon by th... 2 WHEN it was first perceived, in early times, t... 3 Friends and Fellow-Citizens: CALLED upon to u... 4 PROCEEDING, fellow-citizens, to that qualifica... Name: text, dtype: object . # Replace all non letter characters with a whitespace speech_df[&#39;text_clean&#39;] = speech_df[&#39;text&#39;].str.replace(&#39;[^a-zA-Z]&#39;, &#39; &#39;) # Change to lower case speech_df[&#39;text_clean&#39;] = speech_df[&#39;text_clean&#39;].str.lower() # Print the first 5 rows of the text_clean column print(speech_df[&#39;text_clean&#39;].head()) . 0 fellow citizens of the senate and of the house... 1 fellow citizens i am again called upon by th... 2 when it was first perceived in early times t... 3 friends and fellow citizens called upon to u... 4 proceeding fellow citizens to that qualifica... Name: text_clean, dtype: object . Great, now your text strings have been standardized and cleaned up. You can now use this new column (text_clean) to extract information about the speeches. . 4.1.2 High level text features . Once the text has been cleaned and standardized you can begin creating features from the data. The most fundamental information you can calculate about free form text is its size, such as its length and number of words. In this exercise (and the rest of this chapter), you will focus on the cleaned/transformed text column (text_clean) you created in the last exercise. . # Find the length of each text speech_df[&#39;char_cnt&#39;] = speech_df[&#39;text_clean&#39;].str.len() # Count the number of words in each text speech_df[&#39;word_cnt&#39;] = speech_df[&#39;text_clean&#39;].str.split().str.len() # Find the average length of word speech_df[&#39;avg_word_length&#39;] = speech_df[&#39;char_cnt&#39;] / speech_df[&#39;word_cnt&#39;] # Print the first 5 rows of these columns print(speech_df[[&#39;text_clean&#39;, &#39;char_cnt&#39;, &#39;word_cnt&#39;, &#39;avg_word_length&#39;]]) . text_clean char_cnt word_cnt avg_word_length 0 fellow citizens of the senate and of the house... 8616 1432 6.016760 1 fellow citizens i am again called upon by th... 787 135 5.829630 2 when it was first perceived in early times t... 13871 2323 5.971158 . These features may appear basic but can be quite useful in ML models. . 4.2 Word counts . | | | | | | | | | | . 4.2.1 Counting words (I) . Once high level information has been recorded you can begin creating features based on the actual content of each text. One way to do this is to approach it in a similar way to how you worked with categorical variables in the earlier lessons. . For each unique word in the dataset a column is created. | For each entry, the number of times this word occurs is counted and the count value is entered into the respective column. | . These “count” columns can then be used to train machine learning models. . # Import CountVectorizer from sklearn.feature_extraction.text import CountVectorizer # Instantiate CountVectorizer cv = CountVectorizer() # Fit the vectorizer cv.fit(speech_df[&#39;text_clean&#39;]) # Print feature names print(cv.get_feature_names()) . [&#39;abandon&#39;, &#39;abandoned&#39;, &#39;abandonment&#39;, &#39;abate&#39;, &#39;abdicated&#39;, &#39;abeyance&#39;, &#39;abhorring&#39;, &#39;abide&#39;, &#39;abiding&#39;, &#39;abilities&#39;, &#39;ability&#39;, &#39;abject&#39;, &#39;able&#39;, ...] . 4.2.2 Counting words (II) . Once the vectorizer has been fit to the data, it can be used to transform the text to an array representing the word counts. This array will have a row per block of text and a column for each of the features generated by the vectorizer that you observed in the last exercise. . The vectorizer to you fit in the last exercise (cv) is available in your workspace. . # Apply the vectorizer cv_transformed = cv.transform(speech_df[&#39;text_clean&#39;]) # Print the full array cv_array = cv_transformed.toarray() print(cv_array) print(cv_array.shape) # (58, 9043) . [[0 0 0 ... 0 0 0] [0 0 0 ... 0 0 0] [0 1 0 ... 0 0 0] ... [0 1 0 ... 0 0 0] [0 0 0 ... 0 0 0] [0 0 0 ... 0 0 0]] . The speeches have 9043 unique words, which is a lot! In the next exercise, you will see how to create a limited set of features. . 4.2.3 Limiting your features . As you have seen, using the CountVectorizer with its default settings creates a feature for every single word in your corpus. This can create far too many features, often including ones that will provide very little analytical value. . For this purpose CountVectorizer has parameters that you can set to reduce the number of features: . min_df : Use only words that occur in more than this percentage of documents. This can be used to remove outlier words that will not generalize across texts. | max_df : Use only words that occur in less than this percentage of documents. This is useful to eliminate very common words that occur in every corpus without adding value such as “and” or “the”. | . # Import CountVectorizer from sklearn.feature_extraction.text import CountVectorizer # Specify arguements to limit the number of features generated cv = CountVectorizer(min_df=0.2, max_df=0.8) # Fit, transform, and convert into array cv_transformed = cv.fit_transform(speech_df[&#39;text_clean&#39;]) cv_array = cv_transformed.toarray() # Print the array shape print(cv_array.shape) # (58, 818) . 4.2.4 Text to DataFrame . Now that you have generated these count based features in an array you will need to reformat them so that they can be combined with the rest of the dataset. This can be achieved by converting the array into a pandas DataFrame, with the feature names you found earlier as the column names, and then concatenate it with the original DataFrame. . The numpy array (cv_array) and the vectorizer (cv) you fit in the last exercise are available in your workspace. . # Create a DataFrame with these features cv_df = pd.DataFrame(cv_array, columns=cv.get_feature_names()).add_prefix(&#39;Counts_&#39;) # Add the new columns to the original DataFrame speech_df_new = pd.concat([speech_df, cv_df], axis=1, sort=False) print(speech_df_new.head()) . Name Inaugural Address Date text text_clean ... Counts_years 0 George Washington First Inaugural Address Thursday, April 30, 1789 Fellow-Citizens of the Senate and of the House... fellow citizens of the senate and of the house... ... 1 1 George Washington Second Inaugural Address Monday, March 4, 1793 Fellow Citizens: I AM again called upon by th... fellow citizens i am again called upon by th... ... 0 2 John Adams Inaugural Address Saturday, March 4, 1797 WHEN it was first perceived, in early times, t... when it was first perceived in early times t... ... 3 3 Thomas Jefferson First Inaugural Address Wednesday, March 4, 1801 Friends and Fellow-Citizens: CALLED upon to u... friends and fellow citizens called upon to u... ... 0 4 Thomas Jefferson Second Inaugural Address Monday, March 4, 1805 PROCEEDING, fellow-citizens, to that qualifica... proceeding fellow citizens to that qualifica... ... 2 Counts_yet Counts_you Counts_young Counts_your 0 0 5 0 9 1 0 0 0 1 2 0 0 0 1 3 2 7 0 7 4 2 4 0 4 [5 rows x 826 columns] . With the new features combined with the orginial DataFrame they can be now used for ML models or analysis. . 4.3 Term frequency-inverse document frequency . | | | | | | | | . 4.3.1 Tf-idf . While counts of occurrences of words can be useful to build models, words that occur many times may skew the results undesirably. To limit these common words from overpowering your model a form of normalization can be used. In this lesson you will be using Term frequency-inverse document frequency (Tf-idf) as was discussed in the video. Tf-idf has the effect of reducing the value of common words, while increasing the weight of words that do not occur in many documents. . # Import TfidfVectorizer from sklearn.feature_extraction.text import TfidfVectorizer # Instantiate TfidfVectorizer tv = TfidfVectorizer(max_features=100, stop_words=&#39;english&#39;) # Fit the vectroizer and transform the data tv_transformed = tv.fit_transform(speech_df[&#39;text_clean&#39;]) # Create a DataFrame with these features tv_df = pd.DataFrame(tv_transformed.toarray(), columns=tv.get_feature_names()).add_prefix(&#39;TFIDF_&#39;) print(tv_df.head()) . TFIDF_action TFIDF_administration TFIDF_america TFIDF_american TFIDF_americans ... TFIDF_war TFIDF_way TFIDF_work TFIDF_world TFIDF_years 0 0.000000 0.133415 0.000000 0.105388 0.0 ... 0.000000 0.060755 0.000000 0.045929 0.052694 1 0.000000 0.261016 0.266097 0.000000 0.0 ... 0.000000 0.000000 0.000000 0.000000 0.000000 2 0.000000 0.092436 0.157058 0.073018 0.0 ... 0.024339 0.000000 0.000000 0.063643 0.073018 3 0.000000 0.092693 0.000000 0.000000 0.0 ... 0.036610 0.000000 0.039277 0.095729 0.000000 4 0.041334 0.039761 0.000000 0.031408 0.0 ... 0.094225 0.000000 0.000000 0.054752 0.062817 [5 rows x 100 columns] . Did you notice that counting the word occurences and calculating the Tf-idf weights are very similar? This is one of the reasons scikit-learn is very popular, a consistent API. . 4.3.2 Inspecting Tf-idf values . After creating Tf-idf features you will often want to understand what are the most highest scored words for each corpus. This can be achieved by isolating the row you want to examine and then sorting the the scores from high to low. . The DataFrame from the last exercise (tv_df) is available in your workspace. . # Isolate the row to be examined sample_row = tv_df.iloc[0] # Print the top 5 words of the sorted output print(sample_row.sort_values(ascending=False).head()) TFIDF_government 0.367430 TFIDF_public 0.333237 TFIDF_present 0.315182 TFIDF_duty 0.238637 TFIDF_citizens 0.229644 Name: 0, dtype: float64 . When creating vectors from text, any transformations that you perform before training a machine learning model, you also need to apply on the new unseen (test) data. To achieve this follow the same approach from the last chapter: fit the vectorizer only on the training data, and apply it to the test data. . For this exercise the speech_df DataFrame has been split in two: . train_speech_df: The training set consisting of the first 45 speeches. | test_speech_df: The test set consisting of the remaining speeches. | . # Instantiate TfidfVectorizer tv = TfidfVectorizer(max_features=100, stop_words=&#39;english&#39;) # Fit the vectroizer and transform the data tv_transformed = tv.fit_transform(train_speech_df[&#39;text_clean&#39;]) # Transform test data test_tv_transformed = tv.transform(test_speech_df[&#39;text_clean&#39;]) # Create new features for the test set test_tv_df = pd.DataFrame(test_tv_transformed.toarray(), columns=tv.get_feature_names()).add_prefix(&#39;TFIDF_&#39;) print(test_tv_df.head()) . TFIDF_action TFIDF_administration TFIDF_america TFIDF_american TFIDF_authority ... TFIDF_war TFIDF_way TFIDF_work TFIDF_world TFIDF_years 0 0.000000 0.029540 0.233954 0.082703 0.000000 ... 0.079050 0.033313 0.000000 0.299983 0.134749 1 0.000000 0.000000 0.547457 0.036862 0.000000 ... 0.052851 0.066817 0.078999 0.277701 0.126126 2 0.000000 0.000000 0.126987 0.134669 0.000000 ... 0.042907 0.054245 0.096203 0.225452 0.043884 3 0.037094 0.067428 0.267012 0.031463 0.039990 ... 0.030073 0.038020 0.235998 0.237026 0.061516 4 0.000000 0.000000 0.221561 0.156644 0.028442 ... 0.021389 0.081124 0.119894 0.299701 0.153133 [5 rows x 100 columns] . 4.4 N-grams . | | | | . 4.4.1 Using longer n-grams . So far you have created features based on individual words in each of the texts. This can be quite powerful when used in a machine learning model but you may be concerned that by looking at words individually a lot of the context is being ignored. To deal with this when creating models you can use n-grams which are sequence of n words grouped together. For example: . bigrams: Sequences of two consecutive words | trigrams: Sequences of three consecutive words | . These can be automatically created in your dataset by specifying the ngram_range argument as a tuple (n1, n2) where all n-grams in the n1 to n2 range are included. . # Import CountVectorizer from sklearn.feature_extraction.text import CountVectorizer # Instantiate a trigram vectorizer cv_trigram_vec = CountVectorizer(max_features=100, stop_words=&#39;english&#39;, ngram_range=(3,3)) # Fit and apply trigram vectorizer cv_trigram = cv_trigram_vec.fit_transform(speech_df[&#39;text_clean&#39;]) # Print the trigram features print(cv_trigram_vec.get_feature_names()) . [&#39;ability preserve protect&#39;, &#39;agriculture commerce manufactures&#39;, &#39;america ideal freedom&#39;, &#39;amity mutual concession&#39;, &#39;anchor peace home&#39;, &#39;ask bow heads&#39;, ...] . 4.4.2 Finding the most common words . Its always advisable once you have created your features to inspect them to ensure that they are as you would expect. This will allow you to catch errors early, and perhaps influence what further feature engineering you will need to do. . The vectorizer (cv) you fit in the last exercise and the sparse array consisting of word counts (cv_trigram) is available in your workspace. . # Create a DataFrame of the features cv_tri_df = pd.DataFrame(cv_trigram.toarray(), columns=cv_trigram_vec.get_feature_names()).add_prefix(&#39;Counts_&#39;) # Print the top 5 words in the sorted output print(cv_tri_df.sum().sort_values(ascending=False).head()) . Counts_constitution united states 20 Counts_people united states 13 Counts_preserve protect defend 10 Counts_mr chief justice 10 Counts_president united states 8 dtype: int64 . 4.5 Wrap-up . | | | | | . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/blogging/demo/2021/07/06/feature-engineering-for-machine-learning-in-python.html",
            "relUrl": "/blogging/demo/2021/07/06/feature-engineering-for-machine-learning-in-python.html",
            "date": " • Jul 6, 2021"
        }
        
    
  
    
        ,"post42": {
            "title": "Introduction To Tensorflow In Python",
            "content": "Introduction to TensorFlow in Python . This is the memo of the 14th course (23 courses in all) of ‘Machine Learning Scientist with Python’ skill track. . You can find the original course HERE . . #### . Course Description . Not long ago, cutting-edge computer vision algorithms couldn’t differentiate between images of cats and dogs. Today, a skilled data scientist equipped with nothing more than a laptop can classify tens of thousands of objects with greater accuracy than the human eye. In this course, you will use TensorFlow 2.0 to develop, train, and make predictions with the models that have powered major advances in recommendation systems, image classification, and FinTech. You will learn both high-level APIs, which will enable you to design and train deep learning models in 15 lines of code, and low-level APIs, which will allow you to move beyond off-the-shelf routines. You will also learn to accurately predict housing prices, credit card borrower defaults, and images of sign language gestures. . #### . . Introduction to TensorFlow | Linear models | Neural Networks | High Level APIs | . 1. Introduction to TensorFlow . 1.1 Constants and variables . | | | | | | | | . 1.1.1 Defining data as constants . Throughout this course, we will use tensorflow version 2.0 and will exclusively import the submodules needed to complete each exercise. This will usually be done for you, but you will do it in this exercise by importing constant from tensorflow . . After you have imported constant , you will use it to transform a numpy array, credit_numpy , into a tensorflow constant, credit_constant . This array contains feature columns from a dataset on credit card holders and is previewed in the image below. We will return to this dataset in later chapters. . Note that tensorflow version 2.0 allows you to use data as either a numpy array or a tensorflow constant object. Using a constant will ensure that any operations performed with that object are done in tensorflow . . . # Import constant from TensorFlow from tensorflow import constant # Convert the credit_numpy array into a tensorflow constant credit_constant = constant(credit_numpy) # Print constant datatype print(&#39;The datatype is:&#39;, credit_constant.dtype) # Print constant shape print(&#39;The shape is:&#39;, credit_constant.shape) . The datatype is: &lt;dtype: &#39;float64&#39;&gt; The shape is: (30000, 4) . credit_numpy array([[ 2.0000e+00, 1.0000e+00, 2.4000e+01, 3.9130e+03], [ 2.0000e+00, 2.0000e+00, 2.6000e+01, 2.6820e+03], [ 2.0000e+00, 2.0000e+00, 3.4000e+01, 2.9239e+04], ..., [ 2.0000e+00, 2.0000e+00, 3.7000e+01, 3.5650e+03], [ 3.0000e+00, 1.0000e+00, 4.1000e+01, -1.6450e+03], [ 2.0000e+00, 1.0000e+00, 4.6000e+01, 4.7929e+04]]) credit_constant &lt;tf.Tensor: id=0, shape=(30000, 4), dtype=float64, numpy= array([[ 2.0000e+00, 1.0000e+00, 2.4000e+01, 3.9130e+03], [ 2.0000e+00, 2.0000e+00, 2.6000e+01, 2.6820e+03], [ 2.0000e+00, 2.0000e+00, 3.4000e+01, 2.9239e+04], ..., [ 2.0000e+00, 2.0000e+00, 3.7000e+01, 3.5650e+03], [ 3.0000e+00, 1.0000e+00, 4.1000e+01, -1.6450e+03], [ 2.0000e+00, 1.0000e+00, 4.6000e+01, 4.7929e+04]])&gt; . Excellent! You now understand how constants are used in tensorflow . In the following exercise, you’ll practice defining variables. . 1.1.2 Defining variables . Unlike a constant, a variable’s value can be modified. This will be quite useful when we want to train a model by updating its parameters. Constants can’t be used for this purpose, so variables are the natural choice. . Let’s try defining and working with a variable. Note that Variable() , which is used to create a variable tensor, has been imported from tensorflow and is available to use in the exercise. . from tensorflow import Variable # Define the 1-dimensional variable A1 A1 = Variable([1, 2, 3, 4]) # Print the variable A1 print(A1) # Convert A1 to a numpy array and assign it to B1 B1 = A1.numpy() # Print B1 print(B1) . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; [1 2 3 4] . Nice work! In our next exercise, we’ll review how to check the properties of a tensor after it is already defined. . 1.2 Basic operations . | | | | | | | . 1.2.1 Performing element-wise multiplication . Element-wise multiplication in TensorFlow is performed using two tensors with identical shapes. This is because the operation multiplies elements in corresponding positions in the two tensors. An example of an element-wise multiplication, denoted by the ⊙ symbol, is shown below: . . In this exercise, you will perform element-wise multiplication, paying careful attention to the shape of the tensors you multiply. Note that multiply() , constant() , and ones_like() have been imported for you. . # Define tensors A1 and A23 as constants A1 = constant([1, 2, 3, 4]) A23 = constant([[1, 2, 3], [1, 6, 4]]) # Define B1 and B23 to have the correct shape B1 = ones_like(A1) B23 = ones_like(A23) # Perform element-wise multiplication C1 = multiply(A1,B1) C23 = multiply(A23,B23) # Print the tensors C1 and C23 print(&#39;C1: {}&#39;.format(C1.numpy())) print(&#39;C23: {}&#39;.format(C23.numpy())) . C1: [1 2 3 4] C23: [[1 2 3] [1 6 4]] . ones_like(A1) &lt;tf.Tensor: id=12, shape=(4,), dtype=int32, numpy=array([1, 1, 1, 1], dtype=int32)&gt; ones_like(A23) &lt;tf.Tensor: id=15, shape=(2, 3), dtype=int32, numpy= array([[1, 1, 1], [1, 1, 1]], dtype=int32)&gt; . Excellent work! Notice how performing element-wise multiplication with tensors of ones leaves the original tensors unchanged. . 1.2.2 Making predictions with matrix multiplication . In later chapters, you will learn to train linear regression models. This process will yield a vector of parameters that can be multiplied by the input data to generate predictions. In this exercise, you will use input data, features , and a target vector, bill , which are taken from a credit card dataset we will use later in the course. . . The matrix of input data, features , contains two columns: education level and age. The target vector, bill , is the size of the credit card borrower’s bill. . Since we have not trained the model, you will enter a guess for the values of the parameter vector, params . You will then use matmul() to perform matrix multiplication of features by params to generate predictions, billpred , which you will compare with bill . Note that we have imported matmul() and constant() . . # Define features, params, and bill as constants features = constant([[2, 24], [2, 26], [2, 57], [1, 37]]) params = constant([[1000], [150]]) bill = constant([[3913], [2682], [8617], [64400]]) # Compute billpred using features and params billpred = matmul(features,params) # Compute and print the error error = bill - billpred print(error.numpy()) . [[-1687] [-3218] [-1933] [57850]] billpred &lt;tf.Tensor: id=15, shape=(4, 1), dtype=int32, numpy= array([[ 5600], [ 5900], [10550], [ 6550]], dtype=int32)&gt; . Nice job! Understanding matrix multiplication will make things simpler when we start making predictions with linear models. . 1.2.3 Summing over tensor dimensions . You’ve been given a matrix, wealth . This contains the value of bond and stock wealth for five individuals in thousands of dollars. . . The first column corresponds to bonds and the second corresponds to stocks. Each row gives the bond and stock wealth for a single individual. Use wealth , reduce_sum() , and .numpy() to determine which statements are correct about wealth . . reduce_sum(wealth,0).numpy() # array([ 50, 122], dtype=int32) reduce_sum(wealth,1).numpy() # array([61, 9, 64, 3, 35], dtype=int32) reduce_sum(wealth).numpy() # 172 . Combined, the 5 individuals hold $50,000 in bonds. . Excellent work! Understanding how to sum over tensor dimensions will be helpful when preparing datasets and training models. . 1.3 Advanced operations . | | | | | | | | | . 1.3.1 Reshaping tensors . Later in the course, you will classify images of sign language letters using a neural network. In some cases, the network will take 1-dimensional tensors as inputs, but your data will come in the form of images, which will either be either 2- or 3-dimensional tensors, depending on whether they are grayscale or color images. . The figure below shows grayscale and color images of the sign language letter A. The two images have been imported for you and converted to the numpy arrays gray_tensor and color_tensor . Reshape these arrays into 1-dimensional vectors using the reshape operation, which has been imported for you from tensorflow . Note that the shape of gray_tensor is 28×28 and the shape of color_tensor is 28x28x3. . . # Reshape the grayscale image tensor into a vector gray_vector = reshape(gray_tensor, (28*28, 1)) # Reshape the color image tensor into a vector color_vector = reshape(color_tensor, (28*28*3, 1)) . Excellent work! Notice that there are 3 times as many elements in color_vector as there are in gray_vector , since color_tensor has 3 color channels. . 1.3.2 Optimizing with gradients . You are given a loss function, y=x2y=x2, which you want to minimize. You can do this by computing the slope using the GradientTape() operation at different values of x . If the slope is positive, you can decrease the loss by lowering x . If it is negative, you can decrease it by increasing x . This is how gradient descent works. . . In practice, you will use a high level tensorflow operation to perform gradient descent automatically. In this exercise, however, you will compute the slope at x values of -1, 1, and 0. The following operations are available: GradientTape() , multiply() , and Variable() . . def compute_gradient(x0): # Define x as a variable with an initial value of x0 x = Variable(x0) with GradientTape() as tape: tape.watch(x) # Define y using the multiply operation y = multiply(x,x) # Return the gradient of y with respect to x return tape.gradient(y, x).numpy() # Compute and print gradients at x = -1, 1, and 0 print(compute_gradient(-1.0)) # -2.0 print(compute_gradient(1.0)) # 2.0 print(compute_gradient(0.0)) # 0.0 . Excellent work! Notice that the slope is positive at x = 1, which means that we can lower the loss by reducing x . The slope is negative at x = -1, which means that we can lower the loss by increasing x . The slope at x = 0 is 0, which means that we cannot lower the loss by either increasing or decreasing x . This is because the loss is minimized at x = 0. . 1.3.3 Working with image data . You are given a black-and-white image of a letter, which has been encoded as a tensor, letter . You want to determine whether the letter is an X or a K. You don’t have a trained neural network, but you do have a simple model, model , which can be used to classify letter . . The 3×3 tensor, letter , and the 1×3 tensor, model , are available in the Python shell. You can determine whether letter is a K by multiplying letter by model , summing over the result, and then checking if it is equal to 1. As with more complicated models, such as neural networks, model is a collection of weights, arranged in a tensor. . Note that the functions reshape() , matmul() , and reduce_sum() have been imported from tensorflow and are available for use. . letter array([[1., 0., 1.], [1., 1., 0.], [1., 0., 1.]], dtype=float32) . # Reshape model from a 1x3 to a 3x1 tensor model = reshape(model, (3, 1)) # Multiply letter by model output = matmul(letter, model) # Sum over output and print prediction using the numpy method prediction = reduce_sum(output) print(prediction.numpy()) # 1.0 . Excellent work! Your model found that prediction =1.0 and correctly classified the letter as a K. In the coming chapters, you will use data to train a model, model , and then combine this with matrix multiplication, matmul(letter, model) , as we have done here, to make predictions about the classes of objects. . 2. Linear models . 2.1 Input data . | | | | | | | . 2.1.1 Load data using pandas . Before you can train a machine learning model, you must first import data. There are several valid ways to do this, but for now, we will use a simple one-liner from pandas : pd.read_csv() . Recall from the video that the first argument specifies the path or URL. All other arguments are optional. . In this exercise, you will import the King County housing dataset, which we will use to train a linear model later in the chapter. . # Import pandas under the alias pd import pandas as pd # Assign the path to a string variable named data_path data_path = &#39;kc_house_data.csv&#39; # Load the dataset as a dataframe named housing housing = pd.read_csv(data_path) # Print the price column of housing print(housing[&#39;price&#39;]) . Excellent work! Notice that you did not have to specify a delimiter with the sep parameter, since the dataset was stored in the default, comma-separated format. . 2.1.2 Setting the data type . In this exercise, you will both load data and set its type. Note that housing is available and pandas has been imported as pd . You will import numpy and tensorflow , and define tensors that are usable in tensorflow using columns in housing with a given data type. Recall that you can select the price column, for instance, from housing using housing[&#39;price&#39;] . . # Import numpy and tensorflow with their standard aliases import numpy as np import tensorflow as tf # Use a numpy array to define price as a 32-bit float price = np.array(housing[&#39;price&#39;], np.float) # Define waterfront as a Boolean using cast waterfront = tf.cast(housing[&#39;waterfront&#39;], tf.bool) # Print price and waterfront print(price) print(waterfront) . [221900. 538000. 180000. ... 402101. 400000. 325000.] tf.Tensor([False False False ... False False False], shape=(21613,), dtype=bool) . Great job! Notice that printing price yielded a numpy array; whereas printing waterfront yielded a tf.Tensor() . . 2.2 Loss functions . | | | | | | . 2.2.1 Loss functions in TensorFlow . In this exercise, you will compute the loss using data from the King County housing dataset. You are given a target, price , which is a tensor of house prices, and predictions , which is a tensor of predicted house prices. You will evaluate the loss function and print out the value of the loss. . # Import the keras module from tensorflow from tensorflow import keras # Compute the mean squared error (mse) loss = keras.losses.mse(price, predictions) # Print the mean squared error (mse) print(loss.numpy()) # 141171604777.12717 # Compute the mean absolute error (mae) loss = keras.losses.mae(price, predictions) # Print the mean absolute error (mae) print(loss.numpy()) # 268827.99302087986 . Great work! You may have noticed that the MAE was much smaller than the MSE, even though price and predictions were the same. This is because the different loss functions penalize deviations of predictions from price differently. MSE does not like large deviations and punishes them harshly. . 2.2.2 Modifying the loss function . In the previous exercise, you defined a tensorflow loss function and then evaluated it once for a set of actual and predicted values. In this exercise, you will compute the loss within another function called loss_function() , which first generates predicted values from the data and variables. The purpose of this is to construct a function of the trainable model variables that returns the loss. You can then repeatedly evaluate this function for different variable values until you find the minimum. In practice, you will pass this function to an optimizer in tensorflow . Note that features and targets have been defined and are available. Additionally, Variable , float32 , and keras are available. . import tensorflow as tf from tensorflow import Variable from tensorflow import keras # Initialize a variable named scalar scalar = Variable(1.0, tf.float32) # Define the model def model(scalar, features = features): return scalar * features # Define a loss function def loss_function(scalar, features = features, targets = targets): # Compute the predicted values predictions = model(scalar, features) # Return the mean absolute error loss return keras.losses.mae(targets, predictions) # Evaluate the loss function and print the loss print(loss_function(scalar).numpy()) # 3.0 . Great work! As you will see in the following lessons, this exercise was the equivalent of evaluating the loss function for a linear regression where the intercept is 0. . 2.3 Linear regression . | | | | . 2.3.1 Set up a linear regression . A univariate linear regression identifies the relationship between a single feature and the target tensor. In this exercise, we will use a property’s lot size and price. Just as we discussed in the video, we will take the natural logarithms of both tensors, which are available as price_log and size_log . . In this exercise, you will define the model and the loss function. You will then evaluate the loss function for two different values of intercept and slope . Remember that the predicted values are given by intercept + features*slope . Additionally, note that keras.losses.mse() is available for you. Furthermore, slope and intercept have been defined as variables. . # Define a linear regression model def linear_regression(intercept, slope, features = size_log): return intercept + slope*features # Set loss_function() to take the variables as arguments def loss_function(intercept, slope, features = size_log, targets = price_log): # Set the predicted values predictions = linear_regression(intercept, slope, features) # Return the mean squared error loss return keras.losses.mse(targets, predictions) # Compute the loss for different slope and intercept values print(loss_function(0.1, 0.1).numpy()) print(loss_function(0.1, 0.5).numpy()) # 145.44652 # 71.866 . Great work! In the next exercise, you will actually run the regression and train intercept and slope . . 2.3.2 Train a linear model . In this exercise, we will pick up where the previous exercise ended. The intercept and slope, intercept and slope , have been defined and initialized. Additionally, a function has been defined, loss_function(intercept, slope) , which computes the loss using the data and model variables. . You will now define an optimization operation as opt . You will then train a univariate linear model by minimizing the loss to find the optimal values of intercept and slope . Note that the opt operation will try to move closer to the optimum with each step, but will require many steps to find it. Thus, you must repeatedly execute the operation. . # Initialize an adam optimizer opt = keras.optimizers.Adam(0.5) for j in range(100): # Apply minimize, pass the loss function, and supply the variables opt.minimize(lambda: loss_function(intercept, slope), var_list=[intercept, slope]) # Print every 10th value of the loss if j % 10 == 0: print(loss_function(intercept, slope).numpy()) # Plot data and regression line plot_results(intercept, slope) . 9.669481 11.726705 1.1193314 1.6605749 0.7982892 0.8017315 0.6106562 0.59997994 0.5811015 0.5576157 . . Excellent! Notice that we printed loss_function(intercept, slope) every 10th execution for 100 executions. Each time, the loss got closer to the minimum as the optimizer moved the slope and intercept parameters closer to their optimal values. . 2.3.3 Multiple linear regression . In most cases, performing a univariate linear regression will not yield a model that is useful for making accurate predictions. In this exercise, you will perform a multiple regression, which uses more than one feature. . You will use price_log as your target and size_log and bedrooms as your features. Each of these tensors has been defined and is available. You will also switch from using the the mean squared error loss to the mean absolute error loss: keras.losses.mae() . Finally, the predicted values are computed as follows: params[0] + feature1*params[1] + feature2*params[2] . Note that we’ve defined a vector of parameters, params , as a variable, rather than using three variables. Here, params[0] is the intercept and params[1] and params[2] are the slopes. . # Define the linear regression model def linear_regression(params, feature1 = size_log, feature2 = bedrooms): return params[0] + feature1*params[1] + feature2*params[2] # Define the loss function def loss_function(params, targets = price_log, feature1 = size_log, feature2 = bedrooms): # Set the predicted values predictions = linear_regression(params, feature1, feature2) # Use the mean absolute error loss return keras.losses.mae(targets, predictions) # Define the optimize operation opt = keras.optimizers.Adam() # Perform minimization and print trainable variables for j in range(10): opt.minimize(lambda: loss_function(params), var_list=[params]) print_results(params) . loss: 12.418, intercept: 0.101, slope_1: 0.051, slope_2: 0.021 loss: 12.404, intercept: 0.102, slope_1: 0.052, slope_2: 0.022 loss: 12.391, intercept: 0.103, slope_1: 0.053, slope_2: 0.023 loss: 12.377, intercept: 0.104, slope_1: 0.054, slope_2: 0.024 loss: 12.364, intercept: 0.105, slope_1: 0.055, slope_2: 0.025 loss: 12.351, intercept: 0.106, slope_1: 0.056, slope_2: 0.026 loss: 12.337, intercept: 0.107, slope_1: 0.057, slope_2: 0.027 loss: 12.324, intercept: 0.108, slope_1: 0.058, slope_2: 0.028 loss: 12.311, intercept: 0.109, slope_1: 0.059, slope_2: 0.029 loss: 12.297, intercept: 0.110, slope_1: 0.060, slope_2: 0.030 . Great job! Note that params[2] tells us how much the price will increase in percentage terms if we add one more bedroom. You could train params[2] and the other model parameters by increasing the number of times we iterate over opt . . 2.4 Batch training . | | | | | | . 2.4.1 Preparing to batch train . Before we can train a linear model in batches, we must first define variables, a loss function, and an optimization operation. In this exercise, we will prepare to train a model that will predict price_batch , a batch of house prices, using size_batch , a batch of lot sizes in square feet. In contrast to the previous lesson, we will do this by loading batches of data using pandas , converting it to numpy arrays, and then using it to minimize the loss function in steps. . Variable() , keras() , and float32 have been imported for you. Note that you should not set default argument values for either the model or loss function, since we will generate the data in batches during the training process. . # Define the intercept and slope intercept = Variable(10.0, float32) slope = Variable(0.5, float32) # Define the model def linear_regression(intercept, slope, features): # Define the predicted values return intercept + slope*features # Define the loss function def loss_function(intercept, slope, targets, features): # Define the predicted values predictions = linear_regression(intercept, slope, features) # Define the MSE loss return keras.losses.mse(targets, predictions) . Excellent work! Notice that we did not use default argument values for the input data, features and targets . This is because the input data has not been defined in advance. Instead, with batch training, we will load it during the training process. . 2.4.2 Training a linear model in batches . In this exercise, we will train a linear regression model in batches, starting where we left off in the previous exercise. We will do this by stepping through the dataset in batches and updating the model’s variables, intercept and slope , after each step. This approach will allow us to train with datasets that are otherwise too large to hold in memory. . Note that the loss function, loss_function(intercept, slope, targets, features) , has been defined for you. Additionally, keras has been imported for you and numpy is available as np . The trainable variables should be entered into var_list in the order in which they appear as loss function arguments. . # Initialize adam optimizer opt = keras.optimizers.Adam() # Load data in batches for batch in pd.read_csv(&#39;kc_house_data.csv&#39;, chunksize=100): size_batch = np.array(batch[&#39;sqft_lot&#39;], np.float32) # Extract the price values for the current batch price_batch = np.array(batch[&#39;price&#39;], np.float32) # Complete the loss, fill in the variable list, and minimize opt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch), var_list=[intercept, slope]) # Print trained parameters print(intercept.numpy(), slope.numpy()) # 10.217888 0.7016 . Great work! Batch training will be very useful when you train neural networks, which we will do next. . 3. Neural Networks . 3.1 Dense layers . | | | | | | | | . 3.1.1 The linear algebra of dense layers . There are two ways to define a dense layer in tensorflow . The first involves the use of low-level, linear algebraic operations. The second makes use of high-level keras operations. In this exercise, we will use the first method to construct the network shown in the image below. . The input layer contains 3 features — education, marital status, and age — which are available as borrower_features . The hidden layer contains 2 nodes and the output layer contains a single node. . For each layer, you will take the previous layer as an input, initialize a set of weights, compute the product of the inputs and weights, and then apply an activation function. Note that Variable() , ones() , matmul() , and keras() have been imported from tensorflow . . # Initialize bias1 bias1 = Variable(1.0) # Initialize weights1 as 3x2 variable of ones weights1 = Variable(ones((3, 2))) # Perform matrix multiplication of borrower_features and weights1 product1 = matmul(borrower_features,weights1) # Apply sigmoid activation function to product1 + bias1 dense1 = keras.activations.sigmoid(product1 + bias1) # Print shape of dense1 print(&quot; n dense1&#39;s output shape: {}&quot;.format(dense1.shape)) # dense1&#39;s output shape: (1, 2) . # From previous step bias1 = Variable(1.0) weights1 = Variable(ones((3, 2))) product1 = matmul(borrower_features, weights1) dense1 = keras.activations.sigmoid(product1 + bias1) # Initialize bias2 and weights2 bias2 = Variable(1.0) weights2 = Variable(ones((2, 1))) # Perform matrix multiplication of dense1 and weights2 product2 = matmul(dense1, weights2) # Apply activation to product2 + bias2 and print the prediction prediction = keras.activations.sigmoid(product2 + bias2) print(&#39; n prediction: {}&#39;.format(prediction.numpy()[0,0])) print(&#39; n actual: 1&#39;) . prediction: 0.9525741338729858 actual: 1 . Excellent work! Our model produces predicted values in the interval between 0 and 1. For the example we considered, the actual value was 1 and the predicted value was a probability between 0 and 1. This, of course, is not meaningful, since we have not yet trained our model’s parameters. . 3.1.2 The low-level approach with multiple examples . In this exercise, we’ll build further intuition for the low-level approach by constructing the first dense hidden layer for the case where we have multiple examples. We’ll assume the model is trained and the first layer weights, weights1 , and bias, bias1 , are available. We’ll then perform matrix multiplication of the borrower_features tensor by the weights1 variable. Recall that the borrower_features tensor includes education, marital status, and age. Finally, we’ll apply the sigmoid function to the elements of products1 + bias1 , yielding dense1 . . . Note that matmul() and keras() have been imported from tensorflow . . # Compute the product of borrower_features and weights1 products1 = matmul(borrower_features,weights1) # Apply a sigmoid activation function to products1 + bias1 dense1 = keras.activations.sigmoid(products1+bias1) # Print the shapes of borrower_features, weights1, bias1, and dense1 print(&#39; n shape of borrower_features: &#39;, borrower_features.shape) # shape of borrower_features: (5, 3) print(&#39; n shape of weights1: &#39;, weights1.shape) # shape of weights1: (3, 2) print(&#39; n shape of bias1: &#39;, bias1.shape) # shape of bias1: (1,) print(&#39; n shape of dense1: &#39;, dense1.shape) # shape of dense1: (5, 2) . Good job! Note that our input data, borrower_features , is 5×3 because it consists of 5 examples for 3 features. The shape of weights1 is 3×2, as it was in the previous exercise, since it does not depend on the number of examples. Additionally, bias1 is a scalar. Finally, dense1 is 5×2, which means that we can multiply it by the following set of weights, weights2 , which we defined to be 2×1 in the previous exercise. . 3.1.3 Using the dense layer operation . We’ve now seen how to define dense layers in tensorflow using linear algebra. In this exercise, we’ll skip the linear algebra and let keras work out the details. This will allow us to construct the network below, which has 2 hidden layers and 10 features, using less code than we needed for the network with 1 hidden layer and 3 features. . . To construct this network, we’ll need to define three dense layers, each of which takes the previous layer as an input, multiplies it by weights, and applies an activation function. Note that input data has been defined and is available as a 100×10 tensor: borrower_features . Additionally, the keras.layers module is available. . # Define the first dense layer dense1 = keras.layers.Dense(7, activation=&#39;sigmoid&#39;)(borrower_features) # Define a dense layer with 3 output nodes dense2 = keras.layers.Dense(3, activation=&#39;sigmoid&#39;)(dense1) # Define a dense layer with 1 output node predictions = keras.layers.Dense(1, activation=&#39;sigmoid&#39;)(dense2) # Print the shapes of dense1, dense2, and predictions print(&#39; n shape of dense1: &#39;, dense1.shape) # shape of dense1: (100, 7) print(&#39; n shape of dense2: &#39;, dense2.shape) # shape of dense2: (100, 3) print(&#39; n shape of predictions: &#39;, predictions.shape) # shape of predictions: (100, 1) . Great work! With just 8 lines of code, you were able to define 2 dense hidden layers and an output layer. This is the advantage of using high-level operations in tensorflow . Note that each layer has 100 rows because the input data contains 100 examples. . 3.2 Activation functions . | | | | | | | | | | . 3.2.1 Binary classification problems . In this exercise, you will again make use of credit card data. The target variable, default , indicates whether a credit card holder defaults on her payment in the following period. Since there are only two options–default or not–this is a binary classification problem. While the dataset has many features, you will focus on just three: the size of the three latest credit card bills. Finally, you will compute predictions from your untrained network, outputs , and compare those the target variable, default . . The tensor of features has been loaded and is available as bill_amounts . Additionally, the constant() , float32 , and keras.layers.Dense() operations are available. . # Construct input layer from features inputs = constant(bill_amounts) # Define first dense layer dense1 = keras.layers.Dense(3, activation=&#39;relu&#39;)(inputs) # Define second dense layer dense2 = keras.layers.Dense(2, activation=&#39;relu&#39;)(dense1) # Define output layer outputs = keras.layers.Dense(1, activation=&#39;sigmoid&#39;)(dense2) # Print error for first five examples error = default[:5] - outputs.numpy()[:5] print(error) . [[ 0.0000000e+00] [ 3.4570694e-05] [-1.0000000e+00] [-1.0000000e+00] [-1.0000000e+00]] . Excellent work! If you run the code several times, you’ll notice that the errors change each time. This is because you’re using an untrained model with randomly initialized parameters. Furthermore, the errors fall on the interval between -1 and 1 because default is a binary variable that takes on values of 0 and 1 and outputs is a probability between 0 and 1. . 3.2.2 Multiclass classification problems . In this exercise, we expand beyond binary classification to cover multiclass problems. A multiclass problem has targets that can take on three or more values. In the credit card dataset, the education variable can take on 6 different values, each corresponding to a different level of education. We will use that as our target in this exercise and will also expand the feature set from 3 to 10 columns. . As in the previous problem, you will define an input layer, dense layers, and an output layer. You will also print the untrained model’s predictions, which are probabilities assigned to the classes. The tensor of features has been loaded and is available as borrower_features . Additionally, the constant() , float32 , and keras.layers.Dense() operations are available. . import tensorflow as tf # Construct input layer from borrower features inputs = constant(borrower_features,tf.float32) # Define first dense layer dense1 = keras.layers.Dense(10, activation=&#39;sigmoid&#39;)(inputs) # Define second dense layer dense2 = keras.layers.Dense(8, activation=&#39;relu&#39;)(dense1) # Define output layer outputs = keras.layers.Dense(6, activation=&#39;softmax&#39;)(dense2) # Print first five predictions print(outputs.numpy()[:5]) . [[0.17133032 0.16293828 0.14702542 0.17789574 0.16075517 0.18005505] [0.15597914 0.17065835 0.1275746 0.2044413 0.16524555 0.17610106] [0.15597914 0.17065835 0.1275746 0.2044413 0.16524555 0.17610106] [0.17133032 0.16293828 0.14702542 0.17789574 0.16075517 0.18005505] [0.07605464 0.17264706 0.15399623 0.2247733 0.1516134 0.22091544]] . Great work! Notice that each row of outputs sums to one. This is because a row contains the predicted class probabilities for one example. As with the previous exercise, our predictions are not yet informative, since we are using an untrained model with randomly initialized parameters. This is why the model tends to assign similar probabilities to each class. . 3.3 Optimizers . | | | | | . 3.3.1 The dangers of local minima . Consider the plot of the following loss function, loss_function() , which contains a global minimum, marked by the dot on the right, and several local minima, including the one marked by the dot on the left. . . In this exercise, you will try to find the global minimum of loss_function() using keras.optimizers.SGD() . You will do this twice, each time with a different initial value of the input to loss_function() . First, you will use x_1 , which is a variable with an initial value of 6.0. Second, you will use x_2 , which is a variable with an initial value of 0.3. Note that loss_function() has been defined and is available. . # Initialize x_1 and x_2 x_1 = Variable(6.0,float32) x_2 = Variable(0.3,float32) # Define the optimization operation opt = keras.optimizers.SGD(learning_rate=0.01) for j in range(100): # Perform minimization using the loss function and x_1 opt.minimize(lambda: loss_function(x_1), var_list=[x_1]) # Perform minimization using the loss function and x_2 opt.minimize(lambda: loss_function(x_2), var_list=[x_2]) # Print x_1 and x_2 as numpy arrays print(x_1.numpy(), x_2.numpy()) # 4.3801394 0.42052683 . Great work! Notice that we used the same optimizer and loss function, but two different initial values. When we started at 6.0 with x_1 , we found the global minimum at 4.38, marked by the dot on the right. When we started at 0.3, we stopped around 0.42 with x_2 , the local minimum marked by a dot on the far left. . 3.3.2 Avoiding local minima . The previous problem showed how easy it is to get stuck in local minima. We had a simple optimization problem in one variable and gradient descent still failed to deliver the global minimum when we had to travel through local minima first. One way to avoid this problem is to use momentum, which allows the optimizer to break through local minima. We will again use the loss function from the previous problem, which has been defined and is available for you as loss_function() . . . Several optimizers in tensorflow have a momentum parameter, including SGD and RMSprop . You will make use of RMSprop in this exercise. Note that x_1 and x_2 have been initialized to the same value this time. Furthermore, keras.optimizers.RMSprop() has also been imported for you from tensorflow . . # Initialize x_1 and x_2 x_1 = Variable(0.05,float32) x_2 = Variable(0.05,float32) # Define the optimization operation for opt_1 and opt_2 opt_1 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.99) opt_2 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.00) for j in range(100): opt_1.minimize(lambda: loss_function(x_1), var_list=[x_1]) # Define the minimization operation for opt_2 opt_2.minimize(lambda: loss_function(x_2), var_list=[x_2]) # Print x_1 and x_2 as numpy arrays print(x_1.numpy(), x_2.numpy()) # 4.3150263 0.4205261 . Good work! Recall that the global minimum is approximately 4.38. Notice that opt_1 built momentum, bringing x_1 closer to the global minimum. To the contrary, opt_2 , which had a momentum parameter of 0.0, got stuck in the local minimum on the left. . 3.4 Training a network in TensorFlow . | | | | | | | | . 3.4.1 Initialization in TensorFlow . A good initialization can reduce the amount of time needed to find the global minimum. In this exercise, we will initialize weights and biases for a neural network that will be used to predict credit card default decisions. To build intuition, we will use the low-level, linear algebraic approach, rather than making use of convenience functions and high-level keras operations. We will also expand the set of input features from 3 to 23. Several operations have been imported from tensorflow : Variable() , random() , and ones() . . # Define the layer 1 weights w1 = Variable(random.normal([23, 7])) # Initialize the layer 1 bias b1 = Variable(ones([7])) # Define the layer 2 weights w2 = Variable(random.normal([7, 1])) # Define the layer 2 bias b2 = Variable((0)) . Variable(random.normal([7, 1])) &lt;tf.Variable &#39;Variable:0&#39; shape=(7, 1) dtype=float32, numpy= array([[ 0.654808 ], [ 0.05108023], [-0.4015795 ], [ 0.17105988], [-0.71988714], [ 1.8440487 ], [-0.0194056 ]], dtype=float32)&gt; . Excellent work! In the next exercise, you will start where we’ve ended and will finish constructing the neural network. . 3.4.2 Defining the model and loss function . In this exercise, you will train a neural network to predict whether a credit card holder will default. The features and targets you will use to train your network are available in the Python shell as borrower_features and default . You defined the weights and biases in the previous exercise. . Note that the predictions layer is defined as σ(layer1∗w2+b2)σ(layer1∗w2+b2), where σσ is the sigmoid activation, layer1 is a tensor of nodes for the first hidden dense layer, w2 is a tensor of weights, and b2 is the bias tensor. . The trainable variables are w1 , b1 , w2 , and b2 . Additionally, the following operations have been imported for you: keras.activations.relu() and keras.layers.Dropout() . . # Define the model def model(w1, b1, w2, b2, features = borrower_features): # Apply relu activation functions to layer 1 layer1 = keras.activations.relu(matmul(features, w1) + b1) # Apply dropout dropout = keras.layers.Dropout(0.25)(layer1) return keras.activations.sigmoid(matmul(dropout, w2) + b2) # Define the loss function def loss_function(w1, b1, w2, b2, features = borrower_features, targets = default): predictions = model(w1, b1, w2, b2) # Pass targets and predictions to the cross entropy loss return keras.losses.binary_crossentropy(targets, predictions) . Nice work! One of the benefits of using tensorflow is that you have the option to customize models down to the linear algebraic-level, as we’ve shown in the last two exercises. If you print w1 , you can see that the objects we’re working with are simply tensors. . 3.4.3 Training neural networks with TensorFlow . In the previous exercise, you defined a model, model(w1, b1, w2, b2, features) , and a loss function, loss_function(w1, b1, w2, b2, features, targets) , both of which are available to you in this exercise. You will now train the model and then evaluate its performance by predicting default outcomes in a test set, which consists of test_features and test_targets and is available to you. The trainable variables are w1 , b1 , w2 , and b2 . Additionally, the following operations have been imported for you: keras.activations.relu() and keras.layers.Dropout() . . # Train the model for j in range(100): # Complete the optimizer opt.minimize(lambda: loss_function(w1, b1, w2, b2), var_list=[w1, b1, w2, b2]) # Make predictions with model model_predictions = model(w1, b1, w2, b2, test_features) # Construct the confusion matrix confusion_matrix(test_targets, model_predictions) . . Nice work! The diagram shown is called a “confusion matrix.” The diagonal elements show the number of correct predictions. The off-diagonal elements show the number of incorrect predictions. We can see that the model performs reasonably-well, but does so by overpredicting non-default. This suggests that we may need to train longer, tune the model’s hyperparameters, or change the model’s architecture. . 4. High Level APIs . 4.1 Defining neural networks with Keras . | | | | | | . 4.1.1 The sequential model in Keras . In chapter 3, we used components of the keras API in tensorflow to define a neural network, but we stopped short of using its full capabilities to streamline model definition and training. In this exercise, you will use the keras sequential model API to define a neural network that can be used to classify images of sign language letters. You will also use the .summary() method to print the model’s architecture, including the shape and number of parameters associated with each layer. . Note that the images were reshaped from (28, 28) to (784,), so that they could be used as inputs to a dense layer. Additionally, note that keras has been imported from tensorflow for you. . # Define a Keras sequential model model = keras.Sequential() # Define the first dense layer model.add(keras.layers.Dense(16, activation=&#39;relu&#39;, input_shape=(784,))) # Define the second dense layer model.add(keras.layers.Dense(8, activation=&#39;relu&#39;)) # Define the output layer model.add(keras.layers.Dense(4, activation=&#39;softmax&#39;)) # Print the model architecture print(model.summary()) . Model: &quot;sequential&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 16) 12560 _________________________________________________________________ dense_1 (Dense) (None, 8) 136 _________________________________________________________________ dense_2 (Dense) (None, 4) 36 ================================================================= Total params: 12,732 Trainable params: 12,732 Non-trainable params: 0 _________________________________________________________________ None . Excellent work! Notice that we’ve defined a model, but we haven’t compiled it. The compilation step in keras allows us to set the optimizer, loss function, and other useful training parameters in a single line of code. Furthermore, the .summary() method allows us to view the model’s architecture. . 4.1.2 Compiling a sequential model . In this exercise, you will work towards classifying letters from the Sign Language MNIST dataset; however, you will adopt a different network architecture than what you used in the previous exercise. There will be fewer layers, but more nodes. You will also apply dropout to prevent overfitting. Finally, you will compile the model to use the adam optimizer and the categorical_crossentropy loss. You will also use a method in keras to summarize your model’s architecture. Note that keras has been imported from tensorflow for you and a sequential keras model has been defined as model . . # Define the first dense layer model.add(keras.layers.Dense(16, activation=&#39;sigmoid&#39;, input_shape=(784,))) # Apply dropout to the first layer&#39;s output model.add(keras.layers.Dropout(0.25)) # Define the output layer model.add(keras.layers.Dense(4, activation=&#39;softmax&#39;)) # Compile the model model.compile(&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;) # Print a model summary print(model.summary()) . Model: &quot;sequential&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 16) 12560 _________________________________________________________________ dense_1 (Dense) (None, 16) 272 _________________________________________________________________ dropout (Dropout) (None, 16) 0 _________________________________________________________________ dense_2 (Dense) (None, 4) 68 ================================================================= Total params: 12,900 Trainable params: 12,900 Non-trainable params: 0 _________________________________________________________________ None . Great work! You’ve now defined and compiled a neural network using the keras sequential model. Notice that printing the .summary() method shows the layer type, output shape, and number of parameters of each layer. . 4.1.3 Defining a multiple input model . In some cases, the sequential API will not be sufficiently flexible to accommodate your desired model architecture and you will need to use the functional API instead. If, for instance, you want to train two models with different architectures jointly, you will need to use the functional API to do this. In this exercise, we will see how to do this. We will also use the .summary() method to examine the joint model’s architecture. . Note that keras has been imported from tensorflow for you. Additionally, the input layers of the first and second models have been defined as m1_inputs and m2_inputs , respectively. Note that the two models have the same architecture, but one of them uses a sigmoid activation in the first layer and the other uses a relu . . # For model 1, pass the input layer to layer 1 and layer 1 to layer 2 m1_layer1 = keras.layers.Dense(12, activation=&#39;sigmoid&#39;)(m1_inputs) m1_layer2 = keras.layers.Dense(4, activation=&#39;softmax&#39;)(m1_layer1) # For model 2, pass the input layer to layer 1 and layer 1 to layer 2 m2_layer1 = keras.layers.Dense(12, activation=&#39;relu&#39;)(m2_inputs) m2_layer2 = keras.layers.Dense(4, activation=&#39;softmax&#39;)(m2_layer1) # Merge model outputs and define a functional model merged = keras.layers.add([m1_layer2, m2_layer2]) model = keras.Model(inputs=[m1_inputs, m2_inputs], outputs=merged) # Print a model summary print(model.summary()) . Model: &quot;model&quot; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 784)] 0 __________________________________________________________________________________________________ input_2 (InputLayer) [(None, 784)] 0 __________________________________________________________________________________________________ dense (Dense) (None, 12) 9420 input_1[0][0] __________________________________________________________________________________________________ dense_2 (Dense) (None, 12) 9420 input_2[0][0] __________________________________________________________________________________________________ dense_1 (Dense) (None, 4) 52 dense[0][0] __________________________________________________________________________________________________ dense_3 (Dense) (None, 4) 52 dense_2[0][0] __________________________________________________________________________________________________ add (Add) (None, 4) 0 dense_1[0][0] dense_3[0][0] ================================================================================================== Total params: 18,944 Trainable params: 18,944 Non-trainable params: 0 __________________________________________________________________________________________________ None . Nice work! Notice that the .summary() method yields a new column: connected to . This column tells you how layers connect to each other within the network. We can see that dense_2 , for instance, is connected to the input_2 layer. We can also see that the add layer, which merged the two models, connected to both dense_1 and dense_3 . . 4.2 Training and validation with Keras . | | | | | | | | | | | . 4.2.1 Training with Keras . In this exercise, we return to our sign language letter classification problem. We have 2000 images of four letters–A, B, C, and D–and we want to classify them with a high level of accuracy. We will complete all parts of the problem, including the model definition, compilation, and training. . Note that keras has been imported from tensorflow for you. Additionally, the features are available as sign_language_features and the targets are available as sign_language_labels . . # Define a sequential model model = keras.Sequential() # Define a hidden layer model.add(keras.layers.Dense(16, activation=&#39;relu&#39;, input_shape=(784,))) # Define the output layer model.add(keras.layers.Dense(4, activation=&#39;softmax&#39;)) # Compile the model model.compile(&#39;SGD&#39;, loss=&#39;categorical_crossentropy&#39;) # Complete the fitting operation model.fit(sign_language_features, sign_language_labels, epochs=5) . Train on 1999 samples Epoch 1/5 32/1999 [..............................] - ETA: 29s - loss: 1.6657 ... Epoch 5/5 ... 1999/1999 [==============================] - 0s 92us/sample - loss: 0.4493 . Great work! You probably noticed that your only measure of performance improvement was the value of the loss function in the training sample, which is not particularly informative. You will improve on this in the next exercise. . 4.2.2 Metrics and validation with Keras . We trained a model to predict sign language letters in the previous exercise, but it is unclear how successful we were in doing so. In this exercise, we will try to improve upon the interpretability of our results. Since we did not use a validation split, we only observed performance improvements within the training set; however, it is unclear how much of that was due to overfitting. Furthermore, since we did not supply a metric, we only saw decreases in the loss function, which do not have any clear interpretation. . Note that keras has been imported for you from tensorflow . . # Define sequential model model = keras.Sequential() # Define the first layer model.add(keras.layers.Dense(32, activation=&#39;sigmoid&#39;, input_shape=(784,))) # Add activation function to classifier model.add(keras.layers.Dense(4, activation=&#39;softmax&#39;)) # Set the optimizer, loss function, and metrics model.compile(optimizer=&#39;RMSprop&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) # Add the number of epochs and the validation split model.fit(sign_language_features, sign_language_labels, epochs=10, validation_split=0.1) . Train on 1799 samples, validate on 200 samples Epoch 1/10 32/1799 [..............................] - ETA: 43s - loss: 1.6457 - accuracy: 0.2500 ... Epoch 10/10 ... 1799/1799 [==============================] - 0s 119us/sample - loss: 0.1381 - accuracy: 0.9772 - val_loss: 0.1356 - val_accuracy: 0.9700 . Nice work! With the keras API, you only needed 14 lines of code to define, compile, train, and validate a model. You may have noticed that your model performed quite well. In just 10 epochs, we achieved a classification accuracy of around 98% in the validation sample! . 4.2.3 Overfitting detection . In this exercise, we’ll work with a small subset of the examples from the original sign language letters dataset. A small sample, coupled with a heavily-parameterized model, will generally lead to overfitting. This means that your model will simply memorize the class of each example, rather than identifying features that generalize to many examples. . You will detect overfitting by checking whether the validation sample loss is substantially higher than the training sample loss and whether it increases with further training. With a small sample and a high learning rate, the model will struggle to converge on an optimum. You will set a low learning rate for the optimizer, which will make it easier to identify overfitting. . Note that keras has been imported from tensorflow . . # Define sequential model model = keras.Sequential() # Define the first layer model.add(keras.layers.Dense(1024, activation=&#39;relu&#39;, input_shape=(784,))) # Add activation function to classifier model.add(keras.layers.Dense(4, activation=&#39;softmax&#39;)) # Finish the model compilation model.compile(optimizer=keras.optimizers.Adam(lr=0.01), loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) # Complete the model fit operation model.fit(sign_language_features, sign_language_labels, epochs=200, validation_split=0.5) . Train on 25 samples, validate on 25 samples Epoch 1/200 25/25 [==============================] - 1s 37ms/sample - loss: 1.5469 - accuracy: 0.2000 - val_loss: 48.8668 - val_accuracy: 0.2400 ... Epoch 200/200 25/25 [==============================] - 0s 669us/sample - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.5236 - val_accuracy: 0.8400 . Excellent work! You may have noticed that the validation loss, val_loss , was substantially higher than the training loss, loss . Furthermore, if val_loss started to increase before the training process was terminated, then we may have overfitted. When this happens, you will want to try decreasing the number of epochs. . 4.2.4 Evaluating models . Two models have been trained and are available: large_model , which has many parameters; and small_model , which has fewer parameters. Both models have been trained using train_features and train_labels , which are available to you. A separate test set, which consists of test_features and test_labels , is also available. . Your goal is to evaluate relative model performance and also determine whether either model exhibits signs of overfitting. You will do this by evaluating large_model and small_model on both the train and test sets. For each model, you can do this by applying the .evaluate(x, y) method to compute the loss for features x and labels y . You will then compare the four losses generated. . # Evaluate the small model using the train data small_train = small_model.evaluate(train_features, train_labels) # Evaluate the small model using the test data small_test = small_model.evaluate(test_features, test_labels) # Evaluate the large model using the train data large_train = large_model.evaluate(train_features, train_labels) # Evaluate the large model using the test data large_test = large_model.evaluate(test_features, test_labels) # Print losses print(&#39; n Small - Train: {}, Test: {}&#39;.format(small_train, small_test)) print(&#39;Large - Train: {}, Test: {}&#39;.format(large_train, large_test)) . Small - Train: 0.7137059640884399, Test: 0.8472499084472657 Large - Train: 0.036491363495588305, Test: 0.1792870020866394 . Great job! Notice that the gap between the test and train set losses is substantially higher for large_model , suggesting that overfitting may be an issue. Furthermore, both test and train set performance is better for large_model . This suggests that we may want to use large_model , but reduce the number of training epochs. . 4.3 Training models with the Estimators API . | | | | | | | . 4.3.1 Preparing to train with Estimators . For this exercise, we’ll return to the King County housing transaction dataset from chapter 2. We will again develop and train a machine learning model to predict house prices; however, this time, we’ll do it using the estimator API. . Rather than completing everything in one step, we’ll break this procedure down into parts. We’ll begin by defining the feature columns and loading the data. In the next exercise, we’ll define and train a premade estimator . Note that feature_column has been imported for you from tensorflow . Additionally, numpy has been imported as np , and the Kings County housing dataset is available as a pandas DataFrame : housing . . housing.columns Index([&#39;id&#39;, &#39;date&#39;, &#39;price&#39;, &#39;bedrooms&#39;, &#39;bathrooms&#39;, &#39;sqft_living&#39;, &#39;sqft_lot&#39;, &#39;floors&#39;, &#39;waterfront&#39;, &#39;view&#39;, &#39;condition&#39;, &#39;grade&#39;, &#39;sqft_above&#39;, &#39;sqft_basement&#39;, &#39;yr_built&#39;, &#39;yr_renovated&#39;, &#39;zipcode&#39;, &#39;lat&#39;, &#39;long&#39;, &#39;sqft_living15&#39;, &#39;sqft_lot15&#39;], dtype=&#39;object&#39;) housing.shape (21613, 21) . # Define feature columns for bedrooms and bathrooms bedrooms = feature_column.numeric_column(&quot;bedrooms&quot;) bathrooms = feature_column.numeric_column(&quot;bathrooms&quot;) # Define the list of feature columns feature_list = [bedrooms, bathrooms] def input_fn(): # Define the labels labels = np.array(housing[&#39;price&#39;]) # Define the features features = {&#39;bedrooms&#39;:np.array(housing[&#39;bedrooms&#39;]), &#39;bathrooms&#39;:np.array(housing[&#39;bathrooms&#39;])} return features, labels . Excellent work! In the next exercise, we’ll use the feature columns and data input function to define and train an estimator. . 4.3.2 Defining Estimators . In the previous exercise, you defined a list of feature columns, feature_list , and a data input function, input_fn() . In this exercise, you will build on that work by defining an estimator that makes use of input data. . # Define the model and set the number of steps model = estimator.DNNRegressor(feature_columns=feature_list, hidden_units=[2,2]) model.train(input_fn, steps=1) . INFO:tensorflow:Using default config. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpwdsztbla INFO:tensorflow:Using config: {&#39;_model_dir&#39;: &#39;/tmp/tmpwdsztbla&#39;, &#39;_tf_random_seed&#39;: None, &#39;_save_summary_steps&#39;: 100, &#39;_save_checkpoints_steps&#39;: None, &#39;_save_checkpoints_secs&#39;: 600, &#39;_session_config&#39;: allow_soft_placement: true graph_options { rewrite_options { meta_optimizer_iterations: ONE } } , &#39;_keep_checkpoint_max&#39;: 5, &#39;_keep_checkpoint_every_n_hours&#39;: 10000, &#39;_log_step_count_steps&#39;: 100, &#39;_train_distribute&#39;: None, &#39;_device_fn&#39;: None, &#39;_protocol&#39;: None, &#39;_eval_distribute&#39;: None, &#39;_experimental_distribute&#39;: None, &#39;_experimental_max_worker_delay_secs&#39;: None, &#39;_session_creation_timeout_secs&#39;: 7200, &#39;_service&#39;: None, &#39;_cluster_spec&#39;: ClusterSpec({}), &#39;_task_type&#39;: &#39;worker&#39;, &#39;_task_id&#39;: 0, &#39;_global_id_in_cluster&#39;: 0, &#39;_master&#39;: &#39;&#39;, &#39;_evaluation_master&#39;: &#39;&#39;, &#39;_is_chief&#39;: True, &#39;_num_ps_replicas&#39;: 0, &#39;_num_worker_replicas&#39;: 1} WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating: If using Keras pass *_constraint arguments to layers. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version. Instructions for updating: Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts. INFO:tensorflow:Calling model_fn. WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer&#39;s dtype of float32, which is new behavior in TensorFlow 2. The layer has dtype float32 because it&#39;s dtype defaults to floatx. If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2. To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx(&#39;float64&#39;)`. To change just this layer, pass dtype=&#39;float64&#39; to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/adagrad.py:103: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version. Instructions for updating: Call initializer instance with the dtype argument instead of passing it to the constructor INFO:tensorflow:Done calling model_fn. INFO:tensorflow:Create CheckpointSaverHook. INFO:tensorflow:Graph was finalized. INFO:tensorflow:Running local_init_op. INFO:tensorflow:Done running local_init_op. INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpwdsztbla/model.ckpt. INFO:tensorflow:loss = 426469720000.0, step = 0 INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpwdsztbla/model.ckpt. INFO:tensorflow:Loss for final step: 426469720000.0. . # Define the model and set the number of steps model = estimator.LinearRegressor(feature_columns=feature_list) model.train(input_fn, steps=2) . Great work! Note that you have other premade estimator options, such as BoostedTreesRegressor() , and can also create your own custom estimators. . #### . Congratulations! . | | | | . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/2021/07/05/introduction-to-tensorflow-in-python.html",
            "relUrl": "/2021/07/05/introduction-to-tensorflow-in-python.html",
            "date": " • Jul 5, 2021"
        }
        
    
  
    
        ,"post43": {
            "title": "Introduction To Natural Language Processing In Python",
            "content": "Introduction to Natural Language Processing in Python . This is the memo of the 12th course (23 courses in all) of ‘Machine Learning Scientist with Python’ skill track. You can find the original course HERE. . reference: Natural Language Toolkit . Course Description . In this course, you’ll learn natural language processing (NLP) basics, such as how to identify and separate words, how to extract topics in a text, and how to build your own fake news classifier. You’ll also learn how to use basic libraries such as NLTK, alongside libraries which utilize deep learning to solve common NLP problems. This course will give you the foundation to process and parse text as you move forward in your Python learning. . . Regular expressions &amp; word tokenization | Simple topic identification | Named-entity recognition | Building a “fake news” classifier | . 1. Regular expressions &amp; word tokenization . 1.1 Introduction to regular expressions . | | | | . 1.1.1 Which pattern? . Which of the following Regex patterns results in the following text? . &gt;&gt;&gt; my_string = &quot;Let&#39;s write RegEx!&quot; re.findall(PATTERN, my_string)[&#39;Let&#39;, &#39;s&#39;, &#39;write&#39;, &#39;RegEx&#39;] . In the IPython Shell, try replacing PATTERN with one of the below options and observe the resulting output. The re module has been pre-imported for you and my_string is available in your namespace. . PATTERN = r&quot; w+&quot; In [2]: re.findall(PATTERN, my_string) Out[2]: [&#39;Let&#39;,&#39;s&#39;,&#39;write&#39;,&#39;RegEx&#39;] . 1.1.2 Practicing regular expressions: re.split() and re.findall() . Now you’ll get a chance to write some regular expressions to match digits, strings and non-alphanumeric characters. Take a look at my_string first by printing it in the IPython Shell, to determine how you might best match the different steps. . Note: It’s important to prefix your regex patterns with r to ensure that your patterns are interpreted in the way you want them to. Else, you may encounter problems to do with escape sequences in strings. For example, &quot; n&quot; in Python is used to indicate a new line, but if you use the r prefix, it will be interpreted as the raw string &quot; n&quot; – that is, the character &quot; &quot; followed by the character &quot;n&quot; – and not as a new line. . The regular expression module re has already been imported for you. . Remember from the video that the syntax for the regex library is to always to pass the pattern first, and then the string second. . my_string &quot;Let&#39;s write RegEx! Won&#39;t that be fun? I sure think so. Can you find 4 sentences? Or perhaps, all 19 words?&quot; . # Write a pattern to match sentence endings: sentence_endings sentence_endings = r&quot;[.?!]&quot; # Split my_string on sentence endings and print the result print(re.split(sentence_endings, my_string)) # [&quot;Let&#39;s write RegEx&quot;, &quot; Won&#39;t that be fun&quot;, &#39; I sure think so&#39;, &#39; Can you find 4 sentences&#39;, &#39; Or perhaps, all 19 words&#39;, &#39;&#39;] # Find all capitalized words in my_string and print the result capitalized_words = r&quot;[A-Z] w+&quot; print(re.findall(capitalized_words, my_string)) # [&#39;Let&#39;, &#39;RegEx&#39;, &#39;Won&#39;, &#39;Can&#39;, &#39;Or&#39;] # Split my_string on spaces and print the result spaces = r&quot; s+&quot; print(re.split(spaces, my_string)) # [&quot;Let&#39;s&quot;, &#39;write&#39;, &#39;RegEx!&#39;, &quot;Won&#39;t&quot;, &#39;that&#39;, &#39;be&#39;, &#39;fun?&#39;, &#39;I&#39;, &#39;sure&#39;, &#39;think&#39;, &#39;so.&#39;, &#39;Can&#39;, &#39;you&#39;, &#39;find&#39;, &#39;4&#39;, &#39;sentences?&#39;, &#39;Or&#39;, &#39;perhaps,&#39;, &#39;all&#39;, &#39;19&#39;, &#39;words?&#39;] # Find all digits in my_string and print the result digits = r&quot; d+&quot; print(re.findall(digits, my_string)) # [&#39;4&#39;, &#39;19&#39;] . 1.2 Introduction to tokenization** . | | | | | . 1.2.1 Word tokenization with NLTK . Here, you’ll be using the first scene of Monty Python’s Holy Grail, which has been pre-loaded as scene_one. Feel free to check it out in the IPython Shell! . Your job in this exercise is to utilize word_tokenize and sent_tokenize from nltk.tokenize to tokenize both words and sentences from Python strings – in this case, the first scene of Monty Python’s Holy Grail. . scene_one &quot;SCENE 1: [wind] [clop clop clop] nKING ARTHUR: Whoa there! [clop clop clop] nSOLDIER #1: Halt! Who goes there? nARTHUR: It is I, Arthur, son of ... creeper! nSOLDIER #1: What, held under the dorsal guiding feathers? nSOLDIER #2: Well, why not? n&quot; . # Import necessary modules from nltk.tokenize import sent_tokenize from nltk.tokenize import word_tokenize # Split scene_one into sentences: sentences sentences = sent_tokenize(scene_one) # Use word_tokenize to tokenize the fourth sentence: tokenized_sent tokenized_sent = word_tokenize(sentences[3]) # Make a set of unique tokens in the entire scene: unique_tokens unique_tokens = set(word_tokenize(scene_one)) # Print the unique tokens result print(unique_tokens) . {&#39;ridden&#39;, &#39;bird&#39;, &#39;King&#39;, &#39;winter&#39;, &#39;right&#39;, &#39;under&#39;, &#39;needs&#39;, &#39;them&#39;, &#39;with&#39;, &#39;use&#39;, &#39;Mercea&#39;, &#39;simple&#39;, &#39;No&#39;, &#39;!&#39;, &#39;Ridden&#39;, &#39;Pendragon&#39;, ... &#39;minute&#39;, &#39;Whoa&#39;, &#39;...&#39;, &quot;&#39;m&quot;, &#39;[&#39;, &#39;#&#39;, &#39;will&#39;, &quot;&#39;ve&quot;, &#39;an&#39;, &#39;In&#39;, &#39;interested&#39;, &#39;England&#39;, &quot;&#39;re&quot;} . Excellent! Tokenization is fundamental to NLP, and you’ll end up using it a lot in text mining and information retrieval projects. . 1.2.2 More regex with re.search() . In this exercise, you’ll utilize re.search() and re.match() to find specific tokens. Both search and match expect regex patterns, similar to those you defined in an earlier exercise. You’ll apply these regex library methods to the same Monty Python text from the nltk corpora. . You have both scene_one and sentences available from the last exercise; now you can use them with re.search() and re.match() to extract and match more text. . # Search for the first occurrence of &quot;coconuts&quot; in scene_one: match match = re.search(&quot;coconuts&quot;, scene_one) # Print the start and end indexes of match print(match.start(), match.end()) # 580 588 # Write a regular expression to search for anything in square brackets: pattern1 pattern1 = r&quot; [.*]&quot; # Use re.search to find the first text in square brackets print(re.search(pattern1, scene_one)) # &lt;_sre.SRE_Match object; span=(9, 32), match=&#39;[wind] [clop clop clop]&#39;&gt; # Find the script notation at the beginning of the fourth sentence and print it pattern2 = r&quot;[ w s]+:&quot; print(re.match(pattern2, sentences[3])) # &lt;_sre.SRE_Match object; span=(0, 7), match=&#39;ARTHUR:&#39;&gt; . Fantastic work! Now that you’re familiar with the basics of tokenization and regular expressions, it’s time to learn about more advanced tokenization. . 1.3 Advanced tokenization with NLTK and regex . | | | . 1.3.1 Choosing a tokenizer . Given the following string, which of the below patterns is the best tokenizer? If possible, you want to retain sentence punctuation as separate tokens, but have &#39;#1&#39; remain a single token. . my_string = &quot;SOLDIER #1: Found them? In Mercea? The coconut&#39;s tropical!&quot; . The string is available in your workspace as my_string, and the patterns have been pre-loaded as pattern1, pattern2, pattern3, and pattern4, respectively. . Additionally, regexp_tokenize has been imported from nltk.tokenize. You can use regexp_tokenize(string, pattern) with my_string and one of the patterns as arguments to experiment for yourself and see which is the best tokenizer. . my_string # &quot;SOLDIER #1: Found them? In Mercea? The coconut&#39;s tropical!&quot; pattern2 # &#39;( w+|# d| ?|!)&#39; . regexp_tokenize(my_string, pattern2) [&#39;SOLDIER&#39;, &#39;#1&#39;, &#39;Found&#39;, &#39;them&#39;, &#39;?&#39;, &#39;In&#39;, &#39;Mercea&#39;, &#39;?&#39;, &#39;The&#39;, &#39;coconut&#39;, &#39;s&#39;, &#39;tropical&#39;, &#39;!&#39;] . 1.3.2 Regex with NLTK tokenization . Twitter is a frequently used source for NLP text and tasks. In this exercise, you’ll build a more complex tokenizer for tweets with hashtags and mentions using nltk and regex. The nltk.tokenize.TweetTokenizer class gives you some extra methods and attributes for parsing tweets. . Here, you’re given some example tweets to parse using both TweetTokenizer and regexp_tokenize from the nltk.tokenize module. These example tweets have been pre-loaded into the variable tweets. Feel free to explore it in the IPython Shell! . Unlike the syntax for the regex library, with nltk_tokenize() you pass the pattern as the second argument. . tweets[0] &#39;This is the best #nlp exercise ive found online! #python&#39; . # Import the necessary modules from nltk.tokenize import regexp_tokenize from nltk.tokenize import TweetTokenizer # Define a regex pattern to find hashtags: pattern1 pattern1 = r&quot;# w+&quot; # Use the pattern on the first tweet in the tweets list hashtags = regexp_tokenize(tweets[0], pattern1) print(hashtags) [&#39;#nlp&#39;, &#39;#python&#39;] . tweets[-1] &#39;Thanks @datacamp &lt;img draggable=&quot;false&quot; role=&quot;img&quot; class=&quot;emoji&quot; alt=&quot;&quot; src=&quot;https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/svg/1f642.svg&quot;&gt; #nlp #python&#39; . # Import the necessary modules from nltk.tokenize import regexp_tokenize from nltk.tokenize import TweetTokenizer # Write a pattern that matches both mentions (@) and hashtags pattern2 = r&quot;([@#] w+)&quot; # Use the pattern on the last tweet in the tweets list mentions_hashtags = regexp_tokenize(tweets[-1], pattern2) print(mentions_hashtags) # [&#39;@datacamp&#39;, &#39;#nlp&#39;, &#39;#python&#39;] . tweets [&#39;This is the best #nlp exercise ive found online! #python&#39;, &#39;#NLP is super fun! &lt;img draggable=&quot;false&quot; role=&quot;img&quot; class=&quot;emoji&quot; alt=&quot;&quot; src=&quot;https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/svg/2764.svg&quot;&gt; #learning&#39;, &#39;Thanks @datacamp &lt;img draggable=&quot;false&quot; role=&quot;img&quot; class=&quot;emoji&quot; alt=&quot;&quot; src=&quot;https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/svg/1f642.svg&quot;&gt; #nlp #python&#39;] . # Import the necessary modules from nltk.tokenize import regexp_tokenize from nltk.tokenize import TweetTokenizer # Use the TweetTokenizer to tokenize all tweets into one list tknzr = TweetTokenizer() all_tokens = [tknzr.tokenize(t) for t in tweets] print(all_tokens) . [[&#39;This&#39;, &#39;is&#39;, &#39;the&#39;, &#39;best&#39;, &#39;#nlp&#39;, &#39;exercise&#39;, &#39;ive&#39;, &#39;found&#39;, &#39;online&#39;, &#39;!&#39;, &#39;#python&#39;], [&#39;#NLP&#39;, &#39;is&#39;, &#39;super&#39;, &#39;fun&#39;, &#39;!&#39;, &#39;&lt;3&#39;, &#39;#learning&#39;], [&#39;Thanks&#39;, &#39;@datacamp&#39;, &#39;:)&#39;, &#39;#nlp&#39;, &#39;#python&#39;]] . 1.3.3 Non-ascii tokenization . In this exercise, you’ll practice advanced tokenization by tokenizing some non-ascii based text. You’ll be using German with emoji! . Here, you have access to a string called german_text, which has been printed for you in the Shell. Notice the emoji and the German characters! . The following modules have been pre-imported from nltk.tokenize: regexp_tokenize and word_tokenize. . Unicode ranges for emoji are: . (&#39; U0001F300&#39;-&#39; U0001F5FF&#39;), (&#39; U0001F600- U0001F64F&#39;), (&#39; U0001F680- U0001F6FF&#39;), and (&#39; u2600&#39;- u26FF- u2700- u27BF&#39;). . german_text &#39;Wann gehen wir Pizza essen? &lt;img draggable=&quot;false&quot; role=&quot;img&quot; class=&quot;emoji&quot; alt=&quot;&quot; src=&quot;https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/svg/1f355.svg&quot;&gt; Und fährst du mit Über? &lt;img draggable=&quot;false&quot; role=&quot;img&quot; class=&quot;emoji&quot; alt=&quot;&quot; src=&quot;https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/svg/1f695.svg&quot;&gt;&#39; . # Tokenize and print all words in german_text all_words = word_tokenize(german_text) print(all_words) [&#39;Wann&#39;, &#39;gehen&#39;, &#39;wir&#39;, &#39;Pizza&#39;, &#39;essen&#39;, &#39;?&#39;, &#39;&lt;img draggable=&quot;false&quot; role=&quot;img&quot; class=&quot;emoji&quot; alt=&quot;&quot; src=&quot;https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/svg/1f355.svg&quot;&gt;&#39;, &#39;Und&#39;, &#39;fährst&#39;, &#39;du&#39;, &#39;mit&#39;, &#39;Über&#39;, &#39;?&#39;, &#39;&lt;img draggable=&quot;false&quot; role=&quot;img&quot; class=&quot;emoji&quot; alt=&quot;&quot; src=&quot;https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/svg/1f695.svg&quot;&gt;&#39;] # Tokenize and print only capital words capital_words = r&quot;[A-ZÜ] w+&quot; print(regexp_tokenize(german_text, capital_words)) [&#39;Wann&#39;, &#39;Pizza&#39;, &#39;Und&#39;, &#39;Über&#39;] # Tokenize and print only emoji emoji = &quot;[&#39; U0001F300- U0001F5FF&#39;|&#39; U0001F600- U0001F64F&#39;|&#39; U0001F680- U0001F6FF&#39;|&#39; u2600- u26FF u2700- u27BF&#39;]&quot; print(regexp_tokenize(german_text, emoji)) [&#39;&lt;img draggable=&quot;false&quot; role=&quot;img&quot; class=&quot;emoji&quot; alt=&quot;&quot; src=&quot;https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/svg/1f355.svg&quot;&gt;&#39;, &#39;&lt;img draggable=&quot;false&quot; role=&quot;img&quot; class=&quot;emoji&quot; alt=&quot;&quot; src=&quot;https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/svg/1f695.svg&quot;&gt;&#39;] . 1.4 Charting word length with NLTK . | | | | | . Charting practice . Try using your new skills to find and chart the number of words per line in the script using matplotlib. The Holy Grail script is loaded for you, and you need to use regex to find the words per line. . Using list comprehensions here will speed up your computations. For example: my_lines = [tokenize(l) for l in lines] will call a function tokenize on each line in the list lines. The new transformed list will be saved in the my_lines variable. . You have access to the entire script in the variable holy_grail. Go for it! . holy_grail &quot;SCENE 1: [wind] [clop clop clop] nKING ARTHUR: Whoa there! [clop clop clop] nSOLDIER #1: Halt! Who goes there? nARTHUR: It is I, Arthur, ... along. nINSPECTOR: Everything? [squeak] nOFFICER #1: All right, sonny. That&#39;s enough. Just pack that in. [crash] nCAMERAMAN: Christ! n&quot; . # Split the script into lines: lines lines = holy_grail.split(&#39; n&#39;) # Replace all script lines for speaker pattern = &quot;[A-Z]{2,}( s)?(# d)?([A-Z]{2,})?:&quot; lines = [re.sub(pattern, &#39;&#39;, l) for l in lines] # Tokenize each line: tokenized_lines tokenized_lines = [regexp_tokenize(s, &#39; w+&#39;) for s in lines] # Make a frequency list of lengths: line_num_words line_num_words = [len(t_line) for t_line in tokenized_lines] # Plot a histogram of the line lengths plt.hist(line_num_words) # Show the plot plt.show() . . 2. Simple topic identification . 2.1 Word counts with bag-of-words . | | | . 2.1.1 Bag-of-words picker . It’s time for a quick check on your understanding of bag-of-words. Which of the below options, with basic nltk tokenization, map the bag-of-words for the following text? . “The cat is in the box. The cat box.” . (‘The’, 2), (‘box’, 2), (‘.’, 2), (‘cat’, 2), (‘is’, 1), (‘in’, 1), (‘the’, 1) . 2.1.2 Building a Counter with bag-of-words . In this exercise, you’ll build your first (in this course) bag-of-words counter using a Wikipedia article, which has been pre-loaded as article. Try doing the bag-of-words without looking at the full article text, and guessing what the topic is! If you’d like to peek at the title at the end, we’ve included it as article_title. Note that this article text has had very little preprocessing from the raw Wikipedia database entry. . word_tokenize has been imported for you. . # Import Counter from collections import Counter # Tokenize the article: tokens tokens = word_tokenize(article) # Convert the tokens into lowercase: lower_tokens lower_tokens = [t.lower() for t in tokens] # Create a Counter with the lowercase tokens: bow_simple bow_simple = Counter(lower_tokens) # Print the 10 most common tokens print(bow_simple.most_common(10)) # [(&#39;,&#39;, 151), (&#39;the&#39;, 150), (&#39;.&#39;, 89), (&#39;of&#39;, 81), (&quot;&#39;&#39;&quot;, 68), (&#39;to&#39;, 63), (&#39;a&#39;, 60), (&#39;in&#39;, 44), (&#39;and&#39;, 41), (&#39;debugging&#39;, 40)] . 2.2 Simple text preprocessing . | | | . 2.2.1 Text preprocessing steps . Which of the following are useful text preprocessing steps? . Lemmatization, lowercasing, removing unwanted tokens. . 2.2.2 Text preprocessing practice . Now, it’s your turn to apply the techniques you’ve learned to help clean up text for better NLP results. You’ll need to remove stop words and non-alphabetic characters, lemmatize, and perform a new bag-of-words on your cleaned text. . You start with the same tokens you created in the last exercise: lower_tokens. You also have the Counter class imported. . # Import WordNetLemmatizer from nltk.stem import WordNetLemmatizer # Retain alphabetic words: alpha_only alpha_only = [t for t in lower_tokens if t.isalpha()] # Remove all stop words: no_stops no_stops = [t for t in alpha_only if t not in english_stops] # Instantiate the WordNetLemmatizer wordnet_lemmatizer = WordNetLemmatizer() # Lemmatize all tokens into a new list: lemmatized lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops] # Create the bag-of-words: bow bow = Counter(lemmatized) # Print the 10 most common tokens print(bow.most_common(10)) . [(&#39;debugging&#39;, 40), (&#39;system&#39;, 25), (&#39;software&#39;, 16), (&#39;bug&#39;, 16), (&#39;problem&#39;, 15), (&#39;tool&#39;, 15), (&#39;computer&#39;, 14), (&#39;process&#39;, 13), (&#39;term&#39;, 13), (&#39;used&#39;, 12)] . 2.3 Introduction to gensim . | | | | | . 2.3.1 What are word vectors? . What are word vectors and how do they help with NLP? . Word vectors are multi-dimensional mathematical representations of words created using deep learning methods. They give us insight into relationships between words in a corpus. . 2.3.2 Creating and querying a corpus with gensim . It’s time to apply the methods you learned in the previous video to create your first gensim dictionary and corpus! . You’ll use these data structures to investigate word trends and potential interesting topics in your document set. To get started, we have imported a few additional messy articles from Wikipedia, which were preprocessed by lowercasing all words, tokenizing them, and removing stop words and punctuation. These were then stored in a list of document tokens called articles. You’ll need to do some light preprocessing and then generate the gensim dictionary and corpus. . # Import Dictionary from gensim.corpora.dictionary import Dictionary # Create a Dictionary from the articles: dictionary dictionary = Dictionary(articles) # Select the id for &quot;computer&quot;: computer_id computer_id = dictionary.token2id.get(&quot;computer&quot;) # Use computer_id with the dictionary to print the word print(dictionary.get(computer_id)) # computer # Create a MmCorpus: corpus corpus = [dictionary.doc2bow(article) for article in articles] # Print the first 10 word ids with their frequency counts from the fifth document print(corpus[4][:10]) # [(0, 88), (23, 11), (24, 2), (39, 1), (41, 2), (55, 22), (56, 1), (57, 1), (58, 1), (59, 3)] . 2.3.3 Gensim bag-of-words . Now, you’ll use your new gensim corpus and dictionary to see the most common terms per document and across all documents. You can use your dictionary to look up the terms. Take a guess at what the topics are and feel free to explore more documents in the IPython Shell! . You have access to the dictionary and corpus objects you created in the previous exercise, as well as the Python defaultdict and itertools to help with the creation of intermediate data structures for analysis. . defaultdict allows us to initialize a dictionary that will assign a default value to non-existent keys. By supplying the argument int, we are able to ensure that any non-existent keys are automatically assigned a default value of 0. This makes it ideal for storing the counts of words in this exercise. | itertools.chain.from_iterable() allows us to iterate through a set of sequences as if they were one continuous sequence. Using this function, we can easily iterate through our corpus object (which is a list of lists). | . The fifth document from corpus is stored in the variable doc, which has been sorted in descending order. . # Save the fifth document: doc doc = corpus[4] # Sort the doc for frequency: bow_doc bow_doc = sorted(doc, key=lambda w: w[1], reverse=True) # Print the top 5 words of the document alongside the count for word_id, word_count in bow_doc[:5]: print(dictionary.get(word_id), word_count) # Create the defaultdict: total_word_count total_word_count = defaultdict(int) for word_id, word_count in itertools.chain.from_iterable(corpus): total_word_count[word_id] += word_count . engineering 91 &#39;&#39; 88 reverse 71 software 51 cite 26 total_word_count defaultdict(int, {0: 1042, 1: 1, 2: 1, ... 997: 1, 998: 1, 999: 22, ...}) . # Create a sorted list from the defaultdict: sorted_word_count sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) # Print the top 5 words across all documents alongside the count for word_id, word_count in sorted_word_count[:5]: print(dictionary.get(word_id), word_count) . &#39;&#39; 1042 computer 594 software 450 `` 345 cite 322 . 2.4 Tf-idf with gensim . | | | . 2.4.1 What is tf-idf? . You want to calculate the tf-idf weight for the word &quot;computer&quot;, which appears five times in a document containing 100 words. Given a corpus containing 200 documents, with 20 documents mentioning the word &quot;computer&quot;, tf-idf can be calculated by multiplying term frequency with inverse document frequency. . Term frequency = percentage share of the word compared to all tokens in the document Inverse document frequency = logarithm of the total number of documents in a corpora divided by the number of documents containing the term . Which of the below options is correct? . (5 / 100) * log(200 / 20) . 2.4.2 Tf-idf with Wikipedia . Now it’s your turn to determine new significant terms for your corpus by applying gensim‘s tf-idf. You will again have access to the same corpus and dictionary objects you created in the previous exercises – dictionary, corpus, and doc. Will tf-idf make for more interesting results on the document level? . TfidfModel has been imported for you from gensim.models.tfidfmodel. . # Create a new TfidfModel using the corpus: tfidf tfidf = TfidfModel(corpus) # Calculate the tfidf weights of doc: tfidf_weights tfidf_weights = tfidf[doc] # Print the first five weights print(tfidf_weights[:5]) # [(24, 0.0022836332291091273), (39, 0.0043409401554717324), (41, 0.008681880310943465), (55, 0.011988285029371418), (56, 0.005482756770026296)] . corpus [[(0, 50), (1, 1), (2, 1), ... (10325, 1), (10326, 1), (10327, 1)]] tfidf &lt;gensim.models.tfidfmodel.TfidfModel at 0x7f10f5755978&gt; doc [(0, 88), (23, 11), (24, 2), ... (3627, 1), (3628, 2), ...] tfidf_weights [(24, 0.0022836332291091273), (39, 0.0043409401554717324), (41, 0.008681880310943465), (55, 0.011988285029371418), ... . # Sort the weights from highest to lowest: sorted_tfidf_weights sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True) # Print the top 5 weighted words for term_id, weight in sorted_tfidf_weights[:5]: print(dictionary.get(term_id), weight) reverse 0.4884961428651127 infringement 0.18674529210288995 engineering 0.16395041814479536 interoperability 0.12449686140192663 reverse-engineered 0.12449686140192663 . sorted_tfidf_weights [(1535, 0.4884961428651127), (3796, 0.18674529210288995), (350, 0.16395041814479536), (3804, 0.12449686140192663), ... . 3. Named-entity recognition . 3.1 Named Entity Recognition . | | | | | . 3.1.1 NER with NLTK . You’re now going to have some fun with named-entity recognition! A scraped news article has been pre-loaded into your workspace. Your task is to use nltk to find the named entities in this article. . What might the article be about, given the names you found? . Along with nltk, sent_tokenize and word_tokenize from nltk.tokenize have been pre-imported. . # Tokenize the article into sentences: sentences sentences = sent_tokenize(article) # Tokenize each sentence into words: token_sentences token_sentences = [word_tokenize(sent) for sent in sentences] # Tag each tokenized sentence into parts of speech: pos_sentences pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] # Create the named entity chunks: chunked_sentences chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True) # Test for stems of the tree with &#39;NE&#39; tags for sent in chunked_sentences: for chunk in sent: if hasattr(chunk, &quot;label&quot;) and chunk.label() == &quot;NE&quot;: print(chunk) . (NE Uber/NNP) (NE Beyond/NN) (NE Apple/NNP) (NE Uber/NNP) (NE Uber/NNP) (NE Travis/NNP Kalanick/NNP) (NE Tim/NNP Cook/NNP) (NE Apple/NNP) (NE Silicon/NNP Valley/NNP) (NE CEO/NNP) (NE Yahoo/NNP) (NE Marissa/NNP Mayer/NNP) . 3.1.2 Charting practice . In this exercise, you’ll use some extracted named entities and their groupings from a series of newspaper articles to chart the diversity of named entity types in the articles. . You’ll use a defaultdict called ner_categories, with keys representing every named entity group type, and values to count the number of each different named entity type. You have a chunked sentence list called chunked_sentences similar to the last exercise, but this time with non-binary category names. . You can use hasattr() to determine if each chunk has a &#39;label&#39; and then simply use the chunk’s .label() method as the dictionary key. . type(chunked_sentences) list chunked_sentences [Tree(&#39;S&#39;, [(&#39; ufeffImage&#39;, &#39;NN&#39;), (&#39;copyright&#39;, &#39;NN&#39;), Tree(&#39;ORGANIZATION&#39;, [(&#39;EPA&#39;, &#39;NNP&#39;), (&#39;Image&#39;, &#39;NNP&#39;)]), (&#39;caption&#39;, &#39;NN&#39;), (&#39;Uber&#39;, &#39;NNP&#39;), (&#39;has&#39;, &#39;VBZ&#39;), (&#39;been&#39;, &#39;VBN&#39;), (&#39;criticised&#39;, &#39;VBN&#39;), (&#39;many&#39;, &#39;JJ&#39;), (&#39;times&#39;, &#39;NNS&#39;), (&#39;over&#39;, &#39;IN&#39;), (&#39;the&#39;, &#39;DT&#39;), (&#39;way&#39;, &#39;NN&#39;), (&#39;it&#39;, &#39;PRP&#39;), (&#39;runs&#39;, &#39;VBZ&#39;), (&#39;its&#39;, &#39;PRP$&#39;), (&#39;business&#39;, &#39;NN&#39;), (&#39;Ride-sharing&#39;, &#39;JJ&#39;), (&#39;firm&#39;, &#39;NN&#39;), (&#39;Uber&#39;, &#39;NNP&#39;), (&#39;is&#39;, &#39;VBZ&#39;), (&#39;facing&#39;, &#39;VBG&#39;), (&#39;a&#39;, &#39;DT&#39;), (&#39;criminal&#39;, &#39;JJ&#39;), (&#39;investigation&#39;, &#39;NN&#39;), (&#39;by&#39;, &#39;IN&#39;), (&#39;the&#39;, &#39;DT&#39;), Tree(&#39;GPE&#39;, [(&#39;US&#39;, &#39;JJ&#39;)]), (&#39;government&#39;, &#39;NN&#39;), (&#39;.&#39;, &#39;.&#39;)]), . # Create the defaultdict: ner_categories ner_categories = defaultdict(int) # Create the nested for loop for sent in chunked_sentences: for chunk in sent: if hasattr(chunk, &#39;label&#39;): ner_categories[chunk.label()] += 1 # Create a list from the dictionary keys for the chart labels: labels labels = list(ner_categories.keys()) labels # [&#39;ORGANIZATION&#39;, &#39;GPE&#39;, &#39;PERSON&#39;, &#39;LOCATION&#39;, &#39;FACILITY&#39;] . # Create a list of the values: values values = [ner_categories.get(v) for v in labels] # Create the pie chart plt.pie(values, labels=labels, autopct=&#39;%1.1f%%&#39;, startangle=140) # Display the chart plt.show() . . 3.1.3 Stanford library with NLTK . When using the Stanford library with NLTK, what is needed to get started? . NLTK, the Stanford Java Libraries and some environment variables to help with integration. . 3.2 Introduction to SpaCy . | | | | . 3.2.1 Comparing NLTK with spaCy NER . Using the same text you used in the first exercise of this chapter, you’ll now see the results using spaCy’s NER annotator. How will they compare? . The article has been pre-loaded as article. To minimize execution times, you’ll be asked to specify the keyword arguments tagger=False, parser=False, matcher=False when loading the spaCy model, because you only care about the entity in this exercise. . # Import spacy import spacy # Instantiate the English model: nlp nlp = spacy.load(&#39;en&#39;,tagger=False, parser=False, matcher=False) # Create a new document: doc doc = nlp(article) # Print all of the found entities and their labels for ent in doc.ents: print(ent.label_, ent.text) . ORG Uber ORG Uber ORG Apple ORG Uber ORG Uber PERSON Travis Kalanick ORG Uber PERSON Tim Cook ORG Apple CARDINAL Millions ORG Uber GPE drivers’ LOC Silicon Valley’s ORG Yahoo PERSON Marissa Mayer MONEY $186m . 3.2.2 spaCy NER Categories . Which are the extra categories that spacy uses compared to nltk in its named-entity recognition? . NORP, CARDINAL, MONEY, WORKOFART, LANGUAGE, EVENT . 3.3 Multilingual NER with polyglot . | | . 3.3.1 French NER with polyglot I . In this exercise and the next, you’ll use the polyglot library to identify French entities. The library functions slightly differently than spacy, so you’ll use a few of the new things you learned in the last video to display the named entity text and category. . You have access to the full article string in article. Additionally, the Text class of polyglot has been imported from polyglot.text. . # Create a new text object using Polyglot&#39;s Text class: txt txt = Text(article) # Print each of the entities found for ent in txt.entities: print(ent) # Print the type of ent print(type(ent)) . [&#39;Charles&#39;, &#39;Cuvelliez&#39;] [&#39;Charles&#39;, &#39;Cuvelliez&#39;] [&#39;Bruxelles&#39;] [&#39;l’IA&#39;] [&#39;Julien&#39;, &#39;Maldonato&#39;] [&#39;Deloitte&#39;] [&#39;Ethiquement&#39;] [&#39;l’IA&#39;] [&#39;.&#39;] &lt;class &#39;polyglot.text.Chunk&#39;&gt; . 3.3.2 French NER with polyglot II . Here, you’ll complete the work you began in the previous exercise. . Your task is to use a list comprehension to create a list of tuples, in which the first element is the entity tag, and the second element is the full string of the entity text. . # Create the list of tuples: entities entities = [(ent.tag, &#39; &#39;.join(ent)) for ent in txt.entities] # Print entities print(entities) . [(&#39;I-PER&#39;, &#39;Charles Cuvelliez&#39;), (&#39;I-PER&#39;, &#39;Charles Cuvelliez&#39;), (&#39;I-ORG&#39;, &#39;Bruxelles&#39;), (&#39;I-PER&#39;, &#39;l’IA&#39;), (&#39;I-PER&#39;, &#39;Julien Maldonato&#39;), (&#39;I-ORG&#39;, &#39;Deloitte&#39;), (&#39;I-PER&#39;, &#39;Ethiquement&#39;), (&#39;I-LOC&#39;, &#39;l’IA&#39;), (&#39;I-PER&#39;, &#39;.&#39;)] . 3.3.3 Spanish NER with polyglot . You’ll continue your exploration of polyglot now with some Spanish annotation. This article is not written by a newspaper, so it is your first example of a more blog-like text. How do you think that might compare when finding entities? . The Text object has been created as txt, and each entity has been printed, as you can see in the IPython Shell. . Your specific task is to determine how many of the entities contain the words &quot;Márquez&quot; or &quot;Gabo&quot; – these refer to the same person in different ways! . txt.entities is available. . # Initialize the count variable: count count = 0 # Iterate over all the entities for ent in txt.entities: # Check whether the entity contains &#39;Márquez&#39; or &#39;Gabo&#39; if (&quot;Márquez&quot; in ent) or (&quot;Gabo&quot; in ent): # Increment count count += 1 # Print count print(count) # Calculate the percentage of entities that refer to &quot;Gabo&quot;: percentage percentage = count / len(txt.entities) print(percentage) . 4. Building a “fake news” classifier . 4.1 Classifying fake news using supervised learning with NLP . | | | | . 4.1.1 Which possible features** . Which of the following are possible features for a text classification problem? . Number of words in a document. | Specific named entities. | Language. | . 4.2 Building word count vectors with scikit-learn . | | . 4.2.1 CountVectorizer for text classification . It’s time to begin building your text classifier! The data has been loaded into a DataFrame called df. Explore it in the IPython Shell to investigate what columns you can use. The .head() method is particularly informative. . In this exercise, you’ll use pandas alongSpanish NER with polyglotside scikit-learn to create a sparse text vectorizer you can use to train and test a simple supervised model. To begin, you’ll set up a CountVectorizer and investigate some of its features. . print(df.head()) Unnamed: 0 title 0 8476 You Can Smell Hillary’s Fear 1 10294 Watch The Exact Moment Paul Ryan Committed Pol... 2 3608 Kerry to go to Paris in gesture of sympathy 3 10142 Bernie supporters on Twitter erupt in anger ag... 4 875 The Battle of New York: Why This Primary Matters text label 0 Daniel Greenfield, a Shillman Journalism Fello... FAKE 1 Google Pinterest Digg Linkedin Reddit Stumbleu... FAKE 2 U.S. Secretary of State John F. Kerry said Mon... REAL 3 — Kaydee King (@KaydeeKing) November 9, 2016 T... FAKE 4 It&#39;s primary day in New York and front-runners... REAL . # Import the necessary modules from sklearn.feature_extraction.text import CountVectorizer from sklearn.model_selection import train_test_split # Print the head of df print(df.head()) # Create a series to store the labels: y y = df.label # Create training and test sets X_train, X_test, y_train, y_test = train_test_split(df[&#39;text&#39;],y,test_size=0.33,random_state=53) # Initialize a CountVectorizer object: count_vectorizer count_vectorizer = CountVectorizer(stop_words=&#39;english&#39;) # Transform the training data using only the &#39;text&#39; column values: count_train count_train = count_vectorizer.fit_transform(X_train) # Transform the test data using only the &#39;text&#39; column values: count_test count_test = count_vectorizer.transform(X_test) # Print the first 10 features of the count_vectorizer print(count_vectorizer.get_feature_names()[:10]) # [&#39;00&#39;, &#39;000&#39;, &#39;0000&#39;, &#39;00000031&#39;, &#39;000035&#39;, &#39;00006&#39;, &#39;0001&#39;, &#39;0001pt&#39;, &#39;000ft&#39;, &#39;000km&#39;] . 4.2.2 TfidfVectorizer for text classification . Similar to the sparse CountVectorizer created in the previous exercise, you’ll work on creating tf-idf vectors for your documents. You’ll set up a TfidfVectorizer and investigate some of its features. . In this exercise, you’ll use pandas and sklearn along with the same X_train, y_train and X_test, y_test DataFrames and Series you created in the last exercise. . # Import TfidfVectorizer from sklearn.feature_extraction.text import TfidfVectorizer # Initialize a TfidfVectorizer object: tfidf_vectorizer tfidf_vectorizer = TfidfVectorizer(stop_words=&#39;english&#39;, max_df=0.7) # Transform the training data: tfidf_train tfidf_train = tfidf_vectorizer.fit_transform(X_train) # Transform the test data: tfidf_test tfidf_test = tfidf_vectorizer.transform(X_test) # Print the first 10 features print(tfidf_vectorizer.get_feature_names()[:10]) # Print the first 5 vectors of the tfidf training data print(tfidf_train.A[:5]) . [&#39;00&#39;, &#39;000&#39;, &#39;001&#39;, &#39;008s&#39;, &#39;00am&#39;, &#39;00pm&#39;, &#39;01&#39;, &#39;01am&#39;, &#39;02&#39;, &#39;024&#39;] [[0. 0.01928563 0. ... 0. 0. 0. ] [0. 0. 0. ... 0. 0. 0. ] [0. 0.02895055 0. ... 0. 0. 0. ] [0. 0.03056734 0. ... 0. 0. 0. ] [0. 0. 0. ... 0. 0. 0. ]] . 4.2.3 Inspecting the vectors . To get a better idea of how the vectors work, you’ll investigate them by converting them into pandas DataFrames. . Here, you’ll use the same data structures you created in the previous two exercises (count_train, count_vectorizer, tfidf_train, tfidf_vectorizer) as well as pandas, which is imported as pd. . # Create the CountVectorizer DataFrame: count_df count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names()) # Create the TfidfVectorizer DataFrame: tfidf_df tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names()) # Print the head of count_df print(count_df.head()) # Print the head of tfidf_df print(tfidf_df.head()) # Calculate the difference in columns: difference difference = set(count_df.columns) - set(tfidf_df.columns) print(difference) # set() # Check whether the DataFrames are equal print(count_df.equals(tfidf_df)) # False . print(count_df.head()) 000 00am 0600 10 100 107 11 110 1100 12 ... younger 0 0 0 0 0 0 0 0 0 0 0 ... 0 1 0 0 0 3 0 0 0 0 0 0 ... 0 2 0 0 0 0 0 0 0 0 0 0 ... 0 3 0 0 0 0 0 0 0 0 0 0 ... 1 4 0 0 0 0 0 0 0 0 0 0 ... 0 youth youths youtube ypg yuan zawahiri zeitung zero zerohedge 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 [5 rows x 5111 columns] print(tfidf_df.head()) 000 00am 0600 10 100 107 11 110 1100 12 ... 0 0.0 0.0 0.0 0.000000 0.0 0.0 0.0 0.0 0.0 0.0 ... 1 0.0 0.0 0.0 0.105636 0.0 0.0 0.0 0.0 0.0 0.0 ... 2 0.0 0.0 0.0 0.000000 0.0 0.0 0.0 0.0 0.0 0.0 ... 3 0.0 0.0 0.0 0.000000 0.0 0.0 0.0 0.0 0.0 0.0 ... 4 0.0 0.0 0.0 0.000000 0.0 0.0 0.0 0.0 0.0 0.0 ... younger youth youths youtube ypg yuan zawahiri zeitung zero 0 0.000000 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.033579 1 0.000000 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.000000 2 0.000000 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.000000 3 0.015175 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.000000 4 0.000000 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.000000 zerohedge 0 0.0 1 0.0 2 0.0 3 0.0 4 0.0 [5 rows x 5111 columns] . 4.3 Training and testing a classification model with scikit-learn . | | | . 4.3.1 Text classification models . Which of the below is the most reasonable model to use when training a new supervised model using text vector data? . Naive Bayes . 4.3.2 Training and testing the “fake news” model with CountVectorizer . Now it’s your turn to train the “fake news” model using the features you identified and extracted. In this first exercise you’ll train and test a Naive Bayes model using the CountVectorizer data. . The training and test sets have been created, and count_vectorizer, count_train, and count_test have been computed. . # Import the necessary modules from sklearn.naive_bayes import MultinomialNB from sklearn import metrics # Instantiate a Multinomial Naive Bayes classifier: nb_classifier nb_classifier = MultinomialNB() # Fit the classifier to the training data nb_classifier.fit(count_train, y_train) # Create the predicted tags: pred pred = nb_classifier.predict(count_test) # Calculate the accuracy score: score score = metrics.accuracy_score(y_test,pred) print(score) # 0.893352462936394 # Calculate the confusion matrix: cm cm = metrics.confusion_matrix(y_test,pred,labels=[&#39;FAKE&#39;, &#39;REAL&#39;]) print(cm) #[[ 865 143] [ 80 1003]] . 4.3.3 Training and testing the “fake news” model with TfidfVectorizer . Now that you have evaluated the model using the CountVectorizer, you’ll do the same using the TfidfVectorizer with a Naive Bayes model. . The training and test sets have been created, and tfidf_vectorizer, tfidf_train, and tfidf_test have been computed. Additionally, MultinomialNB and metrics have been imported from, respectively, sklearn.naive_bayes and sklearn. . # Calculate the accuracy score and confusion matrix of # Multinomial Naive Bayes classifier predictions trained on # tfidf_train, y_train and tested against tfidf_test and # y_test # Instantiate a Multinomial Naive Bayes classifier: nb_classifier nb_classifier = MultinomialNB() # Fit the classifier to the training data nb_classifier.fit(tfidf_train, y_train) # Create the predicted tags: pred pred = nb_classifier.predict(tfidf_test) # Calculate the accuracy score: score score = metrics.accuracy_score(y_test,pred) print(score) # 0.8565279770444764 # Calculate the confusion matrix: cm cm = metrics.confusion_matrix(y_test,pred,labels=[&#39;FAKE&#39;, &#39;REAL&#39;]) print(cm) #[[ 739 269] [ 31 1052]] . Fantastic fake detection! The model correctly identifies fake news about 86% of the time. That’s a great start, but for a real world situation, you’d need to improve the score. . 4.4 Simple NLP, complex problems . | | | . 4.4.1 Improving the model** . What are possible next steps you could take to improve the model? . Tweaking alpha levels. | Trying a new classification model. | Training on a larger dataset. | Improving text preprocessing. | . 4.4.2 Improving your model** . Your job in this exercise is to test a few different alpha levels using the Tfidf vectors to determine if there is a better performing combination. . The training and test sets have been created, and tfidf_vectorizer, tfidf_train, and tfidf_test have been computed. . # Create the list of alphas: alphas alphas = np.arange(0,1,0.1) # Define train_and_predict() def train_and_predict(alpha): # Instantiate the classifier: nb_classifier nb_classifier = MultinomialNB(alpha=alpha) # Fit to the training data nb_classifier.fit(tfidf_train,y_train) # Predict the labels: pred pred = nb_classifier.predict(tfidf_test) # Compute accuracy: score score = metrics.accuracy_score(y_test,pred) return score # Iterate over the alphas and print the corresponding score for alpha in alphas: print(&#39;Alpha: &#39;, alpha) print(&#39;Score: &#39;, train_and_predict(alpha)) print() . Alpha: 0.0 Score: 0.8813964610234337 Alpha: 0.1 Score: 0.8976566236250598 Alpha: 0.2 Score: 0.8938307030129125 Alpha: 0.30000000000000004 Score: 0.8900047824007652 Alpha: 0.4 Score: 0.8857006217120995 Alpha: 0.5 Score: 0.8842659014825442 Alpha: 0.6000000000000001 Score: 0.874701099952176 Alpha: 0.7000000000000001 Score: 0.8703969392635102 Alpha: 0.8 Score: 0.8660927785748446 Alpha: 0.9 Score: 0.8589191774270684 . 4.4.3 Inspecting your model . Now that you have built a “fake news” classifier, you’ll investigate what it has learned. You can map the important vector weights back to actual words using some simple inspection techniques. . You have your well performing tfidf Naive Bayes classifier available as nb_classifier, and the vectors as tfidf_vectorizer. . # Get the class labels: class_labels class_labels = nb_classifier.classes_ # Extract the features: feature_names feature_names = tfidf_vectorizer.get_feature_names() # Zip the feature names together with the coefficient array and sort by weights: feat_with_weights feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names)) # Print the first class label and the top 20 feat_with_weights entries print(class_labels[0], feat_with_weights[:20]) # Print the second class label and the bottom 20 feat_with_weights entries print(class_labels[1], feat_with_weights[-20:]) . FAKE [(-12.641778440826338, &#39;0000&#39;), (-12.641778440826338, &#39;000035&#39;), (-12.641778440826338, &#39;0001&#39;), (-12.641778440826338, &#39;0001pt&#39;), (-12.641778440826338, &#39;000km&#39;), (-12.641778440826338, &#39;0011&#39;), (-12.641778440826338, &#39;006s&#39;), (-12.641778440826338, &#39;007&#39;), (-12.641778440826338, &#39;007s&#39;), (-12.641778440826338, &#39;008s&#39;), (-12.641778440826338, &#39;0099&#39;), (-12.641778440826338, &#39;00am&#39;), (-12.641778440826338, &#39;00p&#39;), (-12.641778440826338, &#39;00pm&#39;), (-12.641778440826338, &#39;014&#39;), (-12.641778440826338, &#39;015&#39;), (-12.641778440826338, &#39;018&#39;), (-12.641778440826338, &#39;01am&#39;), (-12.641778440826338, &#39;020&#39;), (-12.641778440826338, &#39;023&#39;)] REAL [(-6.790929954967984, &#39;states&#39;), (-6.765360557845786, &#39;rubio&#39;), (-6.751044290367751, &#39;voters&#39;), (-6.701050756752027, &#39;house&#39;), (-6.695547793099875, &#39;republicans&#39;), (-6.6701912490429685, &#39;bush&#39;), (-6.661945235816139, &#39;percent&#39;), (-6.589623788689862, &#39;people&#39;), (-6.559670340096453, &#39;new&#39;), (-6.489892292073901, &#39;party&#39;), (-6.452319082422527, &#39;cruz&#39;), (-6.452076515575875, &#39;state&#39;), (-6.397696648238072, &#39;republican&#39;), (-6.376343060363355, &#39;campaign&#39;), (-6.324397735392007, &#39;president&#39;), (-6.2546017970213645, &#39;sanders&#39;), (-6.144621899738043, &#39;obama&#39;), (-5.756817248152807, &#39;clinton&#39;), (-5.596085785733112, &#39;said&#39;), (-5.357523914504495, &#39;trump&#39;)] . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/2021/07/05/introduction-to-natural-language-processing-in-python.html",
            "relUrl": "/2021/07/05/introduction-to-natural-language-processing-in-python.html",
            "date": " • Jul 5, 2021"
        }
        
    
  
    
        ,"post44": {
            "title": "Introduction To Deep Learning In Python",
            "content": "Introduction to Deep Learning in Python . This is the memo of the 25th course of ‘Data Scientist with Python’ track. . You can find the original course HERE . . 1. Basics of deep learning and neural networks . 1.1 Introduction to deep learning . . . . 1.2 Forward propagation . . Coding the forward propagation algorithm . In this exercise, you’ll write code to do forward propagation (prediction) for your first neural network: . . Each data point is a customer. The first input is how many accounts they have, and the second input is how many children they have. The model will predict how many transactions the user makes in the next year. . You will use this data throughout the first 2 chapters of this course. . input_data # array([3, 5]) weights # {&#39;node_0&#39;: array([2, 4]), &#39;node_1&#39;: array([ 4, -5]), &#39;output&#39;: array([2, 7])} . input_data * weights[&#39;node_0&#39;] # array([ 6, 20]) np.array([3, 5]) * np.array([2, 4]) # array([ 6, 20]) (input_data * weights[&#39;node_0&#39;]).sum() # 26 . # Calculate node 0 value: node_0_value node_0_value = (input_data * weights[&#39;node_0&#39;]).sum() # Calculate node 1 value: node_1_value node_1_value = (input_data * weights[&#39;node_1&#39;]).sum() # Put node values into array: hidden_layer_outputs hidden_layer_outputs = np.array([node_0_value, node_1_value]) # Calculate output: output output = (hidden_layer_outputs * weights[&#39;output&#39;]).sum() # Print output print(output) # -39 . It looks like the network generated a prediction of -39 . . 1.3 Activation functions . . . . The Rectified Linear Activation Function . An “activation function” is a function applied at each node. It converts the node’s input into some output. . The rectified linear activation function (called ReLU ) has been shown to lead to very high-performance networks. This function takes a single number as an input, returning 0 if the input is negative, and the input if the input is positive. . Here are some examples: . relu(3) = 3 . relu(-3) = 0 . def relu(input): &#39;&#39;&#39;Define your relu activation function here&#39;&#39;&#39; # Calculate the value for the output of the relu function: output output = max(input, 0) # Return the value just calculated return(output) # Calculate node 0 value: node_0_output node_0_input = (input_data * weights[&#39;node_0&#39;]).sum() node_0_output = relu(node_0_input) # Calculate node 1 value: node_1_output node_1_input = (input_data * weights[&#39;node_1&#39;]).sum() node_1_output = relu(node_1_input) # Put node values into array: hidden_layer_outputs hidden_layer_outputs = np.array([node_0_output, node_1_output]) # Calculate model output (do not apply relu) model_output = (hidden_layer_outputs * weights[&#39;output&#39;]).sum() # Print model output print(model_output) # 52 . You predicted 52 transactions. Without this activation function, you would have predicted a negative number! . The real power of activation functions will come soon when you start tuning model weights. . Applying the network to many observations/rows of data . input_data [array([3, 5]), array([ 1, -1]), array([0, 0]), array([8, 4])] weights {&#39;node_0&#39;: array([2, 4]), &#39;node_1&#39;: array([ 4, -5]), &#39;output&#39;: array([2, 7])} . def relu(input): &#39;&#39;&#39;Define relu activation function&#39;&#39;&#39; return(max(input, 0)) # Define predict_with_network() def predict_with_network(input_data_row, weights): # Calculate node 0 value node_0_input = (input_data_row * weights[&#39;node_0&#39;]).sum() node_0_output = relu(node_0_input) # Calculate node 1 value node_1_input = (input_data_row * weights[&#39;node_1&#39;]).sum() node_1_output = relu(node_1_input) # Put node values into array: hidden_layer_outputs hidden_layer_outputs = np.array([node_0_output, node_1_output]) # Calculate model output input_to_final_layer = (hidden_layer_outputs * weights[&#39;output&#39;]).sum() model_output = relu(input_to_final_layer) # Return model output return(model_output) # Create empty list to store prediction results results = [] for input_data_row in input_data: # Append prediction to results results.append(predict_with_network(input_data_row, weights)) # Print results print(results) # [52, 63, 0, 148] . 1.4 Deeper networks . . . Forward propagation in a deeper network . You now have a model with 2 hidden layers. The values for an input data point are shown inside the input nodes. The weights are shown on the edges/lines. What prediction would this model make on this data point? . Assume the activation function at each node is the identity function . That is, each node’s output will be the same as its input. So the value of the bottom node in the first hidden layer is -1, and not 0, as it would be if the ReLU activation function was used. . . | Hidden Layer 1 | Hidden Layer 2 | Prediction | | 6 | -1 | | | | | 0 | | -1 | 5 | | . Multi-layer neural networks . In this exercise, you’ll write code to do forward propagation for a neural network with 2 hidden layers. Each hidden layer has two nodes. . input_data array([3, 5]) weights {&#39;node_0_0&#39;: array([2, 4]), &#39;node_0_1&#39;: array([ 4, -5]), &#39;node_1_0&#39;: array([-1, 2]), &#39;node_1_1&#39;: array([1, 2]), &#39;output&#39;: array([2, 7])} . def predict_with_network(input_data): # Calculate node 0 in the first hidden layer node_0_0_input = (input_data * weights[&#39;node_0_0&#39;]).sum() node_0_0_output = relu(node_0_0_input) # Calculate node 1 in the first hidden layer node_0_1_input = (input_data * weights[&#39;node_0_1&#39;]).sum() node_0_1_output = relu(node_0_1_input) # Put node values into array: hidden_0_outputs hidden_0_outputs = np.array([node_0_0_output, node_0_1_output]) # Calculate node 0 in the second hidden layer node_1_0_input = (hidden_0_outputs * weights[&#39;node_1_0&#39;]).sum() node_1_0_output = relu(node_1_0_input) # Calculate node 1 in the second hidden layer node_1_1_input = (hidden_0_outputs * weights[&#39;node_1_1&#39;]).sum() node_1_1_output = relu(node_1_1_input) # Put node values into array: hidden_1_outputs hidden_1_outputs = np.array([node_1_0_output, node_1_1_output]) # Calculate model output: model_output model_output = (hidden_1_outputs * weights[&#39;output&#39;]).sum() # Return model_output return(model_output) output = predict_with_network(input_data) print(output) # 182 . Representations are learned . How are the weights that determine the features/interactions in Neural Networks created? . The model training process sets them to optimize predictive accuracy. . Levels of representation . Which layers of a model capture more complex or “higher level” interactions? . The last layers capture the most complex interactions. . 2. Optimizing a neural network with backward propagation . 2.1 The need for optimization . Calculating model errors . What is the error (predicted – actual) for the following network when the input data is [3, 2] and the actual value of the target (what you are trying to predict) is 5? . . prediction = (3_2 + 2_1) _2 + (3_0 + 2_0)_2 . =16 . error = 16 – 5 = 11 . Understanding how weights change model accuracy . Imagine you have to make a prediction for a single data point. The actual value of the target is 7. The weight going from node_0 to the output is 2, as shown below. . If you increased it slightly, changing it to 2.01, would the predictions become more accurate, less accurate, or stay the same? . . prediction_before = 16 . error_before = 16 – 7 = 9 . prediction_after = (3_2.01 + 2_1) _2 + (3_0 + 2_0)_2 . =16.x . error_after = 9.x . Increasing the weight to 2.01 would increase the resulting error from 9 to 9.08 , making the predictions less accurate. . Coding how weight changes affect accuracy . Now you’ll get to change weights in a real network and see how they affect model accuracy! . . Have a look at the following neural network: . Its weights have been pre-loaded as weights_0 . Your task in this exercise is to update a single weight in weights_0 to create weights_1 , which gives a perfect prediction (in which the predicted value is equal to target_actual : 3). . # The data point you will make a prediction for input_data = np.array([0, 3]) # Sample weights weights_0 = {&#39;node_0&#39;: [2, 1], &#39;node_1&#39;: [1, 2], &#39;output&#39;: [1, 1] } # The actual target value, used to calculate the error target_actual = 3 # Make prediction using original weights model_output_0 = predict_with_network(input_data, weights_0) # Calculate error: error_0 error_0 = model_output_0 - target_actual # Create weights that cause the network to make perfect prediction (3): weights_1 weights_1 = {&#39;node_0&#39;: [2, 1], &#39;node_1&#39;: [1, 2], &#39;output&#39;: [1, 0] } # Make prediction using new weights: model_output_1 model_output_1 = predict_with_network(input_data, weights_1) # Calculate error: error_1 error_1 = model_output_1 - target_actual # Print error_0 and error_1 print(error_0) print(error_1) # 6 # 0 . Scaling up to multiple data points . You’ve seen how different weights will have different accuracies on a single prediction. But usually, you’ll want to measure model accuracy on many points. . You’ll now write code to compare model accuracies for two different sets of weights, which have been stored as weights_0 and weights_1 . . input_data [array([0, 3]), array([1, 2]), array([-1, -2]), array([4, 0])] target_actuals [1, 3, 5, 7] weights_0 {&#39;node_0&#39;: array([2, 1]), &#39;node_1&#39;: array([1, 2]), &#39;output&#39;: array([1, 1])} weights_1 {&#39;node_0&#39;: array([2, 1]), &#39;node_1&#39;: array([1. , 1.5]), &#39;output&#39;: array([1. , 1.5])} . from sklearn.metrics import mean_squared_error # Create model_output_0 model_output_0 = [] # Create model_output_1 model_output_1 = [] # Loop over input_data for row in input_data: # Append prediction to model_output_0 model_output_0.append(predict_with_network(row, weights_0)) # Append prediction to model_output_1 model_output_1.append(predict_with_network(row, weights_1)) # Calculate the mean squared error for model_output_0: mse_0 mse_0 = mean_squared_error(target_actuals, model_output_0) # Calculate the mean squared error for model_output_1: mse_1 mse_1 = mean_squared_error(target_actuals, model_output_1) # Print mse_0 and mse_1 print(&quot;Mean squared error with weights_0: %f&quot; %mse_0) print(&quot;Mean squared error with weights_1: %f&quot; %mse_1) # Mean squared error with weights_0: 37.500000 # Mean squared error with weights_1: 49.890625 . It looks like model_output_1 has a higher mean squared error. . 2.2 Gradient descent . | . ex. learning rate = 0.01 . | . w.r.t. = with respect to . | | . new weight = 2 – -24 * 0.01 = 2.24 . | . Calculating slopes . You’re now going to practice calculating slopes. . When plotting the mean-squared error loss function against predictions, the slope is 2 * x * (y-xb) , or 2 * input_data * error . . Note that x and b may have multiple numbers ( x is a vector for each data point, and b is a vector). In this case, the output will also be a vector, which is exactly what you want. . You’re ready to write the code to calculate this slope while using a single data point. . input_data array([1, 2, 3]) weights array([0, 2, 1]) target 0 . # Calculate the predictions: preds preds = (weights * input_data).sum() # Calculate the error: error error = target - preds # Calculate the slope: slope slope = 2 * input_data * error # Print the slope print(slope) # [-14 -28 -42] . You can now use this slope to improve the weights of the model! . Improving model weights . You’ve just calculated the slopes you need. Now it’s time to use those slopes to improve your model. . If you add the slopes to your weights, you will move in the right direction. However, it’s possible to move too far in that direction. . So you will want to take a small step in that direction first, using a lower learning rate, and verify that the model is improving. . # Set the learning rate: learning_rate learning_rate = 0.01 # Calculate the predictions: preds preds = (weights * input_data).sum() # weights # array([0, 2, 1]) # Calculate the error: error error = preds - target # Calculate the slope: slope slope = 2 * input_data * error # slope # array([14, 28, 42]) # Update the weights: weights_updated weights_updated = weights - learning_rate * slope # weights_updated # array([-0.14, 1.72, 0.58]) # Get updated predictions: preds_updated preds_updated = (weights_updated * input_data).sum() # Calculate updated error: error_updated error_updated = preds_updated - target # Print the original error print(error) # Print the updated error print(error_updated) # 7 # 5.04 . Updating the model weights did indeed decrease the error! . Making multiple updates to weights . You’re now going to make multiple updates so you can dramatically improve your model weights, and see how the predictions improve with each update. . get_slope? Signature: get_slope(input_data, target, weights) Docstring: &lt;no docstring&gt; File: /tmp/tmpt3wthzls/&lt;ipython-input-1-7b11d278e306&gt; Type: function get_mse? Signature: get_mse(input_data, target, weights) Docstring: &lt;no docstring&gt; File: /tmp/tmpt3wthzls/&lt;ipython-input-1-7b11d278e306&gt; Type: function . n_updates = 20 mse_hist = [] # Iterate over the number of updates for i in range(n_updates): # Calculate the slope: slope slope = get_slope(input_data, target, weights) # Update the weights: weights weights = weights - slope * 0.01 # Calculate mse with new weights: mse mse = get_mse(input_data, target, weights) # Append the mse to mse_hist mse_hist.append(mse) # Plot the mse history plt.plot(mse_hist) plt.xlabel(&#39;Iterations&#39;) plt.ylabel(&#39;Mean Squared Error&#39;) plt.show() . . As you can see, the mean squared error decreases as the number of iterations go up. . 2.3 Back propagation . | | | | | . The relationship between forward and backward propagation . If you have gone through 4 iterations of calculating slopes (using backward propagation) and then updated weights. . How many times must you have done forward propagation? . 4 . Each time you generate predictions using forward propagation, you update the weights using backward propagation. . Thinking about backward propagation . If your predictions were all exactly right, and your errors were all exactly 0, the slope of the loss function with respect to your predictions would also be 0. . In that circumstance, the updates to all weights in the network would also be 0. . 2.4 Backpropagation in practice . . slope = 2 imput error . | . 6 and 18 are slopes just calculated in the above graph . | | . x &lt;= 0: slope = 0 . | . x &gt; 0: slope = 1 . . gradient = input(white) slope(red) ReLU_slope(=1 here) . | . gradient_0 = 0_6_1 = 0 . gradient_3 = 1_18_1 = 18 . | | . A round of backpropagation . In the network shown below, we have done forward propagation, and node values calculated as part of forward propagation are shown in white. . The weights are shown in black. . Layers after the question mark show the slopes calculated as part of back-prop, rather than the forward-prop values. Those slope values are shown in purple. . This network again uses the ReLU activation function, so the slope of the activation function is 1 for any node receiving a positive value as input. . Assume the node being examined had a positive value (so the activation function’s slope is 1). . . What is the slope needed to update the weight with the question mark? . . gradient = input(white) slope(purple) ReLU_slope(=1 here) . = 2_3_1 = 6 . 3. Building deep learning models with keras . 3.1 Creating a keras model . . Understanding your data . You will soon start building models in Keras to predict wages based on various professional and demographic factors. . Before you start building a model, it’s good to understand your data by performing some exploratory analysis. . df.head() wage_per_hour union education_yrs experience_yrs age female marr 0 5.10 0 8 21 35 1 1 1 4.95 0 9 42 57 1 1 2 6.67 0 12 1 19 0 0 3 4.00 0 12 4 22 0 0 4 7.50 0 12 17 35 0 1 south manufacturing construction 0 0 1 0 1 0 1 0 2 0 1 0 3 0 0 0 4 0 0 0 . Specifying a model . Now you’ll get to work with your first model in Keras, and will immediately be able to run more complex neural network models on larger datasets compared to the first two chapters. . To start, you’ll take the skeleton of a neural network and add a hidden layer and an output layer. You’ll then fit that model and see Keras do the optimization so your model continually gets better. . predictors[:3] array([[ 0, 8, 21, 35, 1, 1, 0, 1, 0], [ 0, 9, 42, 57, 1, 1, 0, 1, 0], [ 0, 12, 1, 19, 0, 0, 0, 1, 0]]) . # Import necessary modules import keras from keras.layers import Dense from keras.models import Sequential # Save the number of columns in predictors: n_cols n_cols = predictors.shape[1] # Set up the model: model model = Sequential() # Add the first layer model.add(Dense(50, activation=&#39;relu&#39;, input_shape=(n_cols,))) # Add the second layer model.add(Dense(32, activation=&#39;relu&#39;)) # Add the output layer model.add(Dense(1)) . Now that you’ve specified the model, the next step is to compile it. . 3.2 Compiling and fitting a model . Compiling the model . You’re now going to compile the model you specified earlier. To compile the model, you need to specify the optimizer and loss function to use. . The Adam optimizer is an excellent choice. You can read more about it as well as other keras optimizers here , and if you are really curious to learn more, you can read the original paper that introduced the Adam optimizer. . In this exercise, you’ll use the Adam optimizer and the mean squared error loss function. Go for it! . # Import necessary modules import keras from keras.layers import Dense from keras.models import Sequential # Specify the model n_cols = predictors.shape[1] model = Sequential() model.add(Dense(50, activation=&#39;relu&#39;, input_shape = (n_cols,))) model.add(Dense(32, activation=&#39;relu&#39;)) model.add(Dense(1)) # Compile the model model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;) # Verify that model contains information from compiling print(&quot;Loss function: &quot; + model.loss) # Loss function: mean_squared_error . Fitting the model . # Import necessary modules import keras from keras.layers import Dense from keras.models import Sequential # Specify the model n_cols = predictors.shape[1] model = Sequential() model.add(Dense(50, activation=&#39;relu&#39;, input_shape = (n_cols,))) model.add(Dense(32, activation=&#39;relu&#39;)) model.add(Dense(1)) # Compile the model model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;) # Fit the model model.fit(predictors, target) . Epoch 1/10 32/534 [&gt;.............................] - ETA: 1s - loss: 146.0927 534/534 [==============================] - 0s - loss: 78.1405 Epoch 2/10 32/534 [&gt;.............................] - ETA: 0s - loss: 85.0537 534/534 [==============================] - 0s - loss: 30.3265 Epoch 3/10 32/534 [&gt;.............................] - ETA: 0s - loss: 21.0463 534/534 [==============================] - 0s - loss: 27.0886 Epoch 4/10 32/534 [&gt;.............................] - ETA: 0s - loss: 16.8466 534/534 [==============================] - 0s - loss: 25.1240 Epoch 5/10 32/534 [&gt;.............................] - ETA: 0s - loss: 23.2123 534/534 [==============================] - 0s - loss: 24.0247 Epoch 6/10 32/534 [&gt;.............................] - ETA: 0s - loss: 13.3941 534/534 [==============================] - 0s - loss: 23.2055 Epoch 7/10 32/534 [&gt;.............................] - ETA: 0s - loss: 28.1707 534/534 [==============================] - 0s - loss: 22.4556 Epoch 8/10 32/534 [&gt;.............................] - ETA: 0s - loss: 11.3898 534/534 [==============================] - 0s - loss: 22.0805 Epoch 9/10 32/534 [&gt;.............................] - ETA: 0s - loss: 21.9370 480/534 [=========================&gt;....] - ETA: 0s - loss: 21.9982 534/534 [==============================] - 0s - loss: 21.7470 Epoch 10/10 32/534 [&gt;.............................] - ETA: 0s - loss: 5.4697 534/534 [==============================] - 0s - loss: 21.5538 &lt;keras.callbacks.History at 0x7f0fc2b49390&gt; . You now know how to specify, compile, and fit a deep learning model using keras! . 3.3 Classification models . Understanding your classification data . Now you will start modeling with a new dataset for a classification problem. This data includes information about passengers on the Titanic. . You will use predictors such as age , fare and where each passenger embarked from to predict who will survive. This data is from a tutorial on data science competitions . Look here for descriptions of the features. . df.head(3) survived pclass ... embarked_from_queenstown embarked_from_southampton 0 0 3 ... 0 1 1 1 1 ... 0 0 2 1 3 ... 0 1 [3 rows x 11 columns] df.columns Index([&#39;survived&#39;, &#39;pclass&#39;, &#39;age&#39;, &#39;sibsp&#39;, &#39;parch&#39;, &#39;fare&#39;, &#39;male&#39;, &#39;age_was_missing&#39;, &#39;embarked_from_cherbourg&#39;, &#39;embarked_from_queenstown&#39;, &#39;embarked_from_southampton&#39;], dtype=&#39;object&#39;) . Last steps in classification models . You’ll now create a classification model using the titanic dataset. . Here, you’ll use the &#39;sgd&#39; optimizer, which stands for Stochastic Gradient Descent . You’ll now create a classification model using the titanic dataset. . # Import necessary modules import keras from keras.layers import Dense from keras.models import Sequential from keras.utils import to_categorical # Convert the target to categorical: target target = to_categorical(df.survived) # Set up the model model = Sequential() # Add the first layer model.add(Dense(32, activation=&#39;relu&#39;, input_shape=(n_cols,))) # Add the output layer model.add(Dense(2, activation=&#39;softmax&#39;)) # Compile the model model.compile(optimizer=&#39;sgd&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) # Fit the model model.fit(predictors, target) . Epoch 1/10 32/891 [&gt;.............................] - ETA: 0s - loss: 7.6250 - acc: 0.2188 576/891 [==================&gt;...........] - ETA: 0s - loss: 2.6143 - acc: 0.6024 891/891 [==============================] - 0s - loss: 2.5170 - acc: 0.5948 ... Epoch 10/10 32/891 [&gt;.............................] - ETA: 0s - loss: 0.4892 - acc: 0.7500 736/891 [=======================&gt;......] - ETA: 0s - loss: 0.6318 - acc: 0.6807 891/891 [==============================] - 0s - loss: 0.6444 - acc: 0.6779 . This simple model is generating an accuracy of 68! . 3.4 Using models . . Making predictions . In this exercise, your predictions will be probabilities, which is the most common way for data scientists to communicate their predictions to colleagues. . # Specify, compile, and fit the model model = Sequential() model.add(Dense(32, activation=&#39;relu&#39;, input_shape = (n_cols,))) model.add(Dense(2, activation=&#39;softmax&#39;)) model.compile(optimizer=&#39;sgd&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.fit(predictors, target) # Calculate predictions: predictions predictions = model.predict(pred_data) # Calculate predicted probability of survival: predicted_prob_true predicted_prob_true = predictions[:,1] # print predicted_prob_true print(predicted_prob_true) . predicted_prob_true array([0.20054096, 0.3806974 , 0.6795431 , 0.45789802, 0.16493829, ... 0.12394663], dtype=float32) . You’re now ready to begin learning how to fine-tune your models. . 4. Fine-tuning keras models . 4.1 Understanding model optimization . Diagnosing optimization problems . All of the following could prevent a model from showing an improved loss in its first few epochs. . Learning rate too low. | Learning rate too high. | Poor choice of activation function. | . Changing optimization parameters . It’s time to get your hands dirty with optimization. You’ll now try optimizing a model at a very low learning rate, a very high learning rate, and a “just right” learning rate. . You’ll want to look at the results after running this exercise, remembering that a low value for the loss function is good. . For these exercises, we’ve pre-loaded the predictors and target values from your previous classification models (predicting who would survive on the Titanic). . You’ll want the optimization to start from scratch every time you change the learning rate, to give a fair comparison of how each learning rate did in your results. So we have created a function get_new_model() that creates an unoptimized model to optimize. . # Import the SGD optimizer from keras.optimizers import SGD # Create list of learning rates: lr_to_test lr_to_test = [0.000001, 0.01, 1] # Loop over learning rates for lr in lr_to_test: print(&#39; n nTesting model with learning rate: %f n&#39;%lr ) # Build new model to test, unaffected by previous models model = get_new_model() # Create SGD optimizer with specified learning rate: my_optimizer my_optimizer = SGD(lr=lr) # Compile the model model.compile(optimizer=my_optimizer, loss=&#39;categorical_crossentropy&#39;) # Fit the model model.fit(predictors, target) . Testing model with learning rate: 0.000001 Epoch 1/10 32/891 [&gt;.............................] - ETA: 1s - loss: 3.6053 640/891 [====================&gt;.........] - ETA: 0s - loss: 1.9211 891/891 [==============================] - 0s - loss: 1.6579 ... Epoch 10/10 32/891 [&gt;.............................] - ETA: 0s - loss: 0.5917 672/891 [=====================&gt;........] - ETA: 0s - loss: 0.5966 891/891 [==============================] - 0s - loss: 0.6034 Testing model with learning rate: 0.010000 Epoch 1/10 32/891 [&gt;.............................] - ETA: 1s - loss: 1.0910 576/891 [==================&gt;...........] - ETA: 0s - loss: 1.8064 891/891 [==============================] - 0s - loss: 1.4091 ... Epoch 10/10 32/891 [&gt;.............................] - ETA: 0s - loss: 0.6419 672/891 [=====================&gt;........] - ETA: 0s - loss: 0.5787 891/891 [==============================] - 0s - loss: 0.5823 Testing model with learning rate: 1.000000 Epoch 1/10 32/891 [&gt;.............................] - ETA: 1s - loss: 1.0273 608/891 [===================&gt;..........] - ETA: 0s - loss: 1.9649 891/891 [==============================] - 0s - loss: 1.8966 ... Epoch 10/10 32/891 [&gt;.............................] - ETA: 0s - loss: 0.7226 672/891 [=====================&gt;........] - ETA: 0s - loss: 0.6031 891/891 [==============================] - 0s - loss: 0.6060 . 4.2 Model validation . Evaluating model accuracy on validation dataset . Now it’s your turn to monitor model accuracy with a validation data set. A model definition has been provided as model . Your job is to add the code to compile it and then fit it. You’ll check the validation score in each epoch. . # Save the number of columns in predictors: n_cols n_cols = predictors.shape[1] input_shape = (n_cols,) # Specify the model model = Sequential() model.add(Dense(100, activation=&#39;relu&#39;, input_shape = input_shape)) model.add(Dense(100, activation=&#39;relu&#39;)) model.add(Dense(2, activation=&#39;softmax&#39;)) # Compile the model model.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) # Fit the model hist = model.fit(predictors, target, validation_split=0.3) . Train on 623 samples, validate on 268 samples Epoch 1/10 32/623 [&gt;.............................] - ETA: 0s - loss: 3.3028 - acc: 0.4062 608/623 [============================&gt;.] - ETA: 0s - loss: 1.3320 - acc: 0.5938 623/623 [==============================] - 0s - loss: 1.3096 - acc: 0.6003 - val_loss: 0.6805 - val_acc: 0.7201 ... Epoch 10/10 32/623 [&gt;.............................] - ETA: 0s - loss: 0.4873 - acc: 0.7812 320/623 [==============&gt;...............] - ETA: 0s - loss: 0.5953 - acc: 0.7063 623/623 [==============================] - 0s - loss: 0.6169 - acc: 0.6870 - val_loss: 0.5339 - val_acc: 0.7351 . Early stopping: Optimizing the optimization . Now that you know how to monitor your model performance throughout optimization, you can use early stopping to stop optimization when it isn’t helping any more. Since the optimization stops automatically when it isn’t helping, you can also set a high value for epochs in your call to .fit() . . # Import EarlyStopping from keras.callbacks import EarlyStopping # Save the number of columns in predictors: n_cols n_cols = predictors.shape[1] input_shape = (n_cols,) # Specify the model model = Sequential() model.add(Dense(100, activation=&#39;relu&#39;, input_shape = input_shape)) model.add(Dense(100, activation=&#39;relu&#39;)) model.add(Dense(2, activation=&#39;softmax&#39;)) # Compile the model model.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) # Define early_stopping_monitor early_stopping_monitor = EarlyStopping(patience=2) # Fit the model model.fit(predictors, target, epochs=30, validation_split=0.3, callbacks=[early_stopping_monitor]) . Train on 623 samples, validate on 268 samples Epoch 1/30 32/623 [&gt;.............................] - ETA: 0s - loss: 5.6563 - acc: 0.4688 608/623 [============================&gt;.] - ETA: 0s - loss: 1.6536 - acc: 0.5609 623/623 [==============================] - 0s - loss: 1.6406 - acc: 0.5650 - val_loss: 1.0856 - val_acc: 0.6567 ... Epoch 6/30 32/623 [&gt;.............................] - ETA: 0s - loss: 0.4607 - acc: 0.7812 608/623 [============================&gt;.] - ETA: 0s - loss: 0.6208 - acc: 0.7007 623/623 [==============================] - 0s - loss: 0.6231 - acc: 0.6982 - val_loss: 0.6149 - val_acc: 0.6828 Epoch 7/30 32/623 [&gt;.............................] - ETA: 0s - loss: 0.6697 - acc: 0.6875 608/623 [============================&gt;.] - ETA: 0s - loss: 0.6483 - acc: 0.7072 623/623 [==============================] - 0s - loss: 0.6488 - acc: 0.7063 - val_loss: 0.7276 - val_acc: 0.6493 . Because optimization will automatically stop when it is no longer helpful, it is okay to specify the maximum number of epochs as 30 rather than using the default of 10 that you’ve used so far. Here, it seems like the optimization stopped after 7 epochs. . Experimenting with wider networks . Now you know everything you need to begin experimenting with different models! . A model called model_1 has been pre-loaded. You can see a summary of this model printed in the IPython Shell. This is a relatively small network, with only 10 units in each hidden layer. . In this exercise you’ll create a new model called model_2 which is similar to model_1 , except it has 100 units in each hidden layer. . # Define early_stopping_monitor early_stopping_monitor = EarlyStopping(patience=2) # Create the new model: model_2 model_2 = Sequential() # Add the first and second layers model_2.add(Dense(100, activation=&#39;relu&#39;, input_shape=input_shape)) model_2.add(Dense(100, activation=&#39;relu&#39;)) # Add the output layer model_2.add(Dense(2, activation=&#39;softmax&#39;)) # Compile model_2 model_2.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) # Fit model_1 model_1_training = model_1.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False) # Fit model_2 model_2_training = model_2.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False) # Create the plot plt.plot(model_1_training.history[&#39;val_loss&#39;], &#39;r&#39;, model_2_training.history[&#39;val_loss&#39;], &#39;b&#39;) plt.xlabel(&#39;Epochs&#39;) plt.ylabel(&#39;Validation score&#39;) plt.show() . model_1.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_1 (Dense) (None, 10) 110 _________________________________________________________________ dense_2 (Dense) (None, 10) 110 _________________________________________________________________ dense_3 (Dense) (None, 2) 22 ================================================================= Total params: 242.0 Trainable params: 242 Non-trainable params: 0.0 _________________________________________________________________ model_2.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_4 (Dense) (None, 100) 1100 _________________________________________________________________ dense_5 (Dense) (None, 100) 10100 _________________________________________________________________ dense_6 (Dense) (None, 2) 202 ================================================================= Total params: 11,402.0 Trainable params: 11,402 Non-trainable params: 0.0 _________________________________________________________________ . . The blue model is the one you made, the red is the original model. Your model had a lower loss value, so it is the better model. . Adding layers to a network . You’ve seen how to experiment with wider networks. In this exercise, you’ll try a deeper network (more hidden layers). . # The input shape to use in the first hidden layer input_shape = (n_cols,) # Create the new model: model_2 model_2 = Sequential() # Add the first, second, and third hidden layers model_2.add(Dense(50, activation=&#39;relu&#39;, input_shape=input_shape)) model_2.add(Dense(50, activation=&#39;relu&#39;)) model_2.add(Dense(50, activation=&#39;relu&#39;)) # Add the output layer model_2.add(Dense(2, activation=&#39;softmax&#39;)) # Compile model_2 model_2.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) # Fit model 1 model_1_training = model_1.fit(predictors, target, epochs=20, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=False) # Fit model 2 model_2_training = model_2.fit(predictors, target, epochs=20, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=False) # Create the plot plt.plot(model_1_training.history[&#39;val_loss&#39;], &#39;r&#39;, model_2_training.history[&#39;val_loss&#39;], &#39;b&#39;) plt.xlabel(&#39;Epochs&#39;) plt.ylabel(&#39;Validation score&#39;) plt.show() . model_1.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_1 (Dense) (None, 50) 550 _________________________________________________________________ dense_2 (Dense) (None, 2) 102 ================================================================= Total params: 652.0 Trainable params: 652 Non-trainable params: 0.0 _________________________________________________________________ model_2.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_3 (Dense) (None, 50) 550 _________________________________________________________________ dense_4 (Dense) (None, 50) 2550 _________________________________________________________________ dense_5 (Dense) (None, 50) 2550 _________________________________________________________________ dense_6 (Dense) (None, 2) 102 ================================================================= Total params: 5,752.0 Trainable params: 5,752 Non-trainable params: 0.0 _________________________________________________________________ . . 4.3 Thinking about model capacity . . . . Experimenting with model structures . You’ve just run an experiment where you compared two networks that were identical except that the 2nd network had an extra hidden layer. . You see that this 2nd network (the deeper network) had better performance. Given that, How to get an even better performance? . Increasing the number of units in each hidden layer would be a good next step to try achieving even better performance. . 4.4 Stepping up to images . . Building your own digit recognition model . You’ve reached the final exercise of the course – you now know everything you need to build an accurate model to recognize handwritten digits! . To add an extra challenge, we’ve loaded only 2500 images, rather than 60000 which you will see in some published results. Deep learning models perform better with more data, however, they also take longer to train, especially when they start becoming more complex. . If you have a computer with a CUDA compatible GPU, you can take advantage of it to improve computation time. If you don’t have a GPU, no problem! You can set up a deep learning environment in the cloud that can run your models on a GPU. Here is a blog post by Dan that explains how to do this – check it out after completing this exercise! It is a great next step as you continue your deep learning journey. . Ready to take your deep learning to the next level? Check out Advanced Deep Learning with Keras in Python to see how the Keras functional API lets you build domain knowledge to solve new types of problems. Once you know how to use the functional API, take a look at “Convolutional Neural Networks for Image Processing” to learn image-specific applications of Keras. . # feature of 28 * 28 = 784 image of a handwriting digit image. # each value is a number between 0 ~ 255, stands for the darkness of that pixel X array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], dtype=float32) X.shape (2500, 784) # target: 0 ~ 9 y array([[0., 0., 0., ..., 0., 0., 0.], [0., 1., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 1., 0.], [0., 1., 0., ..., 0., 0., 0.]]) y.shape (2500, 10) . # Create the model: model model = Sequential() # Add the first hidden layer model.add(Dense(50, activation=&#39;relu&#39;, input_shape=(X.shape[1],))) # Add the second hidden layer model.add(Dense(50, activation=&#39;relu&#39;)) # Add the output layer model.add(Dense(y.shape[1], activation=&#39;softmax&#39;)) # Compile the model model.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) # Fit the model model.fit(X, y, validation_split=0.3) . Train on 1750 samples, validate on 750 samples Epoch 1/10 32/1750 [..............................] - ETA: 3s - loss: 2.1979 - acc: 0.2188 480/1750 [=======&gt;......................] - ETA: 0s - loss: 2.1655 - acc: 0.2333 960/1750 [===============&gt;..............] - ETA: 0s - loss: 1.9699 - acc: 0.3354 1440/1750 [=======================&gt;......] - ETA: 0s - loss: 1.7895 - acc: 0.4153 1750/1750 [==============================] - 0s - loss: 1.6672 - acc: 0.4737 - val_loss: 1.0023 - val_acc: 0.7707 ... Epoch 10/10 32/1750 [..............................] - ETA: 0s - loss: 0.1482 - acc: 1.0000 480/1750 [=======&gt;......................] - ETA: 0s - loss: 0.1109 - acc: 0.9792 960/1750 [===============&gt;..............] - ETA: 0s - loss: 0.1046 - acc: 0.9812 1440/1750 [=======================&gt;......] - ETA: 0s - loss: 0.1028 - acc: 0.9812 1696/1750 [============================&gt;.] - ETA: 0s - loss: 0.1014 - acc: 0.9817 1750/1750 [==============================] - 0s - loss: 0.0999 - acc: 0.9823 - val_loss: 0.3186 - val_acc: 0.9053 . You’ve done something pretty amazing. You should see better than 90% accuracy recognizing handwritten digits, even while using a small training set of only 1750 images! . The End. . Thank you for reading. .",
            "url": "https://islamalam.github.io/blog/2021/07/05/introduction-to-deep-learning-in-python.html",
            "relUrl": "/2021/07/05/introduction-to-deep-learning-in-python.html",
            "date": " • Jul 5, 2021"
        }
        
    
  
    
        ,"post45": {
            "title": "Feature Engineering For Nlp In Python",
            "content": "Feature Engineering for NLP in Python . This is the memo of the 13th course (23 courses in all) of ‘Machine Learning Scientist with Python’ skill track. . You can find the original course HERE . . #### . Course Description . In this course, you will learn techniques that will allow you to extract useful information from text and process them into a format suitable for applying ML models. More specifically, you will learn about POS tagging, named entity recognition, readability scores, the n-gram and tf-idf models, and how to implement them using scikit-learn and spaCy. You will also learn to compute how similar two documents are to each other. In the process, you will predict the sentiment of movie reviews and build movie and Ted Talk recommenders. Following the course, you will be able to engineer critical features out of any text and solve some of the most challenging problems in data science! . #### . . Basic features and readability scores | Text preprocessing, POS tagging and NER | N-Gram models | TF-IDF and similarity scores | . 1. Basic features and readability scores . 1.1 Introduction to NLP feature engineering . | | | | | | | | | | . 1.1.1 Data format for ML algorithms . In this exercise, you have been given four dataframes df1 , df2 , df3 and df4 . The final column of each dataframe is the predictor variable and the rest of the columns are training features. . Using the console, determine which dataframe is in a suitable format to be trained by a classifier. . df3 feature 1 feature 2 feature 3 feature 4 feature 5 label 0 1 85 66 29 0 0 1 8 183 64 0 0 1 2 1 89 66 23 94 0 3 0 137 40 35 168 1 4 5 116 74 0 0 0 ... . 1.1.2 One-hot encoding . In the previous exercise, we encountered a dataframe df1 which contained categorical features and therefore, was unsuitable for applying ML algorithms to. . In this exercise, your task is to convert df1 into a format that is suitable for machine learning. . # Print the features of df1 print(df1.columns) # Perform one-hot encoding df1 = pd.get_dummies(df1, columns=[&#39;feature 5&#39;]) # Print the new features of df1 print(df1.columns) # Print first five rows of df1 print(df1.head()) . Index([&#39;feature 1&#39;, &#39;feature 2&#39;, &#39;feature 3&#39;, &#39;feature 4&#39;, &#39;feature 5&#39;, &#39;label&#39;], dtype=&#39;object&#39;) Index([&#39;feature 1&#39;, &#39;feature 2&#39;, &#39;feature 3&#39;, &#39;feature 4&#39;, &#39;label&#39;, &#39;feature 5_female&#39;, &#39;feature 5_male&#39;], dtype=&#39;object&#39;) feature 1 feature 2 feature 3 feature 4 label feature 5_female feature 5_male 0 29.0000 0 0 211.3375 1 1 0 1 0.9167 1 2 151.5500 1 0 1 2 2.0000 1 2 151.5500 0 1 0 3 30.0000 1 2 151.5500 0 0 1 4 25.0000 1 2 151.5500 0 1 0 . 1.2 Basic feature extraction . | | | | | | | . 1.2.1 Character count of Russian tweets . In this exercise, you have been given a dataframe tweets which contains some tweets associated with Russia’s Internet Research Agency and compiled by FiveThirtyEight. . Your task is to create a new feature ‘char_count’ in tweets which computes the number of characters for each tweet. Also, compute the average length of each tweet. The tweets are available in the content feature of tweets . . Be aware that this is real data from Twitter and as such there is always a risk that it may contain profanity or other offensive content (in this exercise, and any following exercises that also use real Twitter data). . tweets content 0 LIVE STREAM VIDEO=&gt; Donald Trump Rallies in Co... 1 Muslim Attacks NYPD Cops with Meat Cleaver. Me... 2 .@vfpatlas well that&#39;s a swella word there (di... ... . # Create a feature char_count tweets[&#39;char_count&#39;] = tweets[&#39;content&#39;].apply(len) # Print the average character count print(tweets[&#39;char_count&#39;].mean()) # 103.462 . Great job! Notice that the average character count of these tweets is approximately 104, which is much higher than the overall average tweet length of around 40 characters. Depending on what you’re working on, this may be something worth investigating into. For your information, there is research that indicates that fake news articles tend to have longer titles! Therefore, even extremely basic features such as character counts can prove to be very useful in certain applications. . 1.2.2 Word count of TED talks . ted is a dataframe that contains the transcripts of 500 TED talks. Your job is to compute a new feature word_count which contains the approximate number of words for each talk. Consequently, you also need to compute the average word count of the talks. The transcripts are available as the transcript feature in ted . . In order to complete this task, you will need to define a function count_words that takes in a string as an argument and returns the number of words in the string. You will then need to apply this function to the transcript feature of ted to create the new feature word_count and compute its mean. . # Function that returns number of words in a string def count_words(string): # Split the string into words words = string.split() # Return the number of words return len(words) # Create a new feature word_count ted[&#39;word_count&#39;] = ted[&#39;transcript&#39;].apply(count_words) # Print the average word count of the talks print(ted[&#39;word_count&#39;].mean()) # 1987.1 . Amazing work! You now know how to compute the number of words in a given piece of text. Also, notice that the average length of a talk is close to 2000 words. You can use the word_count feature to compute its correlation with other variables such as number of views, number of comments, etc. and derive extremely interesting insights about TED. . 1.2.3 Hashtags and mentions in Russian tweets . Let’s revisit the tweets dataframe containing the Russian tweets. In this exercise, you will compute the number of hashtags and mentions in each tweet by defining two functions count_hashtags() and count_mentions() respectively and applying them to the content feature of tweets . . In case you don’t recall, the tweets are contained in the content feature of tweets . . # Function that returns numner of hashtags in a string def count_hashtags(string): # Split the string into words words = string.split() # Create a list of words that are hashtags hashtags = [word for word in words if word.startswith(&#39;#&#39;)] # Return number of hashtags return(len(hashtags)) # Create a feature hashtag_count and display distribution tweets[&#39;hashtag_count&#39;] = tweets[&#39;content&#39;].apply(count_hashtags) tweets[&#39;hashtag_count&#39;].hist() plt.title(&#39;Hashtag count distribution&#39;) plt.show() . . # Function that returns number of mentions in a string def count_mentions(string): # Split the string into words words = string.split() # Create a list of words that are mentions mentions = [word for word in words if word.startswith(&#39;@&#39;)] # Return number of mentions return(len(mentions)) # Create a feature mention_count and display distribution tweets[&#39;mention_count&#39;] = tweets[&#39;content&#39;].apply(count_mentions) tweets[&#39;mention_count&#39;].hist() plt.title(&#39;Mention count distribution&#39;) plt.show() . . Excellent work! You now have a good grasp of how to compute various types of summary features. In the next lesson, we will learn about more advanced features that are capable of capturing more nuanced information beyond simple word and character counts. . 1.3 Readability tests . | | | | | | | . 1.3.1 Readability of ‘The Myth of Sisyphus’ . In this exercise, you will compute the Flesch reading ease score for Albert Camus’ famous essay The Myth of Sisyphus . We will then interpret the value of this score as explained in the video and try to determine the reading level of the essay. . The entire essay is in the form of a string and is available as sisyphus_essay . . # Import Textatistic from textatistic import Textatistic # Compute the readability scores readability_scores = Textatistic(sisyphus_essay).scores # Print the flesch reading ease score flesch = readability_scores[&#39;flesch_score&#39;] print(&quot;The Flesch Reading Ease is %.2f&quot; % (flesch)) # The Flesch Reading Ease is 81.67 . Excellent! You now know to compute the Flesch reading ease score for a given body of text. Notice that the score for this essay is approximately 81.67. This indicates that the essay is at the readability level of a 6th grade American student. . 1.3.2 Readability of various publications . In this exercise, you have been given excerpts of articles from four publications. Your task is to compute the readability of these excerpts using the Gunning fog index and consequently, determine the relative difficulty of reading these publications. . The excerpts are available as the following strings: . forbes . – An excerpt from an article from . Forbes . magazine on the Chinese social credit score system. . | harvard_law . – An excerpt from a book review published in . Harvard Law Review . . . | r_digest . – An excerpt from a . Reader’s Digest . article on flight turbulence. . | time_kids . – An excerpt from an article on the ill effects of salt consumption published in . TIME for Kids . . . | . # Import Textatistic from textatistic import Textatistic # List of excerpts excerpts = [forbes, harvard_law, r_digest, time_kids] # Loop through excerpts and compute gunning fog index gunning_fog_scores = [] for excerpt in excerpts: readability_scores = Textatistic(excerpt).scores gunning_fog = readability_scores[&#39;gunningfog_score&#39;] gunning_fog_scores.append(gunning_fog) # Print the gunning fog indices print(gunning_fog_scores) # [14.436002482929858, 20.735401069518716, 11.085587583148559, 5.926785009861934] . Great job! You are now adept at computing readability scores for various pieces of text. Notice that the Harvard Law Review excerpt has the highest Gunning fog index; indicating that it can be comprehended only by readers who have graduated college. On the other hand, the Time for Kids article, intended for children, has a much lower fog index and can be comprehended by 5th grade students. . 2. Text preprocessing, POS tagging and NER . 2.1 Tokenization and Lemmatization . | | | | | | | . 2.1.1 Tokenizing the Gettysburg Address . In this exercise, you will be tokenizing one of the most famous speeches of all time: the Gettysburg Address delivered by American President Abraham Lincoln during the American Civil War. . The entire speech is available as a string named gettysburg . . gettysburg &quot;Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. ... . import spacy # Load the en_core_web_sm model nlp = spacy.load(&#39;en_core_web_sm&#39;) # Create a Doc object doc = nlp(gettysburg) # Generate the tokens tokens = [token.text for token in doc] print(tokens) . [&#39;Four&#39;, &#39;score&#39;, &#39;and&#39;, &#39;seven&#39;, &#39;years&#39;, &#39;ago&#39;, &#39;our&#39;, &#39;fathers&#39;, &#39;brought&#39;, &#39;forth&#39;, &#39;on&#39;, &#39;this&#39;, &#39;continent&#39;, &#39;,&#39;, &#39;a&#39;, &#39;new&#39;, &#39;nation&#39;, ... . Excellent work! You now know how to tokenize a piece of text. In the next exercise, we will perform similar steps and conduct lemmatization. . 2.1.2 Lemmatizing the Gettysburg address . In this exercise, we will perform lemmatization on the same gettysburg address from before. . However, this time, we will also take a look at the speech, before and after lemmatization, and try to adjudge the kind of changes that take place to make the piece more machine friendly. . import spacy # Load the en_core_web_sm model nlp = spacy.load(&#39;en_core_web_sm&#39;) # Create a Doc object doc = nlp(gettysburg) # Generate lemmas lemmas = [token.lemma_ for token in doc] # Convert lemmas into a string print(&#39; &#39;.join(lemmas)) . four score and seven year ago -PRON- father bring forth on this continent , a new nation , conceive in liberty , and dedicate to the proposition that all man be create equal . . Excellent! You’re now proficient at performing lemmatization using spaCy. Observe the lemmatized version of the speech. It isn’t very readable to humans but it is in a much more convenient format for a machine to process. . 2.2 Text cleaning . | | | | | | | | | | . 2.2.1 Cleaning a blog post . In this exercise, you have been given an excerpt from a blog post. Your task is to clean this text into a more machine friendly format. This will involve converting to lowercase, lemmatization and removing stopwords, punctuations and non-alphabetic characters. . The excerpt is available as a string blog and has been printed to the console. The list of stopwords are available as stopwords . . Twenty-first-century politics has witnessed an alarming rise of populism in the U.S. and Europe. The first warning signs came with the UK Brexit Referendum vote in 2016 swinging in the way of Leave. This was followed by a stupendous victory by billionaire Donald Trump to become the 45th President of the United States in November 2016. Since then, Europe has seen a steady rise in populist and far-right parties that have capitalized on Europe’s Immigration Crisis to raise nationalist and anti-Europe sentiments. Some instances include Alternative for Germany (AfD) winning 12.6% of all seats and entering the Bundestag, thus upsetting Germany’s political order for the first time since the Second World War, the success of the Five Star Movement in Italy and the surge in popularity of neo-nazism and neo-fascism in countries such as Hungary, Czech Republic, Poland and Austria. . # Load model and create Doc object nlp = spacy.load(&#39;en_core_web_sm&#39;) doc = nlp(blog) # Generate lemmatized tokens lemmas = [token.lemma_ for token in doc] # Remove stopwords and non-alphabetic tokens a_lemmas = [lemma for lemma in lemmas if lemma.isalpha() and lemma not in stopwords] # Print string after text cleaning print(&#39; &#39;.join(a_lemmas)) . century politic witness alarming rise populism europe warning sign come uk brexit referendum vote swinging way leave follow stupendous victory billionaire donald trump president united states november europe steady rise populist far right party capitalize europe immigration crisis raise nationalist anti europe sentiment instance include alternative germany afd win seat enter bundestag upset germany political order time second world war success star movement italy surge popularity neo nazism neo fascism country hungary czech republic poland austria . Great job! Take a look at the cleaned text; it is lowercased and devoid of numbers, punctuations and commonly used stopwords. Also, note that the word U.S. was present in the original text. Since it had periods in between, our text cleaning process completely removed it. This may not be ideal behavior. It is always advisable to use your custom functions in place of isalpha() for more nuanced cases. . 2.2.2 Cleaning TED talks in a dataframe . In this exercise, we will revisit the TED Talks from the first chapter. You have been a given a dataframe ted consisting of 5 TED Talks. Your task is to clean these talks using techniques discussed earlier by writing a function preprocess and applying it to the transcript feature of the dataframe. . The stopwords list is available as stopwords . . # Function to preprocess text def preprocess(text): # Create Doc object doc = nlp(text, disable=[&#39;ner&#39;, &#39;parser&#39;]) # Generate lemmas lemmas = [token.lemma_ for token in doc] # Remove stopwords and non-alphabetic characters a_lemmas = [lemma for lemma in lemmas if lemma.isalpha() and lemma not in stopwords] return &#39; &#39;.join(a_lemmas) # Apply preprocess to ted[&#39;transcript&#39;] ted[&#39;transcript&#39;] = ted[&#39;transcript&#39;].apply(preprocess) print(ted[&#39;transcript&#39;]) . 0 talk new lecture ted illusion create ted try r... 1 representation brain brain break left half log... 2 great honor today share digital universe creat... ... . Excellent job! You have preprocessed all the TED talk transcripts contained in ted and it is now in a good shape to perform operations such as vectorization (as we will soon see how). You now have a good understanding of how text preprocessing works and why it is important. In the next lessons, we will move on to generating word level features for our texts. . 2.3 Part-of-speech(POS) tagging . | | | . 2.3.1 POS tagging in Lord of the Flies . In this exercise, you will perform part-of-speech tagging on a famous passage from one of the most well-known novels of all time, Lord of the Flies , authored by William Golding. . The passage is available as lotf and has already been printed to the console. . He found himself understanding the wearisomeness of this life, where every path was an improvisation and a considerable part of one’s waking life was spent watching one’s feet. . # Load the en_core_web_sm model nlp = spacy.load(&#39;en_core_web_sm&#39;) # Create a Doc object doc = nlp(lotf) # Generate tokens and pos tags pos = [(token.text, token.pos_) for token in doc] print(pos) . [(&#39;He&#39;, &#39;PRON&#39;), (&#39;found&#39;, &#39;VERB&#39;), (&#39;himself&#39;, &#39;PRON&#39;), (&#39;understanding&#39;, &#39;VERB&#39;), (&#39;the&#39;, &#39;DET&#39;), (&#39;wearisomeness&#39;, &#39;NOUN&#39;), (&#39;of&#39;, &#39;ADP&#39;), (&#39;this&#39;, &#39;DET&#39;), (&#39;life&#39;, &#39;NOUN&#39;), (&#39;,&#39;, &#39;PUNCT&#39;), (&#39;where&#39;, &#39;ADV&#39;), (&#39;every&#39;, &#39;DET&#39;), (&#39;path&#39;, &#39;NOUN&#39;), (&#39;was&#39;, &#39;VERB&#39;), (&#39;an&#39;, &#39;DET&#39;), (&#39;improvisation&#39;, &#39;NOUN&#39;), (&#39;and&#39;, &#39;CCONJ&#39;), (&#39;a&#39;, &#39;DET&#39;), (&#39;considerable&#39;, &#39;ADJ&#39;), (&#39;part&#39;, &#39;NOUN&#39;), (&#39;of&#39;, &#39;ADP&#39;), (&#39;one&#39;, &#39;NUM&#39;), (&#39;’s&#39;, &#39;PART&#39;), (&#39;waking&#39;, &#39;NOUN&#39;), (&#39;life&#39;, &#39;NOUN&#39;), (&#39;was&#39;, &#39;VERB&#39;), (&#39;spent&#39;, &#39;VERB&#39;), (&#39;watching&#39;, &#39;VERB&#39;), (&#39;one&#39;, &#39;PRON&#39;), (&#39;’s&#39;, &#39;PART&#39;), (&#39;feet&#39;, &#39;NOUN&#39;), (&#39;.&#39;, &#39;PUNCT&#39;)] . Good job! Examine the various POS tags attached to each token and evaluate if they make intuitive sense to you. You will notice that they are indeed labelled correctly according to the standard rules of English grammar. . 2.3.2 Counting nouns in a piece of text . In this exercise, we will write two functions, nouns() and proper_nouns() that will count the number of other nouns and proper nouns in a piece of text respectively. . These functions will take in a piece of text and generate a list containing the POS tags for each word. It will then return the number of proper nouns/other nouns that the text contains. We will use these functions in the next exercise to generate interesting insights about fake news. . The en_core_web_sm model has already been loaded as nlp in this exercise. . nlp = spacy.load(&#39;en_core_web_sm&#39;) # Returns number of proper nouns def proper_nouns(text, model=nlp): # Create doc object doc = model(text) # Generate list of POS tags pos = [token.pos_ for token in doc] # Return number of proper nouns return pos.count(&#39;PROPN&#39;) print(proper_nouns(&quot;Abdul, Bill and Cathy went to the market to buy apples.&quot;, nlp)) # 3 . nlp = spacy.load(&#39;en_core_web_sm&#39;) # Returns number of other nouns def nouns(text, model=nlp): # Create doc object doc = model(text) # Generate list of POS tags pos = [token.pos_ for token in doc] # Return number of other nouns return pos.count(&#39;NOUN&#39;) print(nouns(&quot;Abdul, Bill and Cathy went to the market to buy apples.&quot;, nlp)) # 2 . Great job! You now know how to write functions that compute the number of instances of a particulat POS tag in a given piece of text. In the next exercise, we will use these functions to generate features from text in a dataframe. . 2.3.3 Noun usage in fake news . In this exercise, you have been given a dataframe headlines that contains news headlines that are either fake or real. Your task is to generate two new features num_propn and num_noun that represent the number of proper nouns and other nouns contained in the title feature of headlines . . Next, we will compute the mean number of proper nouns and other nouns used in fake and real news headlines and compare the values. If there is a remarkable difference, then there is a good chance that using the num_propn and num_noun features in fake news detectors will improve its performance. . To accomplish this task, the functions proper_nouns and nouns that you had built in the previous exercise have already been made available to you. . headlines Unnamed: 0 title label 0 0 You Can Smell Hillary’s Fear FAKE 1 1 Watch The Exact Moment Paul Ryan Committed Pol... FAKE 2 2 Kerry to go to Paris in gesture of sympathy REAL 3 3 Bernie supporters on Twitter erupt in anger ag... FAKE 4 4 The Battle of New York: Why This Primary Matters REAL . headlines[&#39;num_propn&#39;] = headlines[&#39;title&#39;].apply(proper_nouns) headlines[&#39;num_noun&#39;] = headlines[&#39;title&#39;].apply(nouns) # Compute mean of proper nouns real_propn = headlines[headlines[&#39;label&#39;] == &#39;REAL&#39;][&#39;num_propn&#39;].mean() fake_propn = headlines[headlines[&#39;label&#39;] == &#39;FAKE&#39;][&#39;num_propn&#39;].mean() # Compute mean of other nouns real_noun = headlines[headlines[&#39;label&#39;] == &#39;REAL&#39;][&#39;num_noun&#39;].mean() fake_noun = headlines[headlines[&#39;label&#39;] == &#39;FAKE&#39;][&#39;num_noun&#39;].mean() # Print results print(&quot;Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively&quot;%(real_propn, fake_propn)) print(&quot;Mean no. of other nouns in real and fake headlines are %.2f and %.2f respectively&quot;%(real_noun, fake_noun)) . Mean no. of proper nouns in real and fake headlines are 2.46 and 4.86 respectively Mean no. of other nouns in real and fake headlines are 2.30 and 1.44 respectively . Excellent work! You now know to construct features using POS tags information. Notice how the mean number of proper nouns is considerably higher for fake news than it is for real news. The opposite seems to be true in the case of other nouns. This fact can be put to great use in desgning fake news detectors. . 2.4 Named entity recognition(NER) . | | | | | . 2.4.1 Named entities in a sentence . In this exercise, we will identify and classify the labels of various named entities in a body of text using one of spaCy’s statistical models. We will also verify the veracity of these labels. . # Load the required model nlp = spacy.load(&#39;en_core_web_sm&#39;) # Create a Doc instance text = &#39;Sundar Pichai is the CEO of Google. Its headquarters is in Mountain View.&#39; doc = nlp(text) # Print all named entities and their labels for ent in doc.ents: print(ent.text, ent.label_) . Sundar Pichai ORG Google ORG Mountain View GPE . Good job! Notice how the model correctly predicted the labels of Google and Mountain View but mislabeled Sundar Pichai as an organization. As discussed in the video, the predictions of the model depend strongly on the data it is trained on. It is possible to train spaCy models on your custom data. You will learn to do this in more advanced NLP courses. . 2.4.2 Identifying people mentioned in a news article . In this exercise, you have been given an excerpt from a news article published in TechCrunch . Your task is to write a function find_people that identifies the names of people that have been mentioned in a particular piece of text. You will then use find_people to identify the people of interest in the article. . The article is available as the string tc and has been printed to the console. The required spacy model has also been already loaded as nlp . . def find_persons(text): # Create Doc object doc = nlp(text) # Identify the persons persons = [ent.text for ent in doc.ents if ent.label_ == &#39;PERSON&#39;] # Return persons return persons print(find_persons(tc)) # [&#39;Sheryl Sandberg&#39;, &#39;Mark Zuckerberg&#39;] . Excellent work! The article was related to Facebook and our function correctly identified both the people mentioned. You can now see how NER could be used in a variety of applications. Publishers may use a technique like this to classify news articles by the people mentioned in them. A question answering system could also use something like this to answer questions such as ‘Who are the people mentioned in this passage?’. With this, we come to an end of this chapter. In the next, we will learn how to conduct vectorization on documents. . 3. N-Gram models . 3.1 Building a bag of words model . | | | | | | | . 3.1.1 Word vectors with a given vocabulary . You have been given a corpus of documents and you have computed the vocabulary of the corpus to be the following: V : a, an, and, but, can, come, evening, forever, go, i, men, may, on, the, women . Which of the following corresponds to the bag of words vector for the document “men may come and men may go but i go on forever”? . (0, 0, 1, 1, 0, 1, 0, 1, 2, 1, 2, 2, 1, 0, 0) . Good job! That is, indeed, the correct answer. Each value in the vector corresponds to the frequency of the corresponding word in the vocabulary. . 3.1.2 BoW model for movie taglines . In this exercise, you have been provided with a corpus of more than 7000 movie tag lines. Your job is to generate the bag of words representation bow_matrix for these taglines. For this exercise, we will ignore the text preprocessing step and generate bow_matrix directly. . We will also investigate the shape of the resultant bow_matrix . The first five taglines in corpus have been printed to the console for you to examine. . corpus.shape (7033,) . # Import CountVectorizer from sklearn.feature_extraction.text import CountVectorizer # Create CountVectorizer object vectorizer = CountVectorizer() # Generate matrix of word vectors bow_matrix = vectorizer.fit_transform(corpus) # Print the shape of bow_matrix print(bow_matrix.shape) # (7033, 6614) . Excellent! You now know how to generate a bag of words representation for a given corpus of documents. Notice that the word vectors created have more than 6600 dimensions. However, most of these dimensions have a value of zero since most words do not occur in a particular tagline. . 3.1.3 Analyzing dimensionality and preprocessing . In this exercise, you have been provided with a lem_corpus which contains the pre-processed versions of the movie taglines from the previous exercise. In other words, the taglines have been lowercased and lemmatized, and stopwords have been removed. . Your job is to generate the bag of words representation bow_lem_matrix for these lemmatized taglines and compare its shape with that of bow_matrix obtained in the previous exercise. The first five lemmatized taglines in lem_corpus have been printed to the console for you to examine. . # Import CountVectorizer from sklearn.feature_extraction.text import CountVectorizer # Create CountVectorizer object vectorizer = CountVectorizer() # Generate matrix of word vectors bow_lem_matrix = vectorizer.fit_transform(lem_corpus) # Print the shape of bow_lem_matrix print(bow_lem_matrix.shape) # (6959, 5223) . Good job! Notice how the number of features have reduced significantly from around 6600 to around 5223 for pre-processed movie taglines. The reduced number of dimensions on account of text preprocessing usually leads to better performance when conducting machine learning and it is a good idea to consider it. However, as mentioned in a previous lesson, the final decision always depends on the nature of the application. . 3.1.4 Mapping feature indices with feature names . In the lesson video, we had seen that CountVectorizer doesn’t necessarily index the vocabulary in alphabetical order. In this exercise, we will learn to map each feature index to its corresponding feature name from the vocabulary. . We will use the same three sentences on lions from the video. The sentences are available in a list named corpus and has already been printed to the console. . [&#39;The lion is the king of the jungle&#39;, &#39;Lions have lifespans of a decade&#39;, &#39;The lion is an endangered species&#39;] . # Create CountVectorizer object vectorizer = CountVectorizer() # Generate matrix of word vectors bow_matrix = vectorizer.fit_transform(corpus) # Convert bow_matrix into a DataFrame bow_df = pd.DataFrame(bow_matrix.toarray()) # Map the column names to vocabulary bow_df.columns = vectorizer.get_feature_names() # Print bow_df print(bow_df) . an decade endangered have is ... lion lions of species the 0 0 0 0 0 1 ... 1 0 1 0 3 1 0 1 0 1 0 ... 0 1 1 0 0 2 1 0 1 0 1 ... 1 0 0 1 1 [3 rows x 13 columns] . Great job! Observe that the column names refer to the token whose frequency is being recorded. Therefore, since the first column name is an , the first feature represents the number of times the word ‘an’ occurs in a particular sentence. get_feature_names() essentially gives us a list which represents the mapping of the feature indices to the feature name in the vocabulary. . 3.2 Building a BoW Naive Bayes classifier . | | | | | | . 3.2.1 BoW vectors for movie reviews . In this exercise, you have been given two pandas Series, X_train and X_test , which consist of movie reviews. They represent the training and the test review data respectively. Your task is to preprocess the reviews and generate BoW vectors for these two sets using CountVectorizer . . Once we have generated the BoW vector matrices X_train_bow and X_test_bow , we will be in a very good position to apply a machine learning model to it and conduct sentiment analysis. . # Import CountVectorizer from sklearn.feature_extraction.text import CountVectorizer # Create a CountVectorizer object vectorizer = CountVectorizer(lowercase=True, stop_words=&#39;english&#39;) # Fit and transform X_train X_train_bow = vectorizer.fit_transform(X_train) # Transform X_test X_test_bow = vectorizer.transform(X_test) # Print shape of X_train_bow and X_test_bow print(X_train_bow.shape) print(X_test_bow.shape) # (750, 8158) # (250, 8158) . Great job! You now have a good idea of preprocessing text and transforming them into their bag-of-words representation using CountVectorizer . In this exercise, you have set the lowercase argument to True . However, note that this is the default value of lowercase and passing it explicitly is not necessary. Also, note that both X_train_bow and X_test_bow have 8158 features. There were words present in X_test that were not in X_train . CountVectorizer chose to ignore them in order to ensure that the dimensions of both sets remain the same. . 3.2.2 Predicting the sentiment of a movie review . In the previous exercise, you generated the bag-of-words representations for the training and test movie review data. In this exercise, we will use this model to train a Naive Bayes classifier that can detect the sentiment of a movie review and compute its accuracy. Note that since this is a binary classification problem, the model is only capable of classifying a review as either positive (1) or negative (0). It is incapable of detecting neutral reviews. . In case you don’t recall, the training and test BoW vectors are available as X_train_bow and X_test_bow respectively. The corresponding labels are available as y_train and y_test respectively. Also, for you reference, the original movie review dataset is available as df . . # Create a MultinomialNB object clf = MultinomialNB() # Fit the classifier clf.fit(X_train_bow, y_train) # Measure the accuracy accuracy = clf.score(X_test_bow, y_test) print(&quot;The accuracy of the classifier on the test set is %.3f&quot; % accuracy) # Predict the sentiment of a negative review review = &quot;The movie was terrible. The music was underwhelming and the acting mediocre.&quot; prediction = clf.predict(vectorizer.transform([review]))[0] print(&quot;The sentiment predicted by the classifier is %i&quot; % (prediction)) . Excellent work! You have successfully performed basic sentiment analysis. Note that the accuracy of the classifier is 73.2%. Considering the fact that it was trained on only 750 reviews, this is reasonably good performance. The classifier also correctly predicts the sentiment of a mini negative review which we passed into it. . 3.3 Building n-gram models . | | | | | | . 3.3.1 n-gram models for movie tag lines . In this exercise, we have been provided with a corpus of more than 9000 movie tag lines. Our job is to generate n-gram models up to n equal to 1, n equal to 2 and n equal to 3 for this data and discover the number of features for each model. . We will then compare the number of features generated for each model. . # Generate n-grams upto n=1 vectorizer_ng1 = CountVectorizer(ngram_range=(1,1)) ng1 = vectorizer_ng1.fit_transform(corpus) # Generate n-grams upto n=2 vectorizer_ng2 = CountVectorizer(ngram_range=(1,2)) ng2 = vectorizer_ng2.fit_transform(corpus) # Generate n-grams upto n=3 vectorizer_ng3 = CountVectorizer(ngram_range=(1, 3)) ng3 = vectorizer_ng3.fit_transform(corpus) # Print the number of features for each model print(&quot;ng1, ng2 and ng3 have %i, %i and %i features respectively&quot; % (ng1.shape[1], ng2.shape[1], ng3.shape[1])) # ng1, ng2 and ng3 have 6614, 37100 and 76881 features respectively . Good job! You now know how to generate n-gram models containing higher order n-grams. Notice that ng2 has over 37,000 features whereas ng3 has over 76,000 features. This is much greater than the 6,000 dimensions obtained for ng1 . As the n-gram range increases, so does the number of features, leading to increased computational costs and a problem known as the curse of dimensionality. . 3.3.2 Higher order n-grams for sentiment analysis . Similar to a previous exercise, we are going to build a classifier that can detect if the review of a particular movie is positive or negative. However, this time, we will use n-grams up to n=2 for the task. . The n-gram training reviews are available as X_train_ng . The corresponding test reviews are available as X_test_ng . Finally, use y_train and y_test to access the training and test sentiment classes respectively. . # Define an instance of MultinomialNB clf_ng = MultinomialNB() # Fit the classifier clf_ng.fit(X_train_ng, y_train) # Measure the accuracy accuracy = clf_ng.score(X_test_ng, y_test) print(&quot;The accuracy of the classifier on the test set is %.3f&quot; % accuracy) # Predict the sentiment of a negative review review = &quot;The movie was not good. The plot had several holes and the acting lacked panache.&quot; prediction = clf_ng.predict(ng_vectorizer.transform([review]))[0] print(&quot;The sentiment predicted by the classifier is %i&quot; % (prediction)) . The accuracy of the classifier on the test set is 0.758 The sentiment predicted by the classifier is 0 . Excellent job! You’re now adept at performing sentiment analysis using text. Notice how this classifier performs slightly better than the BoW version. Also, it succeeds at correctly identifying the sentiment of the mini-review as negative. In the next chapter, we will learn more complex methods of vectorizing textual data. . 3.3.3 Comparing performance of n-gram models . You now know how to conduct sentiment analysis by converting text into various n-gram representations and feeding them to a classifier. In this exercise, we will conduct sentiment analysis for the same movie reviews from before using two n-gram models: unigrams and n-grams upto n equal to 3. . We will then compare the performance using three criteria: accuracy of the model on the test set, time taken to execute the program and the number of features created when generating the n-gram representation. . start_time = time.time() # Splitting the data into training and test sets train_X, test_X, train_y, test_y = train_test_split(df[&#39;review&#39;], df[&#39;sentiment&#39;], test_size=0.5, random_state=42, stratify=df[&#39;sentiment&#39;]) # Generating ngrams vectorizer = CountVectorizer(ngram_range=(1,1)) train_X = vectorizer.fit_transform(train_X) test_X = vectorizer.transform(test_X) # Fit classifier clf = MultinomialNB() clf.fit(train_X, train_y) # Print accuracy, time and number of dimensions print(&quot;The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.&quot; % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1])) # The program took 0.196 seconds to complete. The accuracy on the test set is 0.75. The ngram representation had 12347 features. . vectorizer = CountVectorizer(ngram_range=(1,3)) # The program took 2.933 seconds to complete. The accuracy on the test set is 0.77. The ngram representation had 178240 features. . Amazing work! The program took around 0.2 seconds in the case of the unigram model and more than 10 times longer for the higher order n-gram model. The unigram model had over 12,000 features whereas the n-gram model for upto n=3 had over 178,000! Despite taking higher computation time and generating more features, the classifier only performs marginally better in the latter case, producing an accuracy of 77% in comparison to the 75% for the unigram model. . 4. TF-IDF and similarity scores . 4.1 Building tf-idf document vectors . | | | | | | . 4.1.1 tf-idf weight of commonly occurring words . The word bottle occurs 5 times in a particular document D and also occurs in every document of the corpus. What is the tf-idf weight of bottle in D ? . 0 . Correct! In fact, the tf-idf weight for bottle in every document will be 0. This is because the inverse document frequency is constant across documents in a corpus and since bottle occurs in every document, its value is log(1), which is 0. . 4.1.2 tf-idf vectors for TED talks . In this exercise, you have been given a corpus ted which contains the transcripts of 500 TED Talks. Your task is to generate the tf-idf vectors for these talks. . In a later lesson, we will use these vectors to generate recommendations of similar talks based on the transcript. . # Import TfidfVectorizer from sklearn.feature_extraction.text import TfidfVectorizer # Create TfidfVectorizer object vectorizer = TfidfVectorizer() # Generate matrix of word vectors tfidf_matrix = vectorizer.fit_transform(ted) # Print the shape of tfidf_matrix print(tfidf_matrix.shape) # (500, 29158) . Good job! You now know how to generate tf-idf vectors for a given corpus of text. You can use these vectors to perform predictive modeling just like we did with CountVectorizer . In the next few lessons, we will see another extremely useful application of the vectorized form of documents: generating recommendations. . 4.2 Cosine similarity . | | | | | | . 4.2.1 Computing dot product . In this exercise, we will learn to compute the dot product between two vectors, A = (1, 3) and B = (-2, 2), using the numpy library. More specifically, we will use the np.dot() function to compute the dot product of two numpy arrays. . # Initialize numpy vectors A = np.array([1,3]) B = np.array([-2,2]) # Compute dot product dot_prod = np.dot(A, B) # Print dot product print(dot_prod) # 4 . Good job! The dot product of the two vectors is 1 -2 + 3 2 = 4, which is indeed the output produced. We will not be using np.dot() too much in this course but it can prove to be a helpful function while computing dot products between two standalone vectors. . 4.2.2 Cosine similarity matrix of a corpus . In this exercise, you have been given a corpus , which is a list containing five sentences. The corpus is printed in the console. You have to compute the cosine similarity matrix which contains the pairwise cosine similarity score for every pair of sentences (vectorized using tf-idf). . Remember, the value corresponding to the ith row and jth column of a similarity matrix denotes the similarity score for the ith and jth vector. . corpus [&#39;The sun is the largest celestial body in the solar system&#39;, &#39;The solar system consists of the sun and eight revolving planets&#39;, &#39;Ra was the Egyptian Sun God&#39;, &#39;The Pyramids were the pinnacle of Egyptian architecture&#39;, &#39;The quick brown fox jumps over the lazy dog&#39;] . # Initialize an instance of tf-idf Vectorizer tfidf_vectorizer = TfidfVectorizer() # Generate the tf-idf vectors for the corpus tfidf_matrix = tfidf_vectorizer.fit_transform(corpus) # Compute and print the cosine similarity matrix cosine_sim = cosine_similarity(tfidf_matrix,tfidf_matrix) print(cosine_sim) . [[1. 0.36413198 0.18314713 0.18435251 0.16336438] [0.36413198 1. 0.15054075 0.21704584 0.11203887] [0.18314713 0.15054075 1. 0.21318602 0.07763512] [0.18435251 0.21704584 0.21318602 1. 0.12960089] [0.16336438 0.11203887 0.07763512 0.12960089 1. ]] . Great work! As you will see in a subsequent lesson, computing the cosine similarity matrix lies at the heart of many practical systems such as recommenders. From our similarity matrix, we see that the first and the second sentence are the most similar. Also the fifth sentence has, on average, the lowest pairwise cosine scores. This is intuitive as it contains entities that are not present in the other sentences. . 4.3 Building a plot line based recommender . | | | | | | | | | . 4.3.1 Comparing linear_kernel and cosine_similarity . In this exercise, you have been given tfidf_matrix which contains the tf-idf vectors of a thousand documents. Your task is to generate the cosine similarity matrix for these vectors first using cosine_similarity and then, using linear_kernel . . We will then compare the computation times for both functions. . # Record start time start = time.time() # Compute cosine similarity matrix cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix) # Print cosine similarity matrix print(cosine_sim) # Print time taken print(&quot;Time taken: %s seconds&quot; %(time.time() - start)) . [[1. 0. 0. ... 0. 0. 0. ] [0. 1. 0. ... 0. 0. 0. ] [0. 0. 1. ... 0. 0.01418221 0. ] ... [0. 0. 0. ... 1. 0.01589009 0. ] [0. 0. 0.01418221 ... 0.01589009 1. 0. ] [0. 0. 0. ... 0. 0. 1. ]] Time taken: 0.33341264724731445 seconds . # Compute cosine similarity matrix cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix) . Good job! Notice how both linear_kernel and cosine_similarity produced the same result. However, linear_kernel took a smaller amount of time to execute. When you’re working with a very large amount of data and your vectors are in the tf-idf representation, it is good practice to default to linear_kernel to improve performance. (NOTE: In case, you see linear_kernel taking more time, it’s because the dataset we’re dealing with is extremely small and Python’s time module is incapable of capture such minute time differences accurately) . 4.3.2 Plot recommendation engine . In this exercise, we will build a recommendation engine that suggests movies based on similarity of plot lines. You have been given a get_recommendations() function that takes in the title of a movie, a similarity matrix and an indices series as its arguments and outputs a list of most similar movies. indices has already been provided to you. . You have also been given a movie_plots Series that contains the plot lines of several movies. Your task is to generate a cosine similarity matrix for the tf-idf vectors of these plots. . Consequently, we will check the potency of our engine by generating recommendations for one of my favorite movies, The Dark Knight Rises. . # Initialize the TfidfVectorizer tfidf = TfidfVectorizer(stop_words=&#39;english&#39;) # Construct the TF-IDF matrix tfidf_matrix = tfidf.fit_transform(movie_plots) # Generate the cosine similarity matrix cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix) # Generate recommendations print(get_recommendations(&#39;The Dark Knight Rises&#39;, cosine_sim, indices)) . 1 Batman Forever 2 Batman 3 Batman Returns 8 Batman: Under the Red Hood 9 Batman: Year One 10 Batman: The Dark Knight Returns, Part 1 11 Batman: The Dark Knight Returns, Part 2 5 Batman: Mask of the Phantasm 7 Batman Begins 4 Batman &amp; Robin Name: title, dtype: object . Congratulations! You’ve just built your very first recommendation system. Notice how the recommender correctly identifies &#39;The Dark Knight Rises&#39; as a Batman movie and recommends other Batman movies as a result. This sytem is, of course, very primitive and there are a host of ways in which it could be improved. One method would be to look at the cast, crew and genre in addition to the plot to generate recommendations. We will not be covering this in this course but you have all the tools necessary to accomplish this. Do give it a try! . 4.3.3 The recommender function . In this exercise, we will build a recommender function get_recommendations() , as discussed in the lesson and the previous exercise. As we know, it takes in a title, a cosine similarity matrix, and a movie title and index mapping as arguments and outputs a list of 10 titles most similar to the original title (excluding the title itself). . You have been given a dataset metadata that consists of the movie titles and overviews. The head of this dataset has been printed to console. . title tagline 938 Cinema Paradiso A celebration of youth, friendship, and the ev... 630 Spy Hard All the action. All the women. Half the intell... 682 Stonewall The fight for the right to love 514 Killer You only hurt the one you love. 365 Jason&#39;s Lyric Love is courage. . # Generate mapping between titles and index indices = pd.Series(metadata.index, index=metadata[&#39;title&#39;]).drop_duplicates() def get_recommendations(title, cosine_sim, indices): # Get index of movie that matches title idx = indices[title] # Sort the movies based on the similarity scores sim_scores = list(enumerate(cosine_sim[idx])) sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) # Get the scores for 10 most similar movies sim_scores = sim_scores[1:11] # Get the movie indices movie_indices = [i[0] for i in sim_scores] # Return the top 10 most similar movies return metadata[&#39;title&#39;].iloc[movie_indices] . Good job! With this recommender function in our toolkit, we are now in a very good place to build the rest of the components of our recommendation engine. . 4.3.4 TED talk recommender . In this exercise, we will build a recommendation system that suggests TED Talks based on their transcripts. You have been given a get_recommendations() function that takes in the title of a talk, a similarity matrix and an indices series as its arguments, and outputs a list of most similar talks. indices has already been provided to you. . You have also been given a transcripts series that contains the transcripts of around 500 TED talks. Your task is to generate a cosine similarity matrix for the tf-idf vectors of the talk transcripts. . Consequently, we will generate recommendations for a talk titled ‘5 ways to kill your dreams’ by Brazilian entrepreneur Bel Pesce. . transcripts 0 I&#39;ve noticed something interesting about socie... 1 Hetain Patel: (In Chinese)Yuyu Rau: Hi, I&#39;m He... 2 (Music)Sophie Hawley-Weld: OK, you don&#39;t have ... . # Initialize the TfidfVectorizer tfidf = TfidfVectorizer(stop_words=&#39;english&#39;) # Construct the TF-IDF matrix tfidf_matrix = tfidf.fit_transform(transcripts) # Generate the cosine similarity matrix cosine_sim = linear_kernel(tfidf_matrix,tfidf_matrix) # Generate recommendations print(get_recommendations(&#39;5 ways to kill your dreams&#39;, cosine_sim, indices)) . 453 Success is a continuous journey 157 Why we do what we do 494 How to find work you love 149 My journey into movies that matter 447 One Laptop per Child 230 How to get your ideas to spread 497 Plug into your hard-wired happiness 495 Why you will fail to have a great career 179 Be suspicious of simple stories 53 To upgrade is human Name: title, dtype: object . Excellent work! You have successfully built a TED talk recommender. This recommender works surprisingly well despite being trained only on a small subset of TED talks. In fact, three of the talks recommended by our system is also recommended by the official TED website as talks to watch next after &#39;5 ways to kill your dreams&#39; ! . 4.4 Beyond n-grams: word embeddings . | | | | | . 4.4.1 Generating word vectors . In this exercise, we will generate the pairwise similarity scores of all the words in a sentence. The sentence is available as sent and has been printed to the console for your convenience. . sent &#39;I like apples and oranges&#39; . # Create the doc object doc = nlp(sent) # Compute pairwise similarity scores for token1 in doc: for token2 in doc: print(token1.text, token2.text, token1.similarity(token2)) . I I 1.0 I like 0.023032807 I apples 0.10175116 I and 0.047492094 I oranges 0.10894456 like I 0.023032807 like like 1.0 like apples 0.015370452 like and 0.189293 like oranges 0.021943133 apples I 0.10175116 apples like 0.015370452 apples apples 1.0 apples and -0.17736834 apples oranges 0.6315578 and I 0.047492094 and like 0.189293 and apples -0.17736834 and and 1.0 and oranges 0.018627528 oranges I 0.10894456 oranges like 0.021943133 oranges apples 0.6315578 oranges and 0.018627528 oranges oranges 1.0 . Good job! Notice how the words &#39;apples&#39; and &#39;oranges&#39; have the highest pairwaise similarity score. This is expected as they are both fruits and are more related to each other than any other pair of words. . 4.4.2 Computing similarity of Pink Floyd songs . In this final exercise, you have been given lyrics of three songs by the British band Pink Floyd, namely ‘High Hopes’, ‘Hey You’ and ‘Mother’. The lyrics to these songs are available as hopes , hey and mother respectively. . Your task is to compute the pairwise similarity between mother and hopes , and mother and hey . . mother &quot; nMother do you think they&#39;ll drop the bomb? nMother do you think they&#39;ll like this song? nMother do you think they&#39;ll try to ... . # Create Doc objects mother_doc = nlp(mother) hopes_doc = nlp(hopes) hey_doc = nlp(hey) # Print similarity between mother and hopes print(mother_doc.similarity(hopes_doc)) # 0.6006234924640204 # Print similarity between mother and hey print(mother_doc.similarity(hey_doc)) # 0.9135920924498578 . Excellent work! Notice that ‘Mother’ and ‘Hey You’ have a similarity score of 0.9 whereas ‘Mother’ and ‘High Hopes’ has a score of only 0.6. This is probably because ‘Mother’ and ‘Hey You’ were both songs from the same album ‘The Wall’ and were penned by Roger Waters. On the other hand, ‘High Hopes’ was a part of the album ‘Division Bell’ with lyrics by David Gilmour and his wife, Penny Samson. Treat yourself by listening to these songs. They’re some of the best! . 4.5 Final thoughts . | | . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/2021/07/05/feature-engineering-for-nlp-in-python.html",
            "relUrl": "/2021/07/05/feature-engineering-for-nlp-in-python.html",
            "date": " • Jul 5, 2021"
        }
        
    
  
    
        ,"post46": {
            "title": "Machine Learning Fundamentals In Python",
            "content": "Machine Learning Fundamentals in Python . Machine Learning Fundamentals in Python . 1 out of 15 . A GradientBoostingClassifier model is fitted on the training data X_train and y_train, and stored as model. Use model and the X_test feature data to predict values for the response variable, and store it in y_pred. . Complete the code to return the output . from sklearn.ensemble import GradientBoostingClassifier from sklearn.metrics import accuracy_score model = GradientBoostingClassifier(n_estimators=300, max_depth=1, random_state=1) model.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) . Expected Output . 0.95 . Your Output . 0.95 . 2 out of 15 . Fill in the blanks . You are supplied with 2 sets of variables; y_pred are predicted using the model supplied and y_test are the actual response values. . Print the main classification metrics for the model. . Complete the code to return the output . from sklearn.tree import DecisionTreeClassifier from sklearn import metrics model = DecisionTreeClassifier(max_depth=4, random_state=42) model.fit(X_train, y_train) y_pred = model.predict(X_test) print(metrics.classification_report(y_test, y_pred )) . Expected Output . precision recall f1-score support 0 0.93 0.94 0.94 54 1 0.97 0.96 0.96 89 accuracy 0.95 143 macro avg 0.95 0.95 0.95 143 weighted avg 0.95 0.95 0.95 143 . Your Output . precision recall f1-score support 0 0.93 0.94 0.94 54 1 0.97 0.96 0.96 89 accuracy 0.95 143 macro avg 0.95 0.95 0.95 143 weighted avg 0.95 0.95 0.95 143 . 3 out of 15 . Fill in the blanks . To understand the impact of weekly income on the amount a family spends each week on groceries, fit a suitable model using the weekly_spend data. What is the value of the intercept of the model? . spend income children car 0 20 19549 0 0 1 52 95248 1 1 2 18 27693 0 1 3 37 50788 1 1 4 46 50312 0 1 . Complete the code to return the output . import numpy as np from sklearn.linear_model import LinearRegression X=np.array(weekly_spend[&#39;income&#39;]).reshape(-1,1) mod=LinearRegression() mod=LinearRegression(fit_intercept=True) mod.fit(X, y) mod.fit(X, weekly_spend[&quot;spend&quot;]) print(mod.intercept_.round(2)) . Expected Output . 14.39 . Your Output . Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 8, in &lt;module&gt; mod.fit(X, y) NameError: name &#39;y&#39; is not defined . 4 out of 15 . A random forest model has been fitted to train data. . Use the results from a random forest classifier to determine the importance of each feature in determining whether a patient does or does not have heart disease. . Complete the code to return the output . import matplotlib.pyplot as plt import seaborn as sns from sklearn.ensemble import RandomForestClassifier model = RandomForestClassifier(n_estimators=15, random_state=1) model.fit(X_train, y_train) # Create a DataFrame with the feature importances feature_importances = pd.DataFrame( {&quot;feature&quot;: list(X.columns), &quot;importance&quot;: model.feature_importances_} ).sort_values(&quot;importance&quot;, ascending=False) sns.barplot(data=feature_importances, x=&quot;importance&quot;, y=&quot;feature&quot;) plt.show() . Expected Output . . 5 out of 15 . Available in your working session is the dataset scaled_samples. Instantiate a principal component analysis model object with 2 components, and fit the model to the scaled_samples object. . Complete the code to return the output . from sklearn.decomposition import PCA pca = PCA(n_components=2) pca.fit(scaled_samples) pca_features = pca.transform(scaled_samples) print(pca_features.shape) . Expected Output . (85, 2) . 6 out of 15 . As part of a brainstorming session for a medical startup idea that predicts the chance of diabetes based on a few key readings, you have a sample DataFrame named df with key readings for diabetes: . Glucose BloodPressure Insulin 0 148 72 0 1 85 66 0 2 183 64 0 3 89 66 94 4 78 50 168 . Apply Min-Max scaling so that all numeric columns are between 0 and 1. . Complete the code to return the output . from sklearn.preprocessing import MinMaxScaler min_max = MinMaxScaler() df_scaled = pd.DataFrame(min_max.fit_transform(df), columns=df.columns) print(df_scaled.head()) . Expected Output . BloodPressure Glucose Insulin 0 1.000000 0.666667 0.000000 1 0.727273 0.066667 0.000000 2 0.636364 1.000000 0.000000 3 0.727273 0.104762 0.559524 4 0.000000 0.000000 1.000000 . 7 out of 15 . Create and fit random forest classifier with 15 trees to predict whether a patient has or does not have heart disease. The data has already been split into train and test sets (X_train, X_test, y_train, y_test). . Complete the code to return the output . from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score model = RandomForestClassifier(n_estimators=15, random_state=1) model.fit(X_train, y_train) y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred)) . Expected Output . 0.8032786885245902 . 8 out of 15 . The scatterplot shows data for a sample of 14 biofuels. Fit a linear regression model and print the intercept and coefficient. . . x = iodine value (g) | y = cetane number | . Complete the code to return the output . from sklearn.linear_model import LinearRegression model = LinearRegression(fit_intercept=True) model.fit(x, y) print(&quot;Regression coefficients: {}&quot;.format(model.coef_)) print(&quot;Regression intercept: {}&quot;.format(model.intercept_)) . Expected Output . Regression coefficients: [[-0.20938742]] Regression intercept: [75.21243193] . 9 out of 15 . Consider the first five rows of the data frame df shown below. Apply a pre-processing step to standardize all numeric features. . Sepal.Length Sepal.Width Petal.Length Petal.Width Species 1 5.1 3.5 1.4 0.2 setosa 2 4.9 3.0 1.4 0.2 setosa 3 4.7 3.2 1.3 0.2 setosa 4 4.6 3.1 1.5 0.2 setosa 5 5.0 3.6 1.4 0.2 setosa 6 5.4 3.9 1.7 0.4 setosa . Complete the code to return the output . from sklearn.preprocessing import StandardScaler scaler = StandardScaler() df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns) print(df_scaled.head()) . Expected Output . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 0 -0.900681 1.019004 -1.340227 -1.315444 1 -1.143017 -0.131979 -1.340227 -1.315444 2 -1.385353 0.328414 -1.397064 -1.315444 3 -1.506521 0.098217 -1.283389 -1.315444 4 -1.021849 1.249201 -1.340227 -1.315444 . 10 out of 15 . Using the original data, df, training (X_train, y_train) and test (X_test, y_test) sets have been created. Complete the code using the data sets in the appropriate places. . Complete the code to return the output . from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error lin_reg = LinearRegression() lin_reg.fit(X_train,y_train) predictions = lin_reg.predict(X_test) print(&quot;Mean squared error: %.2f&quot; % mean_squared_error(y_test, predictions)) . Expected Output . Mean squared error: 8.47 . 11 out of 15 . A dataset has been prepared for you and split into test and training sets (X_train, X_test, y_train, y_test). . Use sklearn to fit a classification gradient boosting model on the training data with 300 estimators and 0.01 learning rate . Complete the code to return the output . from sklearn.ensemble import GradientBoostingClassifier model = GradientBoostingClassifier(n_estimators=300, learning_rate=0.01, random_state=42) model.fit(X_train, y_train) y_pred = model.predict(X_test) print(model.score(X_test, y_test)) . Expected Output . 0.8888888888888888 . 12 out of 15 . A dataset has been prepared for you and fed into a random forest model. . Use sklearn to show the predicted probabilities of a new data point belonging to each class. . &gt;&gt;&gt; print(new) Alcohol Malic.acid Phenols Flavanoids 13.64 3.10 2.70 3.01 . Complete the code to return the output . from sklearn.ensemble import RandomForestClassifier model = RandomForestClassifier(random_state=42) model.fit(X_train, y_train) print(model.predict_proba(new)) . Expected Output . [[0. 1. 0.]] . 13 out of 15 . Consider the data frame df below that shows the total number of observations per month. Fit a suitable imputer to fill the missing values. . Date Ozone Solar Wind 1976-05-31 26 27 31 1976-06-30 9 30 30 1976-07-31 26 31 31 1976-08-31 26 28 31 1976-09-30 29 30 31 . Complete the code to return the output . from sklearn.impute import SimpleImputer imputer = SimpleImputer(strategy=&#39;median&#39;) print(imputer.fit(df)) . Expected Output . SimpleImputer(add_indicator=False, copy=True, fill_value=None, missing_values=nan, strategy=&#39;median&#39;, verbose=0) . 14 out of 15 . A linear regression model has been fitted to X_train with respect to y_train. . Test values are provided in the arrays X_test and y_test. . Diagnose potential problems by plotting the residuals against the fitted values, including a lowess smoother. . Complete the code to return the output . from matplotlib import pyplot as plt import seaborn as sns model = LinearRegression().fit(X_train,y_train) y_fitted = model.predict(X_test) sns.residplot(y_test - y_fitted, y_fitted) plt.show() . Expected Output . . 15 out of 15 . Consider the variable x in the Pandas DataFrame df shown in the plot below. Note that the data contains positive and negative values. Apply a suitable transformation to the data. . . Complete the code to return the output . from sklearn.preprocessing import PowerTransformer log = PowerTransformer(method=&#39;yeo-johnson&#39;) df[&#39;log_x&#39;] = log.fit_transform(df[[&#39;x&#39;]]) print(df[&#39;log_x&#39;].head()) . Expected Output . 0 -0.319518 1 1.714791 2 0.823256 3 0.414669 4 -1.054036 Name: log_x, dtype: float64 . Retake Assessment . Machine Learning Fundamentals in Python . ScoreCompareApr 23rd, 20212:48pmJul 4th, 20213:53pmNoviceIntermediateAdvanced+29 overall134 . Your score increased by 29 overall. You started with a score of 105 and just measured 134. Congratulations . Share Results . Show off your skills and challenge coworkers and friends to do better.LinkedInFacebookTwitter . Knowledge Summary . Your strengths and skill gaps are based on how you performed within each subskill in the assessment. . Strengths . Model Selection and Validation 2/2 | Classification Techniques 3/5 | Unsupervised Learning 1/1 | . Skill Gaps . Regression Models 0/3 | Feature Engineering 2/4 | .",
            "url": "https://islamalam.github.io/blog/2021/07/04/machine-learning-fundamentals-in-python.html",
            "relUrl": "/2021/07/04/machine-learning-fundamentals-in-python.html",
            "date": " • Jul 4, 2021"
        }
        
    
  
    
        ,"post47": {
            "title": "Machine Learning For Time Series Data In Python",
            "content": "Machine Learning for Time Series Data in Python . chapter1.md Details Activity Earlier this year Jul 5 . You uploaded an item Text chapter1.md No recorded activity before July 5, 2021 . This is the memo of the 9th course (23 courses in all) of ‘Machine Learning Scientist with Python’ skill track. . You can find the original course HERE . . ### Course Description . Time series data is ubiquitous. Whether it be stock market fluctuations, sensor data recording climate change, or activity in the brain, any signal that changes over time can be described as a time series. Machine learning has emerged as a powerful method for leveraging complexity in data in order to generate predictions and insights into the problem one is trying to solve. This course is an intersection between these two worlds of machine learning and time series data, and covers feature engineering, spectograms, and other advanced techniques in order to classify heartbeat sounds and predict stock prices. . ### . 1. Time Series and Machine Learning Primer . Time Series as Inputs to a Model | Predicting Time Series Data | Validating and Inspecting Time Series Models | 1. Time Series and Machine Learning Primer . . 1.1 Timeseries kinds and applications . * . * . * . * . * . * . * . * . * . * . 1.1.1 Plotting a time series (I) . In this exercise, you’ll practice plotting the values of two time series without the time component. . Two DataFrames, data and data2 are available in your workspace. . Unless otherwise noted, assume that all required packages are loaded with their common aliases throughout this course. . Note This course assumes some familiarity with time series data, as well as how to use them in data analytics pipelines. For an introduction to time series, we recommend the Introduction to Time Series Analysis in Python and Visualizing Time Series Data with Python courses. # Plot the time series in each dataset fig, axs = plt.subplots(2, 1, figsize=(5, 10)) data.iloc[:1000].plot(y=&#39;data_values&#39;, ax=axs[0]) data2.iloc[:1000].plot(y=&#39;data_values&#39;, ax=axs[1]) plt.show() . . 1.1.2 Plotting a time series (II) . You’ll now plot both the datasets again, but with the included time stamps for each (stored in the column called &quot;time&quot; ). Let’s see if this gives you some more context for understanding each time series data. . # Plot the time series in each dataset fig, axs = plt.subplots(2, 1, figsize=(5, 10)) data.iloc[:1000].plot(x=&#39;time&#39;, y=&#39;data_values&#39;, ax=axs[0]) data2.iloc[:1000].plot(x=&#39;time&#39;, y=&#39;data_values&#39;, ax=axs[1]) plt.show() . . As you can now see, each time series has a very different sampling frequency (the amount of time between samples). The first is daily stock market data, and the second is an audio waveform. . . 1.2 Machine learning basics . * . * . * . 1.2.1 Fitting a simple model: classification . In this exercise, you’ll use the iris dataset (representing petal characteristics of a number of flowers) to practice using the scikit-learn API to fit a classification model. You can see a sample plot of the data to the right. . Note This course assumes some familiarity with Machine Learning and scikit-learn . For an introduction to scikit-learn, we recommend the Supervised Learning with Scikit-Learn and Preprocessing for Machine Learning in Python courses. . from sklearn.svm import LinearSVC # Construct data for the model X = data[[&#39;petal length (cm)&#39;,&#39;petal width (cm)&#39;]] y = data[[&#39;target&#39;]] # Fit the model model = LinearSVC() model.fit(X, y) . 1.2.2 Predicting using a classification model . Now that you have fit your classifier, let’s use it to predict the type of flower (or class) for some newly-collected flowers. . Information about petal width and length for several new flowers is stored in the variable targets . Using the classifier you fit, you’ll predict the type of each flower. . # Create input array X_predict = targets[[&#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;]] # Predict with the model predictions = model.predict(X_predict) print(predictions) # [2 2 2 1 1 2 2 2 2 1 2 1 1 2 1 1 2 1 2 2] # Visualize predictions and actual values plt.scatter(X_predict[&#39;petal length (cm)&#39;], X_predict[&#39;petal width (cm)&#39;], c=predictions, cmap=plt.cm.coolwarm) plt.title(&quot;Predicted class values&quot;) plt.show() . . 1.2.3 Fitting a simple model: regression . In this exercise, you’ll practice fitting a regression model using data from the Boston housing market. A DataFrame called boston is available in your workspace. It contains many variables of data (stored as columns). Can you find a relationship between the following two variables? . &quot;AGE&quot; proportion of owner-occupied units built prior to 1940 | &quot;RM&quot; average number of rooms per dwelling | . . from sklearn import linear_model # Prepare input and output DataFrames X = boston[[&#39;AGE&#39;]] y = boston[[&#39;RM&#39;]] # Fit the model model = linear_model.LinearRegression() model.fit(X,y) . 1.2.4 Predicting using a regression model . Now that you’ve fit a model with the Boston housing data, lets see what predictions it generates on some new data. You can investigate the underlying relationship that the model has found between inputs and outputs by feeding in a range of numbers as inputs and seeing what the model predicts for each input. . A 1-D array new_inputs consisting of 100 “new” values for &quot;AGE&quot; (proportion of owner-occupied units built prior to 1940) is available in your workspace along with the model you fit in the previous exercise. . # Generate predictions with the model using those inputs predictions = model.predict(new_inputs.reshape(-1,1)) # Visualize the inputs and predicted values plt.scatter(new_inputs, predictions, color=&#39;r&#39;, s=3) plt.xlabel(&#39;inputs&#39;) plt.ylabel(&#39;predictions&#39;) plt.show() . . Here the red line shows the relationship that your model found. As the proportion of pre-1940s houses gets larger, the average number of rooms gets slightly lower. . . 1.3 Machine learning and time series data . * . * . * . * . * . * . * . * . 1.3.1 Inspecting the classification data . In these final exercises of this chapter, you’ll explore the two datasets you’ll use in this course. . The first is a collection of heartbeat sounds. Hearts normally have a predictable sound pattern as they beat, but some disorders can cause the heart to beat abnormally . This dataset contains a training set with labels for each type of heartbeat, and a testing set with no labels. You’ll use the testing set to validate your models. . As you have labeled data, this dataset is ideal for classification . In fact, it was originally offered as a part of a public Kaggle competition . . import librosa as lr from glob import glob # List all the wav files in the folder audio_files = glob(data_dir + &#39;/*.wav&#39;) # Read in the first audio file, create the time array audio, sfreq = lr.load(audio_files[0]) time = np.arange(0, len(audio)) / sfreq # Plot audio over time fig, ax = plt.subplots() ax.plot(time, audio) ax.set(xlabel=&#39;Time (s)&#39;, ylabel=&#39;Sound Amplitude&#39;) plt.show() . audio_files [&#39;./files/murmur__201108222248.wav&#39;, &#39;./files/murmur__201108222242.wav&#39;, &#39;./files/murmur__201108222253.wav&#39;, ...] audio array([-0.00039537, -0.00043787, -0.00047949, ..., 0.00376802, 0.00299449, 0.00206312], dtype=float32) sfreq 22050 time array([ 0.00000000e+00, 4.53514739e-05, 9.07029478e-05, ..., 7.93546485e+00, 7.93551020e+00, 7.93555556e+00]) . . There are several seconds of heartbeat sounds in here, though note that most of this time is silence. A common procedure in machine learning is to separate the datapoints with lots of stuff happening from the ones that don’t. . 1.3.2 Inspecting the regression data . The next dataset contains information about company market value over several years of time. This is one of the most popular kind of time series data used for regression. If you can model the value of a company as it changes over time, you can make predictions about where that company will be in the future. This dataset was also originally provided as part of a public Kaggle competition . . In this exercise, you’ll plot the time series for a number of companies to get an understanding of how they are (or aren’t) related to one another. . # Read in the data data = pd.read_csv(&#39;prices.csv&#39;, index_col=0) # Convert the index of the DataFrame to datetime data.index = pd.to_datetime(data.index) print(data.head()) # Loop through each column, plot its values over time fig, ax = plt.subplots() for column in data.columns: data[column].plot(ax=ax, label=column) ax.legend() plt.show() . . Note that each company’s value is sometimes correlated with others, and sometimes not. Also note there are a lot of ‘jumps’ in there – what effect do you think these jumps would have on a predictive model? . chapter1.txt Details Activity Earlier this year Jul 5 . You edited an item Text chapter1.txt Jul 5 . You uploaded an item Text chapter1.txt No recorded activity before July 5, 2021 . 2. Time Series as Inputs to a Model . . 2.1 Classifying a time series . * . * . * . * . * . * . 2.1.1 Many repetitions of sounds . In this exercise, you’ll start with perhaps the simplest classification technique: averaging across dimensions of a dataset and visually inspecting the result. . You’ll use the heartbeat data described in the last chapter. Some recordings are normal heartbeat activity, while others are abnormal activity. Let’s see if you can spot the difference. . Two DataFrames, normal and abnormal , each with the shape of (n_times_points, n_audio_files) containing the audio for several heartbeats are available in your workspace. Also, the sampling frequency is loaded into a variable called sfreq . A convenience plotting function show_plot_and_make_titles() is also available in your workspace. . normal.shape # (8820, 3) normal.head() 3 4 6 time 0.000000 -0.000995 0.000281 0.002953 0.000454 -0.003381 0.000381 0.003034 0.000907 -0.000948 0.000063 0.000292 0.001361 -0.000766 0.000026 -0.005916 0.001814 0.000469 -0.000432 -0.005307 . fig, axs = plt.subplots(3, 2, figsize=(15, 7), sharex=True, sharey=True) # Calculate the time array time = np.arange(normal.shape[0]) / sfreq # Stack the normal/abnormal audio so you can loop and plot stacked_audio = np.hstack([normal, abnormal]).T # Loop through each audio file / ax object and plot # .T.ravel() transposes the array, then unravels it into a 1-D vector for looping for iaudio, ax in zip(stacked_audio, axs.T.ravel()): ax.plot(time, iaudio) show_plot_and_make_titles() . . As you can see there is a lot of variability in the raw data, let’s see if you can average out some of that noise to notice a difference. . 2.1.2 Invariance in time . While you should always start by visualizing your raw data, this is often uninformative when it comes to discriminating between two classes of data points. Data is usually noisy or exhibits complex patterns that aren’t discoverable by the naked eye. . Another common technique to find simple differences between two sets of data is to average across multiple instances of the same class. This may remove noise and reveal underlying patterns (or, it may not). . In this exercise, you’ll average across many instances of each class of heartbeat sound. . The two DataFrames ( normal and abnormal ) and the time array ( time ) from the previous exercise are available in your workspace. . normal.shape # (8820, 10) . # Average across the audio files of each DataFrame mean_normal = np.mean(normal, axis=1) mean_abnormal = np.mean(abnormal, axis=1) # Plot each average over time fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3), sharey=True) ax1.plot(time, mean_normal) ax1.set(title=&quot;Normal Data&quot;) ax2.plot(time, mean_abnormal) ax2.set(title=&quot;Abnormal Data&quot;) plt.show() . . Do you see a noticeable difference between the two? Maybe, but it’s quite noisy. Let’s see how you can dig into the data a bit further. . 2.1.3 Build a classification model . While eye-balling differences is a useful way to gain an intuition for the data, let’s see if you can operationalize things with a model. In this exercise, you will use each repetition as a datapoint, and each moment in time as a feature to fit a classifier that attempts to predict abnormal vs. normal heartbeats using only the raw data . . We’ve split the two DataFrames ( normal and abnormal ) into X_train , X_test , y_train , and y_test . . from sklearn.svm import LinearSVC # Initialize and fit the model model = LinearSVC() model.fit(X_train,y_train) # Generate predictions and score them manually predictions = model.predict(X_test) print(sum(predictions == y_test.squeeze()) / len(y_test)) # 0.555555555556 . Note that your predictions didn’t do so well. That’s because the features you’re using as inputs to the model (raw data) aren’t very good at differentiating classes. Next, you’ll explore how to calculate some more complex features that may improve the results. . . 2.2 Improving features for classification . * . * . * . * . * . * . row . * . abs . * . roll . * . * . * . * . * . #### 2.2.1 Calculating the envelope of sound . One of the ways you can improve the features available to your model is to remove some of the noise present in the data. In audio data, a common way to do this is to smooth the data and then rectify it so that the total amount of sound energy over time is more distinguishable. You’ll do this in the current exercise. . audio.head() time 0.000000 -0.024684 0.000454 -0.060429 0.000907 -0.070080 0.001361 -0.084212 0.001814 -0.085111 Name: 0, dtype: float32 . # Plot the raw data first audio.plot(figsize=(10, 5)) plt.show() . . # Rectify the audio signal audio_rectified = audio.apply(np.abs) # Plot the result audio_rectified.plot(figsize=(10, 5)) plt.show() . . # Smooth by applying a rolling mean audio_rectified_smooth = audio_rectified.rolling(50).mean() # Plot the result audio_rectified_smooth.plot(figsize=(10, 5)) plt.show() . . By calculating the envelope of each sound and smoothing it, you’ve eliminated much of the noise and have a cleaner signal to tell you when a heartbeat is happening. . 2.2.2 Calculating features from the envelope . Now that you’ve removed some of the noisier fluctuations in the audio, let’s see if this improves your ability to classify. . model LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True, intercept_scaling=1, loss=&#39;squared_hinge&#39;, max_iter=1000, multi_class=&#39;ovr&#39;, penalty=&#39;l2&#39;, random_state=None, tol=0.0001, verbose=0) . # Calculate stats means = np.mean(audio_rectified_smooth, axis=0) stds = np.std(audio_rectified_smooth, axis=0) maxs = np.max(audio_rectified_smooth, axis=0) # Create the X and y arrays X = np.column_stack([means, stds, maxs]) y = labels.reshape([-1, 1]) # Fit the model and score on testing data from sklearn.model_selection import cross_val_score percent_score = cross_val_score(model, X, y, cv=5) print(np.mean(percent_score)) # 0.716666666667 . This model is both simpler (only 3 features) and more understandable (features are simple summary statistics of the data). . 2.2.3 Derivative features: The tempogram . One benefit of cleaning up your data is that it lets you compute more sophisticated features. For example, the envelope calculation you performed is a common technique in computing tempo and rhythm features. In this exercise, you’ll use librosa to compute some tempo and rhythm features for heartbeat data, and fit a model once more. . Note that librosa functions tend to only operate on numpy arrays instead of DataFrames, so we’ll access our Pandas data as a Numpy array with the .values attribute. . # Calculate the tempo of the sounds tempos = [] for col, i_audio in audio.items(): tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None)) # Convert the list to an array so you can manipulate it more easily tempos = np.array(tempos) # Calculate statistics of each tempo tempos_mean = tempos.mean(axis=-1) tempos_std = tempos.std(axis=-1) tempos_max = tempos.max(axis=-1) # Create the X and y arrays X = np.column_stack([means, stds, maxs, tempos_mean, tempos_std, tempos_max]) y = labels.reshape([-1, 1]) # Fit the model and score on testing data percent_score = cross_val_score(model, X, y, cv=5) print(np.mean(percent_score)) # 0.516666666667 . Note that your predictive power may not have gone up (because this dataset is quite small), but you now have a more rich feature representation of audio that your model can use! . . 2.3 The spectrogram . * . * . * . * . * . * . * . * . * . 2.3.1 Spectrograms of heartbeat audio . Spectral engineering is one of the most common techniques in machine learning for time series data. The first step in this process is to calculate a spectrogram of sound. This describes what spectral content (e.g., low and high pitches) are present in the sound over time. In this exercise, you’ll calculate a spectrogram of a heartbeat audio file. . # Import the stft function from librosa.core import stft # Prepare the STFT HOP_LENGTH = 2**4 spec = stft(audio, hop_length=HOP_LENGTH, n_fft=2**7) . from librosa.core import amplitude_to_db from librosa.display import specshow # Convert into decibels spec_db = amplitude_to_db(spec) # Compare the raw audio to the spectrogram of the audio fig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True) axs[0].plot(time, audio) specshow(spec_db, sr=sfreq, x_axis=&#39;time&#39;, y_axis=&#39;hz&#39;, hop_length=HOP_LENGTH) plt.show() . . Do you notice that the heartbeats come in pairs, as seen by the vertical lines in the spectrogram? . 2.3.2 Engineering spectral features . As you can probably tell, there is a lot more information in a spectrogram compared to a raw audio file. By computing the spectral features, you have a much better idea of what’s going on. As such, there are all kinds of spectral features that you can compute using the spectrogram as a base. In this exercise, you’ll look at a few of these features. . import librosa as lr # Calculate the spectral centroid and bandwidth for the spectrogram bandwidths = lr.feature.spectral_bandwidth(S=spec)[0] centroids = lr.feature.spectral_centroid(S=spec)[0] . from librosa.core import amplitude_to_db from librosa.display import specshow # Convert spectrogram to decibels for visualization spec_db = amplitude_to_db(spec) # Display these features on top of the spectrogram fig, ax = plt.subplots(figsize=(10, 5)) ax = specshow(spec_db, x_axis=&#39;time&#39;, y_axis=&#39;hz&#39;, hop_length=HOP_LENGTH) ax.plot(times_spec, centroids) ax.fill_between(times_spec, centroids - bandwidths / 2, centroids + bandwidths / 2, alpha=.5) ax.set(ylim=[None, 6000]) plt.show() . . As you can see, the spectral centroid and bandwidth characterize the spectral content in each sound over time. They give us a summary of the spectral content that we can use in a classifier. . 2.3.3 Combining many features in a classifier . You’ve spent this lesson engineering many features from the audio data – some contain information about how the audio changes in time, others contain information about the spectral content that is present. . The beauty of machine learning is that it can handle all of these features at the same time. If there is different information present in each feature, it should improve the classifier’s ability to distinguish the types of audio. Note that this often requires more advanced techniques such as regularization, which we’ll cover in the next chapter. . For the final exercise in the chapter, we’ve loaded many of the features that you calculated before. Combine all of them into an array that can be fed into the classifier, and see how it does. . # Loop through each spectrogram bandwidths = [] centroids = [] for spec in spectrograms: # Calculate the mean spectral bandwidth this_mean_bandwidth = np.mean(lr.feature.spectral_bandwidth(S=spec)) # Calculate the mean spectral centroid this_mean_centroid = np.mean(lr.feature.spectral_centroid(S=spec)) # Collect the values bandwidths.append(this_mean_bandwidth) centroids.append(this_mean_centroid) # Create X and y arrays X = np.column_stack([means, stds, maxs, tempo_mean, tempo_max, tempo_std, bandwidths, centroids]) y = labels.reshape([-1, 1]) # Fit the model and score on testing data percent_score = cross_val_score(model, X, y, cv=5) print(np.mean(percent_score)) # 0.533333333333 . You calculated many different features of the audio, and combined each of them under the assumption that they provide independent information that can be used in classification. You may have noticed that the accuracy of your models varied a lot when using different set of features. This chapter was focused on creating new “features” from raw data and not obtaining the best accuracy. To improve the accuracy, you want to find the right features that provide relevant information and also build models on much larger data. . chapter1.txt Details Activity Earlier this year Jul 5 . You edited an item Text chapter1.txt Jul 5 . You uploaded an item Text chapter1.txt No recorded activity before July 5, 2021 . 3. Predicting Time Series Data . . 3.1 Predicting data over time . * . * . * . * . * . * . * . * . * . * . * . 3.1.1 Introducing the dataset . As mentioned in the video, you’ll deal with stock market prices that fluctuate over time. In this exercise you’ve got historical prices from two tech companies ( Ebay and Yahoo ) in the DataFrame prices . You’ll visualize the raw data for the two companies, then generate a scatter plot showing how the values for each company compare with one another. Finally, you’ll add in a “time” dimension to your scatter plot so you can see how this relationship changes over time. . # Plot the raw values over time prices.plot() plt.show() . . # Scatterplot with one company per axis prices.plot.scatter(&#39;EBAY&#39;, &#39;YHOO&#39;) plt.show() . . # Scatterplot with color relating to time prices.plot.scatter(&#39;EBAY&#39;, &#39;YHOO&#39;, c=&#39;date&#39;, cmap=plt.cm.viridis, colorbar=False) plt.show() . . As you can see, these two time series seem somewhat related to each other, though its a complex relationship that changes over time. . 3.1.2 Fitting a simple regression model . Now we’ll look at a larger number of companies. Recall that we have historical price values for many companies. Let’s use data from several companies to predict the value of a test company. You’ll attempt to predict the value of the Apple stock price using the values of NVidia, Ebay, and Yahoo. . all_prices.head() symbol AAPL ABT AIG AMAT ARNC BAC date 2010-01-04 214.009998 54.459951 29.889999 14.30 16.650013 15.690000 2010-01-05 214.379993 54.019953 29.330000 14.19 16.130013 16.200001 2010-01-06 210.969995 54.319953 29.139999 14.16 16.970013 16.389999 2010-01-07 210.580000 54.769952 28.580000 14.01 16.610014 16.930000 2010-01-08 211.980005 55.049952 29.340000 14.55 17.020014 16.780001 . from sklearn.linear_model import Ridge from sklearn.model_selection import cross_val_score # Use stock symbols to extract training data X = all_prices[[&quot;EBAY&quot;,&quot;NVDA&quot;,&quot;YHOO&quot;]] y = all_prices[[&quot;AAPL&quot;]] # Fit and score the model with cross-validation scores = cross_val_score(Ridge(), X, y, cv=3) print(scores) # [-6.09050633 -0.3179172 -3.72957284] . As you can see, fitting a model with raw data doesn’t give great results. . 3.1.3 Visualizing predicted values . When dealing with time series data, it’s useful to visualize model predictions on top of the “actual” values that are used to test the model. . In this exercise, after splitting the data (stored in the variables X and y ) into training and test sets, you’ll build a model and then visualize the model’s predictions on top of the testing data in order to estimate the model’s performance. . from sklearn.model_selection import train_test_split from sklearn.metrics import r2_score # Split our data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8, shuffle=False, random_state=1) # Fit our model and generate predictions model = Ridge() model.fit(X_train, y_train) predictions = model.predict(X_test) score = r2_score(y_test, predictions) print(score) # -5.70939901949 . # Visualize our predictions along with the &quot;true&quot; values, and print the score fig, ax = plt.subplots(figsize=(15, 5)) ax.plot(y_test, color=&#39;k&#39;, lw=3) ax.plot(predictions, color=&#39;r&#39;, lw=2) plt.show() . . Now you have an explanation for your poor score. The predictions clearly deviate from the true time series values. . . 3.2 Advanced time series prediction . * . * . * . * . * . * . * . * . * . * . * . * . * . 3.2.1 Visualizing messy data . Let’s take a look at a new dataset – this one is a bit less-clean than what you’ve seen before. . As always, you’ll first start by visualizing the raw data. Take a close look and try to find datapoints that could be problematic for fitting models. . # Visualize the dataset prices.plot(legend=False) plt.tight_layout() plt.show() # Count the missing values of each time series missing_values = prices.isna().sum() print(missing_values) . . In the plot, you can see there are clearly missing chunks of time in your data. There also seem to be a few ‘jumps’ in the data. How can you deal with this? . 3.2.2 Imputing missing values . When you have missing data points, how can you fill them in? . In this exercise, you’ll practice using different interpolation methods to fill in some missing values, visualizing the result each time. But first, you will create the function ( interpolate_and_plot() ) you’ll use to interpolate missing data points and plot them. . # Create a function we&#39;ll use to interpolate and plot def interpolate_and_plot(prices, interpolation): # Create a boolean mask for missing values missing_values = prices.isna() # Interpolate the missing values prices_interp = prices.interpolate(interpolation) # Plot the results, highlighting the interpolated values in black fig, ax = plt.subplots(figsize=(10, 5)) prices_interp.plot(color=&#39;k&#39;, alpha=.6, ax=ax, legend=False) # Now plot the interpolated values on top in red prices_interp[missing_values].plot(ax=ax, color=&#39;r&#39;, lw=3, legend=False) plt.show() . # Interpolate using the latest non-missing value interpolation_type = &#39;zero&#39; interpolate_and_plot(prices, interpolation_type) . . # Interpolate linearly interpolation_type = &#39;linear&#39; interpolate_and_plot(prices, interpolation_type) . . # Interpolate with a quadratic function interpolation_type = &#39;quadratic&#39; interpolate_and_plot(prices, interpolation_type) . . When you interpolate, the pre-existing data is used to infer the values of missing data. As you can see, the method you use for this has a big effect on the outcome. . 3.2.3 Transforming raw data . In the last chapter, you calculated the rolling mean. In this exercise, you will define a function that calculates the percent change of the latest data point from the mean of a window of previous data points. This function will help you calculate the percent change over a rolling window. . This is a more stable kind of time series that is often useful in machine learning. . # Your custom function def percent_change(series): # Collect all *but* the last value of this window, then the final value previous_values = series[:-1] last_value = series[-1] # Calculate the % difference between the last value and the mean of earlier values percent_change = (last_value - np.mean(previous_values)) / np.mean(previous_values) return percent_change # Apply your custom function and plot prices_perc = prices.rolling(20).aggregate(percent_change) prices_perc.loc[&quot;2014&quot;:&quot;2015&quot;].plot() plt.show() . . You’ve converted the data so it’s easier to compare one time point to another. This is a cleaner representation of the data. . 3.2.4 Handling outliers . In this exercise, you’ll handle outliers – data points that are so different from the rest of your data, that you treat them differently from other “normal-looking” data points. You’ll use the output from the previous exercise (percent change over time) to detect the outliers. First you will write a function that replaces outlier data points with the median value from the entire time series. . def replace_outliers(series): # Calculate the absolute difference of each timepoint from the series mean absolute_differences_from_mean = np.abs(series - np.mean(series)) # Calculate a mask for the differences that are &gt; 3 standard deviations from zero this_mask = absolute_differences_from_mean &gt; (np.std(series) * 3) # Replace these values with the median accross the data series[this_mask] = np.nanmedian(series) return series # Apply your preprocessing function to the timeseries and plot the results prices_perc = prices_perc.apply(replace_outliers) prices_perc.loc[&quot;2014&quot;:&quot;2015&quot;].plot() plt.show() . . Since you’ve converted the data to % change over time, it was easier to spot and correct the outliers. . . 3.3 Creating features over time . * . * . * . * . * . * . * . * . 3.3.1 Engineering multiple rolling features at once . Now that you’ve practiced some simple feature engineering, let’s move on to something more complex. You’ll calculate a collection of features for your time series data and visualize what they look like over time. This process resembles how many other time series models operate. . # Define a rolling window with Pandas, excluding the right-most datapoint of the window prices_perc_rolling = prices_perc.rolling(20, min_periods=5, closed=&#39;right&#39;) # Define the features you&#39;ll calculate for each window features_to_calculate = [np.min, np.max, np.mean, np.std] # Calculate these features for your rolling window object features = prices_perc_rolling.agg(features_to_calculate) # Plot the results ax = features.loc[:&quot;2011-01&quot;].plot() prices_perc.loc[:&quot;2011-01&quot;].plot(ax=ax, color=&#39;k&#39;, alpha=.2, lw=3) ax.legend(loc=(1.01, .6)) plt.show() . . 3.3.2 Percentiles and partial functions . In this exercise, you’ll practice how to pre-choose arguments of a function so that you can pre-configure how it runs. You’ll use this to calculate several percentiles of your data using the same percentile() function in numpy . . # Import partial from functools from functools import partial percentiles = [1, 10, 25, 50, 75, 90, 99] # Use a list comprehension to create a partial function for each quantile percentile_functions = [partial(np.percentile, q=percentile) for percentile in percentiles] # Calculate each of these quantiles on the data using a rolling window prices_perc_rolling = prices_perc.rolling(20, min_periods=5, closed=&#39;right&#39;) features_percentiles = prices_perc_rolling.agg(percentile_functions) # Plot a subset of the result ax = features_percentiles.loc[:&quot;2011-01&quot;].plot(cmap=plt.cm.viridis) ax.legend(percentiles, loc=(1.01, .5)) plt.show() . . 3.3.3 Using “date” information . It’s easy to think of timestamps as pure numbers, but don’t forget they generally correspond to things that happen in the real world. That means there’s often extra information encoded in the data such as “is it a weekday?” or “is it a holiday?”. This information is often useful in predicting timeseries data. . prices_perc.head() EBAY date 2014-01-02 0.017938 2014-01-03 0.002268 2014-01-06 -0.027365 2014-01-07 -0.006665 2014-01-08 -0.017206 . # Extract date features from the data, add them as columns prices_perc[&#39;day_of_week&#39;] = prices_perc.index.dayofweek prices_perc[&#39;week_of_year&#39;] = prices_perc.index.weekofyear prices_perc[&#39;month_of_year&#39;] = prices_perc.index.month # Print prices_perc print(prices_perc) . EBAY day_of_week week_of_year month_of_year date 2014-01-02 0.017938 3 1 1 2014-01-03 0.002268 4 1 1 2014-01-06 -0.027365 0 2 1 2014-01-07 -0.006665 1 2 1 ... . This concludes the third chapter. In the next chapter, you will learn advanced techniques to validate and inspect your time series models. . chapter1.txt Details Activity Earlier this year Jul 5 . You edited an item Text chapter1.txt Jul 5 . You uploaded an item Text chapter1.txt No recorded activity before July 5, 2021 . 4. Validating and Inspecting Time Series Models . . 4.1 Creating features from the past . * . * . * . * . * . * . * . * . * . 4.1.1 Creating time-shifted features . In machine learning for time series, it’s common to use information about previous time points to predict a subsequent time point. . In this exercise, you’ll “shift” your raw data and visualize the results. You’ll use the percent change time series that you calculated in the previous chapter, this time with a very short window. A short window is important because, in a real-world scenario, you want to predict the day-to-day fluctuations of a time series, not its change over a longer window of time. . # These are the &quot;time lags&quot; shifts = np.arange(1, 11).astype(int) # Use a dictionary comprehension to create name: value pairs, one pair per shift shifted_data = {&quot;lag_{}_day&quot;.format(day_shift): prices_perc.shift(day_shift) for day_shift in shifts} # Convert into a DataFrame for subsequent use prices_perc_shifted = pd.DataFrame(shifted_data) # Plot the first 100 samples of each ax = prices_perc_shifted.iloc[:100].plot(cmap=plt.cm.viridis) prices_perc.iloc[:100].plot(color=&#39;r&#39;, lw=2) ax.legend(loc=&#39;best&#39;) plt.show() . . 4.1.2 Special case: Auto-regressive models . Now that you’ve created time-shifted versions of a single time series, you can fit an auto-regressive model. This is a regression model where the input features are time-shifted versions of the output time series data. You are using previous values of a timeseries to predict current values of the same timeseries (thus, it is auto-regressive). . By investigating the coefficients of this model, you can explore any repetitive patterns that exist in a timeseries, and get an idea for how far in the past a data point is predictive of the future. . # Replace missing values with the median for each column X = prices_perc_shifted.fillna(np.nanmedian(prices_perc_shifted)) y = prices_perc.fillna(np.nanmedian(prices_perc)) # Fit the model model = Ridge() model.fit(X, y) . X.head(1) lag_1_day lag_2_day lag_3_day lag_4_day lag_5_day lag_6_day date 2010-01-04 0.000756 0.000756 0.000756 0.000756 0.000756 0.000756 lag_7_day lag_8_day lag_9_day lag_10_day date 2010-01-04 0.000756 0.000756 0.000756 0.000756 y.head(1) date 2010-01-04 0.000756 Name: AAPL, dtype: float64 . You’ve filled in the missing values with the median so that it behaves well with scikit-learn. Now let’s take a look at what your model found. . 4.1.3 Visualize regression coefficients . Now that you’ve fit the model, let’s visualize its coefficients. This is an important part of machine learning because it gives you an idea for how the different features of a model affect the outcome. . The shifted time series DataFrame ( prices_perc_shifted ) and the regression model ( model ) are available in your workspace. . In this exercise, you will create a function that, given a set of coefficients and feature names, visualizes the coefficient values. . def visualize_coefficients(coefs, names, ax): # Make a bar plot for the coefficients, including their names on the x-axis ax.bar(names, coefs) ax.set(xlabel=&#39;Coefficient name&#39;, ylabel=&#39;Coefficient value&#39;) # Set formatting so it looks nice plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment=&#39;right&#39;) return ax . # Visualize the output data up to &quot;2011-01&quot; fig, axs = plt.subplots(2, 1, figsize=(10, 5)) y.loc[:&#39;2011-01&#39;].plot(ax=axs[0]) # Run the function to visualize model&#39;s coefficients visualize_coefficients(model.coef_, prices_perc_shifted.columns, ax=axs[1]) plt.show() . . When you use time-lagged features on the raw data, you see that the highest coefficient by far is the first one. This means that the N-1th time point is useful in predicting the Nth timepoint, but no other points are useful. . 4.1.4 Auto-regression with a smoother time series . Now, let’s re-run the same procedure using a smoother signal. You’ll use the same percent change algorithm as before, but this time use a much larger window (40 instead of 20). As the window grows, the difference between neighboring timepoints gets smaller, resulting in a smoother signal. What do you think this will do to the auto-regressive model? . prices_perc_shifted and model (updated to use a window of 40) are available in your workspace. . # Visualize the output data up to &quot;2011-01&quot; fig, axs = plt.subplots(2, 1, figsize=(10, 5)) y.loc[:&#39;2011-01&#39;].plot(ax=axs[0]) # Run the function to visualize model&#39;s coefficients visualize_coefficients(model.coef_, prices_perc_shifted.columns, ax=axs[1]) plt.show() . . As you can see here, by transforming your data with a larger window, you’ve also changed the relationship between each timepoint and the ones that come just before it. This model’s coefficients gradually go down to zero, which means that the signal itself is smoother over time. Be careful when you see something like this, as it means your data is not i.i.d . . . 4.2 Cross-validating time series data . * . * . * . * . * . * . * . * . * . * . * . 4.2.1 Cross-validation with shuffling . As you’ll recall, cross-validation is the process of splitting your data into training and test sets multiple times. Each time you do this, you choose a different training and test set. In this exercise, you’ll perform a traditional ShuffleSplit cross-validation on the company value data from earlier. Later we’ll cover what changes need to be made for time series data. The data we’ll use is the same historical price data for several large companies. . An instance of the Linear regression object ( model ) is available in your workspace along with the function r2_score() for scoring. Also, the data is stored in arrays X and y . We’ve also provided a helper function ( visualize_predictions() ) to help visualize the results. . # Import ShuffleSplit and create the cross-validation object from sklearn.model_selection import ShuffleSplit cv = ShuffleSplit(n_splits=10, random_state=1) # Iterate through CV splits results = [] for tr, tt in cv.split(X, y): # Fit the model on training data model.fit(X[tr], y[tr]) # Generate predictions on the test data, score the predictions, and collect prediction = model.predict(X[tt]) score = r2_score(y[tt], prediction) results.append((prediction, score, tt)) # Custom function to quickly visualize predictions visualize_predictions(results) . . You’ve correctly constructed and fit the model. If you look at the plot to the right, see that the order of datapoints in the test set is scrambled. Let’s see how it looks when we shuffle the data in blocks. . 4.2.2 Cross-validation without shuffling . Now, re-run your model fit using block cross-validation (without shuffling all datapoints). In this case, neighboring time-points will be kept close to one another. How do you think the model predictions will look in each cross-validation loop? . An instance of the Linear regression model object is available in your workspace. Also, the arrays X and y (training data) are available too. . # Create KFold cross-validation object from sklearn.model_selection import KFold cv = KFold(n_splits=10, shuffle=False, random_state=1) # Iterate through CV splits results = [] for tr, tt in cv.split(X, y): # Fit the model on training data model.fit(X[tr], y[tr]) # Generate predictions on the test data and collect prediction = model.predict(X[tt]) results.append((prediction, tt)) # Custom function to quickly visualize predictions visualize_predictions(results) . . This time, the predictions generated within each CV loop look ‘smoother’ than they were before – they look more like a real time series because you didn’t shuffle the data. This is a good sanity check to make sure your CV splits are correct. . 4.2.3 Time-based cross-validation . Finally, let’s visualize the behavior of the time series cross-validation iterator in scikit-learn. Use this object to iterate through your data one last time, visualizing the training data used to fit the model on each iteration. . An instance of the Linear regression model object is available in your workpsace. Also, the arrays X and y (training data) are available too. . # Import TimeSeriesSplit from sklearn.model_selection import TimeSeriesSplit # Create time-series cross-validation object cv = TimeSeriesSplit(n_splits=10) # Iterate through CV splits fig, ax = plt.subplots() for ii, (tr, tt) in enumerate(cv.split(X, y)): # Plot the training data on each iteration, to see the behavior of the CV ax.plot(tr, ii + y[tr]) ax.set(title=&#39;Training data on each CV iteration&#39;, ylabel=&#39;CV iteration&#39;) plt.show() . . Note that the size of the training set grew each time when you used the time series cross-validation object. This way, the time points you predict are always after the timepoints we train on. . . 4.3 Stationarity and stability . * . * . * . * . * . * . * . * . * . * . * . * . * . 4.3.1 Stationarity . First, let’s confirm what we know about stationarity. Take a look at these time series. . Which of the following time series do you think are not stationary? . * . * . * . * . B and C . C begins to trend upward partway through, while B shows a large increase in variance mid-way through, making both of them non-stationary. . 4.3.2 Bootstrapping a confidence interval . A useful tool for assessing the variability of some data is the bootstrap. In this exercise, you’ll write your own bootstrapping function that can be used to return a bootstrapped confidence interval. . This function takes three parameters: a 2-D array of numbers ( data ), a list of percentiles to calculate ( percentiles ), and the number of boostrap iterations to use ( n_boots ). It uses the resample function to generate a bootstrap sample, and then repeats this many times to calculate the confidence interval. . from sklearn.utils import resample def bootstrap_interval(data, percentiles=(2.5, 97.5), n_boots=100): &quot;&quot;&quot;Bootstrap a confidence interval for the mean of columns of a 2-D dataset.&quot;&quot;&quot; # Create our empty array to fill the results bootstrap_means = np.zeros([n_boots, data.shape[-1]]) for ii in range(n_boots): # Generate random indices for our data *with* replacement, then take the sample mean random_sample = resample(data) bootstrap_means[ii] = random_sample.mean(axis=0) # Compute the percentiles of choice for the bootstrapped means percentiles = np.percentile(bootstrap_means, percentiles, axis=0) return percentiles . You can use this function to assess the variability of your model coefficients. . 4.3.3 Calculating variability in model coefficients . In this lesson, you’ll re-run the cross-validation routine used before, but this time paying attention to the model’s stability over time. You’ll investigate the coefficients of the model, as well as the uncertainty in its predictions. . Begin by assessing the stability (or uncertainty) of a model’s coefficients across multiple CV splits. Remember, the coefficients are a reflection of the pattern that your model has found in the data. . An instance of the Linear regression object ( model ) is available in your workpsace. Also, the arrays X and y (the data) are available too. . # Iterate through CV splits n_splits = 100 cv = TimeSeriesSplit(n_splits=n_splits) # Create empty array to collect coefficients coefficients = np.zeros([n_splits, X.shape[1]]) for ii, (tr, tt) in enumerate(cv.split(X, y)): # Fit the model on training data and collect the coefficients model.fit(X[tr], y[tr]) coefficients[ii] = model.coef_ . # Calculate a confidence interval around each coefficient bootstrapped_interval = bootstrap_interval(coefficients, (2.5,97.5)) # Plot it fig, ax = plt.subplots() ax.scatter(feature_names, bootstrapped_interval[0], marker=&#39;_&#39;, lw=3) ax.scatter(feature_names, bootstrapped_interval[1], marker=&#39;_&#39;, lw=3) ax.set(title=&#39;95% confidence interval for model coefficients&#39;) plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment=&#39;right&#39;) plt.show() . . You’ve calculated the variability around each coefficient, which helps assess which coefficients are more stable over time! . 4.3.4 Visualizing model score variability over time . Now that you’ve assessed the variability of each coefficient, let’s do the same for the performance (scores) of the model. Recall that the TimeSeriesSplit object will use successively-later indices for each test set. This means that you can treat the scores of your validation as a time series. You can visualize this over time in order to see how the model’s performance changes over time. . An instance of the Linear regression model object is stored in model , a cross-validation object in cv , and data in X and y . . from sklearn.model_selection import cross_val_score # Generate scores for each split to see how the model performs over time scores = cross_val_score(model, X, y, cv=cv, scoring=my_pearsonr) # Convert to a Pandas Series object scores_series = pd.Series(scores, index=times_scores, name=&#39;score&#39;) # Bootstrap a rolling confidence interval for the mean score scores_lo = scores_series.rolling(20).aggregate(partial(bootstrap_interval, percentiles=2.5)) scores_hi = scores_series.rolling(20).aggregate(partial(bootstrap_interval, percentiles=97.5)) . # Plot the results fig, ax = plt.subplots() scores_lo.plot(ax=ax, label=&quot;Lower confidence interval&quot;) scores_hi.plot(ax=ax, label=&quot;Upper confidence interval&quot;) ax.legend() plt.show() . . You plotted a rolling confidence interval for scores over time. This is useful in seeing when your model predictions are correct. . 4.3.5 Accounting for non-stationarity . In this exercise, you will again visualize the variations in model scores, but now for data that changes its statistics over time. . An instance of the Linear regression model object is stored in model , a cross-validation object in cv , and the data in X and y . . # Pre-initialize window sizes window_sizes = [25, 50, 75, 100] # Create an empty DataFrame to collect the stores all_scores = pd.DataFrame(index=times_scores) # Generate scores for each split to see how the model performs over time for window in window_sizes: # Create cross-validation object using a limited lookback window cv = TimeSeriesSplit(n_splits=100, max_train_size=window) # Calculate scores across all CV splits and collect them in a DataFrame this_scores = cross_val_score(model, X, y, cv=cv, scoring=my_pearsonr) all_scores[&#39;Length {}&#39;.format(window)] = this_scores . # Visualize the scores ax = all_scores.rolling(10).mean().plot(cmap=plt.cm.coolwarm) ax.set(title=&#39;Scores for multiple windows&#39;, ylabel=&#39;Correlation (r)&#39;) plt.show() . . Wonderful – notice how in some stretches of time, longer windows perform worse than shorter ones. This is because the statistics in the data have changed, and the longer window is now using outdated information. . . ### 4.4 Wrap-up . * . * . * . * . * . * . * . . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/2021/07/04/machine-learning-for-time-series-data-in-python.html",
            "relUrl": "/2021/07/04/machine-learning-for-time-series-data-in-python.html",
            "date": " • Jul 4, 2021"
        }
        
    
  
    
        ,"post48": {
            "title": "Dimensionality Reduction in Python",
            "content": "Dimensionality Reduction in Python . This is the memo of the 7th course (23 courses in all) of ‘Machine Learning Scientist with Python’ skill track. You can find the original course HERE. . Course Description . High-dimensional datasets can be overwhelming and leave you not knowing where to start. Typically, you’d visually explore a new dataset first, but when you have too many dimensions the classical approaches will seem insufficient. Fortunately, there are visualization techniques designed specifically for high-dimensional data and you’ll be introduced to these in this course. After exploring the data, you’ll often find that many features hold little information because they don’t show any variance or because they are duplicates of other features. You’ll learn how to detect these features and drop them from the dataset so that you can focus on the informative ones. In a next step, you might want to build a model on these features, and it may turn out that some don’t have any effect on the thing you’re trying to predict. You’ll learn how to detect and drop these irrelevant features too, in order to reduce dimensionality and thus complexity. Finally, you’ll learn how feature extraction techniques can reduce dimensionality for you through the calculation of uncorrelated principal components. . . Exploring high dimensional data | Feature selection I, selecting for feature information | Feature selection II, selecting for model accuracy | Feature extraction . | Exploring high dimensional data . You’ll be introduced to the concept of dimensionality reduction and will learn when an why this is important. You’ll learn the difference between feature selection and feature extraction and will apply both techniques for data exploration. The chapter ends with a lesson on t-SNE, a powerful feature extraction technique that will allow you to visualize a high-dimensional dataset. . | Feature selection I, selecting for feature information . In this first out of two chapters on feature selection, you’ll learn about the curse of dimensionality and how dimensionality reduction can help you overcome it. You’ll be introduced to a number of techniques to detect and remove features that bring little added value to the dataset. Either because they have little variance, too many missing values, or because they are strongly correlated to other features. . | Feature selection II, selecting for model accuracy . In this second chapter on feature selection, you’ll learn how to let models help you find the most important features in a dataset for predicting a particular target feature. In the final lesson of this chapter, you’ll combine the advice of multiple, different, models to decide on which features are worth keeping. . | Feature extraction . This chapter is a deep-dive on the most frequently used dimensionality reduction algorithm, Principal Component Analysis (PCA). You’ll build intuition on how and why this algorithm is so powerful and will apply it both for data exploration and data pre-processing in a modeling pipeline. You’ll end with a cool image compression use case. . | 1. Exploring high dimensional data . You’ll be introduced to the concept of dimensionality reduction and will learn when an why this is important. You’ll learn the difference between feature selection and feature extraction and will apply both techniques for data exploration. The chapter ends with a lesson on t-SNE, a powerful feature extraction technique that will allow you to visualize a high-dimensional dataset. . 1.1 Introduction . . 1.1.1 Finding the number of dimensions in a dataset . A larger sample of the Pokemon dataset has been loaded for you as the Pandas dataframe pokemon_df. . How many dimensions, or columns are in this dataset? . | 12 | pokemon_df.shape(160, 7) | | :— | :— | . 1.1.2 Removing features without variance . A sample of the Pokemon dataset has been loaded as pokemon_df. To get an idea of which features have little variance you should use the IPython Shell to calculate summary statistics on this sample. Then adjust the code to create a smaller, easier to understand, dataset. . | 12345678910 | pokemon_df.describe() HP Attack Defense Generationcount 160.00000 160.00000 160.000000 160.0mean 64.61250 74.98125 70.175000 1.0std 27.92127 29.18009 28.883533 0.0min 10.00000 5.00000 5.000000 1.025% 45.00000 52.00000 50.000000 1.050% 60.00000 71.00000 65.000000 1.075% 80.00000 95.00000 85.000000 1.0max 250.00000 155.00000 180.000000 1.0 | | :— | :— | . | 123456 | pokemon_df.describe(exclude=&#39;number&#39;) Name Type Legendarycount 160 160 160unique 160 15 1top Weepinbell Water Falsefreq 1 31 160 | | :— | :— | . | 1234567891011 | # Leave this list as isnumber_cols = [&#39;HP&#39;, &#39;Attack&#39;, &#39;Defense&#39;] # Remove the feature without variance from this listnon_number_cols = [&#39;Name&#39;, &#39;Type&#39;] # Create a new dataframe by subselecting the chosen featuresdf_selected = pokemon_df[number_cols + non_number_cols] # Prints the first 5 lines of the new dataframeprint(df_selected.head()) | | :— | :— | . | 123456 | HP Attack Defense Name Type0 45 49 49 Bulbasaur Grass1 60 62 63 Ivysaur Grass2 80 82 83 Venusaur Grass3 80 100 123 VenusaurMega Venusaur Grass4 39 52 43 Charmander Fire | | :— | :— | . 1.2 Feature selection vs feature extraction . | | | | | | . 1.2.1 Visually detecting redundant features . Data visualization is a crucial step in any data exploration. Let’s use Seaborn to explore some samples of the US Army ANSUR body measurement dataset. . Two data samples have been pre-loaded as ansur_df_1 and ansur_df_2. . Seaborn has been imported as sns. . | 12345 | # Create a pairplot and color the points using the &#39;Gender&#39; featuresns.pairplot(ansur_df_1, hue=&#39;Gender&#39;, diag_kind=&#39;hist&#39;) # Show the plotplt.show() | | :— | :— | . . | 12345678 | # Remove one of the redundant featuresreduced_df = ansur_df_1.drop(&#39;stature_m&#39;, axis=1) # Create a pairplot and color the points using the &#39;Gender&#39; featuresns.pairplot(reduced_df, hue=&#39;Gender&#39;) # Show the plotplt.show() | | :— | :— | . . | 123456 | # Create a pairplot and color the points using the &#39;Gender&#39; featuresns.pairplot(ansur_df_2, hue=&#39;Gender&#39;, diag_kind=&#39;hist&#39;) # Show the plotplt.show() | | :— | :— | . . | 12345678 | # Remove the redundant featurereduced_df = ansur_df_2.drop(&#39;n_legs&#39;, axis=1) # Create a pairplot and color the points using the &#39;Gender&#39; featuresns.pairplot(reduced_df, hue=&#39;Gender&#39;, diag_kind=&#39;hist&#39;) # Show the plotplt.show() | | :— | :— | . . The body height (inches) and stature (meters) hold the same information in a different unit + all the individuals in the second sample have two legs. . 1.2.2 Advantage of feature selection . What advantage does feature selection have over feature extraction? . The selected features remain unchanged, and are therefore easy to interpret. . 1.3 t-SNE visualization of high-dimensional data . | | | | | | | | | . 1.3.1 t-SNE intuition . t-SNE is super powerful, but do you know exactly when to use it? . When you want to visually explore the patterns in a high dimensional dataset. . 1.3.2 Fitting t-SNE to the ANSUR data . t-SNE is a great technique for visual exploration of high dimensional datasets. In this exercise, you’ll apply it to the ANSUR dataset. You’ll remove non-numeric columns from the pre-loaded dataset df and fit TSNE to his numeric dataset. . | 12345678910111213141516 | # Non-numerical columns in the datasetnon_numeric = [&#39;Branch&#39;, &#39;Gender&#39;, &#39;Component&#39;] # Drop the non-numerical columns from dfdf_numeric = df.drop(non_numeric, axis=1) # Create a t-SNE model with learning rate 50m = TSNE(learning_rate=50) # Fit and transform the t-SNE model on the numeric datasettsne_features = m.fit_transform(df_numeric)print(tsne_features.shape)(6068, 2) df.shape(6068, 94) | | :— | :— | . 1.3.3 t-SNE visualisation of dimensionality . Time to look at the results of your hard work. In this exercise, you will visualize the output of t-SNE dimensionality reduction on the combined male and female Ansur dataset. You’ll create 3 scatterplots of the 2 t-SNE features (&#39;x&#39; and &#39;y&#39;) which were added to the dataset df. In each scatterplot you’ll color the points according to a different categorical variable. . seaborn has already been imported as sns and matplotlib.pyplot as plt. . | 12345 | # Color the points according to Army Componentsns.scatterplot(x=&quot;x&quot;, y=&quot;y&quot;, hue=&#39;Component&#39;, data=df) # Show the plotplt.show() | | :— | :— | . . | 12345 | # Color the points by Army Branchsns.scatterplot(x=&quot;x&quot;, y=&quot;y&quot;, hue=&#39;Branch&#39;, data=df) # Show the plotplt.show() | | :— | :— | . . | 12345 | # Color the points by Gendersns.scatterplot(x=&quot;x&quot;, y=&quot;y&quot;, hue=&#39;Gender&#39;, data=df) # Show the plotplt.show() | | :— | :— | . . There is a Male and a Female cluster. t-SNE found these gender differences in body shape without being told about them explicitly! From the second plot you learned there are more males in the Combat Arms Branch. . 2. Feature selection I, selecting for feature information . 2.1 The curse of dimensionality . 2.1.1 Train – test split . In this chapter, you will keep working with the ANSUR dataset. Before you can build a model on your dataset, you should first decide on which feature you want to predict. In this case, you’re trying to predict gender. . You need to extract the column holding this feature from the dataset and then split the data into a training and test set. The training set will be used to train the model and the test set will be used to check its performance on unseen data. . ansur_df has been pre-loaded for you. . | 1234567891011121314 | # Import train_test_split()from sklearn.model_selection import train_test_split # Select the Gender column as the feature to be predicted (y)y = ansur_df[&#39;Gender&#39;] # Remove the Gender column to create the training dataX = ansur_df.drop(&#39;Gender&#39;, axis=1) # Perform a 70% train and 30% test data splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) print(&quot;{} rows in test set vs. {} in training set. {} Features.&quot;.format(X_test.shape[0], X_train.shape[0], X_test.shape[1]))# 300 rows in test set vs. 700 in training set. 91 Features. | | :— | :— | . 2.1.2 Fitting and testing the model . In the previous exercise, you split the dataset into X_train, X_test, y_train, and y_test. These datasets have been pre-loaded for you. You’ll now create a support vector machine classifier model (SVC()) and fit that to the training data. You’ll then calculate the accuracy on both the test and training set to detect overfitting. . | 12345678910111213141516 | # Import SVC from sklearn.svm and accuracy_score from sklearn.metricsfrom sklearn.svm import SVCfrom sklearn.metrics import accuracy_score # Create an instance of the Support Vector Classification classsvc = SVC() # Fit the model to the training datasvc.fit(X_train, y_train) # Calculate accuracy scores on both train and test dataaccuracy_train = accuracy_score(y_train, svc.predict(X_train))accuracy_test = accuracy_score(y_test, svc.predict(X_test)) print(&quot;{0:.1%} accuracy on test set vs. {1:.1%} on training set&quot;.format(accuracy_test, accuracy_train))# 49.7% accuracy on test set vs. 100.0% on training set | | :— | :— | . Looks like the model badly overfits on the training data. On unseen data it performs worse than a random selector would. . 2.1.3 Accuracy after dimensionality reduction . You’ll reduce the overfit with the help of dimensionality reduction. In this case, you’ll apply a rather drastic form of dimensionality reduction by only selecting a single column that has some good information to distinguish between genders. You’ll repeat the train-test split, model fit and prediction steps to compare the accuracy on test vs. training data. . | 1234567891011121314 | # Assign just the &#39;neckcircumferencebase&#39; column from ansur_df to XX = ansur_df[[&#39;neckcircumferencebase&#39;]] # Split the data, instantiate a classifier and fit the dataX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)svc = SVC()svc.fit(X_train, y_train) # Calculate accuracy scores on both train and test dataaccuracy_train = accuracy_score(y_train, svc.predict(X_train))accuracy_test = accuracy_score(y_test, svc.predict(X_test)) print(&quot;{0:.1%} accuracy on test set vs. {1:.1%} on training set&quot;.format(accuracy_test, accuracy_train))# 93.3% accuracy on test set vs. 94.9% on training set | | :— | :— | . Wow, what just happened!? On the full dataset the model is rubbish but with a single feature we can make good predictions? This is an example of the curse of dimensionality! The model badly overfits when we feed it too many features. It overlooks that neck circumference by itself is pretty different for males and females. . 2.2 Features with missing values or little variance . | | | | | | . 2.2.1 Finding a good variance threshold . You’ll be working on a slightly modified subsample of the ANSUR dataset with just head measurements pre-loaded as head_df. . | 1234 | # Create the boxplothead_df.boxplot() plt.show() | | :— | :— | . . | 12345 | # Normalize the datanormalized_df = head_df / head_df.mean() normalized_df.boxplot()plt.show() | | :— | :— | . . | 12345 | # Normalize the datanormalized_df = head_df / head_df.mean() # Print the variances of the normalized dataprint(normalized_df.var()) | | :— | :— | . | 1234567 | headbreadth 1.678952e-03headcircumference 1.029623e-03headlength 1.867872e-03tragiontopofhead 2.639840e-03n_hairs 1.002552e-08measurement_error 3.231707e-27dtype: float64 | | :— | :— | . Inspect the printed variances. If you want to remove the 2 very low variance features. What would be a good variance threshold? . 1.0e-03 . 2.2.2 Features with low variance . In the previous exercise you established that 0.001 is a good threshold to filter out low variance features in head_df after normalization. Now use the VarianceThreshold feature selector to remove these features. . | 12345678910111213141516 | from sklearn.feature_selection import VarianceThreshold # Create a VarianceThreshold feature selectorsel = VarianceThreshold(threshold=10**-3) # Fit the selector to normalized head_dfsel.fit(head_df / head_df.mean()) # Create a boolean maskmask = sel.get_support() # Apply the mask to create a reduced dataframereduced_df = head_df.loc[:, mask] print(&quot;Dimensionality reduced from {} to {}.&quot;.format(head_df.shape[1], reduced_df.shape[1]))# Dimensionality reduced from 6 to 4. | | :— | :— | . 2.2.3 Removing features with many missing values . You’ll apply feature selection on the Boston Public Schools dataset which has been pre-loaded as school_df. Calculate the missing value ratio per feature and then create a mask to remove features with many missing values. . | 1234567891011121314151617181920212223 | school_df.isna().sum() / len(school_df)x 0.000000y 0.000000objectid_1 0.000000objectid 0.000000bldg_id 0.000000bldg_name 0.000000address 0.000000city 0.000000zipcode 0.000000csp_sch_id 0.000000sch_id 0.000000sch_name 0.000000sch_label 0.000000sch_type 0.000000shared 0.877863complex 0.984733label 0.000000tlt 0.000000pl 0.000000point_x 0.000000point_y 0.000000dtype: float64 | | :— | :— | . | 12345678910 | # Create a boolean mask on whether each feature less than 50% missing values.mask = school_df.isna().sum() / len(school_df) &lt; 0.5 # Create a reduced dataset by applying the maskreduced_df = school_df.loc[:,mask] print(school_df.shape)print(reduced_df.shape)# (131, 21)# (131, 19) | | :— | :— | . 2.3 Pairwise correlation . | | | . 2.3.1 Correlation intuition . The correlation coefficient of A to B is equal to that of B to A. . 2.3.2 Inspecting the correlation matrix . A sample of the ANSUR body measurements dataset has been pre-loaded as ansur_df. Use the terminal to create a correlation matrix for this dataset. . What is the correlation coefficient between wrist and ankle circumference? . | 1234567 | ansur_df.corr() Elbow rest height Wrist circumference Ankle circumference Buttock height Crotch heightElbow rest height 1.000000 0.294753 0.301963 -0.007013 -0.026090Wrist circumference 0.294753 1.000000 0.702178 0.576679 0.606582Ankle circumference 0.301963 0.702178 1.000000 0.367548 0.386502Buttock height -0.007013 0.576679 0.367548 1.000000 0.929411Crotch height -0.026090 0.606582 0.386502 0.929411 1.000000 | | :— | :— | . 0.702178 . 2.3.3 Visualizing the correlation matrix . Reading the correlation matrix of ansur_df in its raw, numeric format doesn’t allow us to get a quick overview. Let’s improve this by removing redundant values and visualizing the matrix using seaborn. . | 123456 | # Create the correlation matrixcorr = ansur_df.corr() # Draw the heatmapsns.heatmap(corr, cmap=cmap, center=0, linewidths=1, annot=True, fmt=&quot;.2f&quot;)plt.show() | | :— | :— | . . | 123456789 | # Create the correlation matrixcorr = ansur_df.corr() # Generate a mask for the upper trianglemask = np.triu(np.ones_like(corr, dtype=bool)) # Add the mask to the heatmapsns.heatmap(corr, mask=mask, cmap=cmap, center=0, linewidths=1, annot=True, fmt=&quot;.2f&quot;)plt.show() | | :— | :— | . . 2.4 Removing highly correlated features . | | . 2.4.1 Filtering out highly correlated features . You’re going to automate the removal of highly correlated features in the numeric ANSUR dataset. You’ll calculate the correlation matrix and filter out columns that have a correlation coefficient of more than 0.95 or less than -0.95. . Since each correlation coefficient occurs twice in the matrix (correlation of A to B equals correlation of B to A) you’ll want to ignore half of the correlation matrix so that only one of the two correlated features is removed. Use a mask trick for this purpose. . | 123456789101112131415 | # Calculate the correlation matrix and take the absolute valuecorr_matrix = ansur_df.corr().abs() # Create a True/False mask and apply itmask = np.triu(np.ones_like(corr_matrix, dtype=bool))tri_df = corr_matrix.mask(mask) # List column names of highly correlated features (r &gt; 0.95)to_drop = [c for c in tri_df.columns if any(tri_df[c] &gt; 0.95)] # Drop the features in the to_drop listreduced_df = ansur_df.drop(to_drop, axis=1) print(&quot;The reduced dataframe has {} columns.&quot;.format(reduced_df.shape[1]))# The reduced dataframe has 88 columns. | | :— | :— | . You’ve automated the removal of highly correlated features. . 2.4.2 Nuclear energy and pool drownings . The dataset that has been pre-loaded for you as weird_df contains actual data provided by the US Centers for Disease Control &amp; Prevention and Department of Energy. . Let’s see if we can find a pattern. . | 123 | # Put nuclear energy production on the x-axis and the number of pool drownings on the y-axissns.scatterplot(x=&#39;nuclear_energy&#39;, y=&#39;pool_drownings&#39;, data=weird_df)plt.show() | | :— | :— | . . | 12 | # Print out the correlation matrix of weird_dfprint(weird_df.corr()) | | :— | :— | . | 123 | pool_drownings nuclear_energypool_drownings 1.000000 0.901179nuclear_energy 0.901179 1.000000 | | :— | :— | . What can you conclude from the strong correlation (r=0.9) between these features? Not much, correlation does not imply causation. . 3. Feature selection II, selecting for model accuracy . 3.1 Selecting features for model performance . | | . 3.1.1 Building a diabetes classifier . You’ll be using the Pima Indians diabetes dataset to predict whether a person has diabetes using logistic regression. There are 8 features and one target in this dataset. The data has been split into a training and test set and pre-loaded for you as X_train, y_train, X_test, and y_test. . A StandardScaler() instance has been predefined as scaler and a LogisticRegression() one as lr. . | 123456789101112131415 | # Fit the scaler on the training features and transform these in one goX_train_std = scaler.fit_transform(X_train) # Fit the logistic regression model on the scaled training datalr.fit(X_train_std, y_train) # Scale the test featuresX_test_std = scaler.transform(X_test) # Predict diabetes presence on the scaled test sety_pred = lr.predict(X_test_std) # Prints accuracy metrics and feature coefficientsprint(&quot;{0:.1%} accuracy on test set.&quot;.format(accuracy_score(y_test, y_pred)))print(dict(zip(X.columns, abs(lr.coef_[0]).round(2)))) | | :— | :— | . | 12 | 79.6% accuracy on test set.{&#39;family&#39;: 0.34, &#39;diastolic&#39;: 0.03, &#39;glucose&#39;: 1.23, &#39;triceps&#39;: 0.24, &#39;age&#39;: 0.34, &#39;insulin&#39;: 0.19, &#39;bmi&#39;: 0.38, &#39;pregnant&#39;: 0.04} | | :— | :— | . 3.1.2 Manual Recursive Feature Elimination . Now that we’ve created a diabetes classifier, let’s see if we can reduce the number of features without hurting the model accuracy too much. . On the second line of code the features are selected from the original dataframe. Adjust this selection. . A StandardScaler() instance has been predefined as scaler and a LogisticRegression() one as lr. . | 12345 | # Remove the feature with the lowest model coefficientX = diabetes_df[[&#39;pregnant&#39;, &#39;glucose&#39;, &#39;triceps&#39;, &#39;insulin&#39;, &#39;bmi&#39;, &#39;family&#39;, &#39;age&#39;]] 80.6% accuracy on test set.{&#39;family&#39;: 0.34, &#39;glucose&#39;: 1.23, &#39;triceps&#39;: 0.24, &#39;age&#39;: 0.35, &#39;insulin&#39;: 0.2, &#39;bmi&#39;: 0.39, &#39;pregnant&#39;: 0.05} | | :— | :— | . | 12345 | # Remove the 2 features with the lowest model coefficientsX = diabetes_df[[&#39;glucose&#39;, &#39;triceps&#39;, &#39;bmi&#39;, &#39;family&#39;, &#39;age&#39;]] 79.6% accuracy on test set.{&#39;family&#39;: 0.34, &#39;age&#39;: 0.37, &#39;bmi&#39;: 0.34, &#39;glucose&#39;: 1.13, &#39;triceps&#39;: 0.25} | | :— | :— | . | 12345 | # Only keep the feature with the highest coefficientX = diabetes_df[[&#39;glucose&#39;]] 76.5% accuracy on test set.{&#39;glucose&#39;: 1.27} | | :— | :— | . | 12345678910 | # Performs a 25-75% train test splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0) # Scales features and fits the logistic regression model to the datalr.fit(scaler.fit_transform(X_train), y_train) # Calculates the accuracy on the test set and prints coefficientsacc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))print(&quot;{0:.1%} accuracy on test set.&quot;.format(acc))print(dict(zip(X.columns, abs(lr.coef_[0]).round(2)))) | | :— | :— | . Removing all but one feature only reduced the accuracy by a few percent. . 3.1.3 Automatic Recursive Feature Elimination . Now let’s automate this recursive process. Wrap a Recursive Feature Eliminator (RFE) around our logistic regression estimator and pass it the desired number of features. . | 123456789101112131415 | # Create the RFE with a LogisticRegression estimator and 3 features to selectrfe = RFE(estimator=LogisticRegression(), n_features_to_select=3, verbose=1) # Fits the eliminator to the datarfe.fit(X_train, y_train) # Print the features and their ranking (high = dropped early on)print(dict(zip(X.columns, rfe.ranking_))) # Print the features that are not eliminatedprint(X.columns[rfe.support_]) # Calculates the test set accuracyacc = accuracy_score(y_test, rfe.predict(X_test))print(&quot;{0:.1%} accuracy on test set.&quot;.format(acc)) | | :— | :— | . | 12345678 | Fitting estimator with 8 features.Fitting estimator with 7 features.Fitting estimator with 6 features.Fitting estimator with 5 features.Fitting estimator with 4 features.{&#39;family&#39;: 2, &#39;diastolic&#39;: 6, &#39;glucose&#39;: 1, &#39;triceps&#39;: 3, &#39;age&#39;: 1, &#39;insulin&#39;: 4, &#39;bmi&#39;: 1, &#39;pregnant&#39;: 5}Index([&#39;glucose&#39;, &#39;bmi&#39;, &#39;age&#39;], dtype=&#39;object&#39;)80.6% accuracy on test set. | | :— | :— | . When we eliminate all but the 3 most relevant features we get a 80.6% accuracy on the test set. . 3.2 Tree-based feature selection . . 3.2.1 Building a random forest model . You’ll again work on the Pima Indians dataset to predict whether an individual has diabetes. This time using a random forest classifier. You’ll fit the model on the training data after performing the train-test split and consult the feature importance values. . | 123456789101112131415 | # Perform a 75% training and 25% test data splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0) # Fit the random forest model to the training datarf = RandomForestClassifier(random_state=0)rf.fit(X_train, y_train) # Calculate the accuracyacc = accuracy_score(y_test, rf.predict(X_test)) # Print the importances per featureprint(dict(zip(X.columns, rf.feature_importances_.round(2)))) # Print accuracyprint(&quot;{0:.1%} accuracy on test set.&quot;.format(acc)) | | :— | :— | . | 12 | {&#39;family&#39;: 0.12, &#39;diastolic&#39;: 0.08, &#39;glucose&#39;: 0.21, &#39;triceps&#39;: 0.11, &#39;age&#39;: 0.16, &#39;insulin&#39;: 0.13, &#39;bmi&#39;: 0.09, &#39;pregnant&#39;: 0.09}77.6% accuracy on test set. | | :— | :— | . The random forest model gets 78% accuracy on the test set and &#39;glucose&#39; is the most important feature (0.21). . 3.2.2 Random forest for feature selection . | 123456789 | # Create a mask for features importances above the thresholdmask = rf.feature_importances_ &gt; 0.15 # Apply the mask to the feature dataset Xreduced_X = X.loc[:,mask] # prints out the selected column namesprint(reduced_X.columns)# Index([&#39;glucose&#39;, &#39;age&#39;], dtype=&#39;object&#39;) | | :— | :— | . Only the features &#39;glucose&#39; and &#39;age&#39; were considered sufficiently important. . 3.2.3 Recursive Feature Elimination with random forests . You’ll wrap a Recursive Feature Eliminator around a random forest model to remove features step by step. This method is more conservative compared to selecting features after applying a single importance threshold. Since dropping one feature can influence the relative importances of the others. . | 123456789101112 | # Wrap the feature eliminator around the random forest modelrfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, verbose=1) # Fit the model to the training datarfe.fit(X_train, y_train) # Create a mask using an attribute of rfemask = rfe.support_ # Apply the mask to the feature dataset X and print the resultreduced_X = X.loc[:,mask]print(reduced_X.columns) | | :— | :— | . | 1234567 | Fitting estimator with 8 features.Fitting estimator with 7 features.Fitting estimator with 6 features.Fitting estimator with 5 features.Fitting estimator with 4 features.Fitting estimator with 3 features.Index([&#39;glucose&#39;, &#39;bmi&#39;], dtype=&#39;object&#39;) | | :— | :— | . | 123456789101112 | # Set the feature eliminator to remove 2 features on each steprfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, step=2, verbose=1) # Fit the model to the training datarfe.fit(X_train, y_train) # Create a maskmask = rfe.support_ # Apply the mask to the feature dataset X and print the resultreduced_X = X.loc[:, mask]print(reduced_X.columns) | | :— | :— | . | 1234 | Fitting estimator with 8 features.Fitting estimator with 6 features.Fitting estimator with 4 features.Index([&#39;glucose&#39;, &#39;insulin&#39;], dtype=&#39;object&#39;) | | :— | :— | . Compared to the quick and dirty single threshold method from the previous exercise one of the selected features is different. . 3.3 Regularized linear regression . 3.3.1 Creating a LASSO regressor . You’ll be working on the numeric ANSUR body measurements dataset to predict a persons Body Mass Index (BMI) using the pre-imported Lasso() regressor. BMI is a metric derived from body height and weight but those two features have been removed from the dataset to give the model a challenge. . You’ll standardize the data first using the StandardScaler() that has been instantiated for you as scaler to make sure all coefficients face a comparable regularizing force trying to bring them down. . 1234567891011 # Set the test size to 30% to get a 70-30% train test split . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) . &lt;code&gt;&lt;/code&gt; . # Fit the scaler on the training features and transform these in one go . X_train_std = scaler.fit_transform(X_train) . &lt;code&gt;&lt;/code&gt; . # Create the Lasso model . la = Lasso() . &lt;code&gt;&lt;/code&gt; . # Fit it to the standardized training data . la.fit(X_train_std, y_train) . . 3.3.2 Lasso model results . Now that you’ve trained the Lasso model, you’ll score its predictive capacity (R2) on the test set and count how many features are ignored because their coefficient is reduced to zero. . 12345678910111213 # Transform the test set with the pre-fitted scaler . X_test_std = scaler.transform(X_test) . &lt;code&gt;&lt;/code&gt; . # Calculate the coefficient of determination (R squared) on X_test_std . r_squared = la.score(X_test_std, y_test) . print(&quot;The model can predict {0:.1%} of the variance in the test set.&quot;.format(r_squared)) . &lt;code&gt;&lt;/code&gt; . # Create a list that has True values when coefficients equal 0 . zero_coef = la.coef_ == 0 . &lt;code&gt;&lt;/code&gt; . # Calculate how many features have a zero coefficient . n_ignored = sum(zero_coef) . print(&quot;The model has ignored {} out of {} features.&quot;.format(n_ignored, len(la.coef_))) . . | 12 | The model can predict 84.7% of the variance in the test set.The model has ignored 82 out of 91 features. | | :— | :— | . We can predict almost 85% of the variance in the BMI value using just 9 out of 91 of the features. The R^2 could be higher though. . 3.3.3 Adjusting the regularization strength . Your current Lasso model has an R2R2 score of 84.7%. When a model applies overly powerful regularization it can suffer from high bias, hurting its predictive power. . Let’s improve the balance between predictive power and model simplicity by tweaking the alpha parameter. . 1234567891011 # Find the highest alpha value with R-squared above 98% . la = Lasso(alpha=0.1, random_state=0) . &lt;code&gt;&lt;/code&gt; . # Fits the model and calculates performance stats . la.fit(X_train_std, y_train) . r_squared = la.score(X_test_std, y_test) . n_ignored_features = sum(la.coef_ == 0) . &lt;code&gt;&lt;/code&gt; . # Print peformance stats . print(&quot;The model can predict {0:.1%} of the variance in the test set.&quot;.format(r_squared)) . print(&quot;{} out of {} features were ignored.&quot;.format(n_ignored_features, len(la.coef_))) . . | 12 | The model can predict 98.3% of the variance in the test set.64 out of 91 features were ignored. | | :— | :— | . With this more appropriate regularization strength we can predict 98% of the variance in the BMI value while ignoring 2/3 of the features. . 3.4 Combining feature selectors . | | | | | . 3.4.1 Creating a LassoCV regressor . You’ll be predicting biceps circumference on a subsample of the male ANSUR dataset using the LassoCV() regressor that automatically tunes the regularization strength (alpha value) using Cross-Validation. . 1234567891011121314 from sklearn.linear_model import LassoCV . . # Create and fit the LassoCV model on the training set . lcv = LassoCV() . lcv.fit(X_train, y_train) . print(&apos;Optimal alpha = {0:.3f}&apos;.format(lcv.alpha_)) . . # Calculate R squared on the test set . r_squared = lcv.score(X_test, y_test) . print(&apos;The model explains {0:.1%} of the test set variance&apos;.format(r_squared)) . . # Create a mask for coefficients not equal to zero . lcv_mask = lcv.coef_ != 0 . print(&apos;{} features out of {} selected&apos;.format(sum(lcv_mask), len(lcv_mask))) . . | 123 | Optimal alpha = 0.089The model explains 88.2% of the test set variance26 features out of 32 selected | | :— | :— | . We got a decent R squared and removed 6 features. We’ll save the lcv_mask for later on. . 3.4.2 Ensemble models for extra votes . The LassoCV() model selected 26 out of 32 features. Not bad, but not a spectacular dimensionality reduction either. Let’s use two more models to select the 10 features they consider most important using the Recursive Feature Eliminator (RFE). . 1234567891011121314 from sklearn.feature_selection import RFE . from sklearn.ensemble import GradientBoostingRegressor . &lt;code&gt;&lt;/code&gt; . # Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step . rfe_gb = RFE(estimator=GradientBoostingRegressor(), . n_features_to_select=10, step=3, verbose=1) . rfe_gb.fit(X_train, y_train) . &lt;code&gt;&lt;/code&gt; . # Calculate the R squared on the test set . r_squared = rfe_gb.score(X_test, y_test) . print(&apos;The model can explain {0:.1%} of the variance in the test set&apos;.format(r_squared)) . . # Assign the support array to gb_mask . gb_mask = rfe_gb.support_ . . | 123456789 | Fitting estimator with 32 features.Fitting estimator with 29 features.Fitting estimator with 26 features.Fitting estimator with 23 features.Fitting estimator with 20 features.Fitting estimator with 17 features.Fitting estimator with 14 features.Fitting estimator with 11 features.The model can explain 85.6% of the variance in the test set | | :— | :— | . 1234567891011121314 from sklearn.feature_selection import RFE . from sklearn.ensemble import RandomForestRegressor . &lt;code&gt;&lt;/code&gt; . # Select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step . rfe_rf = RFE(estimator=RandomForestRegressor(), . n_features_to_select=10, step=3, verbose=1) . rfe_rf.fit(X_train, y_train) . &lt;code&gt;&lt;/code&gt; . # Calculate the R squared on the test set . r_squared = rfe_rf.score(X_test, y_test) . print(&apos;The model can explain {0:.1%} of the variance in the test set&apos;.format(r_squared)) . &lt;code&gt;&lt;/code&gt; . # Assign the support array to gb_mask . rf_mask = rfe_rf.support_ . . | 123456789 | Fitting estimator with 32 features.Fitting estimator with 29 features.Fitting estimator with 26 features.Fitting estimator with 23 features.Fitting estimator with 20 features.Fitting estimator with 17 features.Fitting estimator with 14 features.Fitting estimator with 11 features.The model can explain 84.0% of the variance in the test set | | :— | :— | . Inluding the Lasso linear model from the previous exercise, we now have the votes from 3 models on which features are important. . 3.4.3 Combining 3 feature selectors . We’ll combine the votes of the 3 models you built in the previous exercises, to decide which features are important into a meta mask. We’ll then use this mask to reduce dimensionality and see how a simple linear regressor performs on the reduced dataset. . 123456789101112131415 # Sum the votes of the three models . votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0) . &lt;code&gt;&lt;/code&gt; . # Create a mask for features selected by all 3 models . meta_mask = votes &gt;= 3 . print(meta_mask) . &lt;code&gt;&lt;/code&gt; . # Apply the dimensionality reduction on X . X_reduced = X.loc[:, meta_mask] . print(X_reduced.columns) . &lt;code&gt;&lt;/code&gt; . # Plug the reduced dataset into a linear regression pipeline . X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0) . lm.fit(scaler.fit_transform(X_train), y_train) . r_squared = lm.score(scaler.transform(X_test), y_test) . print(&apos;The model can explain {0:.1%} of the variance in the test set using {1:} features.&apos;.format(r_squared, len(lm.coef_))) . &lt;code&gt;&lt;/code&gt; . r_squared = lm.score(scaler.transform(X_test), y_test) . print(&apos;The model can explain {0:.1%} of the variance in the test set using {1:} features.&apos;.format(r_squared, len(lm.coef_))) . &lt;code&gt;&lt;/code&gt; . # The model can explain 86.8% of the variance in the test set using 7 features. . . Using the votes from 3 models you were able to select just 7 features that allowed a simple linear model to get a high accuracy! . 4. Feature extraction . 4.1 Feature extraction . | | | | . 4.1.1 Manual feature extraction I . You want to compare prices for specific products between stores. The features in the pre-loaded dataset sales_df are: storeID, product, quantity and revenue. The quantity and revenue features tell you how many items of a particular product were sold in a store and what the total revenue was. For the purpose of your analysis it’s more interesting to know the average price per product. . | 123456 | storeID product quantity revenue0 A Apples 1811 9300.61 A Bananas 1003 3375.22 A Oranges 1604 8528.53 B Apples 1785 9181.04 B Bananas 944 3680.2 | | :— | :— | . 1234567 # Calculate the price from the quantity sold and revenue . sales_df[&apos;price&apos;] = sales_df[&apos;revenue&apos;] / sales_df[&apos;quantity&apos;] . &lt;code&gt;&lt;/code&gt; . # Drop the quantity and revenue features . reduced_df = sales_df.drop([&apos;revenue&apos;, &apos;quantity&apos;], axis=1) . &lt;code&gt;&lt;/code&gt; . print(reduced_df.head()) . . 123456 &apos;pregnant&apos;, . weight_kg height_1 height_2 height_3 . 0 81.5 1.78 1.80 1.80 . 1 72.6 1.70 1.70 1.69 . 2 92.9 1.74 1.75 1.73 . 3 79.4 1.66 1.68 1.67 . 4 94.6 1.91 1.93 1.90 . . When you understand the dataset well, always check if you can calculate relevant features and drop irrelevant ones. . 4.1.2 Manual feature extraction II . You’re working on a variant of the ANSUR dataset, height_df, where a person’s height was measured 3 times. Add a feature with the mean height to the dataset and then drop the 3 original features. . 123456 weight_kg height_1 height_2 height_3 . 0 81.5 1.78 1.80 1.80 . 1 72.6 1.70 1.70 1.69 . 2 92.9 1.74 1.75 1.73 . 3 79.4 1.66 1.68 1.67 . . 1234567 # Calculate the mean height . height_df[&apos;height&apos;] = height_df[[&apos;height_1&apos;,&apos;height_2&apos;,&apos;height_3&apos;]].mean(axis=1) . &lt;code&gt;&lt;/code&gt; . # Drop the 3 original height features . reduced_df = height_df.drop([&apos;height_1&apos;,&apos;height_2&apos;,&apos;height_3&apos;], axis=1) . &lt;code&gt;&lt;/code&gt; . print(reduced_df.head()) . . 123456 weight_kg height . 0 81.5 1.793333 . 1 72.6 1.696667 . 2 92.9 1.740000 . 3 79.4 1.670000 . 4 94.6 1.913333 . . 4.1.3 Principal component intuition . . After standardizing the lower and upper arm lengths from the ANSUR dataset we’ve added two perpendicular vectors that are aligned with the main directions of variance. We can describe each point in the dataset as a combination of these two vectors multiplied with a value each. These values are then called principal components. . People with a negative component for the yellow vector have long forearms relative to their upper arms. . 4.2 Principal component analysis . | | | | | | . 4.2.1 Calculating Principal Components . You’ll visually inspect a 4 feature sample of the ANSUR dataset before and after PCA using Seaborn’s pairplot(). This will allow you to inspect the pairwise correlations between the features. . 1234 # Create a pairplot to inspect ansur_df . sns.pairplot(ansur_df) . &lt;code&gt;&lt;/code&gt; . plt.show() . . . 123456789101112131415 from sklearn.preprocessing import StandardScaler . from sklearn.decomposition import PCA . &lt;code&gt;&lt;/code&gt; . # Create the scaler . scaler = StandardScaler() . ansur_std = scaler.fit_transform(ansur_df) . &lt;code&gt;&lt;/code&gt; . # Create the PCA instance and fit and transform the data with pca . pca = PCA() . pc = pca.fit_transform(ansur_std) . &lt;code&gt;&lt;/code&gt; . # This changes the numpy array output back to a dataframe . pc_df = pd.DataFrame(pc, columns=[&apos;PC 1&apos;, &apos;PC 2&apos;, &apos;PC 3&apos;, &apos;PC 4&apos;]) . &lt;code&gt;&lt;/code&gt; . # Create a pairplot of the principal component dataframe . sns.pairplot(pc_df) . plt.show() . . . Notice how, in contrast to the input features, none of the principal components are correlated to one another. . 4.2.2 PCA on a larger dataset . You’ll now apply PCA on a somewhat larger ANSUR datasample with 13 dimensions, once again pre-loaded as ansur_df. The fitted model will be used in the next exercise. Since we are not using the principal components themselves there is no need to transform the data, instead, it is sufficient to fit pca to the data. . 12345678910 from sklearn.preprocessing import StandardScaler . from sklearn.decomposition import PCA . &lt;code&gt;&lt;/code&gt; . # Scale the data . scaler = StandardScaler() . ansur_std = scaler.fit_transform(ansur_df) . &lt;code&gt;&lt;/code&gt; . # Apply PCA . pca = PCA() . pca.fit(ansur_std) . . You’ve fitted PCA on our 13 feature data samples. Now let’s see how the components explain the variance. . 4.2.3 PCA explained variance . You’ll be inspecting the variance explained by the different principal components of the pca instance you created in the previous exercise. . 123456 # Inspect the explained variance ratio per component . print(pca.explained_variance_ratio_) . &lt;code&gt;&lt;/code&gt; . &lt;script.py&gt; output: . [0.61449404 0.19893965 0.06803095 0.03770499 0.03031502 0.0171759 . 0.01072762 0.00656681 0.00634743 0.00436015 0.0026586 0.00202617 . 0.00065268] . . 123456 # Print the cumulative sum of the explained variance ratio . print(pca.explained_variance_ratio_.cumsum()) . &lt;code&gt;&lt;/code&gt; . &lt;code&gt;&lt;/code&gt; . &lt;script.py&gt; output: . [0.61449404 0.81343368 0.88146463 0.91916962 0.94948464 0.96666054 . 0.97738816 0.98395496 0.99030239 0.99466254 0.99732115 0.99934732 . 1. ] . . What’s the lowest number of principal components you should keep if you don’t want to lose more than 10% of explained variance during dimensionality reduction? 4 principal components . Using no more than 4 principal components we can explain more than 90% of the variance in the 13 feature dataset. . 4.3 PCA applications . | | | | | | | | . 4.3.1 Understanding the components . You’ll apply PCA to the numeric features of the Pokemon dataset, poke_df, using a pipeline to combine the feature scaling and PCA in one go. You’ll then interpret the meanings of the first two components. . 1234567891011121314 # Build the pipeline . pipe = Pipeline([(&apos;scaler&apos;, StandardScaler()), . (&apos;reducer&apos;, PCA(n_components=2))]) . . # Fit it to the dataset and extract the component vectors . pipe.fit(poke_df) . vectors = pipe.steps[1][1].components_.round(2) . . # Print feature effects . print(&apos;PC 1 effects = &apos; + str(dict(zip(poke_df.columns, vectors[0])))) . print(&apos;PC 2 effects = &apos; + str(dict(zip(poke_df.columns, vectors[1])))) . &lt;code&gt;&lt;/code&gt; . PC 1 effects = {&apos;HP&apos;: 0.39, &apos;Attack&apos;: 0.44, &apos;Defense&apos;: 0.36, &apos;Sp. Atk&apos;: 0.46, &apos;Sp. Def&apos;: 0.45, &apos;Speed&apos;: 0.34} . PC 2 effects = {&apos;HP&apos;: 0.08, &apos;Attack&apos;: -0.01, &apos;Defense&apos;: 0.63, &apos;Sp. Atk&apos;: -0.31, &apos;Sp. Def&apos;: 0.24, &apos;Speed&apos;: -0.67} . . PC1: All features have a similar positive effect. PC 1 can be interpreted as a measure of overall quality (high stats). | PC2: Defense has a strong positive effect on the second component and speed a strong negative one. This component quantifies an agility vs. armor &amp; protection trade-off. . You’ve used the pipeline for the first time and understand how the features relate to the components. . | . Question . Inspect the PC 1 effects. Which statement is true? | . Possible Answers . Sp. Atk has the biggest effect on this feature by far. PC 1 can be interpreted as a measure of how good a Pokemon’s special attack is. | All features have a similar positive effect. PC 1 can be interpreted as a measure of overall quality (high stats). | . **** . Question . Inspect the PC 2 effects. Which statement is true? | . Possible Answers . Defense has a strong positive effect on the second component and speed a strong negative one. This component quantifies an agility vs. armor &amp; protection trade-off. | Fast Pokemon have high values for this component. | . 4.3.2 PCA for feature exploration . You’ll use the PCA pipeline you’ve built in the previous exercise to visually explore how some categorical features relate to the variance in poke_df. These categorical features (Type &amp; Legendary) can be found in a separate dataframe poke_cat_df. . 1234567 poke_df.head() . &lt;code&gt;&lt;/code&gt; . HP Attack Defense Sp. Atk Sp. Def Speed . 0 45 49 49 65 65 45 . 1 60 62 63 80 80 60 . 2 80 82 83 100 100 80 . 3 80 100 123 122 120 80 . 4 39 52 43 60 50 65 . . 12345 # Build the pipeline . pipe = Pipeline([(&apos;scaler&apos;, StandardScaler()), . (&apos;reducer&apos;, PCA(n_components=2))]) . . # Fit the pipeline to poke_df and transform the data . pc = pipe.fit_transform(poke_df) . &lt;code&gt;&lt;/code&gt; . print(pc) . . 12345678 &lt;script.py&gt; output: . [[-1.5563747 -0.02148212] . [-0.36286656 -0.05026854] . [ 1.28015158 -0.06272022] . ... . [ 2.45821626 -0.51588158] . [ 3.5303971 -0.95106516] . [ 2.23378629 0.53762985]] . . 123 # Add the 2 components to poke_cat_df . # Add the 2 components to poke_cat_df . poke_cat_df[&apos;PC 1&apos;] = pc[:, 0] . poke_cat_df[&apos;PC 2&apos;] = pc[:, 1] . . print(poke_cat_df.head()) . . 1234 # Use the Type feature to color the PC 1 vs PC 2 scatterplot . sns.scatterplot(data=poke_cat_df, . x=&apos;PC 1&apos;, y=&apos;PC 2&apos;, hue=&apos;Type&apos;) . plt.show() . . . 1234 # Use the Legendary feature to color the PC 1 vs PC 2 scatterplot . sns.scatterplot(data=poke_cat_df, . x=&apos;PC 1&apos;, y=&apos;PC 2&apos;, hue=&apos;Legendary&apos;) . plt.show() . . . Looks like the different types are scattered all over the place while the legendary pokemon always score high for PC 1 meaning they have high stats overall. Their spread along the PC 2 axis tells us they aren’t consistently fast and vulnerable or slow and armored. . 4.3.3 PCA in a model pipeline . We just saw that legendary pokemon tend to have higher stats overall. Let’s see if we can add a classifier to our pipeline that detects legendary versus non-legendary pokemon based on the principal components. . The data has been pre-loaded for you and split into training and tests datasets: X_train, X_test, y_train, y_test. . Same goes for all relevant packages and classes(Pipeline(), StandardScaler(), PCA(), RandomForestClassifier()). . 123456789101112131415 # Build the pipeline . pipe = Pipeline([ . (&apos;scaler&apos;, StandardScaler()), . (&apos;reducer&apos;, PCA(n_components=2)), . (&apos;classifier&apos;, RandomForestClassifier(random_state=0))]) . &lt;code&gt;&lt;/code&gt; . # Fit the pipeline to the training data . pipe.fit(X_train, y_train) . &lt;code&gt;&lt;/code&gt; . # Prints the explained variance ratio . print(pipe.steps[1][1].explained_variance_ratio_) . &lt;code&gt;&lt;/code&gt; . # Score the accuracy on the test set . accuracy = pipe.score(X_test, y_test) . . # Prints the model accuracy . print(&apos;{0:.1%} test set accuracy&apos;.format(accuracy)) . . | # Build the pipeline . pipe = Pipeline([ . (&apos;scaler&apos;, StandardScaler()), . (&apos;reducer&apos;, PCA(n_components=3)), . (&apos;classifier&apos;, RandomForestClassifier(random_state=0))]) . . # Fit the pipeline to the training data . pipe.fit(X_train, y_train) . . # Score the accuracy on the test set . accuracy = pipe.score(X_test, y_test) . . # Prints the explained variance ratio and accuracy . print(pipe.steps[1][1].explained_variance_ratio_) . print(&apos;{0:.1%} test set accuracy&apos;.format(accuracy)) . | . | &lt;script.py&gt; output: . [0.45624044 0.17767414] . &lt;code&gt;&lt;/code&gt; . &lt;script.py&gt; output: . 95.8% test set accuracy . . &lt;script.py&gt; output: . [0.45624044 0.17767414 0.12858833] . 95.0% test set accuracy . | . Looks like adding the third component does not increase the model accuracy, even though it adds information to the dataset. . 4.4 Principal Component selection . | | | | | | | . 4.4.1 Selecting the proportion of variance to keep . You’ll let PCA determine the number of components to calculate based on an explained variance threshold that you decide. . 12345678 # Pipe a scaler to PCA selecting 80% of the variance . pipe = Pipeline([(&apos;scaler&apos;, StandardScaler()), . (&apos;reducer&apos;, PCA(n_components=0.8))]) . . # Fit the pipe to the data . pipe.fit(ansur_df) . . print(&apos;{} components selected&apos;.format(len(pipe.steps[1][1].components_))) . &lt;code&gt;&lt;/code&gt; . # Pipe a scaler to PCA selecting 80% of the variance . pipe = Pipeline([(&apos;scaler&apos;, StandardScaler()), . (&apos;reducer&apos;, PCA(n_components=0.9))]) . &lt;code&gt;&lt;/code&gt; . # Fit the pipe to the data . pipe.fit(ansur_df) . . print(&apos;{} components selected&apos;.format(len(pipe.steps[1][1].components_))) . . 12345 &lt;script.py&gt; output: . 11 components selected . . &lt;script.py&gt; output: . 23 components selected . . We need to more than double the components to go from 80% to 90% explained variance. . Question . How many additional features do you need to explain 90% instead of 80% of the variance? | . Possible Answers . 11 | 12 | 23 | . 4.4.2 Choosing the number of components . You’ll now make a more informed decision on the number of principal components to reduce your data to using the “elbow in the plot” technique. . 12345678910111213 # Pipeline a scaler and PCA selecting 10 components . pipe = Pipeline([(&apos;scaler&apos;, StandardScaler()), . (&apos;reducer&apos;, PCA(n_components=10))]) . &lt;code&gt;&lt;/code&gt; . # Fit the pipe to the data . pipe.fit(ansur_df) . &lt;code&gt;&lt;/code&gt; . # Plot the explained variance ratio . plt.plot(pipe.steps[1][1].explained_variance_ratio_) . . plt.xlabel(&apos;Principal component index&apos;) . plt.ylabel(&apos;Explained variance ratio&apos;) . plt.show() . . . To how many components can you reduce the dataset without compromising too much on explained variance? Note that the x-axis is zero indexed. . The ‘elbow’ in the plot is at 3 components (the 3rd component has index 2). . Question . To how many components can you reduce the dataset without compromising too much on explained variance? Note that the x-axis is zero indexed. | . Possible Answers . 1 | 2 | 3 | 4 | . 4.4.3 PCA for image compression . You’ll reduce the size of 16 images with hand written digits (MNIST dataset) using PCA. . The samples are 28 by 28 pixel gray scale images that have been flattened to arrays with 784 elements each (28 x 28 = 784) and added to the 2D numpy array X_test. Each of the 784 pixels has a value between 0 and 255 and can be regarded as a feature. . A pipeline with a scaler and PCA model to select 78 components has been pre-loaded for you as pipe. This pipeline has already been fitted to the entire MNIST dataset except for the 16 samples in X_test. . Finally, a function plot_digits has been created for you that will plot 16 images in a grid. . 12 # Plot the MNIST sample data . plot_digits(X_test) . . . | 1234 | pipePipeline(memory=None, steps=[(&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with_std=True)), (&#39;reducer&#39;, PCA(copy=True, iterated_power=&#39;auto&#39;, n_components=78, random_state=None, svd_solver=&#39;auto&#39;, tol=0.0, whiten=False))]) | | :— | :— | . 12345678910111213141516171819 # Transform the input data to principal components . pc = pipe.transform(X_test) . &lt;code&gt;&lt;/code&gt; . # Prints the number of features per dataset . print(&quot;X_test has {} features&quot;.format(X_test.shape[1])) . print(&quot;pc has {} features&quot;.format(pc.shape[1])) . . # X_test has 784 features . # pc has 78 features . # Inverse transform the components to original feature space . X_rebuilt = pipe.inverse_transform(pc) . &lt;code&gt;&lt;/code&gt; . # Prints the number of features . print(&quot;X_rebuilt has {} features&quot;.format(X_rebuilt.shape[1])) . &lt;code&gt;&lt;/code&gt; . # X_rebuilt has 784 features . &lt;code&gt;&lt;/code&gt; . # Plot the reconstructed data . plot_digits(X_rebuilt) . . . You’ve reduced the size of the data 10 fold but were able to reconstruct images with reasonable quality. . The End . . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/blogging/demo/2021/06/30/dimensionality-reduction-in-python.html",
            "relUrl": "/blogging/demo/2021/06/30/dimensionality-reduction-in-python.html",
            "date": " • Jun 30, 2021"
        }
        
    
  
    
        ,"post49": {
            "title": "Cluster Analysis in Python",
            "content": "Cluster Analysis in Python . This is the memo of the 6th course (23 courses in all) of ‘Machine Learning Scientist with Python’ skill track. You can find the original course HERE. . Course Description . You have probably come across Google News, which automatically groups similar news articles under a topic. Have you ever wondered what process runs in the background to arrive at these groups? In this course, you will be introduced to unsupervised learning through clustering using the SciPy library in Python. This course covers pre-processing of data and application of hierarchical and k-means clustering. Through the course, you will explore player statistics from a popular football video game, FIFA 18. After completing the course, you will be able to quickly apply various clustering algorithms on data, visualize the clusters formed and analyze results. . . Introduction to Clustering | Hierarchical Clustering | K-Means Clustering | Clustering in Real World | 1. Introduction to Clustering . 1.1 Unsupervised learning: basics . . 1.1.1 Unsupervised learning in real world . Which of the following examples can be solved with unsupervised learning? . A list of tweets to be classified based on their sentiment, the data has tweets associated with a positive or negative sentiment. | A spam recognition system that marks incoming emails as spam, the data has emails marked as spam and not spam. | Segmentation of learners at DataCamp based on courses they complete. The training data has no labels.press | . 1.1.2 Pokémon sightings . There have been reports of sightings of rare, legendary Pokémon. You have been asked to investigate! Plot the coordinates of sightings to find out where the Pokémon might be. The X and Y coordinates of the points are stored in list x and y, respectively. . | 12345678 | # Import plotting class from matplotlib libraryfrom matplotlib import pyplot as plt # Create a scatter plotplt.scatter(x, y) # Display the scatter plotplt.show() | | :— | :— | . . Notice the areas where the sightings are dense. This indicates that there is not one, but two legendary Pokémon out there! . 1.2 Basics of cluster analysis . | | . Hierarchical clustering algorithms . | | | | | | . K-means clustering algorithms . | | | | | . 1.2.1 Pokémon sightings: hierarchical clustering . We are going to continue the investigation into the sightings of legendary Pokémon from the previous exercise. Remember that in the scatter plot of the previous exercise, you identified two areas where Pokémon sightings were dense. This means that the points seem to separate into two clusters. In this exercise, you will form two clusters of the sightings using hierarchical clustering. . | 1234567 | df.head() x y0 9 81 6 42 2 103 3 64 1 0 | | :— | :— | . | 123456789101112 | # Import linkage and fcluster functionsfrom scipy.cluster.hierarchy import linkage, fcluster # Use the linkage() function to compute distanceZ = linkage(df, &#39;ward&#39;) # Generate cluster labelsdf[&#39;cluster_labels&#39;] = fcluster(Z, 2, criterion=&#39;maxclust&#39;) # Plot the points with seabornsns.scatterplot(x=&#39;x&#39;, y=&#39;y&#39;, hue=&#39;cluster_labels&#39;, data=df)plt.show() | | :— | :— | . | 123456789101112131415161718 | type(Z)numpy.ndarray Z[:3]array([[10., 13., 0., 2.], [15., 19., 0., 2.], [ 1., 5., 1., 2.]]) df x y cluster_labels0 9 8 21 6 4 2...8 1 6 29 7 1 210 23 29 111 26 25 1... | | :— | :— | . . Notice that the cluster labels are plotted with different colors. You will notice that the resulting plot has an extra cluster labelled 0 in the legend. This will be explained later in the course. . 1.2.2 Pokémon sightings: k-means clustering . We are going to continue the investigation into the sightings of legendary Pokémon from the previous exercise. Just like the previous exercise, we will use the same example of Pokémon sightings. In this exercise, you will form clusters of the sightings using k-means clustering. . | 123456789101112 | # Import kmeans and vq functionsfrom scipy.cluster.vq import kmeans, vq # Compute cluster centerscentroids,_ = kmeans(df, 2) # Assign cluster labelsdf[&#39;cluster_labels&#39;], _ = vq(df, centroids) # Plot the points with seabornsns.scatterplot(x=&#39;x&#39;, y=&#39;y&#39;, hue=&#39;cluster_labels&#39;, data=df)plt.show() | | :— | :— | . | 123 | centroidsarray([[23.7, 28. ], [ 4.3, 5.9]]) | | :— | :— | . . Notice that in this case, the results of both types of clustering are similar. We will look at distinctly different results later in the course. . 1.3 Data preparation for cluster analysis . | | | . 1.3.1 Normalize basic list data . Now that you are aware of normalization, let us try to normalize some data. goals_for is a list of goals scored by a football team in their last ten matches. Let us standardize the data using the whiten() function. . | 12345678910 | # Import the whiten functionfrom scipy.cluster.vq import whiten goals_for = [4,3,2,3,1,1,2,0,1,4] # Use the whiten() function to standardize the datascaled_data = whiten(goals_for)print(scaled_data)# [3.07692308 2.30769231 1.53846154 2.30769231 0.76923077 0.76923077 1.53846154 0. 0.76923077 3.07692308] | | :— | :— | . 1.3.2 Visualize normalized data . After normalizing your data, you can compare the scaled data to the original data to see the difference. The variables from the last exercise, goals_for and scaled_data are already available to you. . | 1234567891011 | # Plot original dataplt.plot(goals_for, label=&#39;original&#39;) # Plot scaled dataplt.plot(scaled_data, label=&#39;scaled&#39;) # Show the legend in the plotplt.legend() # Display the plotplt.show() | | :— | :— | . . 1.3.3 Normalization of small numbers . In earlier examples, you have normalization of whole numbers. In this exercise, you will look at the treatment of fractional numbers – the change of interest rates in the country of Bangalla over the years. . | 1234567891011121314 | # Prepare datarate_cuts = [0.0025, 0.001, -0.0005, -0.001, -0.0005, 0.0025, -0.001, -0.0015, -0.001, 0.0005] # Use the whiten() function to standardize the datascaled_data = whiten(rate_cuts) # Plot original dataplt.plot(rate_cuts, label=&#39;original&#39;) # Plot scaled dataplt.plot(scaled_data, label=&#39;scaled&#39;) plt.legend()plt.show() | | :— | :— | . . Notice how the changes in the original data are negligible as compared to the scaled data . 1.3.4 FIFA 18: Normalize data . FIFA 18 is a football video game that was released in 2017 for PC and consoles. The dataset that you are about to work on contains data on the 1000 top individual players in the game. You will explore various features of the data as we move ahead in the course. In this exercise, you will work with two columns, eur_wage, the wage of a player in Euros and eur_value, their current transfer market value. . The data for this exercise is stored in a Pandas dataframe, fifa. whiten from scipy.cluster.vq and matplotlib.pyplot as plt have been pre-loaded. . | 12345678910 | # Scale wage and valuefifa[&#39;scaled_wage&#39;] = whiten(fifa[&#39;eur_wage&#39;])fifa[&#39;scaled_value&#39;] = whiten(fifa[&#39;eur_value&#39;]) # Plot the two columns in a scatter plotfifa.plot(x=&#39;scaled_wage&#39;, y=&#39;scaled_value&#39;, kind = &#39;scatter&#39;)plt.show() # Check mean and standard deviation of scaled valuesprint(fifa[[&#39;scaled_wage&#39;, &#39;scaled_value&#39;]].describe()) | | :— | :— | . | 123456789 | scaled_wage scaled_valuecount 1000.00 1000.00mean 1.12 1.31std 1.00 1.00min 0.00 0.0025% 0.47 0.7350% 0.85 1.0275% 1.41 1.54max 9.11 8.98 | | :— | :— | . . As you can see the scaled values have a standard deviation of 1. . 2. Hierarchical Clustering . 2.1 Basics of hierarchical clustering . | | | | | | | . 2.1.1 Hierarchical clustering: ward method . It is time for Comic-Con! Comic-Con is an annual comic-based convention held in major cities in the world. You have the data of last year’s footfall, the number of people at the convention ground at a given time. . You would like to decide the location of your stall to maximize sales. Using the ward method, apply hierarchical clustering to find the two points of attraction in the area. . | 123456789 | comic_con x_coordinate y_coordinate x_scaled y_scaled0 17 4 0.509349 0.0900101 20 6 0.599234 0.1350152 35 0 1.048660 0.0000003 14 0 0.419464 0.0000004 37 4 1.108583 0.0900105 33 3 0.988736 0.0675076 14 1 0.419464 0.022502 | | :— | :— | . | 12345678910111213 | # Import the fcluster and linkage functionsfrom scipy.cluster.hierarchy import linkage, fcluster # Use the linkage() functiondistance_matrix = linkage(comic_con[[&#39;x_scaled&#39;, &#39;y_scaled&#39;]], method = &#39;ward&#39;, metric = &#39;euclidean&#39;) # Assign cluster labelscomic_con[&#39;cluster_labels&#39;] = fcluster(distance_matrix, 2, criterion=&#39;maxclust&#39;) # Plot clusterssns.scatterplot(x=&#39;x_scaled&#39;, y=&#39;y_scaled&#39;, hue=&#39;cluster_labels&#39;, data = comic_con)plt.show() | | :— | :— | . . Notice the two clusters correspond to the points of attractions in the figure towards the bottom (a stage) and the top right (an interesting stall). . 2.1.2 Hierarchical clustering: single method . Let us use the same footfall dataset and check if any changes are seen if we use a different method for clustering. . | 12345678910111213 | # Import the fcluster and linkage functionsfrom scipy.cluster.hierarchy import fcluster, linkage # Use the linkage() functiondistance_matrix = linkage(comic_con[[&#39;x_scaled&#39;, &#39;y_scaled&#39;]], method = &#39;single&#39;, metric = &#39;euclidean&#39;) # Assign cluster labelscomic_con[&#39;cluster_labels&#39;] = fcluster(distance_matrix, 2, criterion=&#39;maxclust&#39;) # Plot clusterssns.scatterplot(x=&#39;x_scaled&#39;, y=&#39;y_scaled&#39;, hue=&#39;cluster_labels&#39;, data = comic_con)plt.show() | | :— | :— | . Notice that in this example, the clusters formed are not different from the ones created using the ward method. . 2.1.3 Hierarchical clustering: complete method . For the third and final time, let us use the same footfall dataset and check if any changes are seen if we use a different method for clustering. . | 12345678910111213 | # Import the fcluster and linkage functionsfrom scipy.cluster.hierarchy import linkage, fcluster # Use the linkage() functiondistance_matrix = linkage(comic_con[[&#39;x_scaled&#39;, &#39;y_scaled&#39;]], method=&#39;complete&#39;, metric=&#39;euclidean&#39;) # Assign cluster labelscomic_con[&#39;cluster_labels&#39;] = fcluster(distance_matrix,2,criterion=&#39;maxclust&#39;) # Plot clusterssns.scatterplot(x=&#39;x_scaled&#39;, y=&#39;y_scaled&#39;, hue=&#39;cluster_labels&#39;, data = comic_con)plt.show() | | :— | :— | . Coincidentally, the clusters formed are not different from the ward or single methods. Next, let us learn how to visualize clusters. . 2.2 Visualize clusters . | | | . 2.2.1 Visualize clusters with matplotlib . We have discussed that visualizations are necessary to assess the clusters that are formed and spot trends in your data. Let us now focus on visualizing the footfall dataset from Comic-Con using the matplotlib module. . | 1234567891011 | # Import the pyplot classfrom matplotlib import pyplot as plt # Define a colors dictionary for clusterscolors = {1:&#39;red&#39;, 2:&#39;blue&#39;} # Plot a scatter plotcomic_con.plot.scatter(x = &#39;x_scaled&#39;, y = &#39;y_scaled&#39;, c = comic_con[&#39;cluster_labels&#39;].apply(lambda x: colors[x]))plt.show() | | :— | :— | . . 2.2.2 Visualize clusters with seaborn . Let us now visualize the footfall dataset from Comic Con using the seaborn module. Visualizing clusters using seaborn is easier with the inbuild hue function for cluster labels. . | 123456789 | # Import the seaborn moduleimport seaborn as sns # Plot a scatter plot using seabornsns.scatterplot(x=&#39;x_scaled&#39;, y=&#39;y_scaled&#39;, hue=&#39;cluster_labels&#39;, data = comic_con)plt.show() | | :— | :— | . . Notice the legend is automatically shown when using the hue argument. . 2.3 How many clusters? . | | | . 2.3.1 Create a dendrogram . Dendrograms are branching diagrams that show the merging of clusters as we move through the distance matrix. Let us use the Comic Con footfall data to create a dendrogram. . | 12345678 | # Import the dendrogram functionfrom scipy.cluster.hierarchy import dendrogram # Create a dendrogramdn = dendrogram(distance_matrix) # Display the dendogramplt.show() | | :— | :— | . . 2.4 Limitations of hierarchical clustering . | | . 2.4.1 FIFA 18: exploring defenders . In the FIFA 18 dataset, various attributes of players are present. Two such attributes are: . sliding tackle: a number between 0-99 which signifies how accurate a player is able to perform sliding tackles | aggression: a number between 0-99 which signifies the commitment and will of a player | . These are typically high in defense-minded players. In this exercise, you will perform clustering based on these attributes in the data. . | 123456789 | fifa.head() sliding_tackle aggression ... scaled_aggression cluster_labels0 23 63 ... 3.72 31 26 48 ... 2.84 32 33 56 ... 3.31 33 38 78 ... 4.61 34 11 29 ... 1.71 2 [5 rows x 5 columns] | | :— | :— | . | 123456789101112 | # Fit the data into a hierarchical clustering algorithmdistance_matrix = linkage(fifa[[&#39;scaled_sliding_tackle&#39;, &#39;scaled_aggression&#39;]], &#39;ward&#39;) # Assign cluster labels to each row of datafifa[&#39;cluster_labels&#39;] = fcluster(distance_matrix, 3, criterion=&#39;maxclust&#39;) # Display cluster centers of each clusterprint(fifa[[&#39;scaled_sliding_tackle&#39;, &#39;scaled_aggression&#39;, &#39;cluster_labels&#39;]].groupby(&#39;cluster_labels&#39;).mean()) # Create a scatter plot through seabornsns.scatterplot(x=&#39;scaled_sliding_tackle&#39;, y=&#39;scaled_aggression&#39;, hue=&#39;cluster_labels&#39;, data=fifa)plt.show() | | :— | :— | . | 12345 | scaled_sliding_tackle scaled_aggressioncluster_labels 1 2.99 4.352 0.74 1.943 1.34 3.62 | | :— | :— | . . 3. K-Means Clustering . 3.1 Basics of k-means clustering . | | | | | | . 3.1.1 K-means clustering: first exercise . This exercise will familiarize you with the usage of k-means clustering on a dataset. Let us use the Comic Con dataset and check how k-means clustering works on it. . Recall the two steps of k-means clustering: . Define cluster centers through kmeans() function. It has two required arguments: observations and number of clusters. | Assign cluster labels through the vq() function. It has two required arguments: observations and cluster centers. | . | 12345678910111213 | # Import the kmeans and vq functionsfrom scipy.cluster.vq import kmeans, vq # Generate cluster centerscluster_centers, distortion = kmeans(comic_con[[&#39;x_scaled&#39;,&#39;y_scaled&#39;]], 2) # Assign cluster labelscomic_con[&#39;cluster_labels&#39;], distortion_list = vq(comic_con[[&#39;x_scaled&#39;,&#39;y_scaled&#39;]],cluster_centers) # Plot clusterssns.scatterplot(x=&#39;x_scaled&#39;, y=&#39;y_scaled&#39;, hue=&#39;cluster_labels&#39;, data = comic_con)plt.show() | | :— | :— | . . Notice that the clusters formed are exactly the same as hierarchical clustering that you did in the previous chapter. . 3.1.2 Runtime of k-means clustering . Recall that it took a significantly long time to run hierarchical clustering. How long does it take to run the kmeans() function on the FIFA dataset? . | 12345 | %timeit kmeans(fifa[[&#39;scaled_sliding_tackle&#39;,&#39;scaled_aggression&#39;]],3)# 10 loops, best of 3: 69.7 ms per loop %timeit linkage(fifa[[&#39;scaled_sliding_tackle&#39;,&#39;scaled_aggression&#39;]], method = &#39;ward&#39;, metric = &#39;euclidean&#39;)# 1 loop, best of 3: 703 ms per loop | | :— | :— | . 3.2 How many clusters? . | | | | | | . 3.2.1 Elbow method on distinct clusters . Let us use the comic con data set to see how the elbow plot looks on a data set with distinct, well-defined clusters. You may want to display the data points before proceeding with the exercise. . | 123456789101112131415 | distortions = []num_clusters = range(1, 7) # Create a list of distortions from the kmeans functionfor i in num_clusters: cluster_centers, distortion = kmeans(comic_con[[&#39;x_scaled&#39;,&#39;y_scaled&#39;]],i) distortions.append(distortion) # Create a data frame with two lists - num_clusters, distortionselbow_plot = pd.DataFrame({&#39;num_clusters&#39;: num_clusters, &#39;distortions&#39;: distortions}) # Creat a line plot of num_clusters and distortionssns.lineplot(x=&#39;num_clusters&#39;, y=&#39;distortions&#39;, data = elbow_plot)plt.xticks(num_clusters)plt.show() | | :— | :— | . . From the elbow plot, there are 2 clusters in the data. . 3.2.2 Elbow method on uniform data . In the earlier exercise, you constructed an elbow plot on data with well-defined clusters. Let us now see how the elbow plot looks on a data set with uniformly distributed points. You may want to display the data points on the console before proceeding with the exercise. . | 123456789101112131415 | distortions = []num_clusters = range(2, 7) # Create a list of distortions from the kmeans functionfor i in num_clusters: cluster_centers, distortion = kmeans(uniform_data[[&#39;x_scaled&#39;,&#39;y_scaled&#39;]],i) distortions.append(distortion) # Create a data frame with two lists - number of clusters and distortionselbow_plot = pd.DataFrame({&#39;num_clusters&#39;: num_clusters, &#39;distortions&#39;: distortions}) # Creat a line plot of num_clusters and distortionssns.lineplot(x=&#39;num_clusters&#39;, y=&#39;distortions&#39;, data = elbow_plot)plt.xticks(num_clusters)plt.show() | | :— | :— | . . From the elbow plot, we can not determine how many clusters in the data. . 3.3 Limitations of k-means clustering . | | | | | . 3.3.1 Impact of seeds on distinct clusters . You noticed the impact of seeds on a dataset that did not have well-defined groups of clusters. In this exercise, you will explore whether seeds impact the clusters in the Comic Con data, where the clusters are well-defined. . | 123456789101112131415 | # Import random classfrom numpy import random # Initialize seedrandom.seed(0)random.seed([1, 2, 1000]) # Run kmeans clusteringcluster_centers, distortion = kmeans(comic_con[[&#39;x_scaled&#39;, &#39;y_scaled&#39;]], 2)comic_con[&#39;cluster_labels&#39;], distortion_list = vq(comic_con[[&#39;x_scaled&#39;, &#39;y_scaled&#39;]], cluster_centers) # Plot the scatterplotsns.scatterplot(x=&#39;x_scaled&#39;, y=&#39;y_scaled&#39;, hue=&#39;cluster_labels&#39;, data = comic_con)plt.show() | | :— | :— | . . Notice that the plots have not changed after changing the seed as the clusters are well-defined. . 3.3.2 Uniform clustering patterns . Now that you are familiar with the impact of seeds, let us look at the bias in k-means clustering towards the formation of uniform clusters. . Let us use a mouse-like dataset for our next exercise. A mouse-like dataset is a group of points that resemble the head of a mouse: it has three clusters of points arranged in circles, one each for the face and two ears of a mouse. . Here is how a typical mouse-like dataset looks like (Source). . | 12345678910111213 | # Import the kmeans and vq functionsfrom scipy.cluster.vq import kmeans, vq # Generate cluster centerscluster_centers, distortion = kmeans(mouse[[&#39;x_scaled&#39;,&#39;y_scaled&#39;]],3) # Assign cluster labelsmouse[&#39;cluster_labels&#39;], distortion_list = vq(mouse[[&#39;x_scaled&#39;,&#39;y_scaled&#39;]],cluster_centers) # Plot clusterssns.scatterplot(x=&#39;x_scaled&#39;, y=&#39;y_scaled&#39;, hue=&#39;cluster_labels&#39;, data = mouse)plt.show() | | :— | :— | . . Notice that kmeans is unable to capture the three visible clusters clearly, and the two clusters towards the top have taken in some points along the boundary. This happens due to the underlying assumption in kmeans algorithm to minimize distortions which leads to clusters that are similar in terms of area. . 3.3.3 FIFA 18: defenders revisited . In the FIFA 18 dataset, various attributes of players are present. Two such attributes are: . defending: a number which signifies the defending attributes of a player | physical: a number which signifies the physical attributes of a player | . These are typically defense-minded players. In this exercise, you will perform clustering based on these attributes in the data. . | 123456789101112131415 | # Set up a random seed in numpyrandom.seed([1000,2000]) # Fit the data into a k-means algorithmcluster_centers,_ = kmeans(fifa[[&#39;scaled_def&#39;, &#39;scaled_phy&#39;]], 3) # Assign cluster labelsfifa[&#39;cluster_labels&#39;], _ = vq(fifa[[&#39;scaled_def&#39;, &#39;scaled_phy&#39;]], cluster_centers) # Display cluster centersprint(fifa[[&#39;scaled_def&#39;, &#39;scaled_phy&#39;, &#39;cluster_labels&#39;]].groupby(&#39;cluster_labels&#39;).mean()) # Create a scatter plot through seabornsns.scatterplot(x=&#39;scaled_def&#39;, y=&#39;scaled_phy&#39;, hue=&#39;cluster_labels&#39;, data=fifa)plt.show() | | :— | :— | . | 12345 | scaled_def scaled_phycluster_labels 0 3.74 8.871 1.87 7.082 2.10 8.94 | | :— | :— | . . Notice that the seed has an impact on clustering as the data is uniformly distributed. . 4. Clustering in Real World . 4.1 Dominant colors in images . | | | | | | | | | | . 4.1.1 Extract RGB values from image . There are broadly three steps to find the dominant colors in an image: . Extract RGB values into three lists. | Perform k-means clustering on scaled RGB values. | Display the colors of cluster centers. | . To extract RGB values, we use the imread() function of the image class of matplotlib. Empty lists, r, g and b have been initialized. . For the purpose of finding dominant colors, we will be using the following image. . | 1234567891011121314 | # Import image class of matplotlibfrom matplotlib import image as img # Read batman image and print dimensionsbatman_image = img.imread(&#39;batman.jpg&#39;)print(batman_image.shape)# (57, 90, 3) # Store RGB values of all pixels in lists r, g and bfor rows in batman_image: for temp_r, temp_g, temp_b in rows: r.append(temp_r) g.append(temp_g) b.append(temp_b) | | :— | :— | . You have successfully extracted the RGB values of the image into three lists, one for each color channel. . 4.1.2 How many dominant colors? . The RGB values are stored in a data frame, batman_df. The RGB values have been standardized used the whiten() function, stored in columns, scaled_red, scaled_blue and scaled_green. . Construct an elbow plot with the data frame. How many dominant colors are present? . | 123456789101112131415 | distortions = []num_clusters = range(1, 7) # Create a list of distortions from the kmeans functionfor i in num_clusters: cluster_centers, distortion = kmeans(batman_df[[&#39;scaled_red&#39;, &#39;scaled_blue&#39;, &#39;scaled_green&#39;]], i) distortions.append(distortion) # Create a data frame with two lists, num_clusters and distortionselbow_plot = pd.DataFrame({&#39;num_clusters&#39;:num_clusters,&#39;distortions&#39;:distortions}) # Create a line plot of num_clusters and distortionssns.lineplot(x=&#39;num_clusters&#39;, y=&#39;distortions&#39;, data = elbow_plot)plt.xticks(num_clusters)plt.show() | | :— | :— | . . Notice that there are three distinct colors present in the image, which is supported by the elbow plot. . 4.1.3 Display dominant colors . To display the dominant colors, convert the colors of the cluster centers to their raw values and then converted them to the range of 0-1, using the following formula: converted_pixel = standardized_pixel * pixel_std / 255 . | 123456789101112131415 | # Get standard deviations of each colorr_std, g_std, b_std = batman_df[[&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;]].std() for cluster_center in cluster_centers: scaled_r, scaled_g, scaled_b = cluster_center # Convert each standardized value to scaled value colors.append(( scaled_r * r_std / 255, scaled_g * g_std / 255, scaled_b * b_std / 255 )) # Display colors of cluster centersplt.imshow([colors])plt.show() | | :— | :— | . . Notice the three colors resemble the three that are indicative from visual inspection of the image. . 4.2 Document clustering . | | | | . 4.2.1 TF-IDF(term frequency–inverse document frequency) of movie plots . Let us use the plots of randomly selected movies to perform document clustering on. Before performing clustering on documents, they need to be cleaned of any unwanted noise (such as special characters and stop words) and converted into a sparse matrix through TF-IDF of the documents. . Use the TfidfVectorizer class to perform the TF-IDF of movie plots stored in the list plots. The remove_noise() function is available to use as a tokenizer in the TfidfVectorizer class. The .fit_transform() method fits the data into the TfidfVectorizer objects and then generates the TF-IDF sparse matrix. . | 12345 | plots[:1][&#39;Cable Hogue is isolated in the desert, awaiting his partners, Taggart and Bowen,......A coyote wanders into the abandoned Cable Springs. But the coyote has a collar – possibly symbolising the taming of the wilderness.&#39;] | | :— | :— | . | 12345678 | # Import TfidfVectorizer class from sklearnfrom sklearn.feature_extraction.text import TfidfVectorizer # Initialize TfidfVectorizertfidf_vectorizer = TfidfVectorizer(min_df=0.1, max_df=0.75, max_features=50, tokenizer=remove_noise) # Use the .fit_transform() method on the list plotstfidf_matrix = tfidf_vectorizer.fit_transform(plots) | | :— | :— | . 4.2.2 Top terms in movie clusters . Now that you have created a sparse matrix, generate cluster centers and print the top three terms in each cluster. Use the .todense() method to convert the sparse matrix, tfidf_matrix to a normal matrix for the kmeans() function to process. Then, use the .get_feature_names() method to get a list of terms in the tfidf_vectorizer object. The zip() function in Python joins two lists. . The tfidf_vectorizer object and sparse matrix, tfidf_matrix, from the previous have been retained in this exercise. kmeans has been imported from SciPy. . With a higher number of data points, the clusters formed would be defined more clearly. However, this requires some computational power, making it difficult to accomplish in an exercise here. . | 12345678910111213141516 | num_clusters = 2 # Generate cluster centers through the kmeans functioncluster_centers, distortion = kmeans(tfidf_matrix.todense(),num_clusters) # Generate terms from the tfidf_vectorizer objectterms = tfidf_vectorizer.get_feature_names() for i in range(num_clusters): # Sort the terms and print top 3 terms center_terms = dict(zip(terms, cluster_centers[i])) sorted_terms = sorted(center_terms, key=center_terms.get, reverse=True) print(sorted_terms[:3]) # [&#39;back&#39;, &#39;father&#39;, &#39;one&#39;]# [&#39;man&#39;, &#39;police&#39;, &#39;killed&#39;] | | :— | :— | . Notice positive, warm words in the first cluster and words referring to action in the second cluster. . 4.3 Clustering with multiple features . | | . 4.3.1 Clustering with many features . What should you do if you have too many features for clustering? . Reduce features using a technique like Factor Analysis . 4.3.2 Basic checks on clusters . In the FIFA 18 dataset, we have concentrated on defenders in previous exercises. Let us try to focus on attacking attributes of a player. Pace (pac), Dribbling (dri) and Shooting (sho) are features that are present in attack minded players. In this exercise, k-means clustering has already been applied on the data using the scaled values of these three attributes. Try some basic checks on the clusters so formed. . The data is stored in a Pandas data frame, fifa. The scaled column names are present in a list scaled_features. The cluster labels are stored in the cluster_labels column. Recall the .count() and .mean() methods in Pandas help you find the number of observations and mean of observations in a data frame. . | 12345 | # Print the size of the clustersprint(fifa.groupby(&#39;cluster_labels&#39;)[&#39;ID&#39;].count()) # Print the mean value of wages in each clusterprint(fifa.groupby(&#39;cluster_labels&#39;)[&#39;eur_wage&#39;].mean()) | | :— | :— | . | 1234567891011 | cluster_labels0 831 1072 60Name: ID, dtype: int64 cluster_labels0 132108.431 130308.412 117583.33Name: eur_wage, dtype: float64 | | :— | :— | . In this example, the cluster sizes are not very different, and there are no significant differences that can be seen in the wages. Further analysis is required to validate these clusters. . 4.3.3 FIFA 18: what makes a complete player? . The overall level of a player in FIFA 18 is defined by six characteristics: pace (pac), shooting (sho), passing (pas), dribbling (dri), defending (def), physical (phy). . | 1234567891011121314 | # Create centroids with kmeans for 2 clusterscluster_centers,_ = kmeans(fifa[scaled_features], 2) # Assign cluster labels and print cluster centersfifa[&#39;cluster_labels&#39;], _ = vq(fifa[scaled_features], cluster_centers)print(fifa.groupby(&#39;cluster_labels&#39;)[scaled_features].mean()) # Plot cluster centers to visualize clustersfifa.groupby(&#39;cluster_labels&#39;)[scaled_features].mean().plot(legend=True, kind=&#39;bar&#39;)plt.show() # Get the name column of top 5 players in each clusterfor cluster in fifa[&#39;cluster_labels&#39;].unique(): print(cluster, fifa[fifa[&#39;cluster_labels&#39;] == cluster][&#39;name&#39;].values[:5]) | | :— | :— | . | 12345678910111213 | scaled_pac scaled_sho scaled_pas scaled_dri scaled_def cluster_labels 0 6.68 5.43 8.46 8.51 2.50 1 5.44 3.66 7.17 6.76 3.97 scaled_phy cluster_labels 0 8.34 1 9.21 0 [&#39;Cristiano Ronaldo&#39; &#39;L. Messi&#39; &#39;Neymar&#39; &#39;L. Suárez&#39; &#39;M. Neuer&#39;]1 [&#39;Sergio Ramos&#39; &#39;G. Chiellini&#39; &#39;D. Godín&#39; &#39;Thiago Silva&#39; &#39;M. Hummels&#39;] | | :— | :— | . . Notice the top players in each cluster are representative of the overall characteristics of the cluster – one of the clusters primarily represents attackers, whereas the other represents defenders. . Surprisingly, a top goalkeeper Manuel Neuer is seen in the attackers group, but he is known for going out of the box and participating in open play, which are reflected in his FIFA 18 attributes. .",
            "url": "https://islamalam.github.io/blog/blogging/demo/2021/06/30/cluster-analysis-in-python.html",
            "relUrl": "/blogging/demo/2021/06/30/cluster-analysis-in-python.html",
            "date": " • Jun 30, 2021"
        }
        
    
  
    
        ,"post50": {
            "title": "Preprocessing For Machine Learning In Python",
            "content": "Preprocessing for Machine Learning in Python . Preprocessing for Machine Learning in Python . This is the memo of the 8th course (23 courses in all) of ‘Machine Learning Scientist with Python’ skill track. You can find the original course HERE. . Course Description . This course covers the basics of how and when to perform data preprocessing. This essential step in any machine learning project is when you get your data ready for modeling. Between importing and cleaning your data and fitting your machine learning model is when preprocessing comes into play. You’ll learn how to standardize your data so that it’s in the right form for your model, create new features to best leverage the information in your dataset, and select the best features to improve your model fit. Finally, you’ll have some practice preprocessing by getting a dataset on UFO sightings ready for modeling. . . Introduction to Data Preprocessing | Standardizing Data | Feature Engineering | Selecting features for modeling | Putting it all together | 1. Introduction to Data Preprocessing . 1.1 What is data preprocessing? . . 1.1.1 Missing data – columns . We have a dataset comprised of volunteer information from New York City. The dataset has a number of features, but we want to get rid of features that have at least 3 missing values. . How many features are in the original dataset, and how many features are in the set after columns with at least 3 missing values are removed? . `volunteer.shape . (665, 35) . volunteer.dropna(axis=1,thresh=3).shape . (665, 24)` . 1.1.2 Missing data – rows . Taking a look at the volunteer dataset again, we want to drop rows where the category_desc column values are missing. We’re going to do this using boolean indexing, by checking to see if we have any null values, and then filtering the dataset so that we only have rows with those values. . `# Check how many values are missing in the category_desc column . print(volunteer[‘category_desc’].isnull().sum()) . 48 . Subset the volunteer dataset . volunteer_subset = volunteer[volunteer[‘category_desc’].notnull()] . Print out the shape of the subset . print(volunteer_subset.shape) . (617, 35)` . 1.2 Working with data types . . 1.2.1 Exploring data types . Taking another look at the dataset comprised of volunteer information from New York City, we want to know what types we’ll be working with as we start to do more preprocessing. . Which data types are present in the volunteer dataset? . set(volunteer.dtypes.values) {dtype(&#39;int64&#39;), dtype(&#39;float64&#39;), dtype(&#39;O&#39;)} . 1.2.2 Converting a column type . If you take a look at the volunteer dataset types, you’ll see that the column hits is type object. But, if you actually look at the column, you’ll see that it consists of integers. Let’s convert that column to type int. . `volunteer[“hits”].dtype . dtype(‘O’) . Convert the hits column to type int . volunteer[“hits”] = volunteer[“hits”].astype(‘int’) . volunteer[“hits”].dtype . dtype(‘int64’)` . 1.3 Class distribution . | | . 1.3.1 Class imbalance . In the volunteer dataset, we’re thinking about trying to predict the category_desc variable using the other features in the dataset. First, though, we need to know what the class distribution (and imbalance) is for that label. . Which descriptions occur less than 50 times in the volunteer dataset? . volunteer.category_desc.value_counts() Strengthening Communities 307 Helping Neighbors in Need 119 Education 92 Health 52 Environment 32 Emergency Preparedness 15 Name: category_desc, dtype: int64 . 1.3.2 Stratified sampling . We know that the distribution of variables in the category_desc column in the volunteer dataset is uneven. If we wanted to train a model to try to predict category_desc, we would want to train the model on a sample of data that is representative of the entire dataset. Stratified sampling is a way to achieve this. . `# Create a data with all columns except category_desc . volunteer_X = volunteer.drop(‘category_desc’, axis=1) . Create a category_desc labels dataset . volunteer_y = volunteer[[‘category_desc’]] . Use stratified sampling to split up the dataset according to the volunteer_y dataset . X_train, X_test, y_train, y_test = train_test_split(volunteer_X, volunteer_y, stratify=volunteer_y) . Print out the category_desc counts on the training y labels . print(y_train[‘category_desc’].value_counts())` . Strengthening Communities 230 Helping Neighbors in Need 89 Education 69 Health 39 Environment 24 Emergency Preparedness 11 Name: category_desc, dtype: int64 . `` . `` . 2. Standardizing Data . 2.1 Standardizing Data . | | . 2.1.1 When to standardize . A column you want to use for modeling has extremely high variance. | You have a dataset with several continuous columns on different scales and you’d like to use a linear model to train the data. | The models you’re working with use some sort of distance metric in a linear space, like the Euclidean metric. | . 2.1.2 Modeling without normalizing . Let’s take a look at what might happen to your model’s accuracy if you try to model data without doing some sort of standardization first. Here we have a subset of the wine dataset. One of the columns, Proline, has an extremely high variance compared to the other columns. This is an example of where a technique like log normalization would come in handy, which you’ll learn about in the next section. . | 123456789 | &lt;p&gt;# Split the dataset and labels into training and test sets&lt;/p&gt;&lt;p&gt;X_train, X_test, y_train, y_test = train_test_split(X, y)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Fit the k-nearest neighbors model to the training data&lt;/p&gt;&lt;p&gt;knn.fit(X_train,y_train)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Score the model on the test data&lt;/p&gt;&lt;p&gt;print(knn.score(X_test,y_test))&lt;/p&gt;&lt;p&gt;# 0.5333333333333333&lt;/p&gt; | | ——— | ———————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————- | . 2.2 Log normalization . | | . 2.2.1 Checking the variance . Check the variance of the columns in the wine dataset. Which column is a candidate for normalization? . | 12345678910111213141516 | &lt;p&gt;wine.var()&lt;/p&gt;&lt;p&gt;Type 0.600679&lt;/p&gt;&lt;p&gt;Alcohol 0.659062&lt;/p&gt;&lt;p&gt;Malic acid 1.248015&lt;/p&gt;&lt;p&gt;Ash 0.075265&lt;/p&gt;&lt;p&gt;Alcalinity of ash 11.152686&lt;/p&gt;&lt;p&gt;Magnesium 203.989335&lt;/p&gt;&lt;p&gt;Total phenols 0.391690&lt;/p&gt;&lt;p&gt;Flavanoids 0.997719&lt;/p&gt;&lt;p&gt;Nonflavanoid phenols 0.015489&lt;/p&gt;&lt;p&gt;Proanthocyanins 0.327595&lt;/p&gt;&lt;p&gt;Color intensity 5.374449&lt;/p&gt;&lt;p&gt;Hue 0.052245&lt;/p&gt;&lt;p&gt;OD280/OD315 of diluted wines 0.504086&lt;/p&gt;&lt;p&gt;Proline 99166.717355&lt;/p&gt;&lt;p&gt;dtype: float64&lt;/p&gt; | | ———————– | ————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————– | . Proline 99166.717355 . 2.2.2 Log normalization in Python . Now that we know that the Proline column in our wine dataset has a large amount of variance, let’s log normalize it. . | 12345678910 | &lt;p&gt;# Print out the variance of the Proline column&lt;/p&gt;&lt;p&gt;print(wine.Proline.var())&lt;/p&gt;&lt;p&gt;# 99166.71735542436&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Apply the log normalization function to the Proline column&lt;/p&gt;&lt;p&gt;wine[&#39;Proline_log&#39;] = np.log(wine.Proline)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Check the variance of the Proline column again&lt;/p&gt;&lt;p&gt;print(wine.Proline_log.var())&lt;/p&gt;&lt;p&gt;# 0.17231366191842012&lt;/p&gt;&lt;p&gt;&amp;#x3C;code&gt;&amp;#x3C;/code&gt;&lt;/p&gt; | | ———– | —————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— | . 2.3 Scaling data for feature comparison . | | | . 2.3.1 Scaling data – investigating columns . We want to use the Ash, Alcalinity of ash, and Magnesium columns in the wine dataset to train a linear model, but it’s possible that these columns are all measured in different ways, which would bias a linear model. . | 12345678910 | &lt;p&gt;wine[[&#39;Ash&#39;,&#39;Alcalinity of ash&#39;,&#39;Magnesium&#39;]].describe()&lt;/p&gt;&lt;p&gt;Ash Alcalinity of ash Magnesium&lt;/p&gt;&lt;p&gt;count 178.000000 178.000000 178.000000&lt;/p&gt;&lt;p&gt;mean 2.366517 19.494944 99.741573&lt;/p&gt;&lt;p&gt;std 0.274344 3.339564 14.282484&lt;/p&gt;&lt;p&gt;min 1.360000 10.600000 70.000000&lt;/p&gt;&lt;p&gt;25% 2.210000 17.200000 88.000000&lt;/p&gt;&lt;p&gt;50% 2.360000 19.500000 98.000000&lt;/p&gt;&lt;p&gt;75% 2.557500 21.500000 107.000000&lt;/p&gt;&lt;p&gt;max 3.230000 30.000000 162.000000&lt;/p&gt; | | ———– | ———————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————– | . 2.3.2 Scaling data – standardizing columns . Since we know that the Ash, Alcalinity of ash, and Magnesium columns in the wine dataset are all on different scales, let’s standardize them in a way that allows for use in a linear model. . | 1234567891011 | &lt;p&gt;# Import StandardScaler from scikit-learn&lt;/p&gt;&lt;p&gt;from sklearn.preprocessing import StandardScaler&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Create the scaler&lt;/p&gt;&lt;p&gt;ss = StandardScaler()&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Take a subset of the DataFrame you want to scale&lt;/p&gt;&lt;p&gt;wine_subset = wine[[&#39;Ash&#39;,&#39;Alcalinity of ash&#39;,&#39;Magnesium&#39;]]&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Apply the scaler to the DataFrame subset&lt;/p&gt;&lt;p&gt;wine_subset_scaled = ss.fit_transform(wine_subset)&lt;/p&gt; | | ————- | ———————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————– | . | 123456 | &lt;p&gt;wine_subset_scaled[:5]&lt;/p&gt;&lt;p&gt;array([[ 0.23205254, -1.16959318, 1.91390522],&lt;/p&gt;&lt;p&gt;[-0.82799632, -2.49084714, 0.01814502],&lt;/p&gt;&lt;p&gt;[ 1.10933436, -0.2687382 , 0.08835836],&lt;/p&gt;&lt;p&gt;[ 0.4879264 , -0.80925118, 0.93091845],&lt;/p&gt;&lt;p&gt;[ 1.84040254, 0.45194578, 1.28198515]])&lt;/p&gt; | | —— | —————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————- | . 2.4 Standardized data and modeling . 2.4.1 KNN on non-scaled data . Let’s first take a look at the accuracy of a K-nearest neighbors model on the wine dataset without standardizing the data. . | 123456789 | &lt;p&gt;# Split the dataset and labels into training and test sets&lt;/p&gt;&lt;p&gt;X_train, X_test, y_train, y_test = train_test_split(X,y)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Fit the k-nearest neighbors model to the training data&lt;/p&gt;&lt;p&gt;knn.fit(X_train, y_train)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Score the model on the test data&lt;/p&gt;&lt;p&gt;print(knn.score(X_test, y_test))&lt;/p&gt;&lt;p&gt;# 0.6444444444444445&lt;/p&gt; | | ——— | ———————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————– | . 2.4.2 KNN on scaled data . The accuracy score on the unscaled wine dataset was decent, but we can likely do better if we scale the dataset. The process is mostly the same as the previous exercise, with the added step of scaling the data. . 12345678910111213 &lt;p&gt;# Create the scaling method.&lt;/p&gt;&lt;p&gt;ss = StandardScaler()&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Apply the scaling method to the dataset used for modeling.&lt;/p&gt;&lt;p&gt;X_scaled = ss.fit_transform(X)&lt;/p&gt;&lt;p&gt;X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Fit the k-nearest neighbors model to the training data.&lt;/p&gt;&lt;p&gt;knn.fit(X_train,y_train)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Score the model on the test data.&lt;/p&gt;&lt;p&gt;print(knn.score(X_test,y_test))&lt;/p&gt;&lt;p&gt;# 0.9555555555555556&lt;/p&gt; .   |   | . 3. Feature Engineering . 3.1 Feature engineering . | | | . 3.1.1 Examples for creating new features . timestamps | newspaper headlines | . Timestamps can be broken into days or months, and headlines can be used for natural language processing. . 3.1.2 Identifying areas for feature engineering . | 123 | &lt;p&gt;volunteer[[&#39;title&#39;,&#39;created_date&#39;,&#39;category_desc&#39;]].head(1)&lt;/p&gt;&lt;p&gt;title created_date category_desc&lt;/p&gt;&lt;p&gt;0 Volunteers Needed For Rise Up &amp;#x26; Stay Put! Home... January 13 2011 Strengthening Communities&lt;/p&gt; | | — | —————————————————————————————————————————————————————————————————————————————————————– | . All of these columns will require some feature engineering before modeling. . 3.2 Encoding categorical variables . | | | | . 3.2.1 Encoding categorical variables – binary . Take a look at the hiking dataset. There are several columns here that need encoding, one of which is the Accessible column, which needs to be encoded in order to be modeled. Accessible is a binary feature, so it has two values – either Y or N – so it needs to be encoded into 1s and 0s. Use scikit-learn’s LabelEncoder method to do that transformation. . | 12345678 | &lt;p&gt;# Set up the LabelEncoder object&lt;/p&gt;&lt;p&gt;enc = LabelEncoder()&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Apply the encoding to the &quot;Accessible&quot; column&lt;/p&gt;&lt;p&gt;hiking[&#39;Accessible_enc&#39;] = enc.fit_transform(hiking.Accessible)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Compare the two columns&lt;/p&gt;&lt;p&gt;print(hiking[[&#39;Accessible&#39;, &#39;Accessible_enc&#39;]].head())&lt;/p&gt; | | ——– | ——————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————- | . | 123456 | &lt;p&gt;Accessible Accessible_enc&lt;/p&gt;&lt;p&gt;0 Y 1&lt;/p&gt;&lt;p&gt;1 N 0&lt;/p&gt;&lt;p&gt;2 N 0&lt;/p&gt;&lt;p&gt;3 N 0&lt;/p&gt;&lt;p&gt;4 N 0&lt;/p&gt; | | —— | ———————————————————————————————————————————————————————————————- | . 3.2.2 Encoding categorical variables – one-hot . One of the columns in the volunteer dataset, category_desc, gives category descriptions for the volunteer opportunities listed. Because it is a categorical variable with more than two categories, we need to use one-hot encoding to transform this column numerically. . | 12345 | &lt;p&gt;# Transform the category_desc column&lt;/p&gt;&lt;p&gt;category_enc = pd.get_dummies(volunteer[&quot;category_desc&quot;])&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Take a look at the encoded columns&lt;/p&gt;&lt;p&gt;print(category_enc.head())&lt;/p&gt; | | —– | —————————————————————————————————————————————————————————————————————————————————————— | . | 12345678 | &lt;p&gt;Education Emergency Preparedness ... Helping Neighbors in Need Strengthening Communities&lt;/p&gt;&lt;p&gt;0 0 0 ... 0 0&lt;/p&gt;&lt;p&gt;1 0 0 ... 0 1&lt;/p&gt;&lt;p&gt;2 0 0 ... 0 1&lt;/p&gt;&lt;p&gt;3 0 0 ... 0 1&lt;/p&gt;&lt;p&gt;4 0 0 ... 0 0&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;[5 rows x 6 columns]&lt;/p&gt; | | ——– | ———————————————————————————————————————————————————————————————————————————————————————————————————————————————————— | . 3.3 Engineering numerical features . | | . 3.3.1 Engineering numerical features – taking an average . A good use case for taking an aggregate statistic to create a new feature is to take the mean of columns. Here, you have a DataFrame of running times named running_times_5k. For each name in the dataset, take the mean of their 5 run times. . | 123456789 | &lt;p&gt;# Create a list of the columns to average&lt;/p&gt;&lt;p&gt;run_columns = [&#39;run1&#39;, &#39;run2&#39;, &#39;run3&#39;, &#39;run4&#39;, &#39;run5&#39;]&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Use apply to create a mean column&lt;/p&gt;&lt;p&gt;# axis=1 = row wise&lt;/p&gt;&lt;p&gt;running_times_5k[&quot;mean&quot;] = running_times_5k.apply(lambda row: row[run_columns].mean(), axis=1)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Take a look at the results&lt;/p&gt;&lt;p&gt;print(running_times_5k)&lt;/p&gt; | | ——— | ———————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— | . | 1234567 | &lt;p&gt;name run1 run2 run3 run4 run5 mean&lt;/p&gt;&lt;p&gt;0 Sue 20.1 18.5 19.6 20.3 18.3 19.36&lt;/p&gt;&lt;p&gt;1 Mark 16.5 17.1 16.9 17.6 17.3 17.08&lt;/p&gt;&lt;p&gt;2 Sean 23.5 25.1 25.2 24.6 23.9 24.46&lt;/p&gt;&lt;p&gt;3 Erin 21.7 21.1 20.9 22.1 22.2 21.60&lt;/p&gt;&lt;p&gt;4 Jenny 25.8 27.1 26.1 26.7 26.9 26.52&lt;/p&gt;&lt;p&gt;5 Russell 30.9 29.6 31.4 30.4 29.9 30.44&lt;/p&gt; | | ——- | ————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— | . 3.3.2 Engineering numerical features – datetime . There are several columns in the volunteer dataset comprised of datetimes. Let’s take a look at the start_date_date column and extract just the month to use as a feature for modeling. . | 12345678 | &lt;p&gt;# First, convert string column to date column&lt;/p&gt;&lt;p&gt;volunteer[&quot;start_date_converted&quot;] = pd.to_datetime(volunteer[&quot;start_date_date&quot;])&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Extract just the month from the converted column&lt;/p&gt;&lt;p&gt;volunteer[&quot;start_date_month&quot;] = volunteer[&quot;start_date_converted&quot;].apply(lambda row: row.month)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Take a look at the converted and new month columns&lt;/p&gt;&lt;p&gt;print(volunteer[[&#39;start_date_converted&#39;, &#39;start_date_month&#39;]].head())&lt;/p&gt; | | ——– | ———————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— | . | 123456 | &lt;p&gt;start_date_converted start_date_month&lt;/p&gt;&lt;p&gt;0 2011-07-30 7&lt;/p&gt;&lt;p&gt;1 2011-02-01 2&lt;/p&gt;&lt;p&gt;2 2011-01-29 1&lt;/p&gt;&lt;p&gt;3 2011-02-14 2&lt;/p&gt;&lt;p&gt;4 2011-02-05 2&lt;/p&gt; | | —— | ——————————————————————————————————————————————————————————————————————————————————- | . 3.4 Text classification . | | | | . 3.4.1 Engineering features from strings – extraction . The Length column in the hiking dataset is a column of strings, but contained in the column is the mileage for the hike. We’re going to extract this mileage using regular expressions, and then use a lambda in Pandas to apply the extraction to the DataFrame. . | 1234567891011121314 | &lt;p&gt;# Write a pattern to extract numbers and decimals&lt;/p&gt;&lt;p&gt;def return_mileage(length):&lt;/p&gt;&lt;p&gt;pattern = re.compile(r&quot; d+ . d+&quot;)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Search the text for matches&lt;/p&gt;&lt;p&gt;mile = re.match(pattern, length)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# If a value is returned, use group(0) to return the found value&lt;/p&gt;&lt;p&gt;if mile is not None:&lt;/p&gt;&lt;p&gt;return float(mile.group(0))&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Apply the function to the Length column and take a look at both columns&lt;/p&gt;&lt;p&gt;hiking[&quot;Length_num&quot;] = hiking[&#39;Length&#39;].apply(lambda row: return_mileage(row))&lt;/p&gt;&lt;p&gt;print(hiking[[&quot;Length&quot;, &quot;Length_num&quot;]].head())&lt;/p&gt; | | ——————- | ————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— | . | 123456 | &lt;p&gt;Length Length_num&lt;/p&gt;&lt;p&gt;0 0.8 miles 0.80&lt;/p&gt;&lt;p&gt;1 1.0 mile 1.00&lt;/p&gt;&lt;p&gt;2 0.75 miles 0.75&lt;/p&gt;&lt;p&gt;3 0.5 miles 0.50&lt;/p&gt;&lt;p&gt;4 0.5 miles 0.50&lt;/p&gt; | | —— | ——————————————————————————————————————————————————————————————————————————————— | . 3.4.2 Engineering features from strings – tf/idf . Let’s transform the volunteer dataset’s title column into a text vector, to use in a prediction task in the next exercise. . | 12345678 | &lt;p&gt;# Take the title text&lt;/p&gt;&lt;p&gt;title_text = volunteer[&#39;title&#39;]&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Create the vectorizer method&lt;/p&gt;&lt;p&gt;tfidf_vec = TfidfVectorizer()&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Transform the text into tf-idf vectors&lt;/p&gt;&lt;p&gt;text_tfidf = tfidf_vec.fit_transform(title_text)&lt;/p&gt; | | ——– | ————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————- | . | 123 | &lt;p&gt;text_tfidf&lt;/p&gt;&lt;p&gt;&amp;#x3C;665x1136 sparse matrix of type &#39;&amp;#x3C;class &#39;numpy.float64&#39;&gt;&#39;&lt;/p&gt;&lt;p&gt;with 3397 stored elements in Compressed Sparse Row format&gt;&lt;/p&gt; | | — | ———————————————————————————————————————————————————————————————————– | . 3.4.3 Text classification using tf/idf vectors . Now that we’ve encoded the volunteer dataset’s title column into tf/idf vectors, let’s use those vectors to try to predict the category_desc column. . | 12345678910111213141516171819 | &lt;p&gt;text_tfidf.toarray()&lt;/p&gt;&lt;p&gt;array([[0., 0., 0., ..., 0., 0., 0.],&lt;/p&gt;&lt;p&gt;[0., 0., 0., ..., 0., 0., 0.],&lt;/p&gt;&lt;p&gt;[0., 0., 0., ..., 0., 0., 0.],&lt;/p&gt;&lt;p&gt;...,&lt;/p&gt;&lt;p&gt;[0., 0., 0., ..., 0., 0., 0.],&lt;/p&gt;&lt;p&gt;[0., 0., 0., ..., 0., 0., 0.],&lt;/p&gt;&lt;p&gt;[0., 0., 0., ..., 0., 0., 0.]])&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;text_tfidf.toarray().shape&lt;/p&gt;&lt;p&gt;# (617, 1089)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;volunteer[&quot;category_desc&quot;].head()&lt;/p&gt;&lt;p&gt;1 Strengthening Communities&lt;/p&gt;&lt;p&gt;2 Strengthening Communities&lt;/p&gt;&lt;p&gt;3 Strengthening Communities&lt;/p&gt;&lt;p&gt;4 Environment&lt;/p&gt;&lt;p&gt;5 Environment&lt;/p&gt;&lt;p&gt;Name: category_desc, dtype: object&lt;/p&gt; | | —————————– | ———————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————– | . | 12345678910 | &lt;p&gt;# Split the dataset according to the class distribution of category_desc&lt;/p&gt;&lt;p&gt;y = volunteer[&quot;category_desc&quot;]&lt;/p&gt;&lt;p&gt;X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Fit the model to the training data&lt;/p&gt;&lt;p&gt;nb.fit(X_train, y_train)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Print out the model&#39;s accuracy&lt;/p&gt;&lt;p&gt;print(nb.score(X_test, y_test))&lt;/p&gt;&lt;p&gt;# 0.567741935483871&lt;/p&gt; | | ———– | ————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————– | . Notice that the model doesn’t score very well. We’ll work on selecting the best features for modeling in the next chapter. . 4. Selecting features for modeling . 4.1 Feature selection . . 4.1.1 Identifying areas for feature selection . Take an exploratory look at the post-feature engineering hiking dataset. Which of the following columns is a good candidate for feature selection? . | 123456 | &lt;p&gt;hiking.columns&lt;/p&gt;&lt;p&gt;Index([&#39;Accessible&#39;, &#39;Difficulty&#39;, &#39;Length&#39;, &#39;Limited_Access&#39;, &#39;Location&#39;,&lt;/p&gt;&lt;p&gt;&#39;Name&#39;, &#39;Other_Details&#39;, &#39;Park_Name&#39;, &#39;Prop_ID&#39;, &#39;lat&#39;, &#39;lon&#39;,&lt;/p&gt;&lt;p&gt;&#39;Length_extract&#39;, &#39;accessible_enc&#39;, &#39;&#39;, &#39;Easy&#39;, &#39;Easy &#39;,&lt;/p&gt;&lt;p&gt;&#39;Easy/Moderate&#39;, &#39;Moderate&#39;, &#39;Moderate/Difficult&#39;, &#39;Various&#39;],&lt;/p&gt;&lt;p&gt;dtype=&#39;object&#39;)&lt;/p&gt; | | —— | ————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— | . Length | Difficulty | Accessible | . All three of these columns are good candidates for feature selection. . 4.2 Removing redundant features . | | . 4.2.1 Selecting relevant features . Now let’s identify the redundant columns in the volunteer dataset and perform feature selection on the dataset to return a DataFrame of the relevant features. . For example, if you explore the volunteer dataset in the console, you’ll see three features which are related to location: locality, region, and postalcode. They contain repeated information, so it would make sense to keep only one of the features. . There are also features that have gone through the feature engineering process: columns like Education and Emergency Preparedness are a product of encoding the categorical variable category_desc, so category_desc itself is redundant now. . Take a moment to examine the features of volunteer in the console, and try to identify the redundant features. . | 12345678 | &lt;p&gt;# Create a list of redundant column names to drop&lt;/p&gt;&lt;p&gt;to_drop = [&quot;locality&quot;, &quot;region&quot;, &quot;category_desc&quot;, &quot;created_date&quot;, &quot;vol_requests&quot;]&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Drop those columns from the dataset&lt;/p&gt;&lt;p&gt;volunteer_subset = volunteer.drop(to_drop, axis=1)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Print out the head of the new dataset&lt;/p&gt;&lt;p&gt;print(volunteer_subset.head())&lt;/p&gt; | | ——– | —————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————- | . | 12345 | &lt;p&gt;volunteer_subset.columns&lt;/p&gt;&lt;p&gt;Index([&#39;title&#39;, &#39;hits&#39;, &#39;postalcode&#39;, &#39;vol_requests_lognorm&#39;, &#39;created_month&#39;,&lt;/p&gt;&lt;p&gt;&#39;Education&#39;, &#39;Emergency Preparedness&#39;, &#39;Environment&#39;, &#39;Health&#39;,&lt;/p&gt;&lt;p&gt;&#39;Helping Neighbors in Need&#39;, &#39;Strengthening Communities&#39;],&lt;/p&gt;&lt;p&gt;dtype=&#39;object&#39;)&lt;/p&gt; | | —– | —————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— | . 4.2.2 Checking for correlated features . Let’s take a look at the wine dataset again, which is made up of continuous, numerical features. Run Pearson’s correlation coefficient on the dataset to determine which columns are good candidates for eliminating. Then, remove those columns from the DataFrame. . | 12345678 | &lt;p&gt;# Print out the column correlations of the wine dataset&lt;/p&gt;&lt;p&gt;print(wine.corr())&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Take a minute to find the column where the correlation value is greater than 0.75 at least twice&lt;/p&gt;&lt;p&gt;to_drop = &quot;Flavanoids&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Drop that column from the DataFrame&lt;/p&gt;&lt;p&gt;wine = wine.drop(to_drop, axis=1)&lt;/p&gt; | | ——– | —————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————– | . | 1234567 | &lt;p&gt;print(wine.corr())&lt;/p&gt;&lt;p&gt;Flavanoids Total phenols Malic acid OD280/OD315 of diluted wines Hue&lt;/p&gt;&lt;p&gt;Flavanoids 1.000000 0.864564 -0.411007 0.787194 0.543479&lt;/p&gt;&lt;p&gt;Total phenols 0.864564 1.000000 -0.335167 0.699949 0.433681&lt;/p&gt;&lt;p&gt;Malic acid -0.411007 -0.335167 1.000000 -0.368710 -0.561296&lt;/p&gt;&lt;p&gt;OD280/OD315 of diluted wines 0.787194 0.699949 -0.368710 1.000000 0.565468&lt;/p&gt;&lt;p&gt;Hue 0.543479 0.433681 -0.561296 0.565468 1.000000&lt;/p&gt;&lt;p&gt;&amp;#x3C;code&gt;&amp;#x3C;/code&gt;&lt;/p&gt; | | ——- | —————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————– | . 4.3 Selecting features using text vectors . | | | . 4.3.1 Exploring text vectors, part 1 . Let’s expand on the text vector exploration method we just learned about, using the volunteer dataset’s title tf/idf vectors. In this first part of text vector exploration, we’re going to add to that function we learned about in the slides. We’ll return a list of numbers with the function. In the next exercise, we’ll write another function to collect the top words across all documents, extract them, and then use that list to filter down our text_tfidf vector. . | | &lt;p&gt;vocab&lt;/p&gt;&lt;p&gt;{1048: &#39;web&#39;,&lt;/p&gt;&lt;p&gt;278: &#39;designer&#39;,&lt;/p&gt;&lt;p&gt;1017: &#39;urban&#39;,&lt;/p&gt;&lt;p&gt;...}&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;tfidf_vec&lt;/p&gt;&lt;p&gt;TfidfVectorizer(analyzer=&#39;word&#39;, binary=False, decode_error=&#39;strict&#39;,&lt;/p&gt;&lt;p&gt;dtype=&amp;#x3C;class &#39;numpy.int64&#39;&gt;, encoding=&#39;utf-8&#39;, input=&#39;content&#39;,&lt;/p&gt;&lt;p&gt;lowercase=True, max_df=1.0, max_features=None, min_df=1,&lt;/p&gt;&lt;p&gt;ngram_range=(1, 1), norm=&#39;l2&#39;, preprocessor=None, smooth_idf=True,&lt;/p&gt;&lt;p&gt;stop_words=None, strip_accents=None, sublinear_tf=False,&lt;/p&gt;&lt;p&gt;token_pattern=&#39;(?u) b w w+ b&#39;, tokenizer=None, use_idf=True,&lt;/p&gt;&lt;p&gt;vocabulary=None)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;tfidf_vec.vocabulary_&lt;/p&gt;&lt;p&gt;{&#39;web&#39;: 1048,&lt;/p&gt;&lt;p&gt;&#39;designer&#39;: 278,&lt;/p&gt;&lt;p&gt;&#39;urban&#39;: 1017,&lt;/p&gt;&lt;p&gt;...}&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;text_tfidf&lt;/p&gt;&lt;p&gt;&amp;#x3C;617x1089 sparse matrix of type &#39;&amp;#x3C;class &#39;numpy.float64&#39;&gt;&#39;&lt;/p&gt;&lt;p&gt;with 3172 stored elements in Compressed Sparse Row format&gt;&lt;/p&gt; | | - | —————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————- | . | | &lt;p&gt;# Add in the rest of the parameters&lt;/p&gt;&lt;p&gt;def return_weights(vocab, original_vocab, vector, vector_index, top_n):&lt;/p&gt;&lt;p&gt;zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Let&#39;s transform that zipped dict into a series&lt;/p&gt;&lt;p&gt;zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Let&#39;s sort the series to pull out the top n weighted words&lt;/p&gt;&lt;p&gt;zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index&lt;/p&gt;&lt;p&gt;return [original_vocab[i] for i in zipped_index]&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Print out the weighted words&lt;/p&gt;&lt;p&gt;print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, vector_index=8, top_n=3))&lt;/p&gt;&lt;p&gt;# [189, 942, 466]&lt;/p&gt; | | - | ——————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— | . 4.3.2 Exploring text vectors, part 2 . Using the function we wrote in the previous exercise, we’re going to extract the top words from each document in the text vector, return a list of the word indices, and use that list to filter the text vector down to those top words. . | | &lt;p&gt;def words_to_filter(vocab, original_vocab, vector, top_n):&lt;/p&gt;&lt;p&gt;filter_list = []&lt;/p&gt;&lt;p&gt;for i in range(0, vector.shape[0]):&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Here we&#39;ll call the function from the previous exercise, and extend the list we&#39;re creating&lt;/p&gt;&lt;p&gt;filtered = return_weights(vocab, original_vocab, vector, i, top_n)&lt;/p&gt;&lt;p&gt;filter_list.extend(filtered)&lt;/p&gt;&lt;p&gt;# Return the list in a set, so we don&#39;t get duplicate word indices&lt;/p&gt;&lt;p&gt;return set(filter_list)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Call the function to get the list of word indices&lt;/p&gt;&lt;p&gt;filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# By converting filtered_words back to a list, we can use it to filter the columns in the text vector&lt;/p&gt;&lt;p&gt;filtered_text = text_tfidf[:, list(filtered_words)]&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;filtered_text&lt;/p&gt;&lt;p&gt;&amp;#x3C;617x1008 sparse matrix of type &#39;&amp;#x3C;class &#39;numpy.float64&#39;&gt;&#39;&lt;/p&gt;&lt;p&gt;with 2948 stored elements in Compressed Sparse Row format&gt;&lt;/p&gt; | | - | ————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— | . 4.3.3 Training Naive Bayes with feature selection . Let’s re-run the Naive Bayes text classification model we ran at the end of chapter 3, with our selection choices from the previous exercise, on the volunteer dataset’s title and category_desc columns. . | | &lt;p&gt;# Split the dataset according to the class distribution of category_desc, using the filtered_text vector&lt;/p&gt;&lt;p&gt;train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Fit the model to the training data&lt;/p&gt;&lt;p&gt;nb.fit(train_X, train_y)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Print out the model&#39;s accuracy&lt;/p&gt;&lt;p&gt;print(nb.score(test_X,test_y))&lt;/p&gt;&lt;p&gt;# 0.567741935483871&lt;/p&gt; | | - | —————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— | . You can see that our accuracy score wasn’t that different from the score at the end of chapter 3. That’s okay; the title field is a very small text field, appropriate for demonstrating how filtering vectors works. . 4.4 Dimensionality reduction . | . 4.4.1 Using PCA . Let’s apply PCA to the wine dataset, to see if we can get an increase in our model’s accuracy. . | | &lt;p&gt;from sklearn.decomposition import PCA&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Set up PCA and the X vector for diminsionality reduction&lt;/p&gt;&lt;p&gt;pca = PCA()&lt;/p&gt;&lt;p&gt;wine_X = wine.drop(&quot;Type&quot;, axis=1)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Apply PCA to the wine dataset X vector&lt;/p&gt;&lt;p&gt;transformed_X = pca.fit_transform(wine_X)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Look at the percentage of variance explained by the different components&lt;/p&gt;&lt;p&gt;print(pca.explained_variance_ratio_)&lt;/p&gt; | | - | ———————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— | . | | &lt;p&gt;[9.98091230e-01 1.73591562e-03 9.49589576e-05 5.02173562e-05&lt;/p&gt;&lt;p&gt;1.23636847e-05 8.46213034e-06 2.80681456e-06 1.52308053e-06&lt;/p&gt;&lt;p&gt;1.12783044e-06 7.21415811e-07 3.78060267e-07 2.12013755e-07&lt;/p&gt;&lt;p&gt;8.25392788e-08]&lt;/p&gt; | | - | ——————————————————————————————————————————————————————————————————————————————————————————————— | . 4.4.2 Training a model with PCA . Now that we have run PCA on the wine dataset, let’s try training a model with it. . | | &lt;p&gt;# Split the transformed X and the y labels into training and test sets&lt;/p&gt;&lt;p&gt;X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(transformed_X,y)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Fit knn to the training data&lt;/p&gt;&lt;p&gt;knn.fit(X_wine_train,y_wine_train)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Score knn on the test data and print it out&lt;/p&gt;&lt;p&gt;knn.score(X_wine_test,y_wine_test)&lt;/p&gt;&lt;p&gt;# 0.6444444444444445&lt;/p&gt; | | - | ————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— | . 5. Putting it all together . 5.1 UFOs and preprocessing . | | &lt;p&gt;ufo.head()&lt;/p&gt;&lt;p&gt;date city state country type seconds &lt;/p&gt;&lt;p&gt;2 2002-11-21 05:45:00 clemmons nc us triangle 300.0&lt;/p&gt;&lt;p&gt;4 2012-06-16 23:00:00 san diego ca us light 600.0&lt;/p&gt;&lt;p&gt;7 2013-06-09 00:00:00 oakville (canada) on ca light 120.0&lt;/p&gt;&lt;p&gt;8 2013-04-26 23:27:00 lacey wa us light 120.0&lt;/p&gt;&lt;p&gt;9 2013-09-13 20:30:00 ben avon pa us sphere 300.0&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;length_of_time desc &lt;/p&gt;&lt;p&gt;2 about 5 minutes It was a large, triangular shaped flying ob...&lt;/p&gt;&lt;p&gt;4 10 minutes Dancing lights that would fly around and then ...&lt;/p&gt;&lt;p&gt;7 2 minutes Brilliant orange light or chinese lantern at o...&lt;/p&gt;&lt;p&gt;8 2 minutes Bright red light moving north to north west fr...&lt;/p&gt;&lt;p&gt;9 5 minutes North-east moving south-west. First 7 or so li...&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;recorded lat long&lt;/p&gt;&lt;p&gt;2 12/23/2002 36.021389 -80.382222&lt;/p&gt;&lt;p&gt;4 7/4/2012 32.715278 -117.156389&lt;/p&gt;&lt;p&gt;7 7/3/2013 43.433333 -79.666667&lt;/p&gt;&lt;p&gt;8 5/15/2013 47.034444 -122.821944&lt;/p&gt;&lt;p&gt;9 9/30/2013 40.508056 -80.083333&lt;/p&gt; | | - | ——————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— | . . 5.1.1 Checking column types . Take a look at the UFO dataset’s column types using the dtypes attribute. Two columns jump out for transformation: the seconds column, which is a numeric column but is being read in as object, and the date column, which can be transformed into the datetime type. That will make our feature engineering efforts easier later on. . | | &lt;p&gt;# Check the column types&lt;/p&gt;&lt;p&gt;print(ufo.dtypes)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Change the type of seconds to float&lt;/p&gt;&lt;p&gt;ufo[&quot;seconds&quot;] = ufo.seconds.astype(&#39;float&#39;)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Change the date column to type datetime&lt;/p&gt;&lt;p&gt;ufo[&quot;date&quot;] = pd.to_datetime(ufo[&#39;date&#39;])&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Check the column types&lt;/p&gt;&lt;p&gt;print(ufo[[&#39;seconds&#39;,&#39;date&#39;]].dtypes)&lt;/p&gt; | | - | —————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— | . | | &lt;p&gt;date object&lt;/p&gt;&lt;p&gt;city object&lt;/p&gt;&lt;p&gt;state object&lt;/p&gt;&lt;p&gt;country object&lt;/p&gt;&lt;p&gt;type object&lt;/p&gt;&lt;p&gt;seconds object&lt;/p&gt;&lt;p&gt;length_of_time object&lt;/p&gt;&lt;p&gt;desc object&lt;/p&gt;&lt;p&gt;recorded object&lt;/p&gt;&lt;p&gt;lat object&lt;/p&gt;&lt;p&gt;long float64&lt;/p&gt;&lt;p&gt;dtype: object&lt;/p&gt;&lt;p&gt;seconds float64&lt;/p&gt;&lt;p&gt;date datetime64[ns]&lt;/p&gt;&lt;p&gt;dtype: object&lt;/p&gt; | | - | —————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— | . 5.1.2 Dropping missing data . Let’s remove some of the rows where certain columns have missing values. We’re going to look at the length_of_time column, the state column, and the type column. If any of the values in these columns are missing, we’re going to drop the rows. . | 12345678910 | &lt;p&gt;# Check how many values are missing in the length_of_time, state, and type columns&lt;/p&gt;&lt;p&gt;print(ufo[[&#39;length_of_time&#39;, &#39;state&#39;, &#39;type&#39;]].isnull().sum())&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Keep only rows where length_of_time, state, and type are not null&lt;/p&gt;&lt;p&gt;ufo_no_missing = ufo[ufo[&#39;length_of_time&#39;].notnull() &amp;#x26;&lt;/p&gt;&lt;p&gt;ufo[&#39;state&#39;].notnull() &amp;#x26;&lt;/p&gt;&lt;p&gt;ufo[&#39;type&#39;].notnull()]&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Print out the shape of the new dataset&lt;/p&gt;&lt;p&gt;print(ufo_no_missing.shape)&lt;/p&gt; | | ———– | ———————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————- | . | 12345 | &lt;p&gt;length_of_time 143&lt;/p&gt;&lt;p&gt;state 419&lt;/p&gt;&lt;p&gt;type 159&lt;/p&gt;&lt;p&gt;dtype: int64&lt;/p&gt;&lt;p&gt;(4283, 4)&lt;/p&gt; | | —– | —————————————————————————————————————————————————————————- | . 5.2 Categorical variables and standardization . | | . 5.2.1 Extracting numbers from strings . The length_of_time field in the UFO dataset is a text field that has the number of minutes within the string. Here, you’ll extract that number from that text field using regular expressions. . | 123456789101112131415 | &lt;p&gt;def return_minutes(time_string):&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Use d+ to grab digits&lt;/p&gt;&lt;p&gt;pattern = re.compile(r&quot; d+&quot;)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Use match on the pattern and column&lt;/p&gt;&lt;p&gt;num = re.match(pattern, time_string)&lt;/p&gt;&lt;p&gt;if num is not None:&lt;/p&gt;&lt;p&gt;return int(num.group(0))&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Apply the extraction to the length_of_time column&lt;/p&gt;&lt;p&gt;ufo[&quot;minutes&quot;] = ufo[&quot;length_of_time&quot;].apply(return_minutes)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Take a look at the head of both of the columns&lt;/p&gt;&lt;p&gt;print(ufo[[&#39;length_of_time&#39;,&#39;minutes&#39;]].head())&lt;/p&gt; | | ——————— | ————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————– | . | 123456 | &lt;p&gt;length_of_time minutes&lt;/p&gt;&lt;p&gt;2 about 5 minutes NaN&lt;/p&gt;&lt;p&gt;4 10 minutes 10.0&lt;/p&gt;&lt;p&gt;7 2 minutes 2.0&lt;/p&gt;&lt;p&gt;8 2 minutes 2.0&lt;/p&gt;&lt;p&gt;9 5 minutes 5.0&lt;/p&gt; | | —— | —————————————————————————————————————————————————————————————————————————————————– | . 5.2.2 Identifying features for standardization . In this section, you’ll investigate the variance of columns in the UFO dataset to determine which features should be standardized. After taking a look at the variances of the seconds and minutes column, you’ll see that the variance of the seconds column is extremely high. Because seconds and minutes are related to each other (an issue we’ll deal with when we select features for modeling), let’s log normlize the seconds column. . | 12345678 | &lt;p&gt;# Check the variance of the seconds and minutes columns&lt;/p&gt;&lt;p&gt;print(ufo[[&#39;seconds&#39;,&#39;minutes&#39;]].var())&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Log normalize the seconds column&lt;/p&gt;&lt;p&gt;ufo[&quot;seconds_log&quot;] = np.log(ufo[[&#39;seconds&#39;]])&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Print out the variance of just the seconds_log column&lt;/p&gt;&lt;p&gt;print(ufo[&quot;seconds_log&quot;].var())&lt;/p&gt; | | ——– | ————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————- | . | 1234 | &lt;p&gt;seconds 424087.417474&lt;/p&gt;&lt;p&gt;minutes 117.546372&lt;/p&gt;&lt;p&gt;dtype: float64&lt;/p&gt;&lt;p&gt;1.1223923881183004&lt;/p&gt; | | —- | ——————————————————————————————————————————————————————- | . 5.3 Engineering new features . . 5.3.1 Encoding categorical variables . There are couple of columns in the UFO dataset that need to be encoded before they can be modeled through scikit-learn. You’ll do that transformation here, using both binary and one-hot encoding methods. . | 123456789101112 | &lt;p&gt;# Use Pandas to encode us values as 1 and others as 0&lt;/p&gt;&lt;p&gt;ufo[&quot;country_enc&quot;] = ufo[&quot;country&quot;].apply(lambda x: 1 if x==&#39;us&#39; else 0)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Print the number of unique type values&lt;/p&gt;&lt;p&gt;print(len(ufo[&#39;type&#39;].unique()))&lt;/p&gt;&lt;p&gt;# 21&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Create a one-hot encoded set of the type values&lt;/p&gt;&lt;p&gt;type_set = pd.get_dummies(ufo[&#39;type&#39;])&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Concatenate this set back to the ufo DataFrame&lt;/p&gt;&lt;p&gt;ufo = pd.concat([ufo, type_set], axis=1)&lt;/p&gt; | | ————— | ——————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— | . 5.3.2 Features from dates . Another feature engineering task to perform is month and year extraction. Perform this task on the date column of the ufo dataset. . | | &lt;p&gt;# Look at the first 5 rows of the date column&lt;/p&gt;&lt;p&gt;print(ufo[&#39;date&#39;].head())&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Extract the month from the date column&lt;/p&gt;&lt;p&gt;ufo[&quot;month&quot;] = ufo[&quot;date&quot;].apply(lambda x:x.month)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Extract the year from the date column&lt;/p&gt;&lt;p&gt;ufo[&quot;year&quot;] = ufo[&quot;date&quot;].apply(lambda x:x.year)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Take a look at the head of all three columns&lt;/p&gt;&lt;p&gt;print(ufo[[&#39;date&#39;,&#39;month&#39;,&#39;year&#39;]].head())&lt;/p&gt; | | - | —————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————- | . | | &lt;p&gt;0 2002-11-21 05:45:00&lt;/p&gt;&lt;p&gt;1 2012-06-16 23:00:00&lt;/p&gt;&lt;p&gt;2 2013-06-09 00:00:00&lt;/p&gt;&lt;p&gt;3 2013-04-26 23:27:00&lt;/p&gt;&lt;p&gt;4 2013-09-13 20:30:00&lt;/p&gt;&lt;p&gt;Name: date, dtype: datetime64[ns]&lt;/p&gt;&lt;p&gt;date month year&lt;/p&gt;&lt;p&gt;0 2002-11-21 05:45:00 11 2002&lt;/p&gt;&lt;p&gt;1 2012-06-16 23:00:00 6 2012&lt;/p&gt;&lt;p&gt;2 2013-06-09 00:00:00 6 2013&lt;/p&gt;&lt;p&gt;3 2013-04-26 23:27:00 4 2013&lt;/p&gt;&lt;p&gt;4 2013-09-13 20:30:00 9 2013&lt;/p&gt; | | - | ————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————– | . 5.3.3 Text vectorization . Let’s transform the desc column in the UFO dataset into tf/idf vectors, since there’s likely something we can learn from this field. . | | &lt;p&gt;# Take a look at the head of the desc field&lt;/p&gt;&lt;p&gt;print(ufo[&quot;desc&quot;].head())&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Create the tfidf vectorizer object&lt;/p&gt;&lt;p&gt;vec = TfidfVectorizer()&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Use vec&#39;s fit_transform method on the desc field&lt;/p&gt;&lt;p&gt;desc_tfidf = vec.fit_transform(ufo[&quot;desc&quot;])&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Look at the number of columns this creates&lt;/p&gt;&lt;p&gt;print(desc_tfidf.shape)&lt;/p&gt; | | - | —————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————- | . | | &lt;p&gt;0 It was a large, triangular shaped flying ob...&lt;/p&gt;&lt;p&gt;1 Dancing lights that would fly around and then ...&lt;/p&gt;&lt;p&gt;2 Brilliant orange light or chinese lantern at o...&lt;/p&gt;&lt;p&gt;3 Bright red light moving north to north west fr...&lt;/p&gt;&lt;p&gt;4 North-east moving south-west. First 7 or so li...&lt;/p&gt;&lt;p&gt;Name: desc, dtype: object&lt;/p&gt;&lt;p&gt;(1866, 3422)&lt;/p&gt; | | - | ——————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— | . 5.4 Feature selection and modeling . . 5.4.1 Selecting the ideal dataset . Let’s get rid of some of the unnecessary features. Because we have an encoded country column, country_enc, keep it and drop other columns related to location: city, country, lat, long, state. . We have columns related to month and year, so we don’t need the date or recorded columns. . We vectorized desc, so we don’t need it anymore. For now we’ll keep type. . We’ll keep seconds_log and drop seconds and minutes. . Let’s also get rid of the length_of_time column, which is unnecessary after extracting minutes. . | | &lt;p&gt;# Check the correlation between the seconds, seconds_log, and minutes columns&lt;/p&gt;&lt;p&gt;print(ufo[[&#39;seconds&#39;,&#39;seconds_log&#39;,&#39;minutes&#39;]].corr())&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Make a list of features to drop&lt;/p&gt;&lt;p&gt;to_drop = [&#39;city&#39;, &#39;country&#39;, &#39;date&#39;, &#39;desc&#39;, &#39;lat&#39;, &#39;length_of_time&#39;, &#39;long&#39;, &#39;minutes&#39;, &#39;recorded&#39;, &#39;seconds&#39;, &#39;state&#39;]&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Drop those features&lt;/p&gt;&lt;p&gt;ufo_dropped = ufo.drop(to_drop,axis=1)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Let&#39;s also filter some words out of the text vector we created&lt;/p&gt;&lt;p&gt;filtered_words = words_to_filter(vocab, vec.vocabulary_, desc_tfidf, 4)&lt;/p&gt; | | - | —————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————- | . | | &lt;p&gt;seconds seconds_log minutes&lt;/p&gt;&lt;p&gt;seconds 1.000000 0.853371 0.980341&lt;/p&gt;&lt;p&gt;seconds_log 0.853371 1.000000 0.824493&lt;/p&gt;&lt;p&gt;minutes 0.980341 0.824493 1.000000&lt;/p&gt; | | - | ——————————————————————————————————————————————————————————————————————————— | . 5.4.2 Modeling the UFO dataset, part 1 . In this exercise, we’re going to build a k-nearest neighbor model to predict which country the UFO sighting took place in. Our X dataset has the log-normalized seconds column, the one-hot encoded type columns, as well as the month and year when the sighting took place. The y labels are the encoded country column, where 1 is us and 0 is ca. . | | &lt;p&gt;# Take a look at the features in the X set of data&lt;/p&gt;&lt;p&gt;print(X.columns)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Split the X and y sets using train_test_split, setting stratify=y&lt;/p&gt;&lt;p&gt;train_X, test_X, train_y, test_y = train_test_split(X,y,stratify=y)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Fit knn to the training sets&lt;/p&gt;&lt;p&gt;knn.fit(train_X,train_y)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Print the score of knn on the test sets&lt;/p&gt;&lt;p&gt;print(knn.score(test_X,test_y))&lt;/p&gt;&lt;p&gt;# 0.8693790149892934&lt;/p&gt; | | - | ————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— | . | | &lt;p&gt;Index([&#39;seconds_log&#39;, &#39;changing&#39;, &#39;chevron&#39;, &#39;cigar&#39;, &#39;circle&#39;, &#39;cone&#39;,&lt;/p&gt;&lt;p&gt;&#39;cross&#39;, &#39;cylinder&#39;, &#39;diamond&#39;, &#39;disk&#39;, &#39;egg&#39;, &#39;fireball&#39;, &#39;flash&#39;,&lt;/p&gt;&lt;p&gt;&#39;formation&#39;, &#39;light&#39;, &#39;other&#39;, &#39;oval&#39;, &#39;rectangle&#39;, &#39;sphere&#39;,&lt;/p&gt;&lt;p&gt;&#39;teardrop&#39;, &#39;triangle&#39;, &#39;unknown&#39;, &#39;month&#39;, &#39;year&#39;],&lt;/p&gt;&lt;p&gt;dtype=&#39;object&#39;)&lt;/p&gt; | | - | ———————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————- | . 5.4.3 Modeling the UFO dataset, part 2 . Finally, let’s build a model using the text vector we created, desc_tfidf, using the filtered_words list to create a filtered text vector. Let’s see if we can predict the type of the sighting based on the text. We’ll use a Naive Bayes model for this. . | 123456789101112 | &lt;p&gt;# Use the list of filtered words we created to filter the text vector&lt;/p&gt;&lt;p&gt;filtered_text = desc_tfidf[:, list(filtered_words)]&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Split the X and y sets using train_test_split, setting stratify=y&lt;/p&gt;&lt;p&gt;train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Fit nb to the training sets&lt;/p&gt;&lt;p&gt;nb.fit(train_X,train_y)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;# Print the score of nb on the test sets&lt;/p&gt;&lt;p&gt;print(nb.score(test_X,test_y))&lt;/p&gt;&lt;p&gt;# 0.16274089935760172&lt;/p&gt; | | ————— | —————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— | . As you can see, this model performs very poorly on this text data. This is a clear case where iteration would be necessary to figure out what subset of text improves the model, and if perhaps any of the other features are useful in predicting type. . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/2021/06/30/preprocessing-for-machine-learning-in-python.html",
            "relUrl": "/2021/06/30/preprocessing-for-machine-learning-in-python.html",
            "date": " • Jun 30, 2021"
        }
        
    
  
    
        ,"post51": {
            "title": "Introduction to Deep Learning with Keras",
            "content": "Introduction to Deep Learning with Keras . This is the memo of the 16th course (23 courses in all) of ‘Machine Learning Scientist with Python’ skill track. . You can find the original course HERE . . reference url: https://tensorspace.org/index.html . #### . Course Description . Deep learning is here to stay! It’s the go-to technique to solve complex problems that arise with unstructured data and an incredible tool for innovation. Keras is one of the frameworks that make it easier to start developing deep learning models, and it’s versatile enough to build industry-ready models in no time. In this course, you will learn regression and save the earth by predicting asteroid trajectories, apply binary classification to distinguish between real and fake dollar bills, use multiclass classification to decide who threw which dart at a dart board, learn to use neural networks to reconstruct noisy images and much more. Additionally, you will learn how to better control your models during training and how to tune them to boost their performance. . #### . . Introducing Keras | Going Deeper | Improving Your Model Performance | Advanced Model Architectures | . 1. Introducing Keras . 1.1 What is Keras? . . 1.1.1 Describing Keras . Which of the following statements about Keras is false ? . Keras is integrated into TensorFlow, that means you can call Keras from within TensorFlow and get the best of both worlds. | Keras can work well on its own without using a backend, like TensorFlow. (False) | Keras is an open source project started by François Chollet. | . You’re good at spotting lies! Keras is a wrapper around a backend, so a backend like TensorFlow, Theano, CNTK, etc must be provided. . 1.1.2 Would you use deep learning? . Imagine you’re building an app that allows you to take a picture of your clothes and then shows you a pair of shoes that would match well. This app needs a machine learning module that’s in charge of identifying the type of clothes you are wearing, as well as their color and texture. Would you use deep learning to accomplish this task? . I’d use deep learning, since we are dealing with tabular data and neural networks work well with images. | I’d use deep learning since we are dealing with unstructured data and neural networks work well with images.(True) | This task can be easily accomplished with other machine learning algorithms, so deep learning is not required. | . You’re right! Using deep learning would be the easiest way. The model would generalize well if enough clothing images are provided. . 1.2 Your first neural network . . 1.2.1 Hello nets! . You’re going to build a simple neural network to get a feeling for how quickly it is to accomplish in Keras. . You will build a network that takes two numbers as input , passes them through a hidden layer of 10 neurons , and finally outputs a single non-constrained number . . A non-constrained output can be obtained by avoiding setting an activation function in the output layer . This is useful for problems like regression, when we want our output to be able to take any value. . # Import the Sequential model and Dense layer from keras.models import Sequential from keras.layers import Dense # Create a Sequential model model = Sequential() # Add an input layer and a hidden layer with 10 neurons model.add(Dense(10, input_shape=(2,), activation=&quot;relu&quot;)) # Add a 1-neuron output layer model.add(Dense(1)) # Summarise your model model.summary() . Model: &quot;sequential_1&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_1 (Dense) (None, 10) 30 _________________________________________________________________ dense_2 (Dense) (None, 1) 11 ================================================================= Total params: 41 Trainable params: 41 Non-trainable params: 0 _________________________________________________________________ . You’ve just build your first neural network with Keras, well done! . 1.2.2 Counting parameters . You’ve just created a neural network. Create a new one now and take some time to think about the weights of each layer. The Keras Dense layer and the Sequential model are already loaded for you to use. . This is the network you will be creating: . # Instantiate a new Sequential model model = Sequential() # Add a Dense layer with five neurons and three inputs model.add(Dense(5, input_shape=(3,), activation=&quot;relu&quot;)) # Add a final Dense layer with one neuron and no activation model.add(Dense(1)) # Summarize your model model.summary() . Model: &quot;sequential_1&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_1 (Dense) (None, 5) 20 _________________________________________________________________ dense_2 (Dense) (None, 1) 6 ================================================================= Total params: 26 Trainable params: 26 Non-trainable params: 0 _________________________________________________________________ . Given the model you just built, which answer is correct regarding the number of weights (parameters) in the hidden layer ? . There are 20 parameters, 15 from the connection of our input layer to our hidden layer and 5 from the bias weight of each neuron in the hidden layer. . Great! You certainly know where those parameters come from! . 1.2.3 Build as shown! . You will take on a final challenge before moving on to the next lesson. Build the network shown in the picture below. Prove your mastered Keras basics in no time! . from keras.models import Sequential from keras.layers import Dense # Instantiate a Sequential model model = Sequential() # Build the input and hidden layer model.add(Dense(3, input_shape=(2,))) # Add the ouput layer model.add(Dense(1)) . Perfect! You’ve shown you can already translate a visual representation of a neural network into Keras code. . 1.3 Surviving a meteor strike . . 1.3.1 Specifying a model . You will build a simple regression model to forecast the orbit of the meteor! . Your training data consist of measurements taken at time steps from -10 minutes before the impact region to +10 minutes after . Each time step can be viewed as an X coordinate in our graph, which has an associated position Y for the meteor at that time step. . Note that you can view this problem as approximating a quadratic function via the use of neural networks. . This data is stored in two numpy arrays: one called time_steps , containing the features , and another called y_positions , with the labels . . Feel free to look at these arrays in the console anytime, then build your model! Keras Sequential model and Dense layers are available for you to use. . # Instantiate a Sequential model model = Sequential() # Add a Dense layer with 50 neurons and an input of 1 neuron model.add(Dense(50, input_shape=(1,), activation=&#39;relu&#39;)) # Add two Dense layers with 50 neurons and relu activation model.add(Dense(50,activation=&#39;relu&#39;)) model.add(Dense(50,activation=&#39;relu&#39;)) # End your model with a Dense layer and no activation model.add(Dense(1)) . You are closer to forecasting the meteor orbit! It’s important to note we aren’t using an activation function in our output layer since y_positions aren’t bounded and they can take any value. Your model is performing regression. . 1.3.2 Training . You’re going to train your first model in this course, and for a good cause! . Remember that before training your Keras models you need to compile them . This can be done with the .compile() method. The .compile() method takes arguments such as the optimizer , used for weight updating, and the loss function, which is what we want to minimize. Training your model is as easy as calling the .fit() method, passing on the features , labels and number of epochs to train for. . The model you built in the previous exercise is loaded for you to use, along with the time_steps and y_positions data. . # Compile your model model.compile(optimizer = &#39;adam&#39;, loss = &#39;mse&#39;) print(&quot;Training started..., this can take a while:&quot;) # Fit your model on your data for 30 epochs model.fit(time_steps,y_positions, epochs = 30) # Evaluate your model print(&quot;Final lost value:&quot;,model.evaluate(time_steps, y_positions)) . Training started..., this can take a while: Epoch 1/30 32/2000 [..............................] - ETA: 14s - loss: 2465.2439 928/2000 [============&gt;.................] - ETA: 0s - loss: 1820.2874 1856/2000 [==========================&gt;...] - ETA: 0s - loss: 1439.9186 2000/2000 [==============================] - 0s 177us/step - loss: 1369.6929 ... Epoch 30/30 32/2000 [..............................] - ETA: 0s - loss: 0.1844 896/2000 [============&gt;.................] - ETA: 0s - loss: 0.2483 1696/2000 [========================&gt;.....] - ETA: 0s - loss: 0.2292 2000/2000 [==============================] - 0s 62us/step - loss: 0.2246 . 32/2000 [..............................] - ETA: 1s 1536/2000 [======================&gt;.......] - ETA: 0s 2000/2000 [==============================] - 0s 44us/step Final lost value: 0.14062700100243092 . Amazing! You can check the console to see how the loss function decreased as epochs went by. Your model is now ready to make predictions. . 1.3.3 Predicting the orbit! . You’ve already trained a model that approximates the orbit of the meteor approaching earth and it’s loaded for you to use. . Since you trained your model for values between -10 and 10 minutes, your model hasn’t yet seen any other values for different time steps. You will visualize how your model behaves on unseen data. . To see the source code of plot_orbit , type the following print(inspect.getsource(plot_orbit)) in the console. . Remember np.arange(x,y) produces a range of values from x to y-1 . . Hurry up, you’re running out of time! . # Predict the twenty minutes orbit twenty_min_orbit = model.predict(np.arange(-10, 11)) # Plot the twenty minute orbit plot_orbit(twenty_min_orbit) . # Predict the twenty minutes orbit eighty_min_orbit = model.predict(np.arange(-40, 41)) # Plot the twenty minute orbit plot_orbit(eighty_min_orbit) . . Your model fits perfectly to the scientists trajectory for time values between -10 to +10, the region where the meteor crosses the impact region, so we won’t be hit! However, it starts to diverge when predicting for further values we haven’t trained for. This shows neural networks learn according to the data they are fed with. Data quality and diversity are very important. You’ve barely scratched the surface of what neural networks can do. Are you prepared for the next chapter? . 2. Going Deeper . 2.1 Binary classification . . 2.1.1 Exploring dollar bills . You will practice building classification models in Keras with the Banknote Authentication dataset. . Your goal is to distinguish between real and fake dollar bills. In order to do this, the dataset comes with 4 variables: variance , skewness , curtosis and entropy . These variables are calculated by applying mathematical operations over the dollar bill images. The labels are found in the class variable. . The dataset is pre-loaded in your workspace as banknotes , let’s do some data exploration! . # Import seaborn import seaborn as sns # Use pairplot and set the hue to be our class sns.pairplot(banknotes, hue=&#39;class&#39;) # Show the plot plt.show() # Describe the data print(&#39;Dataset stats: n&#39;, banknotes.describe()) # Count the number of observations of each class print(&#39;Observations per class: n&#39;, banknotes[&#39;class&#39;].value_counts()) . Dataset stats: variance skewness curtosis entropy count 96.000000 96.000000 96.000000 96.000000 mean -0.057791 -0.102829 0.230412 0.081497 std 1.044960 1.059236 1.128972 0.975565 min -2.084590 -2.621646 -1.482300 -3.034187 25% -0.839124 -0.916152 -0.415294 -0.262668 50% -0.026748 -0.037559 -0.033603 0.394888 75% 0.871034 0.813601 0.978766 0.745212 max 1.869239 1.634072 3.759017 1.343345 Observations per class: real 53 fake 43 Name: class, dtype: int64 . . Your pairplot shows that there are variables for which the classes spread out noticeably. This gives us an intuition about our classes being separable. Let’s build a model to find out what it can do! . 2.1.2 A binary classification model . Now that you know what the Banknote Authentication dataset looks like, we’ll build a simple model to distinguish between real and fake bills. . You will perform binary classification by using a single neuron as an output. The input layer will have 4 neurons since we have 4 features in our dataset. The model output will be a value constrained between 0 and 1. . We will interpret this number as the probability of our input variables coming from a fake dollar bill, with 1 meaning we are certain it’s fake. . # Import the sequential model and dense layer from keras.models import Sequential from keras.layers import Dense # Create a sequential model model = Sequential() # Add a dense layer model.add(Dense(1, input_shape=(4,), activation=&#39;sigmoid&#39;)) # Compile your model model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;sgd&#39;, metrics=[&#39;accuracy&#39;]) # Display a summary of your model model.summary() . Model: &quot;sequential_2&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_2 (Dense) (None, 1) 5 ================================================================= Total params: 5 Trainable params: 5 Non-trainable params: 0 _________________________________________________________________ . That was fast! Let’s use this model to make predictions! . 2.1.3 Is this dollar bill fake ? . You are now ready to train your model and check how well it performs when classifying new bills! The dataset has already been partitioned as X_train , X_test , y_train and y_test . . # Train your model for 20 epochs model.fit(X_train, y_train, epochs=20) # Evaluate your model accuracy on the test set accuracy = model.evaluate(X_test, y_test)[1] # Print accuracy print(&#39;Accuracy:&#39;,accuracy) # Accuracy: 0.8252427167105443 . Alright! It looks like you are getting a high accuracy with this simple model! . 2.2 Multi-class classification . . 2.2.1 A multi-class model . You’re going to build a model that predicts who threw which dart only based on where that dart landed! (That is the dart’s x and y coordinates.) . This problem is a multi-class classification problem since each dart can only be thrown by one of 4 competitors. So classes are mutually exclusive, and therefore we can build a neuron with as many output as competitors and use the softmax activation function to achieve a total sum of probabilities of 1 over all competitors. . Keras Sequential model and Dense layer are already loaded for you to use. . # Instantiate a sequential model model = Sequential() # Add 3 dense layers of 128, 64 and 32 neurons each model.add(Dense(128, input_shape=(2,), activation=&#39;relu&#39;)) model.add(Dense(64, activation=&#39;relu&#39;)) model.add(Dense(32, activation=&#39;relu&#39;)) # Add a dense layer with as many neurons as competitors model.add(Dense(4, activation=&#39;softmax&#39;)) # Compile your model using categorical_crossentropy loss model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) . Good job! Your models are getting deeper, just as your knowledge on neural networks! . 2.2.2 Prepare your dataset . In the console you can check that your labels, darts.competitor are not yet in a format to be understood by your network. They contain the names of the competitors as strings. You will first turn these competitors into unique numbers,then use the to_categorical() function from keras.utils to turn these numbers into their one-hot encoded representation. . This is useful for multi-class classification problems, since there are as many output neurons as classes and for every observation in our dataset we just want one of the neurons to be activated. . The dart’s dataset is loaded as darts . Pandas is imported as pd . Let’s prepare this dataset! . darts.head() xCoord yCoord competitor 0 0.196451 -0.520341 Steve 1 0.476027 -0.306763 Susan 2 0.003175 -0.980736 Michael 3 0.294078 0.267566 Kate 4 -0.051120 0.598946 Steve darts.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 800 entries, 0 to 799 Data columns (total 3 columns): xCoord 800 non-null float64 yCoord 800 non-null float64 competitor 800 non-null object dtypes: float64(2), object(1) memory usage: 18.8+ KB . # Transform into a categorical variable darts.competitor = pd.Categorical(darts.competitor) # Assign a number to each category (label encoding) darts.competitor = darts.competitor.cat.codes # Print the label encoded competitors print(&#39;Label encoded competitors: n&#39;,darts.competitor.head()) . Label encoded competitors: 0 2 1 3 2 1 3 0 4 2 Name: competitor, dtype: int8 . # Transform into a categorical variable darts.competitor = pd.Categorical(darts.competitor) # Assign a number to each category (label encoding) darts.competitor = darts.competitor.cat.codes # Import to_categorical from keras utils module from keras.utils import to_categorical # Use to_categorical on your labels coordinates = darts.drop([&#39;competitor&#39;], axis=1) competitors = to_categorical(darts.competitor) # Now print the to_categorical() result print(&#39;One-hot encoded competitors: n&#39;,competitors) . One-hot encoded competitors: [[0. 0. 1. 0.] [0. 0. 0. 1.] [0. 1. 0. 0.] ... [0. 1. 0. 0.] [0. 1. 0. 0.] [0. 0. 0. 1.]] . Great! Each competitor is now a vector of length 4, full of zeroes except for the position representing her or himself. . 2.2.3 Training on dart throwers . Your model is now ready, just as your dataset. It’s time to train! . The coordinates and competitors variables you just transformed have been partitioned into coord_train , competitors_train , coord_test and competitors_test . Your model is also loaded. Feel free to visualize your training data or model.summary() in the console. . # Train your model on the training data for 200 epochs model.fit(coord_train,competitors_train,epochs=200) # Evaluate your model accuracy on the test data accuracy = model.evaluate(coord_test, competitors_test)[1] # Print accuracy print(&#39;Accuracy:&#39;, accuracy) # Accuracy: 0.8375 . Your model just trained for 200 epochs! The accuracy on the test set is quite high. What do the predictions look like? . 2.2.4 Softmax predictions . Your recently trained model is loaded for you. This model is generalizing well!, that’s why you got a high accuracy on the test set. . Since you used the softmax activation function, for every input of 2 coordinates provided to your model there’s an output vector of 4 numbers. Each of these numbers encodes the probability of a given dart being thrown by one of the 4 possible competitors. . When computing accuracy with the model’s .evaluate() method, your model takes the class with the highest probability as the prediction. np.argmax() can help you do this since it returns the index with the highest value in an array. . Use the collection of test throws stored in coords_small_test and np.argmax() to check this out! . # Predict on coords_small_test preds = model.predict(coords_small_test) # Print preds vs true values print(&quot;{:45} | {}&quot;.format(&#39;Raw Model Predictions&#39;,&#39;True labels&#39;)) for i,pred in enumerate(preds): print(&quot;{} | {}&quot;.format(pred,competitors_small_test[i])) . Raw Model Predictions | True labels [0.34438723 0.00842557 0.63167274 0.01551455] | [0. 0. 1. 0.] [0.0989717 0.00530467 0.07537904 0.8203446 ] | [0. 0. 0. 1.] [0.33512568 0.00785374 0.28132284 0.37569773] | [0. 0. 0. 1.] [0.8547263 0.01328656 0.11279515 0.01919206] | [1. 0. 0. 0.] [0.3540977 0.00867271 0.6223853 0.01484426] | [0. 0. 1. 0.] . # Predict on coords_small_test preds = model.predict(coords_small_test) # Print preds vs true values print(&quot;{:45} | {}&quot;.format(&#39;Raw Model Predictions&#39;,&#39;True labels&#39;)) for i,pred in enumerate(preds): print(&quot;{} | {}&quot;.format(pred,competitors_small_test[i])) # Extract the indexes of the highest probable predictions preds = [np.argmax(pred) for pred in preds] # Print preds vs true values print(&quot;{:10} | {}&quot;.format(&#39;Rounded Model Predictions&#39;,&#39;True labels&#39;)) for i,pred in enumerate(preds): print(&quot;{:25} | {}&quot;.format(pred,competitors_small_test[i])) . Rounded Model Predictions | True labels 2 | [0. 0. 1. 0.] 3 | [0. 0. 0. 1.] 3 | [0. 0. 0. 1.] 0 | [1. 0. 0. 0.] 2 | [0. 0. 1. 0.] . Well done! As you’ve seen you can easily interpret the softmax output. This can also help you spot those observations where your network is less certain on which class to predict, since you can see the probability distribution among classes. . 2.3 Multi-label classification . . 2.3.1 An irrigation machine . You’re going to automate the watering of parcels by making an intelligent irrigation machine. Multi-label classification problems differ from multi-class problems in that each observation can be labeled with zero or more classes. So classes are not mutually exclusive. . To account for this behavior what we do is have an output layer with as many neurons as classes but this time, unlike in multi-class problems, each output neuron has a sigmoid activation function. This makes the output layer able to output a number between 0 and 1 in any of its neurons. . Keras Sequential() model and Dense() layers are preloaded. It’s time to build an intelligent irrigation machine! . # Instantiate a Sequential model model = Sequential() # Add a hidden layer of 64 neurons and a 20 neuron&#39;s input model.add(Dense(64,input_shape=(20,), activation=&#39;relu&#39;)) # Add an output layer of 3 neurons with sigmoid activation model.add(Dense(3, activation=&#39;sigmoid&#39;)) # Compile your model with adam and binary crossentropy loss model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.summary() . Model: &quot;sequential_2&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_3 (Dense) (None, 64) 1344 _________________________________________________________________ dense_4 (Dense) (None, 3) 195 ================================================================= Total params: 1,539 Trainable params: 1,539 Non-trainable params: 0 _________________________________________________________________ . Great! You’ve already built 3 models for 3 different problems! . 2.3.2 Training with multiple labels . An output of your multi-label model could look like this: [0.76 , 0.99 , 0.66 ] . If we round up probabilities higher than 0.5, this observation will be classified as containing all 3 possible labels [1,1,1] . For this particular problem, this would mean watering all 3 parcels in your field is the right thing to do given the input sensor measurements. . You will now train and predict with the model you just built. sensors_train , parcels_train , sensors_test and parcels_test are already loaded for you to use. Let’s see how well your machine performs! . # Train for 100 epochs using a validation split of 0.2 model.fit(sensors_train, parcels_train, epochs = 100, validation_split = 0.2) # Predict on sensors_test and round up the predictions preds = model.predict(sensors_test) preds_rounded = np.round(preds) # Print rounded preds print(&#39;Rounded Predictions: n&#39;, preds_rounded) # Evaluate your model&#39;s accuracy on the test data accuracy = model.evaluate(sensors_test, parcels_test)[1] # Print accuracy print(&#39;Accuracy:&#39;, accuracy) . ... Epoch 100/100 32/1120 [..............................] - ETA: 0s - loss: 0.0439 - acc: 0.9896 1024/1120 [==========================&gt;...] - ETA: 0s - loss: 0.0320 - acc: 0.9935 1120/1120 [==============================] - 0s 62us/step - loss: 0.0320 - acc: 0.9935 - val_loss: 0.5132 - val_acc: 0.8702 Rounded Predictions: [[1. 1. 0.] [0. 1. 0.] [0. 1. 0.] ... [1. 1. 0.] [0. 1. 0.] [0. 1. 1.]] 32/600 [&gt;.............................] - ETA: 0s 600/600 [==============================] - 0s 26us/step Accuracy: 0.8844444648424784 . Great work on automating this farm! You can see how the validation_split argument is useful for evaluating how your model performs as it trains. . 2.4 Keras callbacks . . 2.4.1 The history callback . The history callback is returned by default every time you train a model with the .fit() method. To access these metrics you can access the history dictionary inside the returned callback object and the corresponding keys. . The irrigation machine model you built in the previous lesson is loaded for you to train, along with its features and labels (X and y). This time you will store the model’s history callback and use the validation_data parameter as it trains. . You will plot the results stored in history with plot_accuracy() and plot_loss() , two simple matplotlib functions. You can check their code in the console by typing print(inspect.getsource(plot_loss)) . . Let’s see the behind the scenes of our training! . # Train your model and save its history h_callback = model.fit(X_train, y_train, epochs = 50, validation_data=(X_test, y_test)) # Plot train vs test loss during training plot_loss(h_callback.history[&#39;loss&#39;], h_callback.history[&#39;val_loss&#39;]) # Plot train vs test accuracy during training plot_accuracy(h_callback.history[&#39;acc&#39;], h_callback.history[&#39;val_acc&#39;]) . . Awesome! These graphs are really useful for detecting overfitting and to know if your neural network would benefit from more training data. More on this on the next chapter! . 2.4.2 Early stopping your model . The early stopping callback is useful since it allows for you to stop the model training if it no longer improves after a given number of epochs. To make use of this functionality you need to pass the callback inside a list to the model’s callback parameter in the .fit() method. . The model you built to detect fake dollar bills is loaded for you to train, this time with early stopping. X_train , y_train , X_test and y_test are also available for you to use. . # Import the early stopping callback from keras.callbacks import EarlyStopping # Define a callback to monitor val_acc monitor_val_acc = EarlyStopping(monitor=&#39;val_acc&#39;, patience=5) # Train your model using the early stopping callback model.fit(X_train, y_train, epochs=1000, validation_data=(X_test, y_test), callbacks=[monitor_val_acc]) . ... Epoch 26/1000 32/960 [&gt;.............................] - ETA: 0s - loss: 0.2096 - acc: 0.9688 800/960 [========================&gt;.....] - ETA: 0s - loss: 0.2079 - acc: 0.9563 960/960 [==============================] - 0s 94us/step - loss: 0.2091 - acc: 0.9531 - val_loss: 0.2116 - val_acc: 0.9417 . Great! Now you won’t ever fall short of epochs! . 2.4.3 A combination of callbacks . Deep learning models can take a long time to train, especially when you move to deeper architectures and bigger datasets. Saving your model every time it improves as well as stopping it when it no longer does allows you to worry less about choosing the number of epochs to train for. You can also restore a saved model anytime. . The model training and validation data are available in your workspace as X_train , X_test , y_train , and y_test . . Use the EarlyStopping() and the ModelCheckpoint() callbacks so that you can go eat a jar of cookies while you leave your computer to work! . # Import the EarlyStopping and ModelCheckpoint callbacks from keras.callbacks import EarlyStopping, ModelCheckpoint # Early stop on validation accuracy monitor_val_acc = EarlyStopping(monitor = &#39;val_acc&#39;, patience=3) # Save the best model as best_banknote_model.hdf5 modelCheckpoint = ModelCheckpoint(&#39;best_banknote_model.hdf5&#39;, save_best_only = True) # Fit your model for a stupid amount of epochs h_callback = model.fit(X_train, y_train, epochs = 1000000000000, callbacks = [monitor_val_acc, modelCheckpoint], validation_data = (X_test, y_test)) . ... Epoch 4/10000000 32/960 [&gt;.............................] - ETA: 0s - loss: 0.2699 - acc: 0.9688 960/960 [==============================] - 0s 59us/step - loss: 0.2679 - acc: 0.9312 - val_loss: 0.2870 - val_acc: 0.9126 . This is a powerful callback combo! Now you always get the model that performed best, even if you early stopped at one that was already performing worse. . 3. Improving Your Model Performance . 3.1 Learning curves . . 3.1.1 Learning the digits . You’re going to build a model on the digits dataset , a sample dataset that comes pre-loaded with scikit learn. The digits dataset consist of 8×8 pixel handwritten digits from 0 to 9 : You want to distinguish between each of the 10 possible digits given an image, so we are dealing with multi-class classification . . The dataset has already been partitioned into X_train , y_train , X_test , and y_test using 30% of the data as testing data. The labels are one-hot encoded vectors, so you don’t need to use Keras to_categorical() function. . Let’s build this new model ! . # Instantiate a Sequential model model = Sequential() # Input and hidden layer with input_shape, 16 neurons, and relu model.add(Dense(16, input_shape = (8*8,), activation = &#39;relu&#39;)) # Output layer with 10 neurons (one per digit) and softmax model.add(Dense(10, activation = &#39;softmax&#39;)) # Compile your model model.compile(optimizer = &#39;adam&#39;, loss = &#39;categorical_crossentropy&#39;, metrics = [&#39;accuracy&#39;]) # Test if your model works and can process input data print(model.predict(X_train)) . Great! Predicting on training data inputs before training can help you quickly check that your model works as expected. . 3.1.2 Is the model overfitting? . Let’s train the model you just built and plot its learning curve to check out if it’s overfitting! You can make use of loaded function plot_loss() to plot training loss against validation loss, you can get both from the history callback. . If you want to inspect the plot_loss() function code, paste this in the console: print(inspect.getsource(plot_loss)) . # Train your model for 60 epochs, using X_test and y_test as validation data history = model.fit(X_train, y_train, epochs=60, validation_data=(X_test, y_test), verbose=0) # Extract from the history object loss and val_loss to plot the learning curve plot_loss(history.history[&#39;loss&#39;], history.history[&#39;val_loss&#39;]) . . Just by looking at the overall picture, do you think the learning curve shows this model is overfitting after having trained for 60 epochs? . No, the test loss is not getting higher as the epochs go by. . Awesome choice! This graph doesn’t show overfitting but convergence. It looks like your model has learned all it could from the data and it no longer improves. . 3.1.3 Do we need more data? . It’s time to check whether the digits dataset model you built benefits from more training examples! . In order to keep code to a minimum, various things are already initialized and ready to use: . The . model . you just built. . | X_train . , . y_train . , . X_test . , and . y_test . . . | The . initial_weights . of your model, saved after using . model.get_weights() . . . | A defined list of training sizes: . training_sizes . . . | A defined . EarlyStopping . callback monitoring loss: . early_stop . . . | Two empty lists to store the evaluation results: . train_accs . and . test_accs . . . | . Train your model on the different training sizes and evaluate the results on X_test . End by plotting the results with plot_results() . . The full code for this exercise can be found on the slides! . train_sizes array([ 125, 502, 879, 1255]) . for size in train_sizes: # Get a fraction of training data (we only care about the training data) X_train_frac, X_test_frac, y_train_frac, y_test_frac = train_test_split( X_train, y_train, train_size = size) # Set the model weights and fit the model on the training data model.set_weights(initial_weights) model.fit(X_train_frac, y_train_frac, epochs = 50, callbacks = [early_stop]) # Evaluate and store the train fraction and the complete test set results train_accs.append(model.evaluate(X_train_frac, y_train_frac)[1]) test_accs.append(model.evaluate(X_test_frac, y_test_frac)[1]) # Plot train vs test accuracies plot_results(train_accs, test_accs) . . Great job, that was a lot of code to understand! The results shows that your model would not benefit a lot from more training data, since the test set is starting to flatten in accuracy already. . 3.2 Activation functions . . 3.2.1 Different activation functions . tanh(hyperbolic tangent) . The sigmoid() , tanh() , ReLU() , and leaky_ReLU() functions have been defined and ready for you to use. Each function receives an input number X and returns its corresponding Y value. . Which of the statements below is false ? . The . sigmoid() . takes a value of . 0.5 . when . X = 0 . whilst . tanh() . takes a value of . 0 . . . | The . leaky-ReLU() . takes a value of . -0.01 . when . X = -1 . whilst . ReLU() . takes a value of . 0 . . . | **The . sigmoid() . and . tanh() . both take values close to . -1 . for big negative numbers.(false)** . | . Great! For big negative numbers the sigmoid approaches 0 not -1 whilst the tanh() does take values close to -1 . . 3.2.2 Comparing activation functions . Comparing activation functions involves a bit of coding, but nothing you can’t do! . You will try out different activation functions on the multi-label model you built for your irrigation machine in chapter 2. The function get_model() returns a copy of this model and applies the activation function, passed on as a parameter, to its hidden layer. . You will build a loop that goes through several activation functions, generates a new model for each and trains it. Storing the history callback in a dictionary will allow you to compare and visualize which activation function performed best in the next exercise! . # Set a seed np.random.seed(27) # Activation functions to try activations = [&#39;relu&#39;, &#39;leaky_relu&#39;, &#39;sigmoid&#39;, &#39;tanh&#39;] # Loop over the activation functions activation_results = {} for act in activations: # Get a new model with the current activation model = get_model(act_function=act) # Fit the model history = model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=20, verbose=0) activation_results[act] = history . Finishing with relu ... Finishing with leaky_relu ... Finishing with sigmoid ... Finishing with tanh ... . Awesome job! You’ve trained 4 models with 4 different activation functions, let’s see how well they performed! . 3.2.3 Comparing activation functions II . The code used in the previous exercise has been executed to obtain the activation_results with the difference that 100 epochs instead of 20 are used. That way you’ll have more epochs to further compare how the training evolves per activation function. . For every history callback of each activation function in activation_results : . The . history.history[&#39;val_loss&#39;] . has been extracted. . | The . history.history[&#39;val_acc&#39;] . has been extracted. . | Both are saved in two dictionaries: . val_loss_per_function . and . val_acc_per_function . . . | . Pandas is also loaded for you to use as pd . Let’s plot some quick comparison validation loss and accuracy charts with pandas! . # Create a dataframe from val_loss_per_function val_loss= pd.DataFrame(val_loss_per_function) # Call plot on the dataframe val_loss.plot() plt.show() # Create a dataframe from val_acc_per_function val_acc = pd.DataFrame(val_acc_per_function) # Call plot on the dataframe val_acc.plot() plt.show() . . 3.3 Batch size and batch normalization . . 3.3.1 Changing batch sizes . You’ve seen models are usually trained in batches of a fixed size. The smaller a batch size, the more weight updates per epoch, but at a cost of a more unstable gradient descent. Specially if the batch size is too small and it’s not representative of the entire training set. . Let’s see how different batch sizes affect the accuracy of a binary classification model that separates red from blue dots. . You’ll use a batch size of one, updating the weights once per sample in your training set for each epoch. Then you will use the entire dataset, updating the weights only once per epoch. . # Get a fresh new model with get_model model = get_model() # Train your model for 5 epochs with a batch size of 1 model.fit(X_train, y_train, epochs=5, batch_size=1) print(&quot; n The accuracy when using a batch of size 1 is: &quot;, model.evaluate(X_test, y_test)[1]) # The accuracy when using a batch of size 1 is: 0.9733333333333334 . model = get_model() # Fit your model for 5 epochs with a batch of size the training set model.fit(X_train, y_train, epochs=5, batch_size=X_train.shape[0]) print(&quot; n The accuracy when using the whole training set as a batch was: &quot;, model.evaluate(X_test, y_test)[1]) # The accuracy when using the whole training set as a batch was: 0.553333334128062 . 3.3.2 Batch normalizing a familiar model . Remember the digits dataset you trained in the first exercise of this chapter? . . A multi-class classification problem that you solved using softmax and 10 neurons in your output layer. . You will now build a new deeper model consisting of 3 hidden layers of 50 neurons each, using batch normalization in between layers. The kernel_initializer parameter is used to initialize weights in a similar way. . # Import batch normalization from keras layers from keras.layers import BatchNormalization # Build your deep network batchnorm_model = Sequential() batchnorm_model.add(Dense(50, input_shape=(64,), activation=&#39;relu&#39;, kernel_initializer=&#39;normal&#39;)) batchnorm_model.add(BatchNormalization()) batchnorm_model.add(Dense(50, activation=&#39;relu&#39;, kernel_initializer=&#39;normal&#39;)) batchnorm_model.add(BatchNormalization()) batchnorm_model.add(Dense(50, activation=&#39;relu&#39;, kernel_initializer=&#39;normal&#39;)) batchnorm_model.add(BatchNormalization()) batchnorm_model.add(Dense(10, activation=&#39;softmax&#39;, kernel_initializer=&#39;normal&#39;)) # Compile your model with sgd batchnorm_model.compile(optimizer=&#39;sgd&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . Congratulations! That was a deep model indeed. Let’s compare how it performs against this very same model without batch normalization! . 3.3.3 Batch normalization effects . Batch normalization tends to increase the learning speed of our models and make their learning curves more stable. Let’s see how two identical models with and without batch normalization compare. . The model you just built batchnorm_model is loaded for you to use. An exact copy of it without batch normalization: standard_model , is available as well. You can check their summary() in the console. X_train , y_train , X_test , and y_test are also loaded so that you can train both models. . You will compare the accuracy learning curves for both models plotting them with compare_histories_acc() . . You can check the function pasting print(inspect.getsource(compare_histories_acc)) in the console. . # Train your standard model, storing its history history1 = standard_model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=10, verbose=0) # Train the batch normalized model you recently built, store its history history2 = batchnorm_model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=10, verbose=0) # Call compare_acc_histories passing in both model histories compare_histories_acc(history1, history2) . . Outstanding! You can see that for this deep model batch normalization proved to be useful, helping the model obtain high accuracy values just over the first 10 training epochs. . 3.4 Hyperparameter tuning . . 3.4.1 Preparing a model for tuning . Let’s tune the hyperparameters of a binary classification model that does well classifying the breast cancer dataset . . You’ve seen that the first step to turn a model into a sklearn estimator is to build a function that creates it. This function is important since you can play with the parameters it receives to achieve the different models you’d like to try out. . Build a simple create_model() function that receives a learning rate and an activation function as parameters. The Adam optimizer has been imported as an object from keras.optimizers so that you can change its learning rate parameter. . # Creates a model given an activation and learning rate def create_model(learning_rate=0.01, activation=&#39;relu&#39;): # Create an Adam optimizer with the given learning rate opt = Adam(lr=learning_rate) # Create your binary classification model model = Sequential() model.add(Dense(128, input_shape=(30,), activation=activation)) model.add(Dense(256, activation=activation)) model.add(Dense(1, activation=&#39;sigmoid&#39;)) # Compile your model with your optimizer, loss, and metrics model.compile(optimizer=opt, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) return model . Well done! With this function ready you can now create a sklearn estimator and perform hyperparameter tuning! . 3.4.2 Tuning the model parameters . It’s time to try out different parameters on your model and see how well it performs! . The create_model() function you built in the previous exercise is loaded for you to use. . Since fitting the RandomizedSearchCV would take too long, the results you’d get are printed in the show_results() function. You could try random_search.fit(X,y) in the console yourself to check it does work after you have built everything else, but you will probably timeout your exercise (so copy your code first if you try it!). . You don’t need to use the optional epochs and batch_size parameters when building your KerasClassifier since you are passing them as params to the random search and this works as well. . # Import KerasClassifier from keras wrappers from keras.wrappers.scikit_learn import KerasClassifier # Create a KerasClassifier model = KerasClassifier(build_fn = create_model) # Define the parameters to try out params = {&#39;activation&#39;: [&#39;relu&#39;, &#39;tanh&#39;], &#39;batch_size&#39;: [32, 128, 256], &#39;epochs&#39;: [50, 100, 200], &#39;learning_rate&#39;: [0.1, 0.01, 0.001]} # Create a randomize search cv object passing in the parameters to try random_search = RandomizedSearchCV(model, param_distributions = params, cv = KFold(3)) # Running random_search.fit(X,y) would start the search,but it takes too long! show_results() . Best: 0.975395 using {learning_rate: 0.001, epochs: 50, batch_size: 128, activation: relu} 0.956063 (0.013236) with: {learning_rate: 0.1, epochs: 200, batch_size: 32, activation: tanh} 0.970123 (0.019838) with: {learning_rate: 0.1, epochs: 50, batch_size: 256, activation: tanh} 0.971880 (0.006524) with: {learning_rate: 0.01, epochs: 100, batch_size: 128, activation: tanh} 0.724077 (0.072993) with: {learning_rate: 0.1, epochs: 50, batch_size: 32, activation: relu} 0.588752 (0.281793) with: {learning_rate: 0.1, epochs: 100, batch_size: 256, activation: relu} 0.966608 (0.004892) with: {learning_rate: 0.001, epochs: 100, batch_size: 128, activation: tanh} 0.952548 (0.019734) with: {learning_rate: 0.1, epochs: 50, batch_size: 256, activation: relu} 0.971880 (0.006524) with: {learning_rate: 0.001, epochs: 200, batch_size: 128, activation: relu} 0.968366 (0.004239) with: {learning_rate: 0.01, epochs: 100, batch_size: 32, activation: relu} 0.910369 (0.055824) with: {learning_rate: 0.1, epochs: 100, batch_size: 128, activation: relu} . That was great! I’m glad that the server is still working. Now that we have a better idea of which parameters are performing best, let’s use them! . 3.4.3 Training with cross-validation . Time to train your model with the best parameters found: 0.001 for the learning rate , 50 epochs , a 128 batch_size and relu activations . . The create_model() function has been redefined so that it now creates a model with those parameters. X and y are loaded for you to use as features and labels. . In this exercise you do pass the best epochs and batch size values found for your model to the KerasClassifier object so that they are used when performing cross validation. . End this chapter by training an awesome tuned model on the breast cancer dataset ! . # Import KerasClassifier from keras wrappers from keras.wrappers.scikit_learn import KerasClassifier # Create a KerasClassifier model = KerasClassifier(build_fn = create_model, epochs = 50, batch_size = 128, verbose = 0) # Calculate the accuracy score for each fold kfolds = cross_val_score(model, X, y, cv = 3) # Print the mean accuracy print(&#39;The mean accuracy was:&#39;, kfolds.mean()) # Print the accuracy standard deviation print(&#39;With a standard deviation of:&#39;, kfolds.std()) . The mean accuracy was: 0.9718834066666666 With a standard deviation of: 0.002448915612216046 . Amazing! Now you can more reliably test out different parameters on your networks and find better models! . 4. Advanced Model Architectures . 4.1 Tensors, layers, and autoencoders . . 4.1.1 It’s a flow of tensors . If you have already built a model, you can use the model.layers and the keras.backend to build functions that, provided with a valid input tensor, return the corresponding output tensor. . This is a useful tool when trying to understand what is going on inside the layers of a neural network. . For instance, if you get the input and output from the first layer of a network, you can build an inp_to_out function that returns the result of carrying out forward propagation through only the first layer for a given input tensor. . So that’s what you’re going to do right now! . X_test from the Banknote Authentication dataset and its model are preloaded. Type model.summary() in the console to check it. . # Import keras backend import keras.backend as K # Input tensor from the 1st layer of the model inp = model.layers[0].input # Output tensor from the 1st layer of the model out = model.layers[0].output # Define a function from inputs to outputs inp_to_out = K.function([inp],[out]) # Print the results of passing X_test through the 1st layer print(inp_to_out([X_test])) . [array([[7.77682841e-01, 0.00000000e+00], [0.00000000e+00, 0.00000000e+00], [0.00000000e+00, 1.50813460e+00], [0.00000000e+00, 1.34600031e+00], ... . Nice job! Let’s use this function for something more interesting. . 4.1.2 Neural separation . Neurons learn by updating their weights to output values that help them distinguish between the input classes. So put on your gloves because you’re going to perform brain surgery! . You will make use of the inp_to_out() function you just built to visualize the output of two neurons in the first layer of the Banknote Authentication model as epochs go by. Plotting the outputs of both of these neurons against each other will show you the difference in output depending on whether each bill was real or fake. . The model you built in chapter 2 is ready for you to use, just like X_test and y_test . Copy print(inspect.getsource(plot)) in the console if you want to check plot() . . You’re performing heavy duty, once it’s done, take a look at the graphs to watch the separation live! . print(inspect.getsource(plot)) def plot(): fig, ax = plt.subplots() plt.scatter(layer_output[:, 0], layer_output[:, 1],c=y_test,edgecolors=&#39;none&#39;) plt.title(&#39;Epoch: {}, Test Acc: {:3.1f} %&#39;.format(i+1, test_accuracy * 100.0)) plt.show() . for i in range(0, 21): # Train model for 1 epoch h = model.fit(X_train, y_train, batch_size=16, epochs=1,verbose=0) if i%4==0: # Get the output of the first layer layer_output = inp_to_out([X_test])[0] # Evaluate model accuracy for this epoch test_accuracy = model.evaluate(X_test, y_test)[1] # Plot 1st vs 2nd neuron output plot() . . That took a while! If you take a look at the graphs you can see how the neurons are learning to spread out the classes during training. Click in between graphs fast, it’s like a movie! . 4.1.3 Building an autoencoder . Autoencoders have several interesting applications like anomaly detection or image denoising. They aim at producing an output identical to its inputs. The input will be compressed into a lower dimensional space, encoded . The model then learns to decode it back to its original form. . You will encode and decode the MNIST dataset of handwritten digits, the hidden layer will encode a 32-dimensional representation of the image, which originally consists of 784 pixels. . The Sequential model and Dense layers are ready for you to use. . Let’s build an autoencoder! . # Start with a sequential model autoencoder = Sequential() # Add a dense layer with the original image as input autoencoder.add(Dense(32, input_shape=(784, ), activation=&quot;relu&quot;)) # Add an output layer with as many nodes as the image autoencoder.add(Dense(784, activation=&quot;sigmoid&quot;)) # Compile your model autoencoder.compile(optimizer=&#39;adadelta&#39;, loss=&#39;binary_crossentropy&#39;) # Take a look at your model structure autoencoder.summary() . Model: &quot;sequential_1&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_1 (Dense) (None, 32) 25120 _________________________________________________________________ dense_2 (Dense) (None, 784) 25872 ================================================================= Total params: 50,992 Trainable params: 50,992 Non-trainable params: 0 _________________________________________________________________ . Great start! Your model is now ready. Let’s see what you can do with it! . 4.1.4 De-noising like an autoencoder . Okay, you have just built an autoencoder model. Let’s see how it handles a more challenging task. . First, you will build a model that encodes images, and you will check how different digits are represented with show_encodings() . You can change the number parameter of this function to check other digits in the console. . Then, you will apply your autoencoder to noisy images from MNIST , it should be able to clean the noisy artifacts. . X_test_noise is loaded in your workspace. The digits in this data look like this: . . Apply the power of the autoencoder! . # Build your encoder encoder = Sequential() encoder.add(autoencoder.layers[0]) # Encode the images and show the encodings preds = encoder.predict(X_test_noise) show_encodings(preds) . . # Predict on the noisy images with your autoencoder decoded_imgs = autoencoder.predict(X_test_noise) # Plot noisy vs decoded images compare_plot(X_test_noise, decoded_imgs) . Amazing! The noise is gone now! You could get a better reconstruction by using a convolutional autoencoder. . 4.2 Intro to CNNs . . 4.2.1 Building a CNN model . Building a CNN model in Keras isn’t much more difficult than building any of the models you’ve already built throughout the course! You just need to make use of convolutional layers. . You’re going to build a shallow convolutional model that classifies the MNIST dataset of digits. The same one you de-noised with your autoencoder!. The images are 28×28 pixels and just have one channel. . Go ahead and build this small convolutional model! . # Import the Conv2D and Flatten layers and instantiate model from keras.layers import Conv2D,Flatten model = Sequential() # Add a convolutional layer of 32 filters of size 3x3 model.add(Conv2D(32, input_shape=(28, 28, 1), kernel_size=3, activation=&#39;relu&#39;)) # Add a convolutional layer of 16 filters of size 3x3 model.add(Conv2D(16, kernel_size=3, activation=&#39;relu&#39;)) # Flatten the previous layer output model.add(Flatten()) # Add as many outputs as classes with softmax activation model.add(Dense(10, activation=&#39;softmax&#39;)) . Well done! You can see that the key concepts are the same, you just have to use new layers! . 4.2.2 Looking at convolutions . Inspecting the activations of a convolutional layer is a cool thing. You have to do it at least once in your lifetime! . To do so, you will build a new model with the Keras Model object, which takes in a list of inputs and a list of outputs. The output you will provide to this new model is the first convolutional layer outputs when given an MNIST digit as input image. . The convolutional model you built in the previous exercise has already been trained for you. You can check it with model.summary() in the console. . Let’s look at a couple convolutional masks that were learned in the first convolutional layer of this model! . # Obtain a reference to the outputs of the first layer layer_output = model.layers[0].output # Build a model using the model&#39;s input and the first layer output first_layer_model = Model(inputs = model.input, outputs = layer_output) # Use this model to predict on X_test activations = first_layer_model.predict(X_test) # Plot the activations of first digit of X_test for the 15th filter axs[0].matshow(activations[0,:,:,14], cmap = &#39;viridis&#39;) # Do the same but for the 18th filter now axs[1].matshow(activations[0,:,:,17], cmap = &#39;viridis&#39;) plt.show() . . Hurrah! Each neuron filter of the first layer learned a different convolution. The 15th filter (a.k.a convolutional mask) learned to detect horizontal traces in your digits. On the other hand, filter 18th seems to be checking for vertical traces. . 4.2.3 Preparing your input image . When using an already trained model like ResNet50 , we need to make sure that we fit the network the way it was originally trained. So if we want to use a trained model on our custom images, these images need to have the same dimensions as the one used in the original model. . The original ResNet50 model was trained with images of size 224×224 pixels and a number of preprocessing operations; like the subtraction of the mean pixel value in the training set for all training images. . You will go over these preprocessing steps as you prepare this dog’s (named Ivy) image into one that can be classified by ResNet50 . # Import image and preprocess_input from keras.preprocessing import image from keras.applications.resnet50 import preprocess_input # Load the image with the right target size for your model img = image.load_img(img_path, target_size=(224, 224)) # Turn it into an array img_array = image.img_to_array(img) # Expand the dimensions of the image img_expanded = np.expand_dims(img_array, axis = 0) # Pre-process the img in the same way original images were img_ready = preprocess_input(img_expanded) . Alright! Ivy is now ready for ResNet50. Do you know this dog’s breed? Let’s see what this model thinks it is! . 4.2.4 Using a real world model . Okay, so Ivy’s picture is ready to be used by ResNet50 . It is stored in img_ready and now looks like this: . ResNet50 is a model trained on the Imagenet dataset that is able to distinguish between 1000 different objects. ResNet50 is a deep model with 50 layers, you can check it in 3D here . . ResNet50 and decode_predictions have both been imported from keras.applications.resnet50 for you. . It’s time to use this trained model to find out Ivy’s breed! . # Instantiate a ResNet50 model with imagenet weights model = ResNet50(weights=&#39;imagenet&#39;) # Predict with ResNet50 on your already processed img preds = model.predict(img_ready) # Decode predictions print(&#39;Predicted:&#39;, decode_predictions(preds, top=3)[0]) . Predicted: [(‘n02088364’, ‘beagle’, 0.8280003), (‘n02089867’, ‘Walker_hound’, 0.12915272), (‘n02089973’, ‘English_foxhound’, 0.03711732)] . Amazing! Now you know Ivy is a Beagle and that deep learning models that have already been trained for you are easy to use! . 4.3 Intro to LSTMs . . 4.3.1 Text prediction with LSTMs . During the following exercises you will build an LSTM model that is able to predict the next word using a small text dataset. This dataset consist of cleaned quotes from the The Lord of the Ring movies. You can find them in the text variable. . You will turn this text into sequences of length 4 and make use of the Keras Tokenizer to prepare the features and labels for your model! . The Keras Tokenizer is already imported for you to use. It assigns a unique number to each unique word, and stores the mappings in a dictionary. This is important since the model deals with numbers but we later will want to decode the output numbers back into words. . You’re working with this small chunk of The Lord of The Ring quotes: . It is not the strength of the body but the strength of the spirit. | It is useless to meet revenge with revenge it will heal nothing. | Even the smallest person can change the course of history. | All we have to decide is what to do with the time that is given us. | The burned hand teaches best. After that, advice about fire goes to the heart. | . text &#39;it is not the strength of the body but the strength of the spirit it is useless to meet revenge with revenge it will heal nothing even the smallest person can change the course of history all we have to decide is what to do with the time that is given us the burned hand teaches best after that advice about fire goes to the heart&#39; . # Split text into an array of words words = text.split() # Make lines of 4 words each, moving one word at a time lines = [] for i in range(4, len(words)): lines.append(&#39; &#39;.join(words[i-4:i])) # Instantiate a Tokenizer, then fit it on the lines tokenizer = Tokenizer() tokenizer.fit_on_texts(lines) # Turn lines into a sequence of numbers sequences = tokenizer.texts_to_sequences(lines) print(&quot;Lines: n {} n Sequences: n {}&quot;.format(lines[:5],sequences[:5])) . Lines: [&#39;it is not the&#39;, &#39;is not the strength&#39;, &#39;not the strength of&#39;, &#39;the strength of the&#39;, &#39;strength of the body&#39;] Sequences: [[5, 2, 42, 1], [2, 42, 1, 6], [42, 1, 6, 4], [1, 6, 4, 1], [6, 4, 1, 10]] . Great! Your lines are now sequences of numbers, check that identical words are assigned the same number. . 4.3.2 Build your LSTM model . You’ve already prepared your sequences of text, with each of the sequences consisting of four words. It’s time to build your LSTM model! . Your model will be trained on the first three words of each sequence, predicting the 4th one. You are going to use an Embedding layer that will essentially learn to turn words into vectors. These vectors will then be passed to a simple LSTM layer. Our output is a Dense layer with as many neurons as words in the vocabulary and softmax activation. This is because we want to obtain the highest probable next word out of all possible words. . The size of the vocabulary of words (the unique number of words) is stored in vocab_size . . # Import the Embedding, LSTM and Dense layer from keras.layers import Embedding, LSTM, Dense model = Sequential() # Add an Embedding layer with the right parameters model.add(Embedding(input_dim=vocab_size, output_dim=8, input_length=3)) # Add a 32 unit LSTM layer model.add(LSTM(32)) # Add a hidden Dense layer of 32 units and an output layer of vocab_size with softmax model.add(Dense(32, activation=&#39;relu&#39;)) model.add(Dense(vocab_size, activation=&#39;softmax&#39;)) model.summary() . Model: &quot;sequential_1&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_1 (Embedding) (None, 3, 8) 352 _________________________________________________________________ lstm_1 (LSTM) (None, 32) 5248 _________________________________________________________________ dense_1 (Dense) (None, 32) 1056 _________________________________________________________________ dense_2 (Dense) (None, 44) 1452 ================================================================= Total params: 8,108 Trainable params: 8,108 Non-trainable params: 0 _________________________________________________________________ . That’s a nice looking model you’ve built! You’ll see that this model is powerful enough to learn text relationships. Specially because we aren’t using a lot of text in this tiny example. . 4.3.3 Decode your predictions . Your LSTM model has already been trained for you so that you don’t have to wait. It’s time to define a function that decodes its predictions. . Since you are predicting on a model that uses the softmax function, argmax() is used to obtain the position of the output layer with the highest probability, that is the index representing the most probable next word. . The tokenizer you previously created and fitted, is loaded for you. You will be making use of its internal index_word dictionary to turn the model’s next word prediction (which is an integer) into the actual written word it represents. . You’re very close to experimenting with your model! . def predict_text(test_text): if len(test_text.split())!=3: print(&#39;Text input should be 3 words!&#39;) return False # Turn the test_text into a sequence of numbers test_seq = tokenizer.texts_to_sequences([test_text]) test_seq = np.array(test_seq) # Get the model&#39;s next word prediction by passing in test_seq pred = model.predict(test_seq).argmax(axis = 1)[0] # Return the word associated to the predicted index return tokenizer.index_word[pred] . Great job! It’s finally time to try out your model to see how well it does! . 4.3.4 Test your model! . The function you just built, predict_text() , is ready to use. . Try out these strings on your LSTM model: . &#39;meet revenge with&#39; | &#39;the course of&#39; | &#39;strength of the&#39; | . Which sentence could be made with the word output from the sentences above? . predict_text(&#39;meet revenge with&#39;) &#39;revenge&#39; predict_text(&#39;the course of&#39;) &#39;history&#39; predict_text(&#39;strength of the&#39;) &#39;spirit&#39; . 4.4 You’re done! . . Thank you for reading and hope you’ve learned a lot. .",
            "url": "https://islamalam.github.io/blog/blogging/demo/2021/06/03/introduction-to-deep-learning-with-keras.html",
            "relUrl": "/blogging/demo/2021/06/03/introduction-to-deep-learning-with-keras.html",
            "date": " • Jun 3, 2021"
        }
        
    
  
    
        ,"post52": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://islamalam.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post53": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://islamalam.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://islamalam.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://islamalam.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}